{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ccc40e3",
   "metadata": {},
   "source": [
    "# FractaFormer\n",
    "\n",
    "this base version is going to be absurdly terribly no-good inefficient because we're taking the biggest computational issue ($O(t^2)$ attention) and making it way worse by doing MANY of them at once and then having to keep track of each parameter's gradient from MANY different perspectives. This is basically just an extension of [MatFormer+](https://github.com/evintunador/matryoshkaGPT/blob/main/MatFormer%2B.ipynb) where instead of one inner model, we have 2 (or whatever number you specify) models inside 1 at each layer. Check out the video explanation [here](https://www.youtube.com/watch?v=MJnIxpZhTk0)\n",
    "\n",
    "# TODO\n",
    "- make the verbose tags consistent rather than the global variable\n",
    "- add in drawings from the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a204aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# used for the tokenizer\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Imports used for the config\n",
    "import dataclasses \n",
    "from typing import Optional\n",
    "\n",
    "# Imports used for the model\n",
    "import re\n",
    "from typing import Any, List, Sequence, Tuple, Union\n",
    "import numpy as np\n",
    "\n",
    "# used in the training loop\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e172c181",
   "metadata": {},
   "source": [
    "# The Dataset\n",
    "\n",
    "the dataset we'll be using is just TinyShakespeare for sake of simplicity & ability to do run/train locally on any computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49ca7881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "\n",
      " ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] 65\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# the first 200 characters. It's just one continuous text document with all of the works of shakespeare back-to-back\n",
    "print(text[:200])\n",
    "\n",
    "# here are all the unique characters that occur in this text and how many there are\n",
    "chars = sorted(list(set(text)))\n",
    "v = len(chars)\n",
    "print('\\n', chars, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d23a7e",
   "metadata": {},
   "source": [
    "# The Tokenizer\n",
    "\n",
    "We'll be using a very simple tokenizer I previoiusly trained off of the TinyShakespeare dataset that has 128 total tokens and ignores stuff like special tokens & regex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb1e7cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length:  128\n",
      "Encoded: [22, 33, 24, 21, 17, 32, 71, 27, 1, 30, 53, 83, 53, 66, 30, 53, 83, 53, 2, 1, 61, 87, 93, 105, 43, 1, 77, 58, 1, 65, 67, 1, 30, 53, 83, 53, 12] 37\n",
      "Decoded: JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romeo? 49\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer data using pickle\n",
    "with open('./tokenizers/tokenizer.model', 'rb') as f:\n",
    "    loaded_tokenizer_data = pickle.load(f)\n",
    "\n",
    "# Extract the stoi mapping and merges from the loaded data\n",
    "loaded_stoi = loaded_tokenizer_data['stoi']\n",
    "loaded_merges = loaded_tokenizer_data['merges']\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, stoi, merges):\n",
    "        self.stoi = stoi\n",
    "        self.merges = merges\n",
    "        self.itos = {i: s for s, i in stoi.items()}  # Inverse mapping for decoding\n",
    "\n",
    "        self.vocab_len = len(stoi) + len(merges)\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Convert the text to a list of token IDs, using space for unknown characters\n",
    "        tokens = [self.stoi.get(c, self.stoi[' ']) for c in text]\n",
    "\n",
    "        # Perform merging with the possibility of nested merges\n",
    "        i = 0\n",
    "        while i < len(tokens) - 1:\n",
    "            pair = (tokens[i], tokens[i + 1])\n",
    "            if pair in self.merges:\n",
    "                # Replace the current pair with its merged token\n",
    "                merged_token = self.merges[pair]\n",
    "                tokens[i] = merged_token\n",
    "                del tokens[i + 1]\n",
    "\n",
    "                # Move back to handle possible nested merges\n",
    "                if i > 0:\n",
    "                    i -= 1\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        def expand_token(token):\n",
    "            # Base case: if the token is a direct mapping, return its character\n",
    "            if token in self.itos:\n",
    "                return self.itos[token]\n",
    "            # Recursive case: if the token is a merged token, expand its constituents\n",
    "            elif token in self.merges.values():\n",
    "                pair = next(key for key, value in self.merges.items() if value == token)\n",
    "                return ''.join(expand_token(t) for t in pair)\n",
    "            # Fallback for unknown tokens\n",
    "            else:\n",
    "                return ''\n",
    "\n",
    "        # Decode each token in the list, handling nested merges recursively\n",
    "        return ''.join(expand_token(token) for token in tokens)\n",
    "        \n",
    "# Example usage\n",
    "# Assuming loaded_stoi and loaded_merges are already loaded from the tokenizer.model file\n",
    "\n",
    "tokenizer = SimpleTokenizer(loaded_stoi, loaded_merges)\n",
    "print(\"vocab length: \", tokenizer.vocab_len)\n",
    "\n",
    "# Encoding text\n",
    "encoded_text = tokenizer.encode(\"JULIET:\\nO Romeo, Romeo! wherefore art thou Romeo?\")\n",
    "print(\"Encoded:\", encoded_text, len(encoded_text))\n",
    "\n",
    "# Decoding back\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(\"Decoded:\", decoded_text, len(decoded_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac092b1a",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "dedbb53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single large model -> hierarchy of many smaller models inside\n",
      "model_count: [1, 2, 4]\n",
      "model_dim_list: [128, 64, 32]\n",
      "head_dim_list: [32, 16, 8]\n",
      "verbose: {'RMSNorm': False, 'MLP': False, 'MQA': False, 'Layer': False, 'OutputLayer': False, 'FractalLoss': False, 'FractalFormer': False, 'Model': False, 'Sampler': False, 'Generate': False}\n"
     ]
    }
   ],
   "source": [
    "@dataclasses.dataclass # a class meant specifically to just hold data\n",
    "class Config:\n",
    "    \"\"\" \n",
    "    The default configuration & hyperparameters for FractalFormer\n",
    "    \"\"\"\n",
    "    # The number of tokens in the vocabulary.\n",
    "    vocab_size: int = tokenizer.vocab_len\n",
    "    \n",
    "    # The maximum sequence length that this model might ever be used with.\n",
    "    max_position_embeddings: int = 256\n",
    "    \n",
    "    # The number of layers in the model.\n",
    "    num_hidden_layers: int = 4\n",
    "    \n",
    "    # The number of attention heads used in the attention layers of the model.\n",
    "    num_attention_heads: int = 4\n",
    "    \n",
    "    # The number of key-value heads for implementing multi-query attention.\n",
    "    num_key_value_heads: int = 1\n",
    "    # Ensures that the number of query heads is evenly divisible by the number of KV heads.\n",
    "    assert num_attention_heads % num_key_value_heads == 0\n",
    "    \n",
    "    # The hidden size of the model, AKA the embedding dimension\n",
    "    hidden_size: int = 128\n",
    "    # the attention heads need to cleanly divide up the hidden_size of the model for MQA\n",
    "    assert hidden_size % num_attention_heads == 0\n",
    "\n",
    "    # how much larger the inner dimension of the MLP should be than the hidden size of the model\n",
    "    intermediate_multiplier = 4\n",
    "    # The inner dimension of the MLP part of the decoder layer\n",
    "    @property\n",
    "    def intermediate_size(self):\n",
    "        return self.intermediate_multiplier * self.hidden_size\n",
    "    \n",
    "    # The number of head dimensions\n",
    "    head_dim: int = 32\n",
    "    \n",
    "    # The epsilon used by the rms normalization layers.\n",
    "    rms_norm_eps: float = 1e-6 # this is to promote numerical stability & prevent dividing by 0\n",
    "    \n",
    "    # the scaling factor that determines the frequencies for the rotary positional encodings\n",
    "    rope_theta = 100.0\n",
    "    # smaller models should use a smaller theta, but I'm just guessing here. 1000 might work too. 10,000 is the usual\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # the % of neurons to dropout in the MLP\n",
    "    dropout = 0.1\n",
    "\n",
    "    ####### for debugging & visualization\n",
    "    verbose = {\n",
    "    'RMSNorm': False,\n",
    "    'MLP': False,\n",
    "    'MQA': False,\n",
    "    'Layer': False,\n",
    "    'OutputLayer': False,\n",
    "    'FractalLoss': False,\n",
    "    'FractalFormer': False,\n",
    "    'Model': False,\n",
    "    'Sampler': False,\n",
    "    'Generate': False\n",
    "    }\n",
    "\n",
    "    ####### FractalFormer-specific hyperparameters\n",
    "\n",
    "    # the number of levels for sub-models to exist on\n",
    "    levels = 3\n",
    "    \n",
    "    # the number of splits to make at a given level\n",
    "    split = 2 # i don't recommend choosing any value other than 2\n",
    "    # needs to be divisible by 2 in order to splice cleanly\n",
    "    assert split % 2 == 0\n",
    "    # RoPE requires a head dimension of length larger than 1 in order to work\n",
    "    assert head_dim // (split * (levels-1)) > 1\n",
    "    # really though you shouldn't be getting anywhere near that small of a head dimension even at the lowest level, that'd be useless\n",
    "\n",
    "    @property\n",
    "    def model_count(self):\n",
    "        return [self.split**i for i in range(self.levels)]\n",
    "\n",
    "    @property\n",
    "    def model_dim_list(self):\n",
    "        return [self.hidden_size // (self.split**i) for i in range(self.levels)]\n",
    "\n",
    "    @property\n",
    "    def head_dim_list(self):\n",
    "        return [self.head_dim // (self.split**i) for i in range(self.levels)]\n",
    "\n",
    "config = Config()\n",
    "\n",
    "print(\"single large model -> hierarchy of many smaller models inside\")\n",
    "print(f\"model_count: {config.model_count}\")\n",
    "print(f\"model_dim_list: {config.model_dim_list}\")\n",
    "print(f\"head_dim_list: {config.head_dim_list}\")\n",
    "print(f\"verbose: {config.verbose}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d52c2b9d",
   "metadata": {},
   "source": [
    "# Rotary Positional Encoding (RoPE)\n",
    "\n",
    "i don't think i need to adjust the code for this one as long as i always call it individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c43eba66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_rotary_emb(x: torch.Tensor, dim: int, theta: float = 10000.0) -> torch.Tensor:\n",
    "    \"\"\"Applies the rotary embedding to the inputted query or key tensor\"\"\"\n",
    "    # Get sequence length\n",
    "    seq_len = x.size(1)\n",
    "    device = x.device\n",
    "    \n",
    "    # Dynamically compute frequency cis based on the input sequence length\n",
    "    # dynamic is less efficient but pre-computed was giving me trouble so whatever\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2, device=device).float() / dim))\n",
    "    t = torch.arange(seq_len, device=device)\n",
    "    freqs = torch.outer(t, freqs).float()\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "\n",
    "    # Apply rotary embeddings to the input tensor\n",
    "    x_ = torch.view_as_complex(torch.stack(torch.chunk(x.transpose(1, 2).float(), 2, dim=-1), dim=-1))\n",
    "    x_out = torch.view_as_real(x_ * freqs_cis.unsqueeze(0)).type_as(x)  # Ensure batch dimension is handled\n",
    "    x_out = torch.cat(torch.chunk(x_out, 2, dim=-1), dim=-2)\n",
    "    x_out = x_out.reshape(x_out.shape[0], x_out.shape[1], x_out.shape[2], -1).transpose(1, 2)\n",
    "\n",
    "    return x_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac23cbdc",
   "metadata": {},
   "source": [
    "# RMSNorm\n",
    "\n",
    "Layernorm is relatively simple code-wise. However, of note is the fact that during training, the entire full length vector gets normalized whereas during inference we only layernorm the sub-vector we've been given if we're not using the full model size. This is interesting because RMSNorm puts a vector of length $d$ onto a hypersphere of radius $\\sqrt{d}$ which means that while the embeddings of the largest model exist on a hypersphere of the aforementioned size, for each number of layers $i\\in\\mathbb{N}$ s.t. $0 < i \\leq$ `config.model_count` the embeddings are placed onto a hypersphere of radius $\\sqrt{\\frac{d}{s^i}}$ where $s=$`config.split`. I'm not sure yet exactly how to interpret this concatenation of vectors geometrically. When you combine the entries of two hypserspheres to make a larger hypserspheres, what happens to the feature groupings on the surface of the smaller hyperspheres? I presume there are some type of interaction effects or something. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d706eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Implements the RMS Normalization (Root Mean Square Normalization) layer.\n",
    "    RMSNorm is a variant of layer normalization that normalizes the activations\n",
    "    of the previous layer based on their root mean square value.\n",
    "\n",
    "    Parameters:\n",
    "    - dim (int): The dimension of the input features the normalization is applied to.\n",
    "    - eps (float): A small value added to the denominator for numerical stability. Default is 1e-6.\n",
    "    - add_unit_offset (bool): If True, adds a unit (1) to the learned scaling coefficient, effectively\n",
    "      starting with no scaling. If False, the scaling coefficient starts from zero. Default is True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        eps: float = 1e-6,\n",
    "        verbose: bool = False,\n",
    "        #add_unit_offset: bool = True,\n",
    "    ):\n",
    "        super().__init__() \n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.eps = eps  # Small epsilon value for numerical stability since you can't divide by 0\n",
    "        #self.add_unit_offset = add_unit_offset  # Flag to determine if a unit should be added to the weight\n",
    "        \n",
    "        # Initialize the weight parameter with zeros, which will be learned during training.\n",
    "        # The shape of the weight is [dim], meaning one weight per feature dimension.\n",
    "        self.weight = nn.Parameter(torch.zeros(dim))\n",
    "\n",
    "    def _norm(self, x):\n",
    "        \"\"\"\n",
    "        Private helper function to normalize the input tensor.\n",
    "\n",
    "        Parameters:\n",
    "        - x (Tensor): The input tensor to normalize.\n",
    "\n",
    "        Returns:\n",
    "        - Tensor: The normalized tensor.\n",
    "        \"\"\"\n",
    "        # Calculate the root mean square value for each feature (across the last dimension),\n",
    "        # then use reciprocal square root (rsqrt) for normalization.\n",
    "        # Add self.eps to the denominator for numerical stability.\n",
    "        return x * torch.rsqrt(x.pow(2).mean(-1, keepdim=True) + self.eps)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, model: int = 0) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass of the RMSNorm layer\n",
    "\n",
    "        Parameters:\n",
    "        - x (Tensor): The input tensor to normalize.\n",
    "        - model (int): the index indicating the model being used in this layer. used for splicing self.weight\n",
    "\n",
    "        Returns:\n",
    "        - output: The normalized and scaled tensor.\n",
    "        \"\"\"\n",
    "        if self.verbose: \n",
    "            print(\"------------- RMSNorm.forward() ------------\")\n",
    "            print(f\"x: {x.shape}\\n{x}\")\n",
    "            \n",
    "        # Normalize the input tensor using the _norm function and ensure the data type matches the input.\n",
    "        x = self._norm(x.float()).type_as(x)\n",
    "        if self.verbose: print(f\"normed x: {x.shape}\\n{x}\")\n",
    "        \n",
    "        # grabbing x's dimension to use for splicing\n",
    "        dim = x.shape[-1]\n",
    "        \n",
    "        # calculating skip for our splice\n",
    "        skip = model * dim\n",
    "        if self.verbose: \n",
    "            print(f\"dim: {dim}\")\n",
    "            print(f\"skip: {skip}\")\n",
    "        \n",
    "        # scale the normalized tensor by (1 + self.weight), which effectively starts with no scaling\n",
    "        spliced_scale = self.weight[skip:skip + dim]\n",
    "        output = x * (1 + spliced_scale)\n",
    "        if self.verbose:\n",
    "            print(f\"spliced scale: {spliced_scale.shape}\\n{spliced_scale}\")\n",
    "            print(f\"scaled normed x: {output.shape}\\n{output}\")\n",
    "            print(\"------------- END RMSNorm.forward() ------------\")\n",
    "                          \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e8bdc0",
   "metadata": {},
   "source": [
    "The following cell was designed to help you visualize what's happening with RMSNorm's splicing. With RMSNorm we'll only have to think about doing this with individual tensors, but with future methods like MLP and MQA we'll have to create an entirely separate forward method used during training that deals with tuples of tensors. The thing to pay attention to here is the size of the scale weights. scale_weights' entries are 0's because we've not yet undergone training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "28bbeb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [4, 2, 1]\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 4])\n",
      "tensor([[[0.8579, 0.8341, 0.6083, 0.5798],\n",
      "         [0.5192, 0.0044, 0.2611, 0.1196]]])\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 2, 4])\n",
      "tensor([[[0.8579, 0.8341, 0.6083, 0.5798],\n",
      "         [0.5192, 0.0044, 0.2611, 0.1196]]])\n",
      "normed x: torch.Size([1, 2, 4])\n",
      "tensor([[[1.1735, 1.1409, 0.8321, 0.7931],\n",
      "         [1.7501, 0.0149, 0.8800, 0.4030]]])\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 2, 4])\n",
      "tensor([[[1.1735, 1.1409, 0.8321, 0.7931],\n",
      "         [1.7501, 0.0149, 0.8800, 0.4030]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "y: torch.Size([1, 2, 4])\n",
      "tensor([[[1.1735, 1.1409, 0.8321, 0.7931],\n",
      "         [1.7501, 0.0149, 0.8800, 0.4030]]], grad_fn=<MulBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.1644, 0.8277],\n",
      "         [0.0856, 0.2447]]])\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.1644, 0.8277],\n",
      "         [0.0856, 0.2447]]])\n",
      "normed x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.2756, 1.3871],\n",
      "         [0.4668, 1.3349]]])\n",
      "dim: 2\n",
      "skip: 0\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.2756, 1.3871],\n",
      "         [0.4668, 1.3349]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "y: torch.Size([1, 2, 2])\n",
      "tensor([[[0.2756, 1.3871],\n",
      "         [0.4668, 1.3349]]], grad_fn=<MulBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.3786, 0.8666],\n",
      "         [0.2804, 0.3730]]])\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.3786, 0.8666],\n",
      "         [0.2804, 0.3730]]])\n",
      "normed x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.5662, 1.2959],\n",
      "         [0.8497, 1.1305]]])\n",
      "dim: 2\n",
      "skip: 2\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.5662, 1.2959],\n",
      "         [0.8497, 1.1305]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "y: torch.Size([1, 2, 2])\n",
      "tensor([[[0.5662, 1.2959],\n",
      "         [0.8497, 1.1305]]], grad_fn=<MulBackward0>)\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "# Testing our RMSNorm's forward()\n",
    "config.verbose['RMSNorm'] = True\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold = config.hidden_size\n",
    "config.hidden_size = 4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,config.hidden_size)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "norm = RMSNorm(config.hidden_size, verbose=config.verbose['RMSNorm'])\n",
    "y = norm(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,config.hidden_size//2)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "norm = RMSNorm(config.hidden_size, verbose=config.verbose['RMSNorm'])\n",
    "y = norm(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,config.hidden_size//2)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "norm = RMSNorm(config.hidden_size, verbose=config.verbose['RMSNorm'])\n",
    "y = norm(x, model=1)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "config.verbose['RMSNorm'] = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold\n",
    "print(\"model_count: \", config.model_count)\n",
    "\n",
    "# clear up memory\n",
    "del hold, x, y, norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d5428",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/ffwd.jpeg\" width=\"512\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1ba294f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements a multi-layer perceptron with a GeGLU gating mechanism. The GeGLU\n",
    "    activation combines a standard GeLU activation with a learned gating mechanism, enabling\n",
    "    the network to control the flow of information more dynamically.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        intermediate_size: int,\n",
    "        dropout: float = 0.1,\n",
    "        verbose: bool = False,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the GemmaMLP module.\n",
    "\n",
    "        Parameters:\n",
    "            hidden_size (int): The size of the input and output tensors.\n",
    "            intermediate_size (int): The size of the tensor after the initial transformation\n",
    "                                     and before the gating and final projection. This is typically\n",
    "                                     larger than the hidden size to allow for a richer representation.\n",
    "            dropout (float): the dropout rate to use during training in forwardTuple()\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.verbose = verbose\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        assert intermediate_size % hidden_size == 0\n",
    "        self.intermediate_multiplier = intermediate_size // hidden_size\n",
    "\n",
    "        # Linear transformation for the gating mechanism, projecting input to an intermediate size.\n",
    "        self.Wgate = nn.Parameter(torch.Tensor(hidden_size, intermediate_size))\n",
    "        self.Bgate = nn.Parameter(torch.Tensor(intermediate_size))\n",
    "\n",
    "        # Linear transformation for the input tensor, also projecting to the intermediate size but\n",
    "        # intended for element-wise multiplication with the gated output.\n",
    "        self.Wup = nn.Parameter(torch.Tensor(hidden_size, intermediate_size))\n",
    "        self.Bup = nn.Parameter(torch.Tensor(intermediate_size))\n",
    "\n",
    "        # Linear transformation to project the gated and combined tensor back to the original\n",
    "        # hidden size, completing the MLP structure.\n",
    "        self.Wdown = nn.Parameter(torch.Tensor(intermediate_size, hidden_size))\n",
    "        self.Bdown = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        # Initialize weights with uniform distribution\n",
    "        # For gate & up, where in_features is hidden_size\n",
    "        limit_gateup = 1 / np.sqrt(hidden_size)\n",
    "        nn.init.uniform_(self.Wgate, -limit_gateup, limit_gateup)\n",
    "        nn.init.uniform_(self.Bgate, -limit_gateup, limit_gateup)\n",
    "        nn.init.uniform_(self.Wup, -limit_gateup, limit_gateup)\n",
    "        nn.init.uniform_(self.Bup, -limit_gateup, limit_gateup)\n",
    "        \n",
    "        # For down, where in_features is intermediate_size\n",
    "        limit_down = 1 / np.sqrt(intermediate_size)\n",
    "        nn.init.uniform_(self.Wdown, -limit_down, limit_down)\n",
    "        nn.init.uniform_(self.Bdown, -limit_down, limit_down)\n",
    "        \n",
    "        # defining our dropout for training in forwardTuple()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forwardTensor(self, x, model:int=0):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the MLP module during inference.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tensor): The input tensor to the MLP. \n",
    "                        shape (batch size, sequence length, hidden dimension) where hidden dimension changes by which model was used\n",
    "            model (int): the indicator of which model we're using. \n",
    "                        used in calculating our skip length for splicing. \n",
    "                        defaults to the equivalent of what's used in MatFormer+, meaning no skip, aka we use the top-left-most splice\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output tensor after applying the GeGLU gating mechanism and the MLP transformations.\n",
    "        \"\"\"\n",
    "        if self.verbose: \n",
    "            print(\"------------- MLP.forwardTensor() ------------\")\n",
    "            print(f\"x: {x.shape}\\n{x}\")\n",
    "            \n",
    "        # figuring out how we should do our splicing\n",
    "        d_dim = x.shape[-1]\n",
    "        d_skip = model * d_dim\n",
    "        i_dim = d_dim * self.intermediate_multiplier\n",
    "        i_skip = model * i_dim\n",
    "        if self.verbose: \n",
    "            print(f\"d_dim: {d_dim}\")\n",
    "            print(f\"d_skip: {d_skip}\")\n",
    "            print(f\"i_dim: {i_dim}\")\n",
    "            print(f\"i_skip: {i_skip}\")\n",
    "        \n",
    "        # Applies linear transformation for gating.\n",
    "        Wgate = self.Wgate[d_skip:d_skip + d_dim, i_skip:i_skip + i_dim]\n",
    "        Bgate = self.Bgate[i_skip:i_skip + i_dim]\n",
    "        Xgate = x @ Wgate + Bgate\n",
    "        if self.verbose: \n",
    "            print(f\"Wgate: {self.Wgate.shape}\\n{self.Wgate}\")\n",
    "            print(f\"Wgate spliced: {Wgate.shape}\\n{Wgate}\")\n",
    "            print(f\"Bgate: {self.Bgate.shape}\\n{self.Bgate}\")\n",
    "            print(f\"Bgate spliced: {Bgate.shape}\\n{Bgate}\")\n",
    "            print(f\"Xgate: {Xgate.shape}\\n{Xgate}\")\n",
    "\n",
    "        # Applies GeLU activation to the gate, introducing non-linearity and enabling the gating mechanism.\n",
    "        Xgate = F.gelu(Xgate)\n",
    "        if self.verbose: print(f\"GeLU'ed Xgate: {Xgate.shape}\\n{Xgate}\")\n",
    "\n",
    "        # Applies another linear transformation to the input tensor for subsequent combination with the gate.\n",
    "        Wup = self.Wup[d_skip:d_skip + d_dim, i_skip:i_skip + i_dim]\n",
    "        Bup = self.Bup[i_skip:i_skip + i_dim]\n",
    "        Xup = x @ Wup + Bup\n",
    "        if self.verbose: \n",
    "            print(f\"Wup: {self.Wup.shape}\\n{self.Wup}\")\n",
    "            print(f\"Wup spliced: {Wup.shape}\\n{Wup}\")\n",
    "            print(f\"Bup: {self.Bup.shape}\\n{self.Bup}\")\n",
    "            print(f\"Bup spliced: {Bup.shape}\\n{Bup}\")\n",
    "            print(f\"Xup: {Xup.shape}\\n{Xup}\")\n",
    "\n",
    "        # Element-wise multiplication of the gated tensor with the transformed input tensor, modulating\n",
    "        # the input based on the gate's activation.\n",
    "        Xfuse = Xgate * Xup\n",
    "        if self.verbose: print(f\"Xfuse: {Xfuse.shape}\\n{Xfuse}\")\n",
    "\n",
    "        # Applies the final linear transformation to project the modulated tensor back to the hidden size.\n",
    "        Wdown = self.Wdown[i_skip:i_skip + i_dim, d_skip:d_skip + d_dim]\n",
    "        Bdown = self.Bdown[d_skip:d_skip + d_dim]\n",
    "        outputs = Xfuse @ Wdown + Bdown\n",
    "        if self.verbose: \n",
    "            print(f\"Wdown: {self.Wdown.shape}\\n{self.Wdown}\")\n",
    "            print(f\"Wdown spliced: {Wdown.shape}\\n{Wdown}\")\n",
    "            print(f\"Bdown: {self.Bdown.shape}\\n{self.Bdown}\")\n",
    "            print(f\"Bdown spliced: {Bdown.shape}\\n{Bdown}\")\n",
    "            print(f\"outputs: {outputs.shape}\\n{outputs}\") \n",
    "            print(\"------------- END MLP.forwardTensor() ------------\")\n",
    "\n",
    "        # Returns the final output tensor of the MLP, after gating and modulation.\n",
    "        return outputs\n",
    "\n",
    "    def forwardTuple(self, x, drop_bool: bool = True):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the MLP module during training.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tuple[Tuple[Tensor]]): \n",
    "                The input tuple of tuples of tensors to the MLP. \n",
    "                first tuple is of length config.levels and second layer of tuples have lengths of config.model_count\n",
    "                tensors are shape (batch size, sequence length, hidden dimension) where hidden dimension changes by which model was used\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tuple[Tensor]]: \n",
    "                The output tuple of tuples of tensors after applying the GeGLU gating mechanism and the MLP transformations.\n",
    "        \"\"\"\n",
    "        if self.verbose: \n",
    "            print(\"------------- MLP.forwardTuple() ------------\")\n",
    "            print(f\"x: {x}\")\n",
    "\n",
    "        # if we had sent through the config we could've just grabbed these values from there but too late now\n",
    "        num_levels = len(x)\n",
    "        models_per_level = [len(x[i]) for i in range(num_levels)]\n",
    "        if self.verbose: \n",
    "            print(f\"num_levels: {num_levels}\")\n",
    "            print(f\"models_per_level: {models_per_level}\")\n",
    "        \n",
    "        out = ()\n",
    "        for i in range(num_levels):\n",
    "            if self.verbose: print(f\"i: {i}\")\n",
    "            \n",
    "            out_lvl = ()\n",
    "            for j in range(models_per_level[i]):\n",
    "                if self.verbose: print(f\"j: {j}\")\n",
    "\n",
    "                output = self.forwardTensor(x[i][j], model=j)\n",
    "                if self.verbose: print(f\"forwardTensor() output: {output.shape}\\n{output}\")\n",
    "                    \n",
    "                out_lvl += (self.drop(output),) if drop_bool else (output,)\n",
    "\n",
    "            # pretty sure i have to save & store everything without overwriting to prevent in-place arguments. so annoying\n",
    "            if self.verbose: print(f\"out_lvl: {out_lvl}\")\n",
    "            out += (out_lvl,)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"out: {out}\")\n",
    "            print(\"------------- END MLP.forwardTuple() ------------\")\n",
    "        return out\n",
    "        \n",
    "    def forward(self, x, model=0, drop_bool = True):\n",
    "        train = True if type(x) == tuple else False\n",
    "        if self.verbose: print(f\"---------- MLP Input: {'Tuple' if train else 'torch.Tensor'} ------------\")\n",
    "        return self.forwardTuple(x, drop_bool) if train else self.forwardTensor(x, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5cf879",
   "metadata": {},
   "source": [
    "The following two cells are designed to help you comprehend what's happening. If you walk through every single print statement and follow along even down to watching what happens to each weight, you'll be able to clearly see what's happening with the odd splicing behavior. In order to make this somewhat feasible, I've set very small matrices for these examples. However I will admit it is still inevitably a pain, which is why I included the drawings above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "949b1e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [4, 2]\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 4])\n",
      "tensor([[[0.7281, 0.4765, 0.9686, 0.1499],\n",
      "         [0.6585, 0.3994, 0.1702, 0.6216]]])\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 2, 4])\n",
      "tensor([[[0.7281, 0.4765, 0.9686, 0.1499],\n",
      "         [0.6585, 0.3994, 0.1702, 0.6216]]])\n",
      "d_dim: 4\n",
      "d_skip: 0\n",
      "i_dim: 8\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.4666, -0.4539,  0.0391,  0.0463,  0.0505, -0.2020,  0.4694,  0.4681],\n",
      "        [ 0.1910,  0.0150,  0.4008,  0.2614,  0.1535,  0.1524,  0.0971,  0.4999],\n",
      "        [ 0.0653,  0.0442,  0.3793,  0.4703,  0.1421,  0.1048, -0.3274,  0.0231],\n",
      "        [-0.1394, -0.4897, -0.1767,  0.1544, -0.0580,  0.2819, -0.3009,  0.0760]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([4, 8])\n",
      "tensor([[ 0.4666, -0.4539,  0.0391,  0.0463,  0.0505, -0.2020,  0.4694,  0.4681],\n",
      "        [ 0.1910,  0.0150,  0.4008,  0.2614,  0.1535,  0.1524,  0.0971,  0.4999],\n",
      "        [ 0.0653,  0.0442,  0.3793,  0.4703,  0.1421,  0.1048, -0.3274,  0.0231],\n",
      "        [-0.1394, -0.4897, -0.1767,  0.1544, -0.0580,  0.2819, -0.3009,  0.0760]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bgate: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.2584, -0.3702,  0.4394, -0.2962,  0.2225,  0.0278,  0.2262,  0.1142],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([8])\n",
      "tensor([-0.2584, -0.3702,  0.4394, -0.2962,  0.2225,  0.0278,  0.2262,  0.1142],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 2, 8])\n",
      "tensor([[[ 0.2147, -0.7242,  0.9998,  0.3407,  0.4614,  0.0972,  0.2520,\n",
      "           0.7270],\n",
      "         [ 0.0495, -0.9600,  0.5800,  0.0147,  0.3052,  0.1488,  0.3313,\n",
      "           0.6732]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 2, 8])\n",
      "tensor([[[ 0.1256, -0.1698,  0.8411,  0.2158,  0.3127,  0.0524,  0.1511,\n",
      "           0.5571],\n",
      "         [ 0.0257, -0.1618,  0.4170,  0.0074,  0.1892,  0.0832,  0.2086,\n",
      "           0.5047]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.4082,  0.0920,  0.2611, -0.0904,  0.1290,  0.1308,  0.2393, -0.1437],\n",
      "        [ 0.3852,  0.2982, -0.1930,  0.4519, -0.3951, -0.3305,  0.0811, -0.3198],\n",
      "        [ 0.0345, -0.1241, -0.3458,  0.1164,  0.0871, -0.3432,  0.4646, -0.4793],\n",
      "        [-0.0658, -0.4318, -0.0861, -0.2339,  0.2084,  0.2059,  0.2677, -0.3091]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([4, 8])\n",
      "tensor([[-0.4082,  0.0920,  0.2611, -0.0904,  0.1290,  0.1308,  0.2393, -0.1437],\n",
      "        [ 0.3852,  0.2982, -0.1930,  0.4519, -0.3951, -0.3305,  0.0811, -0.3198],\n",
      "        [ 0.0345, -0.1241, -0.3458,  0.1164,  0.0871, -0.3432,  0.4646, -0.4793],\n",
      "        [-0.0658, -0.4318, -0.0861, -0.2339,  0.2084,  0.2059,  0.2677, -0.3091]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bup: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.3641, -0.0233,  0.2829,  0.2226,  0.1338, -0.4447,  0.0367,  0.4360],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([8])\n",
      "tensor([ 0.3641, -0.0233,  0.2829,  0.2226,  0.1338, -0.4447,  0.0367,  0.4360],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 2, 8])\n",
      "tensor([[[ 0.2741,  0.0009,  0.0332,  0.4498,  0.1551, -0.8085,  0.7397,\n",
      "          -0.3317],\n",
      "         [ 0.2142, -0.1331,  0.2653,  0.2180,  0.2053, -0.4210,  0.4722,\n",
      "          -0.0602]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 2, 8])\n",
      "tensor([[[ 3.4415e-02, -1.4779e-04,  2.7920e-02,  9.7071e-02,  4.8499e-02,\n",
      "          -4.2339e-02,  1.1177e-01, -1.8478e-01],\n",
      "         [ 5.5148e-03,  2.1541e-02,  1.1065e-01,  1.6231e-03,  3.8850e-02,\n",
      "          -3.5029e-02,  9.8516e-02, -3.0358e-02]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([8, 4])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1876, -0.1210, -0.0973, -0.2559],\n",
      "        [ 0.3291,  0.0633, -0.1994,  0.0293],\n",
      "        [ 0.0762, -0.2658,  0.0022,  0.1370],\n",
      "        [-0.2072,  0.0570,  0.2741,  0.3200],\n",
      "        [ 0.1339, -0.0343, -0.2940,  0.0724],\n",
      "        [-0.2308, -0.3167, -0.1596, -0.2470],\n",
      "        [-0.2168,  0.0086,  0.1125, -0.3022],\n",
      "        [-0.1933,  0.0743,  0.1011,  0.0198]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([8, 4])\n",
      "tensor([[ 0.1876, -0.1210, -0.0973, -0.2559],\n",
      "        [ 0.3291,  0.0633, -0.1994,  0.0293],\n",
      "        [ 0.0762, -0.2658,  0.0022,  0.1370],\n",
      "        [-0.2072,  0.0570,  0.2741,  0.3200],\n",
      "        [ 0.1339, -0.0343, -0.2940,  0.0724],\n",
      "        [-0.2308, -0.3167, -0.1596, -0.2470],\n",
      "        [-0.2168,  0.0086,  0.1125, -0.3022],\n",
      "        [-0.1933,  0.0743,  0.1011,  0.0198]], grad_fn=<AliasBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([-0.0904,  0.0239,  0.2365,  0.1560], requires_grad=True)\n",
      "Bdown spliced: torch.Size([4])\n",
      "tensor([-0.0904,  0.0239,  0.2365,  0.1560], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.0742,  0.0168,  0.2463,  0.1586],\n",
      "         [-0.0764,  0.0036,  0.2346,  0.1520]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "y: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.0742,  0.0168,  0.2463,  0.1586],\n",
      "         [-0.0764,  0.0036,  0.2346,  0.1520]]], grad_fn=<AddBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.9328, 0.8152],\n",
      "         [0.2659, 0.9294]]])\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.9328, 0.8152],\n",
      "         [0.2659, 0.9294]]])\n",
      "d_dim: 2\n",
      "d_skip: 0\n",
      "i_dim: 4\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.1603, -0.1406,  0.1485,  0.0903,  0.1290,  0.3250, -0.3064, -0.0195],\n",
      "        [ 0.2678, -0.0759, -0.2823, -0.1156, -0.1626, -0.0823, -0.0640,  0.0250],\n",
      "        [ 0.3265,  0.3226, -0.2220,  0.0952, -0.1639, -0.0530, -0.0545,  0.1449],\n",
      "        [ 0.1405, -0.1856,  0.4620,  0.2754,  0.4613,  0.0037,  0.4151, -0.4744]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([2, 4])\n",
      "tensor([[-0.1603, -0.1406,  0.1485,  0.0903],\n",
      "        [ 0.2678, -0.0759, -0.2823, -0.1156]], grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.3497, -0.3953,  0.0397,  0.2665, -0.2117,  0.3961, -0.3696, -0.3023],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([4])\n",
      "tensor([-0.3497, -0.3953,  0.0397,  0.2665], grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.2810, -0.5884, -0.0520,  0.2564],\n",
      "         [-0.1434, -0.5033, -0.1833,  0.1830]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.1094, -0.1636, -0.0249,  0.1542],\n",
      "         [-0.0635, -0.1547, -0.0783,  0.1048]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.2986,  0.1500, -0.3853, -0.1803, -0.3009, -0.1117, -0.1530,  0.3651],\n",
      "        [-0.4493,  0.1789,  0.2847,  0.2266, -0.0810,  0.0420,  0.4821,  0.4021],\n",
      "        [ 0.2291, -0.1944, -0.2912, -0.0054, -0.3667, -0.1558,  0.3921, -0.0677],\n",
      "        [-0.1669,  0.2887,  0.3949,  0.1228, -0.2188,  0.3552,  0.2134,  0.4843]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([2, 4])\n",
      "tensor([[-0.2986,  0.1500, -0.3853, -0.1803],\n",
      "        [-0.4493,  0.1789,  0.2847,  0.2266]], grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.3611, -0.0902,  0.1049,  0.1280, -0.4021,  0.3923,  0.4543, -0.1759],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([4])\n",
      "tensor([-0.3611, -0.0902,  0.1049,  0.1280], grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 2, 4])\n",
      "tensor([[[-1.0058,  0.1955, -0.0224,  0.1445],\n",
      "         [-0.8580,  0.1159,  0.2671,  0.2907]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.1100, -0.0320,  0.0006,  0.0223],\n",
      "         [ 0.0545, -0.0179, -0.0209,  0.0305]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([8, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.0333, -0.2559, -0.2910,  0.1226],\n",
      "        [ 0.1885,  0.0095,  0.2591,  0.0268],\n",
      "        [-0.0383,  0.1129, -0.2780, -0.1064],\n",
      "        [-0.1413,  0.0807, -0.2098,  0.2152],\n",
      "        [ 0.0016, -0.2805,  0.2901, -0.2784],\n",
      "        [-0.2849,  0.2824, -0.3494, -0.1704],\n",
      "        [ 0.3426,  0.3259, -0.1398,  0.1564],\n",
      "        [ 0.1292,  0.2356,  0.1841, -0.0713]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([4, 2])\n",
      "tensor([[-0.0333, -0.2559],\n",
      "        [ 0.1885,  0.0095],\n",
      "        [-0.0383,  0.1129],\n",
      "        [-0.1413,  0.0807]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([ 0.1420,  0.0024,  0.1947, -0.1396], requires_grad=True)\n",
      "Bdown spliced: torch.Size([2])\n",
      "tensor([0.1420, 0.0024], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 2, 2])\n",
      "tensor([[[ 0.1291, -0.0243],\n",
      "         [ 0.1333, -0.0117]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "y: torch.Size([1, 2, 2])\n",
      "tensor([[[ 0.1291, -0.0243],\n",
      "         [ 0.1333, -0.0117]]], grad_fn=<AddBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.6104, 0.4618],\n",
      "         [0.7933, 0.3477]]])\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.6104, 0.4618],\n",
      "         [0.7933, 0.3477]]])\n",
      "d_dim: 2\n",
      "d_skip: 2\n",
      "i_dim: 4\n",
      "i_skip: 4\n",
      "Wgate: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1106, -0.3882, -0.1092,  0.0216, -0.1389, -0.3548, -0.0959, -0.1275],\n",
      "        [ 0.1596, -0.4198, -0.3157, -0.2740, -0.1352,  0.0379,  0.3492, -0.2751],\n",
      "        [ 0.0263,  0.4214, -0.3824,  0.1251, -0.1106,  0.3826,  0.4409, -0.3513],\n",
      "        [-0.3244, -0.4004,  0.0466,  0.2940, -0.0287,  0.1158, -0.1273,  0.0675]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([2, 4])\n",
      "tensor([[-0.1106,  0.3826,  0.4409, -0.3513],\n",
      "        [-0.0287,  0.1158, -0.1273,  0.0675]], grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.1442,  0.0433, -0.3234,  0.3518, -0.2000, -0.2197,  0.2319,  0.4092],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([4])\n",
      "tensor([-0.2000, -0.2197,  0.2319,  0.4092], grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.2808,  0.0673,  0.4422,  0.2259],\n",
      "         [-0.2977,  0.1240,  0.5374,  0.1540]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.1093,  0.0354,  0.2966,  0.1332],\n",
      "         [-0.1140,  0.0681,  0.3786,  0.0864]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2602,  0.3957, -0.1358, -0.3722,  0.1558, -0.1852,  0.4472,  0.0893],\n",
      "        [-0.1214, -0.0585,  0.4153,  0.3133, -0.2589,  0.3917,  0.3320, -0.3685],\n",
      "        [-0.3705, -0.4212, -0.4741,  0.4532, -0.3409, -0.2342, -0.1513,  0.2227],\n",
      "        [ 0.3566,  0.1751,  0.2538,  0.0364, -0.4931,  0.4059, -0.3821,  0.3116]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([2, 4])\n",
      "tensor([[-0.3409, -0.2342, -0.1513,  0.2227],\n",
      "        [-0.4931,  0.4059, -0.3821,  0.3116]], grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.0318, -0.3966, -0.1813,  0.2081,  0.0968, -0.0543, -0.4510,  0.2367],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([4])\n",
      "tensor([ 0.0968, -0.0543, -0.4510,  0.2367], grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.3390, -0.0098, -0.7198,  0.5165],\n",
      "         [-0.3451, -0.0990, -0.7039,  0.5217]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.0371, -0.0003, -0.2135,  0.0688],\n",
      "         [ 0.0393, -0.0067, -0.2665,  0.0451]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([8, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.2014,  0.3181,  0.2191, -0.2879],\n",
      "        [ 0.3330,  0.1973,  0.2915,  0.3231],\n",
      "        [ 0.0973, -0.1595, -0.1402,  0.0542],\n",
      "        [ 0.2620,  0.0131,  0.1382, -0.2278],\n",
      "        [-0.2590, -0.2430, -0.0590,  0.2199],\n",
      "        [-0.2838,  0.0031, -0.2767, -0.0701],\n",
      "        [ 0.2908,  0.0794,  0.0609,  0.3447],\n",
      "        [ 0.3012, -0.0290, -0.3523,  0.2793]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([4, 2])\n",
      "tensor([[-0.0590,  0.2199],\n",
      "        [-0.2767, -0.0701],\n",
      "        [ 0.0609,  0.3447],\n",
      "        [-0.3523,  0.2793]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([-0.1221, -0.0223, -0.2198, -0.2161], requires_grad=True)\n",
      "Bdown spliced: torch.Size([2])\n",
      "tensor([-0.2198, -0.2161], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 2, 2])\n",
      "tensor([[[-0.2591, -0.2623],\n",
      "         [-0.2523, -0.2863]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "y: torch.Size([1, 2, 2])\n",
      "tensor([[[-0.2591, -0.2623],\n",
      "         [-0.2523, -0.2863]]], grad_fn=<AddBackward0>)\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2]\n"
     ]
    }
   ],
   "source": [
    "# Testing our MLP's forwardTensor()\n",
    "config.verbose['MLP'] = True\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold = config.hidden_size\n",
    "config.hidden_size = 4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,4)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "mlp = MLP(4,8, verbose=config.verbose['MLP'])\n",
    "y = mlp(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,2)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "mlp = MLP(4,8, verbose=config.verbose['MLP'])\n",
    "y = mlp(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,2)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "mlp = MLP(4,8, verbose=config.verbose['MLP'])\n",
    "y = mlp(x, model=1)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "config.verbose['MLP'] = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold\n",
    "print(\"model_count: \", config.model_count)\n",
    "\n",
    "# clear up memory\n",
    "del hold, x, y, mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5b0a4ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [4, 2]\n",
      "x: ((tensor([[[ 0.2455,  0.3505, -0.7284,  1.3426],\n",
      "         [-1.3696,  0.1531,  1.0647, -0.4866]]]),), (tensor([[[ 0.7130,  1.0259],\n",
      "         [-0.4039,  0.4001]]]), tensor([[[-2.2886,  2.0869],\n",
      "         [ 1.7298, -0.1407]]])))\n",
      "---------- MLP Input: Tuple ------------\n",
      "------------- MLP.forwardTuple() ------------\n",
      "x: ((tensor([[[ 0.2455,  0.3505, -0.7284,  1.3426],\n",
      "         [-1.3696,  0.1531,  1.0647, -0.4866]]]),), (tensor([[[ 0.7130,  1.0259],\n",
      "         [-0.4039,  0.4001]]]), tensor([[[-2.2886,  2.0869],\n",
      "         [ 1.7298, -0.1407]]])))\n",
      "num_levels: 2\n",
      "models_per_level: [1, 2]\n",
      "i: 0\n",
      "j: 0\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.2455,  0.3505, -0.7284,  1.3426],\n",
      "         [-1.3696,  0.1531,  1.0647, -0.4866]]])\n",
      "d_dim: 4\n",
      "d_skip: 0\n",
      "i_dim: 8\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.2385, -0.4624, -0.4879, -0.2823,  0.1342,  0.1279,  0.3824, -0.2909],\n",
      "        [ 0.0806,  0.1350,  0.4154,  0.1767,  0.2151, -0.4552,  0.0503, -0.0271],\n",
      "        [ 0.4173, -0.3258, -0.3684, -0.0790, -0.0876,  0.0870, -0.1913,  0.2954],\n",
      "        [ 0.0607,  0.3414, -0.1492, -0.1191,  0.3356, -0.4044,  0.2768,  0.0380]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([4, 8])\n",
      "tensor([[-0.2385, -0.4624, -0.4879, -0.2823,  0.1342,  0.1279,  0.3824, -0.2909],\n",
      "        [ 0.0806,  0.1350,  0.4154,  0.1767,  0.2151, -0.4552,  0.0503, -0.0271],\n",
      "        [ 0.4173, -0.3258, -0.3684, -0.0790, -0.0876,  0.0870, -0.1913,  0.2954],\n",
      "        [ 0.0607,  0.3414, -0.1492, -0.1191,  0.3356, -0.4044,  0.2768,  0.0380]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bgate: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.3556,  0.1845, -0.4956, -0.0281,  0.3586,  0.2287,  0.2795, -0.1238],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([8])\n",
      "tensor([ 0.3556,  0.1845, -0.4956, -0.0281,  0.3586,  0.2287,  0.2795, -0.1238],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 2, 8])\n",
      "tensor([[[ 0.1028,  0.8140, -0.4017, -0.1379,  0.9813, -0.5058,  0.9021,\n",
      "          -0.3690],\n",
      "         [ 1.1094,  0.3254, -0.0836,  0.3593, -0.0487,  0.2733, -0.5749,\n",
      "           0.5665]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 2, 8])\n",
      "tensor([[[ 0.0556,  0.6448, -0.1382, -0.0614,  0.8211, -0.1550,  0.7365,\n",
      "          -0.1314],\n",
      "         [ 0.9611,  0.2042, -0.0390,  0.2301, -0.0234,  0.1661, -0.1625,\n",
      "           0.4048]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.4951, -0.2027, -0.2881,  0.1512,  0.4799,  0.1967, -0.0642,  0.1783],\n",
      "        [ 0.4369,  0.3717, -0.4054, -0.4979,  0.1202, -0.3657,  0.4019,  0.2392],\n",
      "        [ 0.3940, -0.4659, -0.0116, -0.2658, -0.1286,  0.1571,  0.0530,  0.4216],\n",
      "        [-0.0915,  0.2611, -0.3008, -0.0787, -0.0138,  0.3263, -0.2298,  0.0938]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([4, 8])\n",
      "tensor([[-0.4951, -0.2027, -0.2881,  0.1512,  0.4799,  0.1967, -0.0642,  0.1783],\n",
      "        [ 0.4369,  0.3717, -0.4054, -0.4979,  0.1202, -0.3657,  0.4019,  0.2392],\n",
      "        [ 0.3940, -0.4659, -0.0116, -0.2658, -0.1286,  0.1571,  0.0530,  0.4216],\n",
      "        [-0.0915,  0.2611, -0.3008, -0.0787, -0.0138,  0.3263, -0.2298,  0.0938]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bup: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.3944, -0.4840,  0.1827,  0.2914,  0.4656,  0.2429,  0.4486, -0.1979],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([8])\n",
      "tensor([ 0.3944, -0.4840,  0.1827,  0.2914,  0.4656,  0.2429,  0.4486, -0.1979],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 2, 8])\n",
      "tensor([[[ 0.0161,  0.2863, -0.4256,  0.2420,  0.7007,  0.4866,  0.2266,\n",
      "          -0.2515],\n",
      "         [ 1.6034, -0.7726,  0.6493, -0.2366, -0.3035, -0.0740,  0.7663,\n",
      "          -0.0022]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 2, 8])\n",
      "tensor([[[ 8.9672e-04,  1.8465e-01,  5.8805e-02, -1.4856e-02,  5.7535e-01,\n",
      "          -7.5444e-02,  1.6688e-01,  3.3043e-02],\n",
      "         [ 1.5410e+00, -1.5779e-01, -2.5340e-02, -5.4434e-02,  7.1021e-03,\n",
      "          -1.2295e-02, -1.2453e-01, -9.0217e-04]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([8, 4])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0048,  0.0823, -0.3449, -0.1971],\n",
      "        [ 0.3077,  0.2563,  0.1036,  0.0334],\n",
      "        [ 0.3309,  0.2009, -0.1652, -0.0606],\n",
      "        [-0.0582,  0.0444, -0.0429,  0.1130],\n",
      "        [-0.2915, -0.1805, -0.0316,  0.0187],\n",
      "        [ 0.0518,  0.3450,  0.3336,  0.1713],\n",
      "        [-0.0194,  0.2189, -0.2141,  0.2222],\n",
      "        [-0.0215,  0.1645, -0.0528,  0.0638]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([8, 4])\n",
      "tensor([[ 0.0048,  0.0823, -0.3449, -0.1971],\n",
      "        [ 0.3077,  0.2563,  0.1036,  0.0334],\n",
      "        [ 0.3309,  0.2009, -0.1652, -0.0606],\n",
      "        [-0.0582,  0.0444, -0.0429,  0.1130],\n",
      "        [-0.2915, -0.1805, -0.0316,  0.0187],\n",
      "        [ 0.0518,  0.3450,  0.3336,  0.1713],\n",
      "        [-0.0194,  0.2189, -0.2141,  0.2222],\n",
      "        [-0.0215,  0.1645, -0.0528,  0.0638]], grad_fn=<AliasBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([ 0.1824, -0.0911,  0.0709,  0.0190], requires_grad=True)\n",
      "Bdown spliced: torch.Size([4])\n",
      "tensor([ 0.1824, -0.0911,  0.0709,  0.0190], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 2, 4])\n",
      "tensor([[[ 8.3971e-02, -1.2045e-01, -1.8603e-04,  5.6809e-02],\n",
      "         [ 1.3572e-01, -4.5162e-02, -4.4800e-01, -3.2433e-01]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "forwardTensor() output: torch.Size([1, 2, 4])\n",
      "tensor([[[ 8.3971e-02, -1.2045e-01, -1.8603e-04,  5.6809e-02],\n",
      "         [ 1.3572e-01, -4.5162e-02, -4.4800e-01, -3.2433e-01]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "out_lvl: (tensor([[[ 9.3302e-02, -0.0000e+00, -2.0670e-04,  6.3121e-02],\n",
      "         [ 1.5080e-01, -5.0180e-02, -4.9778e-01, -3.6036e-01]]],\n",
      "       grad_fn=<MulBackward0>),)\n",
      "i: 1\n",
      "j: 0\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[ 0.7130,  1.0259],\n",
      "         [-0.4039,  0.4001]]])\n",
      "d_dim: 2\n",
      "d_skip: 0\n",
      "i_dim: 4\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.2385, -0.4624, -0.4879, -0.2823,  0.1342,  0.1279,  0.3824, -0.2909],\n",
      "        [ 0.0806,  0.1350,  0.4154,  0.1767,  0.2151, -0.4552,  0.0503, -0.0271],\n",
      "        [ 0.4173, -0.3258, -0.3684, -0.0790, -0.0876,  0.0870, -0.1913,  0.2954],\n",
      "        [ 0.0607,  0.3414, -0.1492, -0.1191,  0.3356, -0.4044,  0.2768,  0.0380]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([2, 4])\n",
      "tensor([[-0.2385, -0.4624, -0.4879, -0.2823],\n",
      "        [ 0.0806,  0.1350,  0.4154,  0.1767]], grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.3556,  0.1845, -0.4956, -0.0281,  0.3586,  0.2287,  0.2795, -0.1238],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([4])\n",
      "tensor([ 0.3556,  0.1845, -0.4956, -0.0281], grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.2683, -0.0067, -0.4173, -0.0481],\n",
      "         [ 0.4842,  0.4253, -0.1324,  0.1566]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.1625, -0.0033, -0.1411, -0.0231],\n",
      "         [ 0.3321,  0.2827, -0.0592,  0.0880]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.4951, -0.2027, -0.2881,  0.1512,  0.4799,  0.1967, -0.0642,  0.1783],\n",
      "        [ 0.4369,  0.3717, -0.4054, -0.4979,  0.1202, -0.3657,  0.4019,  0.2392],\n",
      "        [ 0.3940, -0.4659, -0.0116, -0.2658, -0.1286,  0.1571,  0.0530,  0.4216],\n",
      "        [-0.0915,  0.2611, -0.3008, -0.0787, -0.0138,  0.3263, -0.2298,  0.0938]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([2, 4])\n",
      "tensor([[-0.4951, -0.2027, -0.2881,  0.1512],\n",
      "        [ 0.4369,  0.3717, -0.4054, -0.4979]], grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.3944, -0.4840,  0.1827,  0.2914,  0.4656,  0.2429,  0.4486, -0.1979],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([4])\n",
      "tensor([ 0.3944, -0.4840,  0.1827,  0.2914], grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.4896, -0.2473, -0.4386, -0.1115],\n",
      "         [ 0.7692, -0.2535,  0.1369,  0.0312]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.0796,  0.0008,  0.0619,  0.0026],\n",
      "         [ 0.2554, -0.0716, -0.0081,  0.0027]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([8, 4])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0048,  0.0823, -0.3449, -0.1971],\n",
      "        [ 0.3077,  0.2563,  0.1036,  0.0334],\n",
      "        [ 0.3309,  0.2009, -0.1652, -0.0606],\n",
      "        [-0.0582,  0.0444, -0.0429,  0.1130],\n",
      "        [-0.2915, -0.1805, -0.0316,  0.0187],\n",
      "        [ 0.0518,  0.3450,  0.3336,  0.1713],\n",
      "        [-0.0194,  0.2189, -0.2141,  0.2222],\n",
      "        [-0.0215,  0.1645, -0.0528,  0.0638]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.0048,  0.0823],\n",
      "        [ 0.3077,  0.2563],\n",
      "        [ 0.3309,  0.2009],\n",
      "        [-0.0582,  0.0444]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([ 0.1824, -0.0911,  0.0709,  0.0190], requires_grad=True)\n",
      "Bdown spliced: torch.Size([2])\n",
      "tensor([ 0.1824, -0.0911], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 2, 2])\n",
      "tensor([[[ 0.2034, -0.0718],\n",
      "         [ 0.1587, -0.0899]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "forwardTensor() output: torch.Size([1, 2, 2])\n",
      "tensor([[[ 0.2034, -0.0718],\n",
      "         [ 0.1587, -0.0899]]], grad_fn=<AddBackward0>)\n",
      "j: 1\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[-2.2886,  2.0869],\n",
      "         [ 1.7298, -0.1407]]])\n",
      "d_dim: 2\n",
      "d_skip: 2\n",
      "i_dim: 4\n",
      "i_skip: 4\n",
      "Wgate: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.2385, -0.4624, -0.4879, -0.2823,  0.1342,  0.1279,  0.3824, -0.2909],\n",
      "        [ 0.0806,  0.1350,  0.4154,  0.1767,  0.2151, -0.4552,  0.0503, -0.0271],\n",
      "        [ 0.4173, -0.3258, -0.3684, -0.0790, -0.0876,  0.0870, -0.1913,  0.2954],\n",
      "        [ 0.0607,  0.3414, -0.1492, -0.1191,  0.3356, -0.4044,  0.2768,  0.0380]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([2, 4])\n",
      "tensor([[-0.0876,  0.0870, -0.1913,  0.2954],\n",
      "        [ 0.3356, -0.4044,  0.2768,  0.0380]], grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.3556,  0.1845, -0.4956, -0.0281,  0.3586,  0.2287,  0.2795, -0.1238],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([4])\n",
      "tensor([ 0.3586,  0.2287,  0.2795, -0.1238], grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[ 1.2594, -0.8146,  1.2951, -0.7208],\n",
      "         [ 0.1599,  0.4362, -0.0903,  0.3819]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[ 1.1285, -0.1692,  1.1686, -0.1698],\n",
      "         [ 0.0901,  0.2916, -0.0419,  0.2477]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.4951, -0.2027, -0.2881,  0.1512,  0.4799,  0.1967, -0.0642,  0.1783],\n",
      "        [ 0.4369,  0.3717, -0.4054, -0.4979,  0.1202, -0.3657,  0.4019,  0.2392],\n",
      "        [ 0.3940, -0.4659, -0.0116, -0.2658, -0.1286,  0.1571,  0.0530,  0.4216],\n",
      "        [-0.0915,  0.2611, -0.3008, -0.0787, -0.0138,  0.3263, -0.2298,  0.0938]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([2, 4])\n",
      "tensor([[-0.1286,  0.1571,  0.0530,  0.4216],\n",
      "        [-0.0138,  0.3263, -0.2298,  0.0938]], grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.3944, -0.4840,  0.1827,  0.2914,  0.4656,  0.2429,  0.4486, -0.1979],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([4])\n",
      "tensor([ 0.4656,  0.2429,  0.4486, -0.1979], grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.7312,  0.5643, -0.1523, -0.9670],\n",
      "         [ 0.2450,  0.4687,  0.5726,  0.5181]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.8251, -0.0955, -0.1779,  0.1642],\n",
      "         [ 0.0221,  0.1367, -0.0240,  0.1284]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([8, 4])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0048,  0.0823, -0.3449, -0.1971],\n",
      "        [ 0.3077,  0.2563,  0.1036,  0.0334],\n",
      "        [ 0.3309,  0.2009, -0.1652, -0.0606],\n",
      "        [-0.0582,  0.0444, -0.0429,  0.1130],\n",
      "        [-0.2915, -0.1805, -0.0316,  0.0187],\n",
      "        [ 0.0518,  0.3450,  0.3336,  0.1713],\n",
      "        [-0.0194,  0.2189, -0.2141,  0.2222],\n",
      "        [-0.0215,  0.1645, -0.0528,  0.0638]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([4, 2])\n",
      "tensor([[-0.0316,  0.0187],\n",
      "        [ 0.3336,  0.1713],\n",
      "        [-0.2141,  0.2222],\n",
      "        [-0.0528,  0.0638]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([ 0.1824, -0.0911,  0.0709,  0.0190], requires_grad=True)\n",
      "Bdown spliced: torch.Size([2])\n",
      "tensor([0.0709, 0.0190], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 2, 2])\n",
      "tensor([[[ 0.0424, -0.0109],\n",
      "         [ 0.1142,  0.0457]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "forwardTensor() output: torch.Size([1, 2, 2])\n",
      "tensor([[[ 0.0424, -0.0109],\n",
      "         [ 0.1142,  0.0457]]], grad_fn=<AddBackward0>)\n",
      "out_lvl: (tensor([[[ 0.0000, -0.0797],\n",
      "         [ 0.1764, -0.0999]]], grad_fn=<MulBackward0>), tensor([[[ 0.0471, -0.0122],\n",
      "         [ 0.1269,  0.0508]]], grad_fn=<MulBackward0>))\n",
      "out: ((tensor([[[ 9.3302e-02, -0.0000e+00, -2.0670e-04,  6.3121e-02],\n",
      "         [ 1.5080e-01, -5.0180e-02, -4.9778e-01, -3.6036e-01]]],\n",
      "       grad_fn=<MulBackward0>),), (tensor([[[ 0.0000, -0.0797],\n",
      "         [ 0.1764, -0.0999]]], grad_fn=<MulBackward0>), tensor([[[ 0.0471, -0.0122],\n",
      "         [ 0.1269,  0.0508]]], grad_fn=<MulBackward0>)))\n",
      "------------- END MLP.forwardTuple() ------------\n",
      "out: ((tensor([[[ 9.3302e-02, -0.0000e+00, -2.0670e-04,  6.3121e-02],\n",
      "         [ 1.5080e-01, -5.0180e-02, -4.9778e-01, -3.6036e-01]]],\n",
      "       grad_fn=<MulBackward0>),), (tensor([[[ 0.0000, -0.0797],\n",
      "         [ 0.1764, -0.0999]]], grad_fn=<MulBackward0>), tensor([[[ 0.0471, -0.0122],\n",
      "         [ 0.1269,  0.0508]]], grad_fn=<MulBackward0>)))\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [4, 2]\n"
     ]
    }
   ],
   "source": [
    "# Testing our MLP's forwardTuple()\n",
    "config.verbose['MLP'] = True\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2 = config.hidden_size, config.levels\n",
    "config.hidden_size = 4\n",
    "config.levels = 2\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "mlp = MLP(4,8, verbose=config.verbose['MLP'])\n",
    "x = ((torch.randn((1,2,4)),),\n",
    "     (torch.randn((1,2,2)),torch.randn((1,2,2)))\n",
    "    )\n",
    "print(f\"x: {x}\")\n",
    "out = mlp(x)\n",
    "print(f\"out: {out}\")\n",
    "\n",
    "config.verbose['MLP'] = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.levels = hold2\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "# clear up memory\n",
    "del hold1, hold2, x, out, mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14cf359",
   "metadata": {},
   "source": [
    "# Multi-Query Attention\n",
    "\n",
    "To subset the attention heads, we have to not only splice according to the model's embedding dimension but also take into account new smaller head sizes and how they're spaced throughout the matrix. I'm assuming you know how self-attention works well enough to look at this weight matrix and get the idea\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/sa.jpeg\" width=\"512\"/>\n",
    "</p>\n",
    "\n",
    "then we've gotta concatenate the outputs of each head\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/mha_concat.jpeg\" width=\"512\"/>\n",
    "</p>\n",
    "\n",
    "and after that linearly project them\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/mha_proj.jpeg\" width=\"512\"/>\n",
    "</p>\n",
    "\n",
    "this is the place where our splicing gets conceptually annoying. instead of just grabbing the matrix in the upper corner, because of the way attention head output concatenation works we actually need to skip over certain parts of the linear projection matrix and then concatenate them together in order to use them. Here's an example of what the matrix multiplication looks like. on the left is a simplified version of the concatenated attention heads where i just showed it as a matrix rather than a tensor, and then on the right is the actual projection matrix. notice how the numbers in the pink output matrix look similar to the first column of the purple output matrix with a positive number, its negative, and then a smaller positive number; that's the self-similarity in action. the yellow arrows point to the parts that get skipped over. obviously this would look a lot uglier with bigger matrices & incorporating the blue/green layer\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/mha_proj_matmul.jpeg\" width=\"512\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3f3dd6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Multi-Query Attention which supports a distinct number of attention heads for queries and key-values (KV).\n",
    "    In the case where the same number of queries and key-values are used, this implemenation is equivalent to regular Multi-Head Attention.  \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.verbose = config.verbose['MQA']\n",
    "\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.num_kv_heads = config.num_key_value_heads\n",
    "        \n",
    "        # Determines the number of query heads associated with each KV head.\n",
    "        self.num_queries_per_kv = self.num_heads // self.num_kv_heads\n",
    "\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.head_dim = config.head_dim\n",
    "        self.theta = config.rope_theta\n",
    "\n",
    "        # Calculates the total size for all query projections.\n",
    "        self.q_size = self.num_heads * self.head_dim\n",
    "        # Calculates the total size for all key and value projections.\n",
    "        self.kv_size = self.num_kv_heads * self.head_dim\n",
    "        \n",
    "        # Initialize our learnable matrices\n",
    "        # the linear projection layer for queries, keys, and values\n",
    "        # no real reason why we're creating one matrix instead of separate ones. cleaner model summary view?\n",
    "        self.Wqkv = nn.Parameter(torch.Tensor(self.hidden_size,\n",
    "                                              (self.num_heads + 2 * self.num_kv_heads) * self.head_dim))\n",
    "        # the output projection layer, mapping the concatenated attention outputs back to the hidden size.\n",
    "        self.Wo = nn.Parameter(torch.Tensor(self.num_heads * self.head_dim, self.hidden_size))\n",
    "        \n",
    "        # Initialize weights with uniform distribution\n",
    "        # For qkv_proj, where in_features is hidden_size\n",
    "        limit_Wqkv = 1 / np.sqrt(self.hidden_size)\n",
    "        nn.init.uniform_(self.Wqkv, -limit_Wqkv, limit_Wqkv)\n",
    "        # for o_proj, where in_features is self.num_heads * self.head_dim\n",
    "        limit_Wo = 1 / np.sqrt(self.num_heads * self.head_dim)\n",
    "        nn.init.uniform_(self.Wo, -limit_Wo, limit_Wo)\n",
    "        \n",
    "        # for our attention mask we'll use very large negative values to prevent attending to certain tokens\n",
    "        mask_negatives = torch.full((1, 1, config.max_position_embeddings, config.max_position_embeddings),\n",
    "                                 -2.3819763e38).to(torch.float)\n",
    "        # then we'll replace the lower triangular ones with 0's to allow attention to see past tokens\n",
    "        mask = torch.triu(mask_negatives, diagonal=1).to(config.device)\n",
    "        # to define self.mask as a tensor that shouldn't undergo gradient descent\n",
    "        self.register_buffer('mask', mask)\n",
    "        \n",
    "        # defining our dropout\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forwardTensor(self,\n",
    "                      x: torch.Tensor,\n",
    "                      model: int = 0,\n",
    "                     ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x (torch.Tensor): Te input tensor to the attention mechanism.\n",
    "                        shape (batch_size, input_len, hidden_size)\n",
    "            model (int): the indicator of which model we're using. \n",
    "                        used in calculating our skip length for splicing. \n",
    "                        defaults to the equivalent of what's used in MatFormer+, meaning no skip, aka we use the top-left-most splice\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The output tensor after applying the attention mechanism\n",
    "        \"\"\"\n",
    "        if self.verbose: print(\"----------------- MultiQueryAttention.forwardTensor() --------------------\")\n",
    "        \n",
    "        # Ensures the input tensor is 3-dimensional (batch_size, input_len, hidden_size).\n",
    "        x_shape = x.shape\n",
    "        assert len(x_shape) == 3\n",
    "        if self.verbose: print(f\"x shape: {x_shape}\")\n",
    "\n",
    "        # Extracts input sequence length and embedding dimension length from the hidden states tensor.\n",
    "        batch_size, input_len, d_dim = x_shape\n",
    "        \n",
    "        # figuring out how we should do our splicing\n",
    "        # first along the embedding dimension\n",
    "        d_skip = model * d_dim  # the size of our skip along the model's embedding dimension\n",
    "        if self.verbose: print(f\"d_skip: {d_skip}\")\n",
    "        \n",
    "        # then for splicing along the head sizes dimension\n",
    "        index = config.model_dim_list.index(d_dim)\n",
    "        models_in_this_level = config.model_count[index] # how many models are in this level\n",
    "        h_dim = config.head_dim_list[index] # the head dimension size of this model in this level\n",
    "        h_skip = model * h_dim # the size of our skip along the head dimension\n",
    "        if self.verbose: \n",
    "            print(f\"models_in_this_level: {models_in_this_level}\")\n",
    "            print(f\"h_dim: {h_dim}\")\n",
    "            print(f\"h_skip: {h_skip}\")\n",
    "\n",
    "        # Splits the Wqkv tensor into separate tensors for queries, keys, and values based on their respective sizes.\n",
    "        if self.verbose: print(f\"self.Wqkv: {self.Wqkv.shape}\\n{self.Wqkv}\")\n",
    "        Wq, Wk, Wv = self.Wqkv.split([self.q_size,\n",
    "                                      self.kv_size,\n",
    "                                      self.kv_size],dim=-1)\n",
    "        if self.verbose: \n",
    "            print(f\"Wq: {Wq.shape}\\n{Wq}\")\n",
    "            print(f\"Wk: {Wk.shape}\\n{Wk}\")\n",
    "            print(f\"Wv: {Wv.shape}\\n{Wv}\")\n",
    "        \n",
    "        # splicing to get our correct weight matrices for each respective head\n",
    "        # d_dim is relatively self-explanatory\n",
    "        # i*self.head_dim is bc we initialized one single q, k, and v matrix for all heads so we have to\n",
    "        # iterate through said matrix to get to the correct head\n",
    "        Wq = torch.cat([Wq[d_skip:d_skip + d_dim,\\\n",
    "                               i*self.head_dim + h_skip:i*self.head_dim + h_skip + h_dim] \\\n",
    "                               for i in range(self.num_heads)], dim=1)\n",
    "        Wk = torch.cat([Wk[d_skip:d_skip + d_dim,\\\n",
    "                               i*self.head_dim + h_skip:i*self.head_dim + h_skip + h_dim] \\\n",
    "                               for i in range(self.num_kv_heads)], dim=1)\n",
    "        Wv = torch.cat([Wv[d_skip:d_skip + d_dim,\\\n",
    "                               i*self.head_dim + h_skip:i*self.head_dim + h_skip + h_dim] \\\n",
    "                               for i in range(self.num_kv_heads)], dim=1)\n",
    "        if self.verbose:\n",
    "            print(f\"Wq spliced: {Wq.shape}\\n{Wq}\")\n",
    "            print(f\"Wk spliced: {Wk.shape}\\n{Wk}\")\n",
    "            print(f\"Wv spliced: {Wv.shape}\\n{Wv}\")\n",
    "        \n",
    "        # this needs to be size (d_dim, (self.num_heads + 2 * self.num_kv_heads) * h_dim) aka (32,24)\n",
    "        # recombine the spliced Wq Wk and Wv. Now they're the right size for matmul against x\n",
    "        Wqkv_spliced = torch.cat((Wq, Wk, Wv), dim=-1)\n",
    "        if self.verbose:\n",
    "            print(f\"Wqkv_spliced: {Wqkv_spliced.shape}\\n{Wqkv_spliced}\")\n",
    "        \n",
    "\n",
    "        # finally we can project x to get our queries, keys and values\n",
    "        xqkv = x @ Wqkv_spliced\n",
    "        if self.verbose: print(f\"xqkv: {xqkv.shape}\\n{xqkv}\")\n",
    "            \n",
    "        # Splits the combined Xqkv tensor into separate tensors for queries (xq), keys (xk), and values (xv) based on their respective sizes.\n",
    "        xq, xk, xv = xqkv.split([self.q_size // models_in_this_level,\n",
    "                                 self.kv_size // models_in_this_level,\n",
    "                                 self.kv_size // models_in_this_level],dim=-1)\n",
    "        if self.verbose:\n",
    "            print(f\"xq: {xq.shape}\\n{xq}\")\n",
    "            print(f\"xk: {xk.shape}\\n{xk}\")\n",
    "            print(f\"xv: {xv.shape}\\n{xv}\")\n",
    "\n",
    "        # Reshapes each of the Q, K, and V tensors to separate the heads and align the dimensions for attention operations.\n",
    "        xq = xq.view(batch_size, input_len, self.num_heads, h_dim)#, self.head_dim)\n",
    "        xk = xk.view(batch_size, input_len, self.num_kv_heads, h_dim)#, self.head_dim)\n",
    "        xv = xv.view(batch_size, input_len, self.num_kv_heads, h_dim)#, self.head_dim)\n",
    "        if self.verbose:\n",
    "            print(f\"xq reshaped: {xq.shape}\\n{xq}\")\n",
    "            print(f\"xk reshaped: {xk.shape}\\n{xk}\")\n",
    "            print(f\"xv reshaped: {xv.shape}\\n{xv}\")\n",
    "\n",
    "        # Applies rotary positional embeddings to queries and keys to incorporate positional information.\n",
    "        xq = apply_rotary_emb(xq, h_dim, self.theta)#self.head_dim\n",
    "        xk = apply_rotary_emb(xk, h_dim, self.theta)#self.head_dim\n",
    "        # is the differring head dimension going to mess with RoPE? Not sure\n",
    "        if self.verbose:\n",
    "            print(f\"rotated xq: {xq.shape}\\n{xq}\")\n",
    "            print(f\"rotated xk: {xk.shape}\\n{xk}\")\n",
    "\n",
    "        # If the number of KV heads is different from the number of query heads, adjusts keys and values to match the query heads count.\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            # [batch_size, input_len, n_local_heads, head_dim]\n",
    "            xk = torch.repeat_interleave(xk, self.num_queries_per_kv, dim=2)\n",
    "            xv = torch.repeat_interleave(xv, self.num_queries_per_kv, dim=2)\n",
    "            if self.verbose:\n",
    "                print(f\"repeat_interleaved xk: {xk.shape}\\n{xk}\")\n",
    "                print(f\"repeat_interleaved xv: {xv.shape}\\n{xv}\")\n",
    "\n",
    "        # Transposes Q, K, and V tensors to align them for the batch matrix multiplication in attention calculation.\n",
    "        # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        q = xq.transpose(1, 2)\n",
    "        k = xk.transpose(1, 2)\n",
    "        v = xv.transpose(1, 2)\n",
    "        if self.verbose:\n",
    "            print(f\"transposed xq: {q.shape}\\n{q}\")\n",
    "            print(f\"transposed xk: {k.shape}\\n{k}\")\n",
    "            print(f\"transposed xv: {v.shape}\\n{v}\")\n",
    "\n",
    "        # Calculates attention scores by performing a batch matrix multiplication between queries and keys, followed by scaling.\n",
    "        # [batch_size, n_local_heads, input_len, input_len]\n",
    "        scores = torch.matmul(q, k.transpose(2, 3)) * h_dim**-0.5#self.scaling\n",
    "        if self.verbose: print(f\"scores: {scores.shape}\\n{scores}\")\n",
    "        \n",
    "        # Applies the lower-triangular mask to the attention scores\n",
    "        if self.verbose: print(f\"mask: {self.mask[...,:input_len, :input_len].shape}\\n{self.mask[...,:input_len, :input_len]}\")\n",
    "        scores = scores + self.mask[...,:input_len, :input_len] # make sure mask is the correct size. input_len <= max_seq_len\n",
    "        if self.verbose: print(f\"masked scores: {scores.shape}\\n{scores}\")\n",
    "\n",
    "        # Applies softmax to the scores to obtain attention probabilities\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        if self.verbose: print(f\"softmaxed scores: {scores.shape}\\n{scores}\")\n",
    "        \n",
    "        # Computes the weighted sum of values based on the attention scores to obtain the output of the attention mechanism.\n",
    "        # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        attention = torch.matmul(scores, v)\n",
    "        if self.verbose: print(f\"attention: {attention.shape}\\n{attention}\")\n",
    "\n",
    "        # Reshapes the attention output to match the expected output dimensions, combining the heads back into the hidden dimension.\n",
    "        # [batch_size, input_len, hidden_dim]\n",
    "        attention = attention.transpose(1, 2).contiguous().view(batch_size, input_len, -1)\n",
    "        if self.verbose: print(f\"reshaped attention: {attention.shape}\\n{attention}\")\n",
    "\n",
    "        # Splice the output projection\n",
    "        Wo = torch.cat([self.Wo[i*self.head_dim + h_skip:i*self.head_dim + h_skip + h_dim,\\\n",
    "                                d_skip:d_skip + d_dim,\\\n",
    "                               ] for i in range(self.num_heads)], dim=0)\n",
    "        if self.verbose: \n",
    "            print(f\"self.Wo: {self.Wo.shape}\\n{self.Wo}\")\n",
    "            print(f\"spliced Wo: {Wo.shape}\\n{Wo}\")\n",
    "            \n",
    "        # Applies the final linear projection to the attention output, mapping it back to the hidden size dimension.\n",
    "        output = attention @ Wo\n",
    "        if self.verbose: \n",
    "            print(f\"projected output: {output.shape}\\n{output}\")\n",
    "            print(\"----------------- END MultiQueryAttention.forwardTensor() --------------------\")\n",
    "            \n",
    "        return output\n",
    "\n",
    "    def forwardTuple(self,\n",
    "                     x: Tuple[Tuple[torch.Tensor]],\n",
    "                     drop_bool: bool = True\n",
    "                    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Attention module during training.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tuple[Tuple[Tensor]]): \n",
    "                The input tuple of tuples of tensors \n",
    "                first tuple is of length config.levels and second layer of tuples have lengths of config.model_count\n",
    "                tensors are shape (batch size, sequence length, hidden dimension) where hidden dimension changes by which model was used\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tuple[Tensor]]: \n",
    "                The output tuple of tuples of tensors after applying the MQA mechanism\n",
    "        \"\"\"\n",
    "        if self.verbose: \n",
    "            print(\"------------- MultiQueryAttention.forwardTuple() ------------\")\n",
    "            print(f\"x: {x}\")\n",
    "            \n",
    "        # forwardTuple() should only be used during training, so we assert input_len == max_position_embeddings\n",
    "        input_len = x[0][0].shape[1]\n",
    "        if self.verbose: print(f\"input_len: {input_len}\")\n",
    "        assert input_len == config.max_position_embeddings\n",
    "\n",
    "        # we could define these from the config but this way the method is more flexible to testing\n",
    "        num_levels = len(x)\n",
    "        models_per_level = [len(x[i]) for i in range(num_levels)]\n",
    "        if self.verbose: \n",
    "            print(f\"num_levels: {num_levels}\")\n",
    "            print(f\"models_per_level: {models_per_level}\")\n",
    "\n",
    "        # the loop that iterates over levels, aka the different potential sizes of models\n",
    "        out = ()\n",
    "        for i in range(num_levels):\n",
    "            if self.verbose: print(f\"Level {i} from range({num_levels})\")\n",
    "\n",
    "            # now for the loop that iterates over models in this level\n",
    "            out_lvl = ()\n",
    "            for j in range(models_per_level[i]):\n",
    "                if self.verbose: print(f\"Model {j} from range({models_per_level[i]})\")\n",
    "\n",
    "                output = self.forwardTensor(x[i][j], model=j)\n",
    "                if self.verbose: print(f\"forwardTensor() output: {output.shape}\\n{output}\")\n",
    "                \n",
    "                out_lvl += (self.drop(output),) if drop_bool else (output,)\n",
    "            \n",
    "            out += (out_lvl,)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"final output: {out}\")\n",
    "            print(\"------------- END MultiQueryAttention.forwardTuple() ------------\")\n",
    "\n",
    "        return out\n",
    "        \n",
    "    def forward(self, x, model=0, drop_bool = True):\n",
    "        train = True if type(x) == tuple else False\n",
    "        if self.verbose: print(f\"---------- Attention Input: {'Tuple' if train else 'torch.Tensor'} ------------\")\n",
    "        return self.forwardTuple(x, drop_bool) if train else self.forwardTensor(x, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c01ce03",
   "metadata": {},
   "source": [
    "And here are the detailed print statements for the attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "aa877863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [8, 4, 2]\n",
      "head_dim_list:  [4, 2, 1]\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[0.3407, 0.0037, 0.2456, 0.9530, 0.2088, 0.0721, 0.6394, 0.3947],\n",
      "         [0.0308, 0.1757, 0.2463, 0.6810, 0.9286, 0.0679, 0.1332, 0.6736],\n",
      "         [0.4515, 0.1483, 0.4882, 0.9509, 0.7283, 0.9410, 0.4479, 0.7546]]])\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 8])\n",
      "d_skip: 0\n",
      "models_in_this_level: 1\n",
      "h_dim: 4\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3391, -0.1365, -0.0472, -0.0195, -0.2014,  0.2541, -0.0274, -0.0497,\n",
      "         -0.1302, -0.3464,  0.3407,  0.0823, -0.0127,  0.0056, -0.2181,  0.3477],\n",
      "        [-0.2259,  0.2537,  0.0384, -0.1120,  0.0393,  0.1315,  0.1131, -0.1200,\n",
      "         -0.0671, -0.2622,  0.2215, -0.0416,  0.2801, -0.1157, -0.0675, -0.3097],\n",
      "        [-0.1958, -0.1534, -0.1093,  0.3113,  0.3204, -0.0552, -0.2740, -0.1926,\n",
      "         -0.2202,  0.2145, -0.0983, -0.1529, -0.1319,  0.1704, -0.0285,  0.0418],\n",
      "        [ 0.1941,  0.1000,  0.3380,  0.0324,  0.1375,  0.1364, -0.1669, -0.0062,\n",
      "         -0.1027, -0.2723, -0.2583,  0.1106,  0.1855, -0.2014, -0.0170, -0.2088],\n",
      "        [-0.0245, -0.0669, -0.2166,  0.0560, -0.1681,  0.0373, -0.2187, -0.3351,\n",
      "         -0.0130, -0.1194,  0.1450,  0.1041, -0.0872,  0.1299,  0.0028,  0.3244],\n",
      "        [ 0.2823,  0.0326, -0.3134, -0.0644, -0.0857,  0.1730, -0.0224, -0.3355,\n",
      "         -0.3412, -0.2753, -0.1672,  0.1635, -0.3393,  0.1295, -0.0222,  0.3276],\n",
      "        [ 0.0465,  0.0275,  0.1777, -0.0083,  0.2585,  0.0433,  0.1025, -0.0118,\n",
      "         -0.1261,  0.0851, -0.1143,  0.0269, -0.3513, -0.0754,  0.0864, -0.0963],\n",
      "        [-0.0815, -0.2194,  0.0028, -0.0334,  0.1717, -0.2002, -0.3491,  0.1401,\n",
      "          0.3243, -0.2509, -0.0860,  0.1195,  0.1826, -0.2126, -0.3440,  0.2877]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[ 0.3391, -0.1365, -0.0472, -0.0195, -0.2014,  0.2541, -0.0274, -0.0497],\n",
      "        [-0.2259,  0.2537,  0.0384, -0.1120,  0.0393,  0.1315,  0.1131, -0.1200],\n",
      "        [-0.1958, -0.1534, -0.1093,  0.3113,  0.3204, -0.0552, -0.2740, -0.1926],\n",
      "        [ 0.1941,  0.1000,  0.3380,  0.0324,  0.1375,  0.1364, -0.1669, -0.0062],\n",
      "        [-0.0245, -0.0669, -0.2166,  0.0560, -0.1681,  0.0373, -0.2187, -0.3351],\n",
      "        [ 0.2823,  0.0326, -0.3134, -0.0644, -0.0857,  0.1730, -0.0224, -0.3355],\n",
      "        [ 0.0465,  0.0275,  0.1777, -0.0083,  0.2585,  0.0433,  0.1025, -0.0118],\n",
      "        [-0.0815, -0.2194,  0.0028, -0.0334,  0.1717, -0.2002, -0.3491,  0.1401]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[-0.1302, -0.3464,  0.3407,  0.0823],\n",
      "        [-0.0671, -0.2622,  0.2215, -0.0416],\n",
      "        [-0.2202,  0.2145, -0.0983, -0.1529],\n",
      "        [-0.1027, -0.2723, -0.2583,  0.1106],\n",
      "        [-0.0130, -0.1194,  0.1450,  0.1041],\n",
      "        [-0.3412, -0.2753, -0.1672,  0.1635],\n",
      "        [-0.1261,  0.0851, -0.1143,  0.0269],\n",
      "        [ 0.3243, -0.2509, -0.0860,  0.1195]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[-0.0127,  0.0056, -0.2181,  0.3477],\n",
      "        [ 0.2801, -0.1157, -0.0675, -0.3097],\n",
      "        [-0.1319,  0.1704, -0.0285,  0.0418],\n",
      "        [ 0.1855, -0.2014, -0.0170, -0.2088],\n",
      "        [-0.0872,  0.1299,  0.0028,  0.3244],\n",
      "        [-0.3393,  0.1295, -0.0222,  0.3276],\n",
      "        [-0.3513, -0.0754,  0.0864, -0.0963],\n",
      "        [ 0.1826, -0.2126, -0.3440,  0.2877]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([8, 8])\n",
      "tensor([[ 0.3391, -0.1365, -0.0472, -0.0195, -0.2014,  0.2541, -0.0274, -0.0497],\n",
      "        [-0.2259,  0.2537,  0.0384, -0.1120,  0.0393,  0.1315,  0.1131, -0.1200],\n",
      "        [-0.1958, -0.1534, -0.1093,  0.3113,  0.3204, -0.0552, -0.2740, -0.1926],\n",
      "        [ 0.1941,  0.1000,  0.3380,  0.0324,  0.1375,  0.1364, -0.1669, -0.0062],\n",
      "        [-0.0245, -0.0669, -0.2166,  0.0560, -0.1681,  0.0373, -0.2187, -0.3351],\n",
      "        [ 0.2823,  0.0326, -0.3134, -0.0644, -0.0857,  0.1730, -0.0224, -0.3355],\n",
      "        [ 0.0465,  0.0275,  0.1777, -0.0083,  0.2585,  0.0433,  0.1025, -0.0118],\n",
      "        [-0.0815, -0.2194,  0.0028, -0.0334,  0.1717, -0.2002, -0.3491,  0.1401]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([8, 4])\n",
      "tensor([[-0.1302, -0.3464,  0.3407,  0.0823],\n",
      "        [-0.0671, -0.2622,  0.2215, -0.0416],\n",
      "        [-0.2202,  0.2145, -0.0983, -0.1529],\n",
      "        [-0.1027, -0.2723, -0.2583,  0.1106],\n",
      "        [-0.0130, -0.1194,  0.1450,  0.1041],\n",
      "        [-0.3412, -0.2753, -0.1672,  0.1635],\n",
      "        [-0.1261,  0.0851, -0.1143,  0.0269],\n",
      "        [ 0.3243, -0.2509, -0.0860,  0.1195]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([8, 4])\n",
      "tensor([[-0.0127,  0.0056, -0.2181,  0.3477],\n",
      "        [ 0.2801, -0.1157, -0.0675, -0.3097],\n",
      "        [-0.1319,  0.1704, -0.0285,  0.0418],\n",
      "        [ 0.1855, -0.2014, -0.0170, -0.2088],\n",
      "        [-0.0872,  0.1299,  0.0028,  0.3244],\n",
      "        [-0.3393,  0.1295, -0.0222,  0.3276],\n",
      "        [-0.3513, -0.0754,  0.0864, -0.0963],\n",
      "        [ 0.1826, -0.2126, -0.3440,  0.2877]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([8, 16])\n",
      "tensor([[ 0.3391, -0.1365, -0.0472, -0.0195, -0.2014,  0.2541, -0.0274, -0.0497,\n",
      "         -0.1302, -0.3464,  0.3407,  0.0823, -0.0127,  0.0056, -0.2181,  0.3477],\n",
      "        [-0.2259,  0.2537,  0.0384, -0.1120,  0.0393,  0.1315,  0.1131, -0.1200,\n",
      "         -0.0671, -0.2622,  0.2215, -0.0416,  0.2801, -0.1157, -0.0675, -0.3097],\n",
      "        [-0.1958, -0.1534, -0.1093,  0.3113,  0.3204, -0.0552, -0.2740, -0.1926,\n",
      "         -0.2202,  0.2145, -0.0983, -0.1529, -0.1319,  0.1704, -0.0285,  0.0418],\n",
      "        [ 0.1941,  0.1000,  0.3380,  0.0324,  0.1375,  0.1364, -0.1669, -0.0062,\n",
      "         -0.1027, -0.2723, -0.2583,  0.1106,  0.1855, -0.2014, -0.0170, -0.2088],\n",
      "        [-0.0245, -0.0669, -0.2166,  0.0560, -0.1681,  0.0373, -0.2187, -0.3351,\n",
      "         -0.0130, -0.1194,  0.1450,  0.1041, -0.0872,  0.1299,  0.0028,  0.3244],\n",
      "        [ 0.2823,  0.0326, -0.3134, -0.0644, -0.0857,  0.1730, -0.0224, -0.3355,\n",
      "         -0.3412, -0.2753, -0.1672,  0.1635, -0.3393,  0.1295, -0.0222,  0.3276],\n",
      "        [ 0.0465,  0.0275,  0.1777, -0.0083,  0.2585,  0.0433,  0.1025, -0.0118,\n",
      "         -0.1261,  0.0851, -0.1143,  0.0269, -0.3513, -0.0754,  0.0864, -0.0963],\n",
      "        [-0.0815, -0.2194,  0.0028, -0.0334,  0.1717, -0.2002, -0.3491,  0.1401,\n",
      "          0.3243, -0.2509, -0.0860,  0.1195,  0.1826, -0.2126, -0.3440,  0.2877]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.2644, -0.0686,  0.3262,  0.0889,  0.3329,  0.1724, -0.3547,\n",
      "          -0.1169, -0.1765, -0.4152, -0.2422,  0.1936, -0.0542, -0.2442,\n",
      "          -0.1793,  0.0719],\n",
      "         [ 0.0024, -0.1333,  0.0117,  0.1025,  0.1614,  0.0275, -0.5882,\n",
      "          -0.3153,  0.0264, -0.4765, -0.1005,  0.2247,  0.1148, -0.1391,\n",
      "          -0.2563,  0.3288],\n",
      "         [ 0.4157, -0.1751, -0.1185,  0.1087,  0.2442,  0.2953, -0.6858,\n",
      "          -0.5993, -0.4161, -0.8467, -0.2748,  0.3934, -0.2547, -0.1006,\n",
      "          -0.3784,  0.6514]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.2644, -0.0686,  0.3262,  0.0889,  0.3329,  0.1724, -0.3547,\n",
      "          -0.1169],\n",
      "         [ 0.0024, -0.1333,  0.0117,  0.1025,  0.1614,  0.0275, -0.5882,\n",
      "          -0.3153],\n",
      "         [ 0.4157, -0.1751, -0.1185,  0.1087,  0.2442,  0.2953, -0.6858,\n",
      "          -0.5993]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.1765, -0.4152, -0.2422,  0.1936],\n",
      "         [ 0.0264, -0.4765, -0.1005,  0.2247],\n",
      "         [-0.4161, -0.8467, -0.2748,  0.3934]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0542, -0.2442, -0.1793,  0.0719],\n",
      "         [ 0.1148, -0.1391, -0.2563,  0.3288],\n",
      "         [-0.2547, -0.1006, -0.3784,  0.6514]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 0.2644, -0.0686,  0.3262,  0.0889],\n",
      "          [ 0.3329,  0.1724, -0.3547, -0.1169]],\n",
      "\n",
      "         [[ 0.0024, -0.1333,  0.0117,  0.1025],\n",
      "          [ 0.1614,  0.0275, -0.5882, -0.3153]],\n",
      "\n",
      "         [[ 0.4157, -0.1751, -0.1185,  0.1087],\n",
      "          [ 0.2442,  0.2953, -0.6858, -0.5993]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[-0.1765, -0.4152, -0.2422,  0.1936]],\n",
      "\n",
      "         [[ 0.0264, -0.4765, -0.1005,  0.2247]],\n",
      "\n",
      "         [[-0.4161, -0.8467, -0.2748,  0.3934]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[-0.0542, -0.2442, -0.1793,  0.0719]],\n",
      "\n",
      "         [[ 0.1148, -0.1391, -0.2563,  0.3288]],\n",
      "\n",
      "         [[-0.2547, -0.1006, -0.3784,  0.6514]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 0.2644, -0.0686,  0.3262,  0.0889],\n",
      "          [ 0.3329,  0.1724, -0.3547, -0.1169]],\n",
      "\n",
      "         [[-0.0085, -0.1429,  0.0084,  0.0887],\n",
      "          [ 0.5821,  0.0588, -0.1820, -0.3110]],\n",
      "\n",
      "         [[-0.0652, -0.1932,  0.4273,  0.0718],\n",
      "          [ 0.5220,  0.4084,  0.5075, -0.5287]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[-0.1765, -0.4152, -0.2422,  0.1936]],\n",
      "\n",
      "         [[ 0.0989, -0.4966, -0.0321,  0.1760]],\n",
      "\n",
      "         [[ 0.4230, -0.9080, -0.2640,  0.2173]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.1765, -0.4152, -0.2422,  0.1936],\n",
      "          [-0.1765, -0.4152, -0.2422,  0.1936]],\n",
      "\n",
      "         [[ 0.0989, -0.4966, -0.0321,  0.1760],\n",
      "          [ 0.0989, -0.4966, -0.0321,  0.1760]],\n",
      "\n",
      "         [[ 0.4230, -0.9080, -0.2640,  0.2173],\n",
      "          [ 0.4230, -0.9080, -0.2640,  0.2173]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.0542, -0.2442, -0.1793,  0.0719],\n",
      "          [-0.0542, -0.2442, -0.1793,  0.0719]],\n",
      "\n",
      "         [[ 0.1148, -0.1391, -0.2563,  0.3288],\n",
      "          [ 0.1148, -0.1391, -0.2563,  0.3288]],\n",
      "\n",
      "         [[-0.2547, -0.1006, -0.3784,  0.6514],\n",
      "          [-0.2547, -0.1006, -0.3784,  0.6514]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[ 0.2644, -0.0686,  0.3262,  0.0889],\n",
      "          [-0.0085, -0.1429,  0.0084,  0.0887],\n",
      "          [-0.0652, -0.1932,  0.4273,  0.0718]],\n",
      "\n",
      "         [[ 0.3329,  0.1724, -0.3547, -0.1169],\n",
      "          [ 0.5821,  0.0588, -0.1820, -0.3110],\n",
      "          [ 0.5220,  0.4084,  0.5075, -0.5287]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.1765, -0.4152, -0.2422,  0.1936],\n",
      "          [ 0.0989, -0.4966, -0.0321,  0.1760],\n",
      "          [ 0.4230, -0.9080, -0.2640,  0.2173]],\n",
      "\n",
      "         [[-0.1765, -0.4152, -0.2422,  0.1936],\n",
      "          [ 0.0989, -0.4966, -0.0321,  0.1760],\n",
      "          [ 0.4230, -0.9080, -0.2640,  0.2173]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.0542, -0.2442, -0.1793,  0.0719],\n",
      "          [ 0.1148, -0.1391, -0.2563,  0.3288],\n",
      "          [-0.2547, -0.1006, -0.3784,  0.6514]],\n",
      "\n",
      "         [[-0.0542, -0.2442, -0.1793,  0.0719],\n",
      "          [ 0.1148, -0.1391, -0.2563,  0.3288],\n",
      "          [-0.2547, -0.1006, -0.3784,  0.6514]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-0.0400,  0.0327,  0.0536],\n",
      "          [ 0.0380,  0.0427,  0.0716],\n",
      "          [ 0.0011,  0.0442,  0.0253]],\n",
      "\n",
      "         [[-0.0335, -0.0309,  0.0263],\n",
      "          [-0.0717, -0.0103,  0.0867],\n",
      "          [-0.2435, -0.1303, -0.1995]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-4.0004e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 3.7999e-02,  4.2734e-02, -2.3820e+38],\n",
      "          [ 1.0554e-03,  4.4197e-02,  2.5289e-02]],\n",
      "\n",
      "         [[-3.3541e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [-7.1654e-02, -1.0268e-02, -2.3820e+38],\n",
      "          [-2.4351e-01, -1.3027e-01, -1.9949e-01]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4988, 0.5012, 0.0000],\n",
      "          [0.3259, 0.3402, 0.3339]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4847, 0.5153, 0.0000],\n",
      "          [0.3160, 0.3539, 0.3302]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.0542, -0.2442, -0.1793,  0.0719],\n",
      "          [ 0.0305, -0.1915, -0.2179,  0.2007],\n",
      "          [-0.0637, -0.1605, -0.2720,  0.3528]],\n",
      "\n",
      "         [[-0.0542, -0.2442, -0.1793,  0.0719],\n",
      "          [ 0.0329, -0.1900, -0.2190,  0.2043],\n",
      "          [-0.0606, -0.1596, -0.2723,  0.3542]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.0542, -0.2442, -0.1793,  0.0719, -0.0542, -0.2442, -0.1793,\n",
      "           0.0719],\n",
      "         [ 0.0305, -0.1915, -0.2179,  0.2007,  0.0329, -0.1900, -0.2190,\n",
      "           0.2043],\n",
      "         [-0.0637, -0.1605, -0.2720,  0.3528, -0.0606, -0.1596, -0.2723,\n",
      "           0.3542]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2249, -0.0852, -0.1809, -0.2176, -0.0884,  0.1549, -0.0740,  0.1094],\n",
      "        [-0.1147,  0.2907, -0.0775,  0.2298, -0.1813, -0.0951, -0.2156,  0.3312],\n",
      "        [ 0.2426, -0.1988,  0.1071,  0.0443,  0.3136,  0.1629,  0.2511,  0.0774],\n",
      "        [-0.0536,  0.0941, -0.2653,  0.3110,  0.2630,  0.0861,  0.2097, -0.3077],\n",
      "        [-0.0939, -0.1611,  0.2851,  0.1241,  0.1322, -0.3023, -0.0914, -0.1603],\n",
      "        [ 0.3523, -0.1293,  0.1468, -0.3036, -0.1442, -0.2202, -0.1024,  0.3532],\n",
      "        [ 0.2378, -0.3286, -0.0826, -0.0071, -0.1057,  0.0885,  0.1916, -0.2735],\n",
      "        [-0.0566, -0.0642,  0.2310,  0.2295, -0.3166,  0.1396,  0.1895,  0.2653]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([8, 8])\n",
      "tensor([[ 0.2249, -0.0852, -0.1809, -0.2176, -0.0884,  0.1549, -0.0740,  0.1094],\n",
      "        [-0.1147,  0.2907, -0.0775,  0.2298, -0.1813, -0.0951, -0.2156,  0.3312],\n",
      "        [ 0.2426, -0.1988,  0.1071,  0.0443,  0.3136,  0.1629,  0.2511,  0.0774],\n",
      "        [-0.0536,  0.0941, -0.2653,  0.3110,  0.2630,  0.0861,  0.2097, -0.3077],\n",
      "        [-0.0939, -0.1611,  0.2851,  0.1241,  0.1322, -0.3023, -0.0914, -0.1603],\n",
      "        [ 0.3523, -0.1293,  0.1468, -0.3036, -0.1442, -0.2202, -0.1024,  0.3532],\n",
      "        [ 0.2378, -0.3286, -0.0826, -0.0071, -0.1057,  0.0885,  0.1916, -0.2735],\n",
      "        [-0.0566, -0.0642,  0.2310,  0.2295, -0.3166,  0.1396,  0.1895,  0.2653]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.1592,  0.0707, -0.0294,  0.0553,  0.0360,  0.0561,  0.0359,\n",
      "          -0.1323],\n",
      "         [-0.1685,  0.0821, -0.0205,  0.1123,  0.0067,  0.0457,  0.0396,\n",
      "          -0.0970],\n",
      "         [-0.2162,  0.1432, -0.0352,  0.1988, -0.0261,  0.0702,  0.0818,\n",
      "          -0.0679]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "y: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.1592,  0.0707, -0.0294,  0.0553,  0.0360,  0.0561,  0.0359,\n",
      "          -0.1323],\n",
      "         [-0.1685,  0.0821, -0.0205,  0.1123,  0.0067,  0.0457,  0.0396,\n",
      "          -0.0970],\n",
      "         [-0.2162,  0.1432, -0.0352,  0.1988, -0.0261,  0.0702,  0.0818,\n",
      "          -0.0679]]], grad_fn=<UnsafeViewBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.3543, 0.4181, 0.1665, 0.5106],\n",
      "         [0.1456, 0.1123, 0.6628, 0.3735],\n",
      "         [0.0509, 0.5593, 0.2095, 0.7324]]])\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 0\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[-1.3307e-02,  1.9413e-01, -2.9237e-01, -2.1514e-01, -7.0423e-02,\n",
      "         -7.6969e-02,  2.3266e-01,  5.4133e-02,  2.3674e-01, -2.4471e-01,\n",
      "         -1.4773e-01,  2.6907e-01,  1.9072e-02,  2.3556e-01,  1.2103e-01,\n",
      "         -1.8642e-01],\n",
      "        [ 2.3889e-01, -3.2365e-05,  2.0315e-01, -1.7578e-01,  3.2352e-01,\n",
      "         -2.1234e-01, -3.7136e-02,  2.7130e-01,  5.0389e-02,  2.8661e-01,\n",
      "          1.2287e-01, -7.6816e-02,  2.8196e-01, -3.2379e-01,  9.9921e-02,\n",
      "          2.7496e-01],\n",
      "        [ 1.7866e-01,  1.0681e-01,  3.3391e-01, -2.1734e-01, -3.1542e-01,\n",
      "         -2.7343e-01,  1.0826e-01, -3.2853e-01, -2.7415e-02,  2.9992e-01,\n",
      "         -1.7689e-01, -2.2745e-01, -8.8778e-02,  2.5378e-01,  1.7359e-01,\n",
      "          7.4994e-02],\n",
      "        [-3.8184e-02,  9.4525e-02, -3.0692e-01,  1.1410e-02, -3.4034e-01,\n",
      "         -2.6830e-01, -2.3927e-01, -3.1330e-01,  4.0844e-02, -2.7617e-01,\n",
      "         -1.9357e-01, -2.5854e-01, -2.7739e-01, -3.1210e-01,  2.5695e-01,\n",
      "          3.0785e-01],\n",
      "        [ 9.4352e-02, -8.2124e-02, -8.0554e-02,  3.1927e-01,  1.0570e-03,\n",
      "          1.6824e-01, -3.5247e-01,  1.0734e-01, -4.3126e-03, -1.1963e-01,\n",
      "          2.0715e-01,  1.9110e-01, -4.1735e-02,  1.6448e-01,  7.4432e-02,\n",
      "         -1.7853e-01],\n",
      "        [ 1.6318e-01,  1.1268e-01,  1.2864e-01,  1.7773e-01, -1.8013e-01,\n",
      "          1.7054e-01, -1.6281e-01,  1.0574e-01,  1.3854e-02,  2.7836e-01,\n",
      "         -1.8598e-01, -3.4682e-02,  9.3460e-02,  1.5853e-01, -2.9245e-01,\n",
      "          1.9608e-01],\n",
      "        [ 1.7626e-01,  2.0783e-01,  1.5363e-01,  2.9549e-01,  3.2927e-01,\n",
      "          2.3230e-01, -1.9765e-01,  2.4775e-01,  2.8927e-01, -1.5622e-01,\n",
      "         -3.0546e-01, -2.0682e-01, -2.1856e-01,  3.4521e-01,  1.6410e-02,\n",
      "         -2.5546e-01],\n",
      "        [ 2.3739e-01,  1.8438e-01,  2.3390e-01,  2.7029e-01, -4.0009e-02,\n",
      "         -5.4088e-02, -3.9289e-02,  3.0379e-01, -3.2908e-01,  3.1583e-02,\n",
      "          3.3833e-01,  3.2841e-01, -1.9709e-01,  1.2886e-01,  3.4696e-01,\n",
      "         -6.2720e-02]], requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[-1.3307e-02,  1.9413e-01, -2.9237e-01, -2.1514e-01, -7.0423e-02,\n",
      "         -7.6969e-02,  2.3266e-01,  5.4133e-02],\n",
      "        [ 2.3889e-01, -3.2365e-05,  2.0315e-01, -1.7578e-01,  3.2352e-01,\n",
      "         -2.1234e-01, -3.7136e-02,  2.7130e-01],\n",
      "        [ 1.7866e-01,  1.0681e-01,  3.3391e-01, -2.1734e-01, -3.1542e-01,\n",
      "         -2.7343e-01,  1.0826e-01, -3.2853e-01],\n",
      "        [-3.8184e-02,  9.4525e-02, -3.0692e-01,  1.1410e-02, -3.4034e-01,\n",
      "         -2.6830e-01, -2.3927e-01, -3.1330e-01],\n",
      "        [ 9.4352e-02, -8.2124e-02, -8.0554e-02,  3.1927e-01,  1.0570e-03,\n",
      "          1.6824e-01, -3.5247e-01,  1.0734e-01],\n",
      "        [ 1.6318e-01,  1.1268e-01,  1.2864e-01,  1.7773e-01, -1.8013e-01,\n",
      "          1.7054e-01, -1.6281e-01,  1.0574e-01],\n",
      "        [ 1.7626e-01,  2.0783e-01,  1.5363e-01,  2.9549e-01,  3.2927e-01,\n",
      "          2.3230e-01, -1.9765e-01,  2.4775e-01],\n",
      "        [ 2.3739e-01,  1.8438e-01,  2.3390e-01,  2.7029e-01, -4.0009e-02,\n",
      "         -5.4088e-02, -3.9289e-02,  3.0379e-01]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[ 0.2367, -0.2447, -0.1477,  0.2691],\n",
      "        [ 0.0504,  0.2866,  0.1229, -0.0768],\n",
      "        [-0.0274,  0.2999, -0.1769, -0.2274],\n",
      "        [ 0.0408, -0.2762, -0.1936, -0.2585],\n",
      "        [-0.0043, -0.1196,  0.2071,  0.1911],\n",
      "        [ 0.0139,  0.2784, -0.1860, -0.0347],\n",
      "        [ 0.2893, -0.1562, -0.3055, -0.2068],\n",
      "        [-0.3291,  0.0316,  0.3383,  0.3284]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.0191,  0.2356,  0.1210, -0.1864],\n",
      "        [ 0.2820, -0.3238,  0.0999,  0.2750],\n",
      "        [-0.0888,  0.2538,  0.1736,  0.0750],\n",
      "        [-0.2774, -0.3121,  0.2569,  0.3079],\n",
      "        [-0.0417,  0.1645,  0.0744, -0.1785],\n",
      "        [ 0.0935,  0.1585, -0.2924,  0.1961],\n",
      "        [-0.2186,  0.3452,  0.0164, -0.2555],\n",
      "        [-0.1971,  0.1289,  0.3470, -0.0627]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[-1.3307e-02,  1.9413e-01, -7.0423e-02, -7.6969e-02],\n",
      "        [ 2.3889e-01, -3.2365e-05,  3.2352e-01, -2.1234e-01],\n",
      "        [ 1.7866e-01,  1.0681e-01, -3.1542e-01, -2.7343e-01],\n",
      "        [-3.8184e-02,  9.4525e-02, -3.4034e-01, -2.6830e-01]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.2367, -0.2447],\n",
      "        [ 0.0504,  0.2866],\n",
      "        [-0.0274,  0.2999],\n",
      "        [ 0.0408, -0.2762]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.0191,  0.2356],\n",
      "        [ 0.2820, -0.3238],\n",
      "        [-0.0888,  0.2538],\n",
      "        [-0.2774, -0.3121]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[-1.3307e-02,  1.9413e-01, -7.0423e-02, -7.6969e-02,  2.3674e-01,\n",
      "         -2.4471e-01,  1.9072e-02,  2.3556e-01],\n",
      "        [ 2.3889e-01, -3.2365e-05,  3.2352e-01, -2.1234e-01,  5.0389e-02,\n",
      "          2.8661e-01,  2.8196e-01, -3.2379e-01],\n",
      "        [ 1.7866e-01,  1.0681e-01, -3.1542e-01, -2.7343e-01, -2.7415e-02,\n",
      "          2.9992e-01, -8.8778e-02,  2.5378e-01],\n",
      "        [-3.8184e-02,  9.4525e-02, -3.4034e-01, -2.6830e-01,  4.0844e-02,\n",
      "         -2.7617e-01, -2.7739e-01, -3.1210e-01]], grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.1054,  0.1348, -0.1160, -0.2986,  0.1212, -0.0579, -0.0318,\n",
      "          -0.1690],\n",
      "         [ 0.1290,  0.1344, -0.3101, -0.3165,  0.0372,  0.0922, -0.1280,\n",
      "           0.0496],\n",
      "         [ 0.1424,  0.1015, -0.1380, -0.3765,  0.0644,  0.0084, -0.0631,\n",
      "          -0.3445]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.1054,  0.1348, -0.1160, -0.2986],\n",
      "         [ 0.1290,  0.1344, -0.3101, -0.3165],\n",
      "         [ 0.1424,  0.1015, -0.1380, -0.3765]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.1212, -0.0579],\n",
      "         [ 0.0372,  0.0922],\n",
      "         [ 0.0644,  0.0084]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.0318, -0.1690],\n",
      "         [-0.1280,  0.0496],\n",
      "         [-0.0631, -0.3445]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.1054,  0.1348],\n",
      "          [-0.1160, -0.2986]],\n",
      "\n",
      "         [[ 0.1290,  0.1344],\n",
      "          [-0.3101, -0.3165]],\n",
      "\n",
      "         [[ 0.1424,  0.1015],\n",
      "          [-0.1380, -0.3765]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.1212, -0.0579]],\n",
      "\n",
      "         [[ 0.0372,  0.0922]],\n",
      "\n",
      "         [[ 0.0644,  0.0084]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.0318, -0.1690]],\n",
      "\n",
      "         [[-0.1280,  0.0496]],\n",
      "\n",
      "         [[-0.0631, -0.3445]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.1054,  0.1348],\n",
      "          [-0.1160, -0.2986]],\n",
      "\n",
      "         [[-0.0433,  0.1812],\n",
      "          [ 0.0988, -0.4319]],\n",
      "\n",
      "         [[-0.1515,  0.0873],\n",
      "          [ 0.3997,  0.0312]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.1212, -0.0579]],\n",
      "\n",
      "         [[-0.0575,  0.0811]],\n",
      "\n",
      "         [[-0.0345,  0.0551]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.1212, -0.0579],\n",
      "          [ 0.1212, -0.0579]],\n",
      "\n",
      "         [[-0.0575,  0.0811],\n",
      "          [-0.0575,  0.0811]],\n",
      "\n",
      "         [[-0.0345,  0.0551],\n",
      "          [-0.0345,  0.0551]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.0318, -0.1690],\n",
      "          [-0.0318, -0.1690]],\n",
      "\n",
      "         [[-0.1280,  0.0496],\n",
      "          [-0.1280,  0.0496]],\n",
      "\n",
      "         [[-0.0631, -0.3445],\n",
      "          [-0.0631, -0.3445]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.1054,  0.1348],\n",
      "          [-0.0433,  0.1812],\n",
      "          [-0.1515,  0.0873]],\n",
      "\n",
      "         [[-0.1160, -0.2986],\n",
      "          [ 0.0988, -0.4319],\n",
      "          [ 0.3997,  0.0312]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.1212, -0.0579],\n",
      "          [-0.0575,  0.0811],\n",
      "          [-0.0345,  0.0551]],\n",
      "\n",
      "         [[ 0.1212, -0.0579],\n",
      "          [-0.0575,  0.0811],\n",
      "          [-0.0345,  0.0551]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.0318, -0.1690],\n",
      "          [-0.1280,  0.0496],\n",
      "          [-0.0631, -0.3445]],\n",
      "\n",
      "         [[-0.0318, -0.1690],\n",
      "          [-0.1280,  0.0496],\n",
      "          [-0.0631, -0.3445]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 0.0035,  0.0034,  0.0027],\n",
      "          [-0.0111,  0.0122,  0.0081],\n",
      "          [-0.0166,  0.0112,  0.0071]],\n",
      "\n",
      "         [[ 0.0023, -0.0124, -0.0088],\n",
      "          [ 0.0262, -0.0288, -0.0192],\n",
      "          [ 0.0330, -0.0145, -0.0085]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 3.5150e-03, -2.3820e+38, -2.3820e+38],\n",
      "          [-1.1137e-02,  1.2153e-02, -2.3820e+38],\n",
      "          [-1.6564e-02,  1.1163e-02,  7.0888e-03]],\n",
      "\n",
      "         [[ 2.2893e-03, -2.3820e+38, -2.3820e+38],\n",
      "          [ 2.6161e-02, -2.8790e-02, -2.3820e+38],\n",
      "          [ 3.2989e-02, -1.4453e-02, -8.5227e-03]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4942, 0.5058, 0.0000],\n",
      "          [0.3276, 0.3369, 0.3355]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5137, 0.4863, 0.0000],\n",
      "          [0.3433, 0.3274, 0.3293]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.0318, -0.1690],\n",
      "          [-0.0804, -0.0585],\n",
      "          [-0.0747, -0.1543]],\n",
      "\n",
      "         [[-0.0318, -0.1690],\n",
      "          [-0.0786, -0.0627],\n",
      "          [-0.0736, -0.1553]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0318, -0.1690, -0.0318, -0.1690],\n",
      "         [-0.0804, -0.0585, -0.0786, -0.0627],\n",
      "         [-0.0747, -0.1543, -0.0736, -0.1553]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2890,  0.2674, -0.1384, -0.0493, -0.3465,  0.2701,  0.3135, -0.0641],\n",
      "        [ 0.2591, -0.0292,  0.0926, -0.1203, -0.3165, -0.1439,  0.0228,  0.2735],\n",
      "        [-0.2411, -0.0699,  0.0447,  0.2625,  0.0514,  0.1217,  0.3456,  0.0080],\n",
      "        [-0.1259,  0.0839, -0.0751, -0.1600,  0.1388, -0.1991,  0.2998, -0.3330],\n",
      "        [-0.2638, -0.1343, -0.1331, -0.1879,  0.1442, -0.0778,  0.1944, -0.0621],\n",
      "        [ 0.1502,  0.2030, -0.3401, -0.1601, -0.1443, -0.0431, -0.1110, -0.1063],\n",
      "        [ 0.0131, -0.3397,  0.1769,  0.3389,  0.0085, -0.0423, -0.0579, -0.0453],\n",
      "        [-0.3518, -0.0880,  0.0810,  0.0862, -0.0661,  0.0007, -0.2156,  0.0073]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[ 0.2890,  0.2674, -0.1384, -0.0493],\n",
      "        [ 0.2591, -0.0292,  0.0926, -0.1203],\n",
      "        [-0.2638, -0.1343, -0.1331, -0.1879],\n",
      "        [ 0.1502,  0.2030, -0.3401, -0.1601]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0700, -0.0336,  0.0505,  0.0549],\n",
      "         [-0.0271, -0.0220,  0.0375,  0.0358],\n",
      "         [-0.0655, -0.0371,  0.0586,  0.0609]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "y: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0700, -0.0336,  0.0505,  0.0549],\n",
      "         [-0.0271, -0.0220,  0.0375,  0.0358],\n",
      "         [-0.0655, -0.0371,  0.0586,  0.0609]]], grad_fn=<UnsafeViewBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.7701, 0.6450, 0.0060, 0.5671],\n",
      "         [0.6063, 0.0502, 0.6508, 0.4886],\n",
      "         [0.2885, 0.1242, 0.3718, 0.8763]]])\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 4\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 2\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.3199, -0.2134,  0.2609, -0.0834,  0.0101,  0.2973, -0.1827,  0.0855,\n",
      "         -0.2577,  0.3122, -0.3447, -0.0917,  0.2445,  0.0138, -0.1744,  0.2022],\n",
      "        [ 0.2960,  0.1731,  0.3336,  0.3242,  0.2522, -0.2064, -0.1800,  0.1786,\n",
      "         -0.0038, -0.1036, -0.2989, -0.2625,  0.1707,  0.1963, -0.0141, -0.0408],\n",
      "        [ 0.0179, -0.3114, -0.0449, -0.0944,  0.0581,  0.3287,  0.3095, -0.2718,\n",
      "         -0.1051, -0.1423,  0.1535,  0.0066,  0.0544,  0.2521,  0.1702,  0.2641],\n",
      "        [ 0.2614,  0.0448,  0.3498, -0.2605,  0.0752,  0.1217, -0.0552,  0.0596,\n",
      "         -0.2542, -0.3211,  0.3158,  0.0860,  0.1068,  0.1737,  0.2795, -0.2581],\n",
      "        [-0.2374, -0.0031,  0.2273,  0.2315,  0.3001,  0.0581, -0.0150, -0.3231,\n",
      "         -0.2704, -0.0658,  0.0434,  0.2440,  0.2281, -0.1278,  0.1139, -0.0786],\n",
      "        [-0.2774, -0.1597, -0.0145, -0.3189, -0.1623,  0.0826, -0.1423, -0.2690,\n",
      "          0.0554,  0.3456,  0.2968,  0.2241,  0.0161,  0.0397, -0.1454,  0.1534],\n",
      "        [ 0.0170, -0.2298, -0.3017, -0.2753,  0.0606,  0.3126, -0.0382, -0.0491,\n",
      "         -0.3221,  0.2502, -0.0199, -0.0254,  0.0981,  0.0090, -0.1770, -0.0219],\n",
      "        [-0.3081, -0.2028,  0.2070,  0.1145, -0.3233,  0.3298, -0.1733, -0.0840,\n",
      "         -0.0113,  0.1043,  0.0705,  0.0744,  0.1917,  0.2773,  0.0342,  0.0875]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[-0.3199, -0.2134,  0.2609, -0.0834,  0.0101,  0.2973, -0.1827,  0.0855],\n",
      "        [ 0.2960,  0.1731,  0.3336,  0.3242,  0.2522, -0.2064, -0.1800,  0.1786],\n",
      "        [ 0.0179, -0.3114, -0.0449, -0.0944,  0.0581,  0.3287,  0.3095, -0.2718],\n",
      "        [ 0.2614,  0.0448,  0.3498, -0.2605,  0.0752,  0.1217, -0.0552,  0.0596],\n",
      "        [-0.2374, -0.0031,  0.2273,  0.2315,  0.3001,  0.0581, -0.0150, -0.3231],\n",
      "        [-0.2774, -0.1597, -0.0145, -0.3189, -0.1623,  0.0826, -0.1423, -0.2690],\n",
      "        [ 0.0170, -0.2298, -0.3017, -0.2753,  0.0606,  0.3126, -0.0382, -0.0491],\n",
      "        [-0.3081, -0.2028,  0.2070,  0.1145, -0.3233,  0.3298, -0.1733, -0.0840]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[-0.2577,  0.3122, -0.3447, -0.0917],\n",
      "        [-0.0038, -0.1036, -0.2989, -0.2625],\n",
      "        [-0.1051, -0.1423,  0.1535,  0.0066],\n",
      "        [-0.2542, -0.3211,  0.3158,  0.0860],\n",
      "        [-0.2704, -0.0658,  0.0434,  0.2440],\n",
      "        [ 0.0554,  0.3456,  0.2968,  0.2241],\n",
      "        [-0.3221,  0.2502, -0.0199, -0.0254],\n",
      "        [-0.0113,  0.1043,  0.0705,  0.0744]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.2445,  0.0138, -0.1744,  0.2022],\n",
      "        [ 0.1707,  0.1963, -0.0141, -0.0408],\n",
      "        [ 0.0544,  0.2521,  0.1702,  0.2641],\n",
      "        [ 0.1068,  0.1737,  0.2795, -0.2581],\n",
      "        [ 0.2281, -0.1278,  0.1139, -0.0786],\n",
      "        [ 0.0161,  0.0397, -0.1454,  0.1534],\n",
      "        [ 0.0981,  0.0090, -0.1770, -0.0219],\n",
      "        [ 0.1917,  0.2773,  0.0342,  0.0875]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[ 0.2273,  0.2315, -0.0150, -0.3231],\n",
      "        [-0.0145, -0.3189, -0.1423, -0.2690],\n",
      "        [-0.3017, -0.2753, -0.0382, -0.0491],\n",
      "        [ 0.2070,  0.1145, -0.1733, -0.0840]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.0434,  0.2440],\n",
      "        [ 0.2968,  0.2241],\n",
      "        [-0.0199, -0.0254],\n",
      "        [ 0.0705,  0.0744]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.1139, -0.0786],\n",
      "        [-0.1454,  0.1534],\n",
      "        [-0.1770, -0.0219],\n",
      "        [ 0.0342,  0.0875]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[ 0.2273,  0.2315, -0.0150, -0.3231,  0.0434,  0.2440,  0.1139, -0.0786],\n",
      "        [-0.0145, -0.3189, -0.1423, -0.2690,  0.2968,  0.2241, -0.1454,  0.1534],\n",
      "        [-0.3017, -0.2753, -0.0382, -0.0491, -0.0199, -0.0254, -0.1770, -0.0219],\n",
      "        [ 0.2070,  0.1145, -0.1733, -0.0840,  0.0705,  0.0744,  0.0342,  0.0875]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.2812,  0.0359, -0.2018, -0.4703,  0.2647,  0.3744,  0.0123,\n",
      "           0.0879],\n",
      "         [ 0.0419,  0.0011, -0.1257, -0.2825,  0.0627,  0.1789, -0.0367,\n",
      "          -0.0115],\n",
      "         [ 0.1330,  0.0251, -0.1880, -0.2185,  0.1037,  0.1539, -0.0210,\n",
      "           0.0649]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.2812,  0.0359, -0.2018, -0.4703],\n",
      "         [ 0.0419,  0.0011, -0.1257, -0.2825],\n",
      "         [ 0.1330,  0.0251, -0.1880, -0.2185]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[0.2647, 0.3744],\n",
      "         [0.0627, 0.1789],\n",
      "         [0.1037, 0.1539]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.0123,  0.0879],\n",
      "         [-0.0367, -0.0115],\n",
      "         [-0.0210,  0.0649]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.2812,  0.0359],\n",
      "          [-0.2018, -0.4703]],\n",
      "\n",
      "         [[ 0.0419,  0.0011],\n",
      "          [-0.1257, -0.2825]],\n",
      "\n",
      "         [[ 0.1330,  0.0251],\n",
      "          [-0.1880, -0.2185]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[0.2647, 0.3744]],\n",
      "\n",
      "         [[0.0627, 0.1789]],\n",
      "\n",
      "         [[0.1037, 0.1539]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.0123,  0.0879]],\n",
      "\n",
      "         [[-0.0367, -0.0115]],\n",
      "\n",
      "         [[-0.0210,  0.0649]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.2812,  0.0359],\n",
      "          [-0.2018, -0.4703]],\n",
      "\n",
      "         [[ 0.0217,  0.0358],\n",
      "          [ 0.1698, -0.2584]],\n",
      "\n",
      "         [[-0.0782,  0.1104],\n",
      "          [ 0.2769, -0.0800]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.2647,  0.3744]],\n",
      "\n",
      "         [[-0.1167,  0.1494]],\n",
      "\n",
      "         [[-0.1831,  0.0303]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.2647,  0.3744],\n",
      "          [ 0.2647,  0.3744]],\n",
      "\n",
      "         [[-0.1167,  0.1494],\n",
      "          [-0.1167,  0.1494]],\n",
      "\n",
      "         [[-0.1831,  0.0303],\n",
      "          [-0.1831,  0.0303]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.0123,  0.0879],\n",
      "          [ 0.0123,  0.0879]],\n",
      "\n",
      "         [[-0.0367, -0.0115],\n",
      "          [-0.0367, -0.0115]],\n",
      "\n",
      "         [[-0.0210,  0.0649],\n",
      "          [-0.0210,  0.0649]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.2812,  0.0359],\n",
      "          [ 0.0217,  0.0358],\n",
      "          [-0.0782,  0.1104]],\n",
      "\n",
      "         [[-0.2018, -0.4703],\n",
      "          [ 0.1698, -0.2584],\n",
      "          [ 0.2769, -0.0800]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.2647,  0.3744],\n",
      "          [-0.1167,  0.1494],\n",
      "          [-0.1831,  0.0303]],\n",
      "\n",
      "         [[ 0.2647,  0.3744],\n",
      "          [-0.1167,  0.1494],\n",
      "          [-0.1831,  0.0303]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.0123,  0.0879],\n",
      "          [-0.0367, -0.0115],\n",
      "          [-0.0210,  0.0649]],\n",
      "\n",
      "         [[ 0.0123,  0.0879],\n",
      "          [-0.0367, -0.0115],\n",
      "          [-0.0210,  0.0649]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 0.0621, -0.0194, -0.0356],\n",
      "          [ 0.0135,  0.0020, -0.0020],\n",
      "          [ 0.0146,  0.0181,  0.0125]],\n",
      "\n",
      "         [[-0.1623, -0.0330,  0.0161],\n",
      "          [-0.0366, -0.0413, -0.0275],\n",
      "          [ 0.0306, -0.0313, -0.0376]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 6.2127e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 1.3540e-02,  1.9960e-03, -2.3820e+38],\n",
      "          [ 1.4604e-02,  1.8121e-02,  1.2487e-02]],\n",
      "\n",
      "         [[-1.6226e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [-3.6631e-02, -4.1310e-02, -2.3820e+38],\n",
      "          [ 3.0644e-02, -3.1307e-02, -3.7567e-02]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5029, 0.4971, 0.0000],\n",
      "          [0.3332, 0.3344, 0.3325]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5012, 0.4988, 0.0000],\n",
      "          [0.3479, 0.3270, 0.3250]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.0123,  0.0879],\n",
      "          [-0.0121,  0.0385],\n",
      "          [-0.0152,  0.0470]],\n",
      "\n",
      "         [[ 0.0123,  0.0879],\n",
      "          [-0.0122,  0.0383],\n",
      "          [-0.0146,  0.0479]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0123,  0.0879,  0.0123,  0.0879],\n",
      "         [-0.0121,  0.0385, -0.0122,  0.0383],\n",
      "         [-0.0152,  0.0470, -0.0146,  0.0479]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0132, -0.0138,  0.1445,  0.2657,  0.0933, -0.1704, -0.1875,  0.0678],\n",
      "        [ 0.0559, -0.1047,  0.0141, -0.0910,  0.3148,  0.2180,  0.0269,  0.0593],\n",
      "        [ 0.1342, -0.0432,  0.0390, -0.1654,  0.2164, -0.1236,  0.1656, -0.2358],\n",
      "        [ 0.1183, -0.2145, -0.2823,  0.3243,  0.0707, -0.3266, -0.2429,  0.1234],\n",
      "        [-0.1972,  0.1842,  0.2296,  0.1905, -0.0321, -0.1419,  0.1353,  0.1101],\n",
      "        [-0.0788, -0.3441,  0.2699,  0.0335,  0.2743, -0.1805,  0.0998, -0.2893],\n",
      "        [-0.3422, -0.0625, -0.0141, -0.3136,  0.2495,  0.3455,  0.1951,  0.2523],\n",
      "        [-0.1177,  0.1849,  0.1595, -0.3283, -0.2948, -0.2507, -0.1316, -0.3017]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[ 0.2164, -0.1236,  0.1656, -0.2358],\n",
      "        [ 0.0707, -0.3266, -0.2429,  0.1234],\n",
      "        [ 0.2495,  0.3455,  0.1951,  0.2523],\n",
      "        [-0.2948, -0.2507, -0.1316, -0.3017]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0140, -0.0480, -0.0285, -0.0155],\n",
      "         [-0.0142, -0.0249, -0.0188, -0.0070],\n",
      "         [-0.0177, -0.0305, -0.0231, -0.0088]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "y: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0140, -0.0480, -0.0285, -0.0155],\n",
      "         [-0.0142, -0.0249, -0.0188, -0.0070],\n",
      "         [-0.0177, -0.0305, -0.0231, -0.0088]]], grad_fn=<UnsafeViewBackward0>)\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n",
      "head_dim_list:  [32, 16, 8]\n"
     ]
    }
   ],
   "source": [
    "# Testing our Attention's forwardTensor()\n",
    "config.verbose['MQA'] = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4 = config.hidden_size, config.num_attention_heads, config.head_dim, config.max_position_embeddings\n",
    "config.hidden_size = 8\n",
    "config.num_attention_heads = 2\n",
    "config.head_dim = 4\n",
    "config.max_position_embeddings = 3\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,8)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "att = MultiQueryAttention(config)\n",
    "y = att(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,4)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "att = MultiQueryAttention(config)\n",
    "y = att(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,4)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "att = MultiQueryAttention(config)\n",
    "y = att(x, model=1)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "config.verbose['MQA'] = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.num_attention_heads = hold2\n",
    "config.head_dim = hold3\n",
    "config.max_position_embeddings = hold4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "# clear up memory\n",
    "del hold1, hold2, hold3, hold4, x, att, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c22983d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [8, 4]\n",
      "head_dim_list:  [4, 2]\n",
      "x: ((tensor([[[ 0.2664,  0.3733,  0.3922,  1.0960,  2.3121, -0.2461, -0.9022,\n",
      "           0.2530],\n",
      "         [-0.3789,  1.6260, -0.5612,  0.4809, -0.9917, -1.2565, -1.7778,\n",
      "           0.1097],\n",
      "         [ 1.1252,  1.3802, -0.1866,  1.2921,  1.9044,  0.3307, -0.7473,\n",
      "           0.0281]]]),), (tensor([[[-0.6271, -0.4791,  0.4411, -0.4742],\n",
      "         [-1.0448, -0.4126,  1.4944,  1.7708],\n",
      "         [ 0.9858, -0.3961, -2.0524, -1.3764]]]), tensor([[[-0.1470, -1.1672,  1.4134,  0.1182],\n",
      "         [-0.1106,  1.4042,  1.4729, -1.1275],\n",
      "         [ 0.3591,  0.6153,  0.9089, -0.5692]]])))\n",
      "---------- Attention Input: Tuple ------------\n",
      "------------- MultiQueryAttention.forwardTuple() ------------\n",
      "x: ((tensor([[[ 0.2664,  0.3733,  0.3922,  1.0960,  2.3121, -0.2461, -0.9022,\n",
      "           0.2530],\n",
      "         [-0.3789,  1.6260, -0.5612,  0.4809, -0.9917, -1.2565, -1.7778,\n",
      "           0.1097],\n",
      "         [ 1.1252,  1.3802, -0.1866,  1.2921,  1.9044,  0.3307, -0.7473,\n",
      "           0.0281]]]),), (tensor([[[-0.6271, -0.4791,  0.4411, -0.4742],\n",
      "         [-1.0448, -0.4126,  1.4944,  1.7708],\n",
      "         [ 0.9858, -0.3961, -2.0524, -1.3764]]]), tensor([[[-0.1470, -1.1672,  1.4134,  0.1182],\n",
      "         [-0.1106,  1.4042,  1.4729, -1.1275],\n",
      "         [ 0.3591,  0.6153,  0.9089, -0.5692]]])))\n",
      "input_len: 3\n",
      "num_levels: 2\n",
      "models_per_level: [1, 2]\n",
      "Level 0 from range(2)\n",
      "Model 0 from range(1)\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 8])\n",
      "d_skip: 0\n",
      "models_in_this_level: 1\n",
      "h_dim: 4\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.1335, -0.1303, -0.1848, -0.2353, -0.2708, -0.0298,  0.0637,  0.1486,\n",
      "         -0.2726, -0.0643,  0.2985,  0.0271,  0.0918, -0.2552, -0.2136, -0.0052],\n",
      "        [-0.3026, -0.1643,  0.1719, -0.1304, -0.0511, -0.0446, -0.0808, -0.0620,\n",
      "          0.2364, -0.0486, -0.1940,  0.1364, -0.0929, -0.2627,  0.2486, -0.1143],\n",
      "        [-0.0366,  0.1024,  0.1159, -0.0483,  0.1303,  0.1464,  0.1210,  0.3225,\n",
      "          0.3037,  0.1669,  0.0698,  0.3314, -0.1166, -0.0120, -0.1210, -0.0676],\n",
      "        [-0.1102, -0.2287,  0.0243, -0.1338, -0.2344, -0.0874,  0.0078, -0.1051,\n",
      "         -0.0120,  0.1125, -0.2447, -0.0139, -0.1205,  0.1822,  0.0661, -0.1941],\n",
      "        [ 0.0983, -0.0353,  0.1414, -0.2113,  0.2630, -0.2893, -0.1788, -0.1863,\n",
      "          0.0779, -0.2327, -0.1869,  0.0561, -0.2564,  0.2620,  0.1596, -0.3362],\n",
      "        [ 0.3457,  0.2794, -0.2533, -0.2063, -0.1942,  0.3105,  0.1679, -0.1309,\n",
      "         -0.0567, -0.1583,  0.0414, -0.2704, -0.0439,  0.1494,  0.1729, -0.1447],\n",
      "        [-0.3464,  0.3313, -0.2952, -0.0014,  0.0009, -0.2870,  0.1910,  0.1955,\n",
      "          0.2215,  0.2274,  0.2571, -0.1387, -0.1163,  0.1354,  0.0825,  0.3315],\n",
      "        [-0.3416, -0.0617, -0.1760, -0.2088, -0.2323, -0.1513, -0.0373,  0.2187,\n",
      "          0.1528,  0.0005, -0.1678, -0.0206, -0.0010,  0.2095,  0.2588,  0.2393]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[-0.1335, -0.1303, -0.1848, -0.2353, -0.2708, -0.0298,  0.0637,  0.1486],\n",
      "        [-0.3026, -0.1643,  0.1719, -0.1304, -0.0511, -0.0446, -0.0808, -0.0620],\n",
      "        [-0.0366,  0.1024,  0.1159, -0.0483,  0.1303,  0.1464,  0.1210,  0.3225],\n",
      "        [-0.1102, -0.2287,  0.0243, -0.1338, -0.2344, -0.0874,  0.0078, -0.1051],\n",
      "        [ 0.0983, -0.0353,  0.1414, -0.2113,  0.2630, -0.2893, -0.1788, -0.1863],\n",
      "        [ 0.3457,  0.2794, -0.2533, -0.2063, -0.1942,  0.3105,  0.1679, -0.1309],\n",
      "        [-0.3464,  0.3313, -0.2952, -0.0014,  0.0009, -0.2870,  0.1910,  0.1955],\n",
      "        [-0.3416, -0.0617, -0.1760, -0.2088, -0.2323, -0.1513, -0.0373,  0.2187]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[-0.2726, -0.0643,  0.2985,  0.0271],\n",
      "        [ 0.2364, -0.0486, -0.1940,  0.1364],\n",
      "        [ 0.3037,  0.1669,  0.0698,  0.3314],\n",
      "        [-0.0120,  0.1125, -0.2447, -0.0139],\n",
      "        [ 0.0779, -0.2327, -0.1869,  0.0561],\n",
      "        [-0.0567, -0.1583,  0.0414, -0.2704],\n",
      "        [ 0.2215,  0.2274,  0.2571, -0.1387],\n",
      "        [ 0.1528,  0.0005, -0.1678, -0.0206]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.0918, -0.2552, -0.2136, -0.0052],\n",
      "        [-0.0929, -0.2627,  0.2486, -0.1143],\n",
      "        [-0.1166, -0.0120, -0.1210, -0.0676],\n",
      "        [-0.1205,  0.1822,  0.0661, -0.1941],\n",
      "        [-0.2564,  0.2620,  0.1596, -0.3362],\n",
      "        [-0.0439,  0.1494,  0.1729, -0.1447],\n",
      "        [-0.1163,  0.1354,  0.0825,  0.3315],\n",
      "        [-0.0010,  0.2095,  0.2588,  0.2393]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([8, 8])\n",
      "tensor([[-0.1335, -0.1303, -0.1848, -0.2353, -0.2708, -0.0298,  0.0637,  0.1486],\n",
      "        [-0.3026, -0.1643,  0.1719, -0.1304, -0.0511, -0.0446, -0.0808, -0.0620],\n",
      "        [-0.0366,  0.1024,  0.1159, -0.0483,  0.1303,  0.1464,  0.1210,  0.3225],\n",
      "        [-0.1102, -0.2287,  0.0243, -0.1338, -0.2344, -0.0874,  0.0078, -0.1051],\n",
      "        [ 0.0983, -0.0353,  0.1414, -0.2113,  0.2630, -0.2893, -0.1788, -0.1863],\n",
      "        [ 0.3457,  0.2794, -0.2533, -0.2063, -0.1942,  0.3105,  0.1679, -0.1309],\n",
      "        [-0.3464,  0.3313, -0.2952, -0.0014,  0.0009, -0.2870,  0.1910,  0.1955],\n",
      "        [-0.3416, -0.0617, -0.1760, -0.2088, -0.2323, -0.1513, -0.0373,  0.2187]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([8, 4])\n",
      "tensor([[-0.2726, -0.0643,  0.2985,  0.0271],\n",
      "        [ 0.2364, -0.0486, -0.1940,  0.1364],\n",
      "        [ 0.3037,  0.1669,  0.0698,  0.3314],\n",
      "        [-0.0120,  0.1125, -0.2447, -0.0139],\n",
      "        [ 0.0779, -0.2327, -0.1869,  0.0561],\n",
      "        [-0.0567, -0.1583,  0.0414, -0.2704],\n",
      "        [ 0.2215,  0.2274,  0.2571, -0.1387],\n",
      "        [ 0.1528,  0.0005, -0.1678, -0.0206]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([8, 4])\n",
      "tensor([[ 0.0918, -0.2552, -0.2136, -0.0052],\n",
      "        [-0.0929, -0.2627,  0.2486, -0.1143],\n",
      "        [-0.1166, -0.0120, -0.1210, -0.0676],\n",
      "        [-0.1205,  0.1822,  0.0661, -0.1941],\n",
      "        [-0.2564,  0.2620,  0.1596, -0.3362],\n",
      "        [-0.0439,  0.1494,  0.1729, -0.1447],\n",
      "        [-0.1163,  0.1354,  0.0825,  0.3315],\n",
      "        [-0.0010,  0.2095,  0.2588,  0.2393]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([8, 16])\n",
      "tensor([[-0.1335, -0.1303, -0.1848, -0.2353, -0.2708, -0.0298,  0.0637,  0.1486,\n",
      "         -0.2726, -0.0643,  0.2985,  0.0271,  0.0918, -0.2552, -0.2136, -0.0052],\n",
      "        [-0.3026, -0.1643,  0.1719, -0.1304, -0.0511, -0.0446, -0.0808, -0.0620,\n",
      "          0.2364, -0.0486, -0.1940,  0.1364, -0.0929, -0.2627,  0.2486, -0.1143],\n",
      "        [-0.0366,  0.1024,  0.1159, -0.0483,  0.1303,  0.1464,  0.1210,  0.3225,\n",
      "          0.3037,  0.1669,  0.0698,  0.3314, -0.1166, -0.0120, -0.1210, -0.0676],\n",
      "        [-0.1102, -0.2287,  0.0243, -0.1338, -0.2344, -0.0874,  0.0078, -0.1051,\n",
      "         -0.0120,  0.1125, -0.2447, -0.0139, -0.1205,  0.1822,  0.0661, -0.1941],\n",
      "        [ 0.0983, -0.0353,  0.1414, -0.2113,  0.2630, -0.2893, -0.1788, -0.1863,\n",
      "          0.0779, -0.2327, -0.1869,  0.0561, -0.2564,  0.2620,  0.1596, -0.3362],\n",
      "        [ 0.3457,  0.2794, -0.2533, -0.2063, -0.1942,  0.3105,  0.1679, -0.1309,\n",
      "         -0.0567, -0.1583,  0.0414, -0.2704, -0.0439,  0.1494,  0.1729, -0.1447],\n",
      "        [-0.3464,  0.3313, -0.2952, -0.0014,  0.0009, -0.2870,  0.1910,  0.1955,\n",
      "          0.2215,  0.2274,  0.2571, -0.1387, -0.1163,  0.1354,  0.0825,  0.3315],\n",
      "        [-0.3416, -0.0617, -0.1760, -0.2088, -0.2323, -0.1513, -0.0373,  0.2187,\n",
      "          0.1528,  0.0005, -0.1678, -0.0206, -0.0010,  0.2095,  0.2588,  0.2393]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.0846, -0.7714,  0.6980, -0.7663,  0.2992, -0.5877, -0.5937,\n",
      "          -0.4920,  0.1545, -0.5507, -0.9505,  0.4889, -0.6654,  0.5288,\n",
      "           0.3784, -1.2636],\n",
      "         [-0.4275, -1.2970,  0.9798,  0.2883, -0.2101,  0.2049, -0.5969,\n",
      "          -0.3631, -0.0717, -0.0686, -0.9276,  0.5473,  0.3378, -0.9012,\n",
      "           0.0910, -0.2873],\n",
      "         [-0.1526, -0.9121,  0.4402, -1.0841, -0.2730, -0.4734, -0.4812,\n",
      "          -0.6525, -0.0842, -0.6907, -0.8002,  0.2594, -0.5747,  0.0409,\n",
      "           0.5175, -1.3309]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.0846, -0.7714,  0.6980, -0.7663,  0.2992, -0.5877, -0.5937,\n",
      "          -0.4920],\n",
      "         [-0.4275, -1.2970,  0.9798,  0.2883, -0.2101,  0.2049, -0.5969,\n",
      "          -0.3631],\n",
      "         [-0.1526, -0.9121,  0.4402, -1.0841, -0.2730, -0.4734, -0.4812,\n",
      "          -0.6525]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.1545, -0.5507, -0.9505,  0.4889],\n",
      "         [-0.0717, -0.0686, -0.9276,  0.5473],\n",
      "         [-0.0842, -0.6907, -0.8002,  0.2594]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.6654,  0.5288,  0.3784, -1.2636],\n",
      "         [ 0.3378, -0.9012,  0.0910, -0.2873],\n",
      "         [-0.5747,  0.0409,  0.5175, -1.3309]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 0.0846, -0.7714,  0.6980, -0.7663],\n",
      "          [ 0.2992, -0.5877, -0.5937, -0.4920]],\n",
      "\n",
      "         [[-0.4275, -1.2970,  0.9798,  0.2883],\n",
      "          [-0.2101,  0.2049, -0.5969, -0.3631]],\n",
      "\n",
      "         [[-0.1526, -0.9121,  0.4402, -1.0841],\n",
      "          [-0.2730, -0.4734, -0.4812, -0.6525]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[ 0.1545, -0.5507, -0.9505,  0.4889]],\n",
      "\n",
      "         [[-0.0717, -0.0686, -0.9276,  0.5473]],\n",
      "\n",
      "         [[-0.0842, -0.6907, -0.8002,  0.2594]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[-0.6654,  0.5288,  0.3784, -1.2636]],\n",
      "\n",
      "         [[ 0.3378, -0.9012,  0.0910, -0.2873]],\n",
      "\n",
      "         [[-0.5747,  0.0409,  0.5175, -1.3309]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 0.0846, -0.7714,  0.6980, -0.7663],\n",
      "          [ 0.2992, -0.5877, -0.5937, -0.4920]],\n",
      "\n",
      "         [[-1.0555, -1.3193,  0.1697,  0.1574],\n",
      "          [ 0.3888,  0.2401, -0.4993, -0.3408]],\n",
      "\n",
      "         [[-0.3368, -0.6786, -0.3220, -1.2437],\n",
      "          [ 0.5511, -0.3344, -0.0480, -0.7335]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[ 0.1545, -0.5507, -0.9505,  0.4889]],\n",
      "\n",
      "         [[ 0.7418, -0.1229, -0.5615,  0.5378]],\n",
      "\n",
      "         [[ 0.7627, -0.7284,  0.2564,  0.1170]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 0.1545, -0.5507, -0.9505,  0.4889],\n",
      "          [ 0.1545, -0.5507, -0.9505,  0.4889]],\n",
      "\n",
      "         [[ 0.7418, -0.1229, -0.5615,  0.5378],\n",
      "          [ 0.7418, -0.1229, -0.5615,  0.5378]],\n",
      "\n",
      "         [[ 0.7627, -0.7284,  0.2564,  0.1170],\n",
      "          [ 0.7627, -0.7284,  0.2564,  0.1170]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.6654,  0.5288,  0.3784, -1.2636],\n",
      "          [-0.6654,  0.5288,  0.3784, -1.2636]],\n",
      "\n",
      "         [[ 0.3378, -0.9012,  0.0910, -0.2873],\n",
      "          [ 0.3378, -0.9012,  0.0910, -0.2873]],\n",
      "\n",
      "         [[-0.5747,  0.0409,  0.5175, -1.3309],\n",
      "          [-0.5747,  0.0409,  0.5175, -1.3309]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[ 0.0846, -0.7714,  0.6980, -0.7663],\n",
      "          [-1.0555, -1.3193,  0.1697,  0.1574],\n",
      "          [-0.3368, -0.6786, -0.3220, -1.2437]],\n",
      "\n",
      "         [[ 0.2992, -0.5877, -0.5937, -0.4920],\n",
      "          [ 0.3888,  0.2401, -0.4993, -0.3408],\n",
      "          [ 0.5511, -0.3344, -0.0480, -0.7335]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[ 0.1545, -0.5507, -0.9505,  0.4889],\n",
      "          [ 0.7418, -0.1229, -0.5615,  0.5378],\n",
      "          [ 0.7627, -0.7284,  0.2564,  0.1170]],\n",
      "\n",
      "         [[ 0.1545, -0.5507, -0.9505,  0.4889],\n",
      "          [ 0.7418, -0.1229, -0.5615,  0.5378],\n",
      "          [ 0.7627, -0.7284,  0.2564,  0.1170]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.6654,  0.5288,  0.3784, -1.2636],\n",
      "          [ 0.3378, -0.9012,  0.0910, -0.2873],\n",
      "          [-0.5747,  0.0409,  0.5175, -1.3309]],\n",
      "\n",
      "         [[-0.6654,  0.5288,  0.3784, -1.2636],\n",
      "          [ 0.3378, -0.9012,  0.0910, -0.2873],\n",
      "          [-0.5747,  0.0409,  0.5175, -1.3309]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-0.3001, -0.3232,  0.3579],\n",
      "          [ 0.2396, -0.3157,  0.1090],\n",
      "          [ 0.0098, -0.3272,  0.0047]],\n",
      "\n",
      "         [[ 0.3468,  0.1815,  0.2232],\n",
      "          [ 0.1179,  0.1780, -0.0231],\n",
      "          [-0.0218,  0.0412,  0.2829]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-3.0009e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 2.3959e-01, -3.1572e-01, -2.3820e+38],\n",
      "          [ 9.8435e-03, -3.2721e-01,  4.6963e-03]],\n",
      "\n",
      "         [[ 3.4685e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 1.1792e-01,  1.7800e-01, -2.3820e+38],\n",
      "          [-2.1839e-02,  4.1232e-02,  2.8290e-01]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.6354, 0.3646, 0.0000],\n",
      "          [0.3692, 0.2635, 0.3673]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4850, 0.5150, 0.0000],\n",
      "          [0.2923, 0.3113, 0.3964]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.6654,  0.5288,  0.3784, -1.2636],\n",
      "          [-0.2996,  0.0073,  0.2736, -0.9076],\n",
      "          [-0.3677, -0.0273,  0.3537, -1.0310]],\n",
      "\n",
      "         [[-0.6654,  0.5288,  0.3784, -1.2636],\n",
      "          [-0.1487, -0.2077,  0.2304, -0.7608],\n",
      "          [-0.3171, -0.1098,  0.3441, -0.9864]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.6654,  0.5288,  0.3784, -1.2636, -0.6654,  0.5288,  0.3784,\n",
      "          -1.2636],\n",
      "         [-0.2996,  0.0073,  0.2736, -0.9076, -0.1487, -0.2077,  0.2304,\n",
      "          -0.7608],\n",
      "         [-0.3677, -0.0273,  0.3537, -1.0310, -0.3171, -0.1098,  0.3441,\n",
      "          -0.9864]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0980, -0.2981,  0.2261,  0.2564, -0.3301,  0.0409,  0.0146, -0.1270],\n",
      "        [ 0.1700, -0.2685, -0.3082, -0.2914, -0.3036,  0.1539, -0.3166,  0.2542],\n",
      "        [ 0.2761, -0.2931, -0.0521,  0.0605,  0.0846, -0.2332, -0.2005,  0.3453],\n",
      "        [-0.2636,  0.2990,  0.1560,  0.1303,  0.0219,  0.0304, -0.2433, -0.0458],\n",
      "        [ 0.2879, -0.1021, -0.2176, -0.0471,  0.1683, -0.1578, -0.2345,  0.2747],\n",
      "        [-0.2572, -0.1831, -0.0564, -0.1827, -0.2013, -0.1825,  0.1696, -0.1463],\n",
      "        [ 0.2083,  0.1612,  0.0095,  0.2926,  0.2143,  0.3186, -0.3243,  0.0742],\n",
      "        [ 0.0136,  0.2395,  0.2041, -0.1826, -0.3425,  0.3096,  0.1156,  0.1286]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([8, 8])\n",
      "tensor([[ 0.0980, -0.2981,  0.2261,  0.2564, -0.3301,  0.0409,  0.0146, -0.1270],\n",
      "        [ 0.1700, -0.2685, -0.3082, -0.2914, -0.3036,  0.1539, -0.3166,  0.2542],\n",
      "        [ 0.2761, -0.2931, -0.0521,  0.0605,  0.0846, -0.2332, -0.2005,  0.3453],\n",
      "        [-0.2636,  0.2990,  0.1560,  0.1303,  0.0219,  0.0304, -0.2433, -0.0458],\n",
      "        [ 0.2879, -0.1021, -0.2176, -0.0471,  0.1683, -0.1578, -0.2345,  0.2747],\n",
      "        [-0.2572, -0.1831, -0.0564, -0.1827, -0.2013, -0.1825,  0.1696, -0.1463],\n",
      "        [ 0.2083,  0.1612,  0.0095,  0.2926,  0.2143,  0.3186, -0.3243,  0.0742],\n",
      "        [ 0.0136,  0.2395,  0.2041, -0.1826, -0.3425,  0.3096,  0.1156,  0.1286]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.1962, -0.7029, -0.6695, -0.1901,  0.3590, -0.3346,  0.0314,\n",
      "           0.0129],\n",
      "         [ 0.3348, -0.3560, -0.3348,  0.0707,  0.4267, -0.2032, -0.0037,\n",
      "           0.0847],\n",
      "         [ 0.3239, -0.4233, -0.3768,  0.1166,  0.5173, -0.2587,  0.0134,\n",
      "           0.0367]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.1962, -0.7029, -0.6695, -0.1901,  0.3590, -0.3346,  0.0314,\n",
      "           0.0129],\n",
      "         [ 0.3348, -0.3560, -0.3348,  0.0707,  0.4267, -0.2032, -0.0037,\n",
      "           0.0847],\n",
      "         [ 0.3239, -0.4233, -0.3768,  0.1166,  0.5173, -0.2587,  0.0134,\n",
      "           0.0367]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Level 1 from range(2)\n",
      "Model 0 from range(2)\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 0\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.1335, -0.1303, -0.1848, -0.2353, -0.2708, -0.0298,  0.0637,  0.1486,\n",
      "         -0.2726, -0.0643,  0.2985,  0.0271,  0.0918, -0.2552, -0.2136, -0.0052],\n",
      "        [-0.3026, -0.1643,  0.1719, -0.1304, -0.0511, -0.0446, -0.0808, -0.0620,\n",
      "          0.2364, -0.0486, -0.1940,  0.1364, -0.0929, -0.2627,  0.2486, -0.1143],\n",
      "        [-0.0366,  0.1024,  0.1159, -0.0483,  0.1303,  0.1464,  0.1210,  0.3225,\n",
      "          0.3037,  0.1669,  0.0698,  0.3314, -0.1166, -0.0120, -0.1210, -0.0676],\n",
      "        [-0.1102, -0.2287,  0.0243, -0.1338, -0.2344, -0.0874,  0.0078, -0.1051,\n",
      "         -0.0120,  0.1125, -0.2447, -0.0139, -0.1205,  0.1822,  0.0661, -0.1941],\n",
      "        [ 0.0983, -0.0353,  0.1414, -0.2113,  0.2630, -0.2893, -0.1788, -0.1863,\n",
      "          0.0779, -0.2327, -0.1869,  0.0561, -0.2564,  0.2620,  0.1596, -0.3362],\n",
      "        [ 0.3457,  0.2794, -0.2533, -0.2063, -0.1942,  0.3105,  0.1679, -0.1309,\n",
      "         -0.0567, -0.1583,  0.0414, -0.2704, -0.0439,  0.1494,  0.1729, -0.1447],\n",
      "        [-0.3464,  0.3313, -0.2952, -0.0014,  0.0009, -0.2870,  0.1910,  0.1955,\n",
      "          0.2215,  0.2274,  0.2571, -0.1387, -0.1163,  0.1354,  0.0825,  0.3315],\n",
      "        [-0.3416, -0.0617, -0.1760, -0.2088, -0.2323, -0.1513, -0.0373,  0.2187,\n",
      "          0.1528,  0.0005, -0.1678, -0.0206, -0.0010,  0.2095,  0.2588,  0.2393]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[-0.1335, -0.1303, -0.1848, -0.2353, -0.2708, -0.0298,  0.0637,  0.1486],\n",
      "        [-0.3026, -0.1643,  0.1719, -0.1304, -0.0511, -0.0446, -0.0808, -0.0620],\n",
      "        [-0.0366,  0.1024,  0.1159, -0.0483,  0.1303,  0.1464,  0.1210,  0.3225],\n",
      "        [-0.1102, -0.2287,  0.0243, -0.1338, -0.2344, -0.0874,  0.0078, -0.1051],\n",
      "        [ 0.0983, -0.0353,  0.1414, -0.2113,  0.2630, -0.2893, -0.1788, -0.1863],\n",
      "        [ 0.3457,  0.2794, -0.2533, -0.2063, -0.1942,  0.3105,  0.1679, -0.1309],\n",
      "        [-0.3464,  0.3313, -0.2952, -0.0014,  0.0009, -0.2870,  0.1910,  0.1955],\n",
      "        [-0.3416, -0.0617, -0.1760, -0.2088, -0.2323, -0.1513, -0.0373,  0.2187]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[-0.2726, -0.0643,  0.2985,  0.0271],\n",
      "        [ 0.2364, -0.0486, -0.1940,  0.1364],\n",
      "        [ 0.3037,  0.1669,  0.0698,  0.3314],\n",
      "        [-0.0120,  0.1125, -0.2447, -0.0139],\n",
      "        [ 0.0779, -0.2327, -0.1869,  0.0561],\n",
      "        [-0.0567, -0.1583,  0.0414, -0.2704],\n",
      "        [ 0.2215,  0.2274,  0.2571, -0.1387],\n",
      "        [ 0.1528,  0.0005, -0.1678, -0.0206]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.0918, -0.2552, -0.2136, -0.0052],\n",
      "        [-0.0929, -0.2627,  0.2486, -0.1143],\n",
      "        [-0.1166, -0.0120, -0.1210, -0.0676],\n",
      "        [-0.1205,  0.1822,  0.0661, -0.1941],\n",
      "        [-0.2564,  0.2620,  0.1596, -0.3362],\n",
      "        [-0.0439,  0.1494,  0.1729, -0.1447],\n",
      "        [-0.1163,  0.1354,  0.0825,  0.3315],\n",
      "        [-0.0010,  0.2095,  0.2588,  0.2393]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[-0.1335, -0.1303, -0.2708, -0.0298],\n",
      "        [-0.3026, -0.1643, -0.0511, -0.0446],\n",
      "        [-0.0366,  0.1024,  0.1303,  0.1464],\n",
      "        [-0.1102, -0.2287, -0.2344, -0.0874]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[-0.2726, -0.0643],\n",
      "        [ 0.2364, -0.0486],\n",
      "        [ 0.3037,  0.1669],\n",
      "        [-0.0120,  0.1125]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.0918, -0.2552],\n",
      "        [-0.0929, -0.2627],\n",
      "        [-0.1166, -0.0120],\n",
      "        [-0.1205,  0.1822]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[-0.1335, -0.1303, -0.2708, -0.0298, -0.2726, -0.0643,  0.0918, -0.2552],\n",
      "        [-0.3026, -0.1643, -0.0511, -0.0446,  0.2364, -0.0486, -0.0929, -0.2627],\n",
      "        [-0.0366,  0.1024,  0.1303,  0.1464,  0.3037,  0.1669, -0.1166, -0.0120],\n",
      "        [-0.1102, -0.2287, -0.2344, -0.0874, -0.0120,  0.1125, -0.1205,  0.1822]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.2648,  0.3140,  0.3629,  0.1461,  0.1973,  0.0839, -0.0074,\n",
      "           0.1942],\n",
      "         [ 0.0145, -0.0481,  0.0838,  0.1136,  0.6199,  0.5358, -0.4452,\n",
      "           0.6797],\n",
      "         [ 0.2150,  0.0413, -0.1917, -0.1918, -0.9691, -0.5415,  0.5325,\n",
      "          -0.3737]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.2648,  0.3140,  0.3629,  0.1461],\n",
      "         [ 0.0145, -0.0481,  0.0838,  0.1136],\n",
      "         [ 0.2150,  0.0413, -0.1917, -0.1918]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.1973,  0.0839],\n",
      "         [ 0.6199,  0.5358],\n",
      "         [-0.9691, -0.5415]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.0074,  0.1942],\n",
      "         [-0.4452,  0.6797],\n",
      "         [ 0.5325, -0.3737]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.2648,  0.3140],\n",
      "          [ 0.3629,  0.1461]],\n",
      "\n",
      "         [[ 0.0145, -0.0481],\n",
      "          [ 0.0838,  0.1136]],\n",
      "\n",
      "         [[ 0.2150,  0.0413],\n",
      "          [-0.1917, -0.1918]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.1973,  0.0839]],\n",
      "\n",
      "         [[ 0.6199,  0.5358]],\n",
      "\n",
      "         [[-0.9691, -0.5415]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.0074,  0.1942]],\n",
      "\n",
      "         [[-0.4452,  0.6797]],\n",
      "\n",
      "         [[ 0.5325, -0.3737]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.2648,  0.3140],\n",
      "          [ 0.3629,  0.1461]],\n",
      "\n",
      "         [[ 0.0483, -0.0138],\n",
      "          [-0.0503,  0.1319]],\n",
      "\n",
      "         [[-0.1271,  0.1783],\n",
      "          [ 0.2542, -0.0945]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.1973,  0.0839]],\n",
      "\n",
      "         [[-0.1160,  0.8111]],\n",
      "\n",
      "         [[ 0.8957, -0.6559]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.1973,  0.0839],\n",
      "          [ 0.1973,  0.0839]],\n",
      "\n",
      "         [[-0.1160,  0.8111],\n",
      "          [-0.1160,  0.8111]],\n",
      "\n",
      "         [[ 0.8957, -0.6559],\n",
      "          [ 0.8957, -0.6559]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.0074,  0.1942],\n",
      "          [-0.0074,  0.1942]],\n",
      "\n",
      "         [[-0.4452,  0.6797],\n",
      "          [-0.4452,  0.6797]],\n",
      "\n",
      "         [[ 0.5325, -0.3737],\n",
      "          [ 0.5325, -0.3737]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.2648,  0.3140],\n",
      "          [ 0.0483, -0.0138],\n",
      "          [-0.1271,  0.1783]],\n",
      "\n",
      "         [[ 0.3629,  0.1461],\n",
      "          [-0.0503,  0.1319],\n",
      "          [ 0.2542, -0.0945]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.1973,  0.0839],\n",
      "          [-0.1160,  0.8111],\n",
      "          [ 0.8957, -0.6559]],\n",
      "\n",
      "         [[ 0.1973,  0.0839],\n",
      "          [-0.1160,  0.8111],\n",
      "          [ 0.8957, -0.6559]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.0074,  0.1942],\n",
      "          [-0.4452,  0.6797],\n",
      "          [ 0.5325, -0.3737]],\n",
      "\n",
      "         [[-0.0074,  0.1942],\n",
      "          [-0.4452,  0.6797],\n",
      "          [ 0.5325, -0.3737]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 0.0556,  0.1584,  0.0221],\n",
      "          [ 0.0059, -0.0119,  0.0370],\n",
      "          [-0.0072,  0.1127, -0.1632]],\n",
      "\n",
      "         [[ 0.0593,  0.0540,  0.1621],\n",
      "          [ 0.0008,  0.0798, -0.0930],\n",
      "          [ 0.0299, -0.0750,  0.2048]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 5.5566e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 5.9163e-03, -1.1856e-02, -2.3820e+38],\n",
      "          [-7.1516e-03,  1.1270e-01, -1.6317e-01]],\n",
      "\n",
      "         [[ 5.9296e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 8.0489e-04,  7.9752e-02, -2.3820e+38],\n",
      "          [ 2.9859e-02, -7.5036e-02,  2.0480e-01]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5044, 0.4956, 0.0000],\n",
      "          [0.3352, 0.3779, 0.2868]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4803, 0.5197, 0.0000],\n",
      "          [0.3235, 0.2912, 0.3853]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.0074,  0.1942],\n",
      "          [-0.2244,  0.4348],\n",
      "          [-0.0180,  0.2148]],\n",
      "\n",
      "         [[-0.0074,  0.1942],\n",
      "          [-0.2349,  0.4466],\n",
      "          [ 0.0731,  0.1168]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0074,  0.1942, -0.0074,  0.1942],\n",
      "         [-0.2244,  0.4348, -0.2349,  0.4466],\n",
      "         [-0.0180,  0.2148,  0.0731,  0.1168]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0980, -0.2981,  0.2261,  0.2564, -0.3301,  0.0409,  0.0146, -0.1270],\n",
      "        [ 0.1700, -0.2685, -0.3082, -0.2914, -0.3036,  0.1539, -0.3166,  0.2542],\n",
      "        [ 0.2761, -0.2931, -0.0521,  0.0605,  0.0846, -0.2332, -0.2005,  0.3453],\n",
      "        [-0.2636,  0.2990,  0.1560,  0.1303,  0.0219,  0.0304, -0.2433, -0.0458],\n",
      "        [ 0.2879, -0.1021, -0.2176, -0.0471,  0.1683, -0.1578, -0.2345,  0.2747],\n",
      "        [-0.2572, -0.1831, -0.0564, -0.1827, -0.2013, -0.1825,  0.1696, -0.1463],\n",
      "        [ 0.2083,  0.1612,  0.0095,  0.2926,  0.2143,  0.3186, -0.3243,  0.0742],\n",
      "        [ 0.0136,  0.2395,  0.2041, -0.1826, -0.3425,  0.3096,  0.1156,  0.1286]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[ 0.0980, -0.2981,  0.2261,  0.2564],\n",
      "        [ 0.1700, -0.2685, -0.3082, -0.2914],\n",
      "        [ 0.2879, -0.1021, -0.2176, -0.0471],\n",
      "        [-0.2572, -0.1831, -0.0564, -0.1827]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0198, -0.0848, -0.0709, -0.0936],\n",
      "         [-0.1306, -0.1077, -0.1588, -0.2547],\n",
      "         [ 0.0258, -0.0812, -0.0928, -0.0920]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0198, -0.0848, -0.0709, -0.0936],\n",
      "         [-0.1306, -0.1077, -0.1588, -0.2547],\n",
      "         [ 0.0258, -0.0812, -0.0928, -0.0920]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Model 1 from range(2)\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 4\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 2\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.1335, -0.1303, -0.1848, -0.2353, -0.2708, -0.0298,  0.0637,  0.1486,\n",
      "         -0.2726, -0.0643,  0.2985,  0.0271,  0.0918, -0.2552, -0.2136, -0.0052],\n",
      "        [-0.3026, -0.1643,  0.1719, -0.1304, -0.0511, -0.0446, -0.0808, -0.0620,\n",
      "          0.2364, -0.0486, -0.1940,  0.1364, -0.0929, -0.2627,  0.2486, -0.1143],\n",
      "        [-0.0366,  0.1024,  0.1159, -0.0483,  0.1303,  0.1464,  0.1210,  0.3225,\n",
      "          0.3037,  0.1669,  0.0698,  0.3314, -0.1166, -0.0120, -0.1210, -0.0676],\n",
      "        [-0.1102, -0.2287,  0.0243, -0.1338, -0.2344, -0.0874,  0.0078, -0.1051,\n",
      "         -0.0120,  0.1125, -0.2447, -0.0139, -0.1205,  0.1822,  0.0661, -0.1941],\n",
      "        [ 0.0983, -0.0353,  0.1414, -0.2113,  0.2630, -0.2893, -0.1788, -0.1863,\n",
      "          0.0779, -0.2327, -0.1869,  0.0561, -0.2564,  0.2620,  0.1596, -0.3362],\n",
      "        [ 0.3457,  0.2794, -0.2533, -0.2063, -0.1942,  0.3105,  0.1679, -0.1309,\n",
      "         -0.0567, -0.1583,  0.0414, -0.2704, -0.0439,  0.1494,  0.1729, -0.1447],\n",
      "        [-0.3464,  0.3313, -0.2952, -0.0014,  0.0009, -0.2870,  0.1910,  0.1955,\n",
      "          0.2215,  0.2274,  0.2571, -0.1387, -0.1163,  0.1354,  0.0825,  0.3315],\n",
      "        [-0.3416, -0.0617, -0.1760, -0.2088, -0.2323, -0.1513, -0.0373,  0.2187,\n",
      "          0.1528,  0.0005, -0.1678, -0.0206, -0.0010,  0.2095,  0.2588,  0.2393]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[-0.1335, -0.1303, -0.1848, -0.2353, -0.2708, -0.0298,  0.0637,  0.1486],\n",
      "        [-0.3026, -0.1643,  0.1719, -0.1304, -0.0511, -0.0446, -0.0808, -0.0620],\n",
      "        [-0.0366,  0.1024,  0.1159, -0.0483,  0.1303,  0.1464,  0.1210,  0.3225],\n",
      "        [-0.1102, -0.2287,  0.0243, -0.1338, -0.2344, -0.0874,  0.0078, -0.1051],\n",
      "        [ 0.0983, -0.0353,  0.1414, -0.2113,  0.2630, -0.2893, -0.1788, -0.1863],\n",
      "        [ 0.3457,  0.2794, -0.2533, -0.2063, -0.1942,  0.3105,  0.1679, -0.1309],\n",
      "        [-0.3464,  0.3313, -0.2952, -0.0014,  0.0009, -0.2870,  0.1910,  0.1955],\n",
      "        [-0.3416, -0.0617, -0.1760, -0.2088, -0.2323, -0.1513, -0.0373,  0.2187]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[-0.2726, -0.0643,  0.2985,  0.0271],\n",
      "        [ 0.2364, -0.0486, -0.1940,  0.1364],\n",
      "        [ 0.3037,  0.1669,  0.0698,  0.3314],\n",
      "        [-0.0120,  0.1125, -0.2447, -0.0139],\n",
      "        [ 0.0779, -0.2327, -0.1869,  0.0561],\n",
      "        [-0.0567, -0.1583,  0.0414, -0.2704],\n",
      "        [ 0.2215,  0.2274,  0.2571, -0.1387],\n",
      "        [ 0.1528,  0.0005, -0.1678, -0.0206]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.0918, -0.2552, -0.2136, -0.0052],\n",
      "        [-0.0929, -0.2627,  0.2486, -0.1143],\n",
      "        [-0.1166, -0.0120, -0.1210, -0.0676],\n",
      "        [-0.1205,  0.1822,  0.0661, -0.1941],\n",
      "        [-0.2564,  0.2620,  0.1596, -0.3362],\n",
      "        [-0.0439,  0.1494,  0.1729, -0.1447],\n",
      "        [-0.1163,  0.1354,  0.0825,  0.3315],\n",
      "        [-0.0010,  0.2095,  0.2588,  0.2393]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[ 0.1414, -0.2113, -0.1788, -0.1863],\n",
      "        [-0.2533, -0.2063,  0.1679, -0.1309],\n",
      "        [-0.2952, -0.0014,  0.1910,  0.1955],\n",
      "        [-0.1760, -0.2088, -0.0373,  0.2187]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[-0.1869,  0.0561],\n",
      "        [ 0.0414, -0.2704],\n",
      "        [ 0.2571, -0.1387],\n",
      "        [-0.1678, -0.0206]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.1596, -0.3362],\n",
      "        [ 0.1729, -0.1447],\n",
      "        [ 0.0825,  0.3315],\n",
      "        [ 0.2588,  0.2393]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[ 0.1414, -0.2113, -0.1788, -0.1863, -0.1869,  0.0561,  0.1596, -0.3362],\n",
      "        [-0.2533, -0.2063,  0.1679, -0.1309,  0.0414, -0.2704,  0.1729, -0.1447],\n",
      "        [-0.2952, -0.0014,  0.1910,  0.1955,  0.2571, -0.1387,  0.0825,  0.3315],\n",
      "        [-0.1760, -0.2088, -0.0373,  0.2187, -0.1678, -0.0206,  0.2588,  0.2393]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.1632,  0.2453,  0.0959,  0.4823,  0.3228,  0.1089, -0.0781,\n",
      "           0.7151],\n",
      "         [-0.6077, -0.0330,  0.5790, -0.1219,  0.6467, -0.5669,  0.0550,\n",
      "           0.0524],\n",
      "         [-0.2732, -0.0853,  0.2339, -0.0943,  0.2876, -0.2606,  0.0915,\n",
      "          -0.0447]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.1632,  0.2453,  0.0959,  0.4823],\n",
      "         [-0.6077, -0.0330,  0.5790, -0.1219],\n",
      "         [-0.2732, -0.0853,  0.2339, -0.0943]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.3228,  0.1089],\n",
      "         [ 0.6467, -0.5669],\n",
      "         [ 0.2876, -0.2606]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.0781,  0.7151],\n",
      "         [ 0.0550,  0.0524],\n",
      "         [ 0.0915, -0.0447]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.1632,  0.2453],\n",
      "          [ 0.0959,  0.4823]],\n",
      "\n",
      "         [[-0.6077, -0.0330],\n",
      "          [ 0.5790, -0.1219]],\n",
      "\n",
      "         [[-0.2732, -0.0853],\n",
      "          [ 0.2339, -0.0943]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.3228,  0.1089]],\n",
      "\n",
      "         [[ 0.6467, -0.5669]],\n",
      "\n",
      "         [[ 0.2876, -0.2606]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.0781,  0.7151]],\n",
      "\n",
      "         [[ 0.0550,  0.0524]],\n",
      "\n",
      "         [[ 0.0915, -0.0447]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.1632,  0.2453],\n",
      "          [ 0.0959,  0.4823]],\n",
      "\n",
      "         [[-0.3006, -0.5292],\n",
      "          [ 0.4154,  0.4213]],\n",
      "\n",
      "         [[ 0.1912, -0.2130],\n",
      "          [-0.0116,  0.2520]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[0.3228, 0.1089]],\n",
      "\n",
      "         [[0.8265, 0.2379]],\n",
      "\n",
      "         [[0.1173, 0.3699]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[0.3228, 0.1089],\n",
      "          [0.3228, 0.1089]],\n",
      "\n",
      "         [[0.8265, 0.2379],\n",
      "          [0.8265, 0.2379]],\n",
      "\n",
      "         [[0.1173, 0.3699],\n",
      "          [0.1173, 0.3699]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.0781,  0.7151],\n",
      "          [-0.0781,  0.7151]],\n",
      "\n",
      "         [[ 0.0550,  0.0524],\n",
      "          [ 0.0550,  0.0524]],\n",
      "\n",
      "         [[ 0.0915, -0.0447],\n",
      "          [ 0.0915, -0.0447]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.1632,  0.2453],\n",
      "          [-0.3006, -0.5292],\n",
      "          [ 0.1912, -0.2130]],\n",
      "\n",
      "         [[ 0.0959,  0.4823],\n",
      "          [ 0.4154,  0.4213],\n",
      "          [-0.0116,  0.2520]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[0.3228, 0.1089],\n",
      "          [0.8265, 0.2379],\n",
      "          [0.1173, 0.3699]],\n",
      "\n",
      "         [[0.3228, 0.1089],\n",
      "          [0.8265, 0.2379],\n",
      "          [0.1173, 0.3699]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.0781,  0.7151],\n",
      "          [ 0.0550,  0.0524],\n",
      "          [ 0.0915, -0.0447]],\n",
      "\n",
      "         [[-0.0781,  0.7151],\n",
      "          [ 0.0550,  0.0524],\n",
      "          [ 0.0915, -0.0447]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-0.0184, -0.0541,  0.0506],\n",
      "          [-0.1093, -0.2647, -0.1633],\n",
      "          [ 0.0272,  0.0759, -0.0398]],\n",
      "\n",
      "         [[ 0.0590,  0.1372,  0.1341],\n",
      "          [ 0.1272,  0.3136,  0.1446],\n",
      "          [ 0.0167,  0.0356,  0.0649]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-1.8367e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [-1.0935e-01, -2.6467e-01, -2.3820e+38],\n",
      "          [ 2.7246e-02,  7.5937e-02, -3.9850e-02]],\n",
      "\n",
      "         [[ 5.9020e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 1.2724e-01,  3.1361e-01, -2.3820e+38],\n",
      "          [ 1.6743e-02,  3.5575e-02,  6.4936e-02]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5388, 0.4612, 0.0000],\n",
      "          [0.3350, 0.3517, 0.3133]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4535, 0.5465, 0.0000],\n",
      "          [0.3259, 0.3321, 0.3420]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.0781,  0.7151],\n",
      "          [-0.0167,  0.4094],\n",
      "          [ 0.0218,  0.2440]],\n",
      "\n",
      "         [[-0.0781,  0.7151],\n",
      "          [-0.0054,  0.3530],\n",
      "          [ 0.0241,  0.2352]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0781,  0.7151, -0.0781,  0.7151],\n",
      "         [-0.0167,  0.4094, -0.0054,  0.3530],\n",
      "         [ 0.0218,  0.2440,  0.0241,  0.2352]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0980, -0.2981,  0.2261,  0.2564, -0.3301,  0.0409,  0.0146, -0.1270],\n",
      "        [ 0.1700, -0.2685, -0.3082, -0.2914, -0.3036,  0.1539, -0.3166,  0.2542],\n",
      "        [ 0.2761, -0.2931, -0.0521,  0.0605,  0.0846, -0.2332, -0.2005,  0.3453],\n",
      "        [-0.2636,  0.2990,  0.1560,  0.1303,  0.0219,  0.0304, -0.2433, -0.0458],\n",
      "        [ 0.2879, -0.1021, -0.2176, -0.0471,  0.1683, -0.1578, -0.2345,  0.2747],\n",
      "        [-0.2572, -0.1831, -0.0564, -0.1827, -0.2013, -0.1825,  0.1696, -0.1463],\n",
      "        [ 0.2083,  0.1612,  0.0095,  0.2926,  0.2143,  0.3186, -0.3243,  0.0742],\n",
      "        [ 0.0136,  0.2395,  0.2041, -0.1826, -0.3425,  0.3096,  0.1156,  0.1286]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[ 0.0846, -0.2332, -0.2005,  0.3453],\n",
      "        [ 0.0219,  0.0304, -0.2433, -0.0458],\n",
      "        [ 0.2143,  0.3186, -0.3243,  0.0742],\n",
      "        [-0.3425,  0.3096,  0.1156,  0.1286]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.2526,  0.2364, -0.0503,  0.0264],\n",
      "         [-0.1145,  0.1239, -0.0537,  0.0205],\n",
      "         [-0.0682,  0.0828, -0.0444,  0.0284]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.2526,  0.2364, -0.0503,  0.0264],\n",
      "         [-0.1145,  0.1239, -0.0537,  0.0205],\n",
      "         [-0.0682,  0.0828, -0.0444,  0.0284]]], grad_fn=<UnsafeViewBackward0>)\n",
      "final output: ((tensor([[[ 0.2180, -0.7810, -0.7439, -0.2112,  0.3988, -0.3718,  0.0349,\n",
      "           0.0143],\n",
      "         [ 0.3720, -0.3956, -0.3720,  0.0785,  0.4741, -0.0000, -0.0041,\n",
      "           0.0941],\n",
      "         [ 0.0000, -0.4703, -0.4187,  0.1295,  0.5748, -0.2874,  0.0148,\n",
      "           0.0408]]], grad_fn=<MulBackward0>),), (tensor([[[-0.0000, -0.0942, -0.0000, -0.1040],\n",
      "         [-0.1451, -0.0000, -0.1764, -0.2830],\n",
      "         [ 0.0286, -0.0902, -0.1031, -0.1022]]], grad_fn=<MulBackward0>), tensor([[[-0.2807,  0.2627, -0.0559,  0.0294],\n",
      "         [-0.1272,  0.1376, -0.0597,  0.0227],\n",
      "         [-0.0000,  0.0920, -0.0493,  0.0315]]], grad_fn=<MulBackward0>)))\n",
      "------------- END MultiQueryAttention.forwardTuple() ------------\n",
      "out: ((tensor([[[ 0.2180, -0.7810, -0.7439, -0.2112,  0.3988, -0.3718,  0.0349,\n",
      "           0.0143],\n",
      "         [ 0.3720, -0.3956, -0.3720,  0.0785,  0.4741, -0.0000, -0.0041,\n",
      "           0.0941],\n",
      "         [ 0.0000, -0.4703, -0.4187,  0.1295,  0.5748, -0.2874,  0.0148,\n",
      "           0.0408]]], grad_fn=<MulBackward0>),), (tensor([[[-0.0000, -0.0942, -0.0000, -0.1040],\n",
      "         [-0.1451, -0.0000, -0.1764, -0.2830],\n",
      "         [ 0.0286, -0.0902, -0.1031, -0.1022]]], grad_fn=<MulBackward0>), tensor([[[-0.2807,  0.2627, -0.0559,  0.0294],\n",
      "         [-0.1272,  0.1376, -0.0597,  0.0227],\n",
      "         [-0.0000,  0.0920, -0.0493,  0.0315]]], grad_fn=<MulBackward0>)))\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n",
      "head_dim_list:  [32, 16, 8]\n"
     ]
    }
   ],
   "source": [
    "# Testing our Attention's forwardTuple()\n",
    "config.verbose['MQA'] = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4, hold5 = config.hidden_size, config.num_attention_heads, config.head_dim, config.levels, config.max_position_embeddings\n",
    "config.hidden_size = 8\n",
    "config.num_attention_heads = 2\n",
    "config.head_dim = 4\n",
    "config.levels = 2\n",
    "config.max_position_embeddings = 3\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "att = MultiQueryAttention(config)\n",
    "# we need to make sure to send in a tuple of the expected size. above we set hidden_size=4 and levels=2\n",
    "x = ((torch.randn((1,3,8)),),(torch.randn((1,3,4)),torch.randn((1,3,4))))\n",
    "print(f\"x: {x}\")\n",
    "out = att(x)\n",
    "print(f\"out: {out}\")\n",
    "\n",
    "config.verbose['MQA'] = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.num_attention_heads = hold2\n",
    "config.head_dim = hold3\n",
    "config.levels = hold4\n",
    "config.max_position_embeddings = hold5\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "# clear up memory\n",
    "del hold1, hold2, hold3, hold4, hold5, x, att, out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67bb47d",
   "metadata": {},
   "source": [
    "# Layer\n",
    "\n",
    "nothing too interesting here besides the absurd amount of memory we're probably taking up with these tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d191c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    A decoder layer that integrates the MultiQueryAttention and MLP. It includes\n",
    "    normalization steps both before and after the attention mechanism to stabilize and accelerate training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.verbose = config.verbose['Layer']\n",
    "\n",
    "        # Initializes the GemmaAttention mechanism with parameters from the config, enabling self-attention within the decoder layer.\n",
    "        self.self_attn = MultiQueryAttention(config)\n",
    "        \n",
    "        # Initializes the GemmaMLP module, providing a non-linear transformation after the attention mechanism.\n",
    "        self.mlp = MLP(\n",
    "            # the hidden dimension of the model\n",
    "            hidden_size = config.hidden_size,\n",
    "            # the number of nodes in the center of the two feedforward layers\n",
    "            intermediate_size = config.intermediate_size,\n",
    "            # the % of neurons to set to 0 during training\n",
    "            dropout = config.dropout,\n",
    "            verbose=config.verbose['MLP']\n",
    "        )\n",
    "        \n",
    "        # Applies RMSNorm normalization to the input of the decoder layer for stable training dynamics.\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size,\n",
    "                                       eps = config.rms_norm_eps,\n",
    "                                      verbose=config.verbose['RMSNorm'])\n",
    "        \n",
    "        # Applies RMSNorm after the attention mechanism and before the MLP to ensure the output is well-conditioned for further processing.\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size,\n",
    "                                                eps = config.rms_norm_eps,\n",
    "                                              verbose=config.verbose['RMSNorm'])\n",
    "\n",
    "    def forwardTensor(self,\n",
    "                # The input tensor to the decoder layer. shape (batch_size, input_len, hidden_size)\n",
    "                x: torch.Tensor,\n",
    "                model: int = 0,\n",
    "                drop_bool: bool = False\n",
    "                ) -> torch.Tensor:\n",
    "        if self.verbose: \n",
    "            print(\"----------------- Layer.forwardTensor() --------------------\")\n",
    "            print(f\"x in layer before MQA:\\n{x}\")\n",
    "        \n",
    "        # Self Attention Block\n",
    "        # Stores the original input for use as a residual connection, aiding in mitigating the vanishing gradient problem\n",
    "        residual_connection = x\n",
    "        # Normalizes the input before processing by the attention mechanism.\n",
    "        x = self.input_layernorm(x, model)\n",
    "        # Processes the normalized input through the GemmaAttention mechanism\n",
    "        x = self.self_attn(x, model, drop_bool)\n",
    "        # The aforementioned residual connection\n",
    "        x = residual_connection + x\n",
    "        if self.verbose: print(f\"x in layer after MQA & resid connection and before MLP:\\n{x}\")\n",
    "\n",
    "        # MLP Block\n",
    "        # Again, stores the output of the attention block for use as a residual connection before processing by the MLP.\n",
    "        residual_connection = x\n",
    "        # Normalizes the output of the attention block before passing it to the MLP, ensuring a stable input distribution.\n",
    "        x = self.post_attention_layernorm(x, model)\n",
    "        # Transforms the normalized attention output through the MLP, introducing additional non-linearity and capacity to the model.\n",
    "        x = self.mlp(x, model, drop_bool)\n",
    "        # Another residual connection\n",
    "        x = residual_connection + x\n",
    "        if self.verbose: \n",
    "            print(f\"layer's final residual state:\\n{x}\")\n",
    "            print(\"----------------- END Layer.forwardTensor() --------------------\")\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forwardTuple(self,\n",
    "                     x: Tuple[Tuple[torch.Tensor]],\n",
    "                    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the forward pass of a decoder layer during training.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tuple[Tuple[Tensor]]): \n",
    "                The input tuple of tuples of tensors \n",
    "                first tuple is of length config.levels and second layer of tuples have lengths of config.model_count\n",
    "                tensors are shape (batch size, sequence length, hidden dimension) where hidden dimension changes by which model was used\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tuple[Tensor]]: \n",
    "                The output tuple of tuples of tensors after applying the decoder layer\n",
    "        \"\"\"\n",
    "        if self.verbose: \n",
    "            print(\"------------- Layer.forwardTuple() ------------\")\n",
    "            print(f\"x:\\n{x}\")\n",
    "            \n",
    "        # forwardTuple() should only be used during training, so we assert input_len == max_position_embeddings\n",
    "        input_len = x[0][0].shape[1]\n",
    "        if self.verbose: print(f\"input_len: {input_len}\")\n",
    "        assert input_len == config.max_position_embeddings\n",
    "\n",
    "        # we could define these from the config but this way the method is more flexible to testing\n",
    "        num_levels = len(x)\n",
    "        models_per_level = [len(x[i]) for i in range(num_levels)]\n",
    "        if self.verbose: \n",
    "            print(f\"num_levels: {num_levels}\")\n",
    "            print(f\"models_per_level: {models_per_level}\")\n",
    "\n",
    "        # the loop that iterates over levels, aka the different potential sizes of models\n",
    "        out = ()\n",
    "        for i in range(num_levels):\n",
    "            if self.verbose: print(f\"Level {i} from range({num_levels})\")\n",
    "\n",
    "            # now for the loop that iterates over models in this level\n",
    "            out_lvl = ()\n",
    "            for j in range(models_per_level[i]):\n",
    "                if self.verbose: print(f\"Model {j} from range({models_per_level[i]})\")\n",
    "\n",
    "                output = self.forwardTensor(x[i][j], model = j, drop_bool = True)\n",
    "                if self.verbose: print(f\"forwardTensor() output: {output.shape}\\n{output}\")\n",
    "                \n",
    "                out_lvl += (output,)\n",
    "            \n",
    "            out += (out_lvl,)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"final output: {out}\")\n",
    "            print(\"------------- END Layer.forwardTuple() ------------\")\n",
    "\n",
    "        return out\n",
    "        \n",
    "    def forward(self, x, model=0):\n",
    "        train = True if type(x) == tuple else False\n",
    "        if self.verbose: print(f\"---------- Layer Input: {'Tuple' if train else 'torch.Tensor'} ------------\")\n",
    "        return self.forwardTuple(x) if train else self.forwardTensor(x, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b27eb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [8, 4, 2]\n",
      "head_dim_list:  [4, 2, 1]\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[0.8526, 0.5234, 0.8879, 0.4372, 0.1502, 0.1810, 0.6843, 0.0201],\n",
      "         [0.8530, 0.6593, 0.4759, 0.7145, 0.5469, 0.7127, 0.3927, 0.2568],\n",
      "         [0.9759, 0.4863, 0.6241, 0.0655, 0.3090, 0.3910, 0.6061, 0.8683]]])\n",
      "---------- Layer Input: torch.Tensor ------------\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "x in layer before MQA:\n",
      "tensor([[[0.8526, 0.5234, 0.8879, 0.4372, 0.1502, 0.1810, 0.6843, 0.0201],\n",
      "         [0.8530, 0.6593, 0.4759, 0.7145, 0.5469, 0.7127, 0.3927, 0.2568],\n",
      "         [0.9759, 0.4863, 0.6241, 0.0655, 0.3090, 0.3910, 0.6061, 0.8683]]])\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[ 0.7829,  0.3952,  1.2740,  0.4850,  0.0605, -0.1140,  1.0439,\n",
      "           0.3679],\n",
      "         [ 0.7438,  0.5189,  0.9024,  0.7338,  0.4395,  0.4228,  0.7173,\n",
      "           0.5553],\n",
      "         [ 0.8266,  0.4075,  1.0623,  0.1134,  0.2441,  0.0972,  0.9496,\n",
      "           1.1519]]], grad_fn=<AddBackward0>)\n",
      "layer's final residual state:\n",
      "tensor([[[ 0.5851,  0.1765,  1.0976,  0.4398,  0.0961, -0.4157,  1.0721,\n",
      "           0.4478],\n",
      "         [ 0.5501,  0.4093,  0.7596,  0.7225,  0.5465,  0.1181,  0.6916,\n",
      "           0.6896],\n",
      "         [ 0.7622,  0.1773,  0.9011, -0.0018,  0.1683, -0.1666,  0.9181,\n",
      "           1.1609]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "y: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.5851,  0.1765,  1.0976,  0.4398,  0.0961, -0.4157,  1.0721,\n",
      "           0.4478],\n",
      "         [ 0.5501,  0.4093,  0.7596,  0.7225,  0.5465,  0.1181,  0.6916,\n",
      "           0.6896],\n",
      "         [ 0.7622,  0.1773,  0.9011, -0.0018,  0.1683, -0.1666,  0.9181,\n",
      "           1.1609]]], grad_fn=<AddBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.8645, 0.8820, 0.3679, 0.9834],\n",
      "         [0.0219, 0.3758, 0.6517, 0.6405],\n",
      "         [0.1958, 0.3704, 0.8007, 0.9679]]])\n",
      "---------- Layer Input: torch.Tensor ------------\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "x in layer before MQA:\n",
      "tensor([[[0.8645, 0.8820, 0.3679, 0.9834],\n",
      "         [0.0219, 0.3758, 0.6517, 0.6405],\n",
      "         [0.1958, 0.3704, 0.8007, 0.9679]]])\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[0.8908, 0.7346, 0.4644, 0.9727],\n",
      "         [0.0305, 0.3244, 0.6834, 0.6338],\n",
      "         [0.1978, 0.3537, 0.8138, 0.9667]]], grad_fn=<AddBackward0>)\n",
      "layer's final residual state:\n",
      "tensor([[[ 0.7536,  0.6155,  0.3861,  0.8641],\n",
      "         [-0.0543,  0.1577,  0.6329,  0.5280],\n",
      "         [ 0.0897,  0.1985,  0.7613,  0.8692]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "y: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.7536,  0.6155,  0.3861,  0.8641],\n",
      "         [-0.0543,  0.1577,  0.6329,  0.5280],\n",
      "         [ 0.0897,  0.1985,  0.7613,  0.8692]]], grad_fn=<AddBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.0870, 0.2759, 0.4709, 0.9381],\n",
      "         [0.6479, 0.6534, 0.9702, 0.2747],\n",
      "         [0.1705, 0.0194, 0.1667, 0.7784]]])\n",
      "---------- Layer Input: torch.Tensor ------------\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "x in layer before MQA:\n",
      "tensor([[[0.0870, 0.2759, 0.4709, 0.9381],\n",
      "         [0.6479, 0.6534, 0.9702, 0.2747],\n",
      "         [0.1705, 0.0194, 0.1667, 0.7784]]])\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[ 0.1008,  0.2674,  0.4455,  0.9357],\n",
      "         [ 0.6996,  0.6122,  1.1242,  0.1885],\n",
      "         [ 0.2158, -0.0160,  0.2538,  0.7134]]], grad_fn=<AddBackward0>)\n",
      "layer's final residual state:\n",
      "tensor([[[-0.0921,  0.0019,  0.3633,  0.8402],\n",
      "         [ 0.4758,  0.3814,  1.0667,  0.1189],\n",
      "         [ 0.0244, -0.2347,  0.1996,  0.6315]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "y: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0921,  0.0019,  0.3633,  0.8402],\n",
      "         [ 0.4758,  0.3814,  1.0667,  0.1189],\n",
      "         [ 0.0244, -0.2347,  0.1996,  0.6315]]], grad_fn=<AddBackward0>)\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [8, 4, 2]\n",
      "head_dim_list:  [4, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "# Testing our Layer's forwardTensor()\n",
    "config.verbose['Layer'] = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4 = config.hidden_size, config.num_attention_heads, config.head_dim, config.max_position_embeddings\n",
    "config.hidden_size = 8\n",
    "config.num_attention_heads = 2\n",
    "config.head_dim = 4\n",
    "config.max_position_embeddings = 3\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "layer = Layer(config)\n",
    "y = layer(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size//config.split)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "y = layer(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size//config.split)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "y = layer(x, model=1)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "config.verbose['Layer'] = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.num_attention_heads = hold2\n",
    "config.head_dim = hold3\n",
    "config.max_position_embeddings = hold4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "# clear up memory\n",
    "del hold1, hold2, hold3, hold4, x, layer, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0531758f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [8, 4]\n",
      "head_dim_list:  [4, 2]\n",
      "x: ((tensor([[[-0.4035,  0.6340, -0.4843,  1.2034, -1.0113, -0.9114,  0.0505,\n",
      "           1.4425],\n",
      "         [ 0.0126,  1.4195, -0.0916,  0.0954,  0.1611,  0.0488,  0.2386,\n",
      "           0.5153],\n",
      "         [ 1.0853, -0.6008, -0.1457,  0.3010,  0.9513,  0.4493, -0.9705,\n",
      "          -1.8265]]]),), (tensor([[[-1.4372, -0.1648, -1.5215,  0.4213],\n",
      "         [ 1.0980, -1.3593, -1.2852,  1.4749],\n",
      "         [ 0.0072,  0.5485,  1.4280, -0.1434]]]), tensor([[[ 0.4308, -0.8585, -1.3597,  1.5565],\n",
      "         [-2.2003, -2.0060,  0.0684, -1.6505],\n",
      "         [-1.8424, -1.8042, -0.7318, -1.7832]]])))\n",
      "---------- Layer Input: Tuple ------------\n",
      "------------- Layer.forwardTuple() ------------\n",
      "x:\n",
      "((tensor([[[-0.4035,  0.6340, -0.4843,  1.2034, -1.0113, -0.9114,  0.0505,\n",
      "           1.4425],\n",
      "         [ 0.0126,  1.4195, -0.0916,  0.0954,  0.1611,  0.0488,  0.2386,\n",
      "           0.5153],\n",
      "         [ 1.0853, -0.6008, -0.1457,  0.3010,  0.9513,  0.4493, -0.9705,\n",
      "          -1.8265]]]),), (tensor([[[-1.4372, -0.1648, -1.5215,  0.4213],\n",
      "         [ 1.0980, -1.3593, -1.2852,  1.4749],\n",
      "         [ 0.0072,  0.5485,  1.4280, -0.1434]]]), tensor([[[ 0.4308, -0.8585, -1.3597,  1.5565],\n",
      "         [-2.2003, -2.0060,  0.0684, -1.6505],\n",
      "         [-1.8424, -1.8042, -0.7318, -1.7832]]])))\n",
      "input_len: 3\n",
      "num_levels: 2\n",
      "models_per_level: [1, 2]\n",
      "Level 0 from range(2)\n",
      "Model 0 from range(1)\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "x in layer before MQA:\n",
      "tensor([[[-0.4035,  0.6340, -0.4843,  1.2034, -1.0113, -0.9114,  0.0505,\n",
      "           1.4425],\n",
      "         [ 0.0126,  1.4195, -0.0916,  0.0954,  0.1611,  0.0488,  0.2386,\n",
      "           0.5153],\n",
      "         [ 1.0853, -0.6008, -0.1457,  0.3010,  0.9513,  0.4493, -0.9705,\n",
      "          -1.8265]]])\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[-0.0895,  1.2291, -0.0186,  0.8623, -0.7188, -1.2110,  0.4174,\n",
      "           2.1501],\n",
      "         [ 0.6635,  1.9928,  0.1979, -0.2344,  0.6836, -0.2191,  0.6107,\n",
      "           0.9871],\n",
      "         [ 1.4336, -0.4659, -0.0230,  0.2295,  1.2903,  0.2905, -0.7792,\n",
      "          -1.6843]]], grad_fn=<AddBackward0>)\n",
      "layer's final residual state:\n",
      "tensor([[[-0.0799,  1.3123,  0.0284,  0.6482, -0.6832, -1.0945,  0.5701,\n",
      "           2.0812],\n",
      "         [ 0.6014,  2.1037,  0.3467, -0.3399,  0.4538, -0.1907,  0.5511,\n",
      "           1.1629],\n",
      "         [ 1.2745, -0.4294, -0.1449,  0.2828,  1.0326,  0.3992, -0.7202,\n",
      "          -1.5498]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.0799,  1.3123,  0.0284,  0.6482, -0.6832, -1.0945,  0.5701,\n",
      "           2.0812],\n",
      "         [ 0.6014,  2.1037,  0.3467, -0.3399,  0.4538, -0.1907,  0.5511,\n",
      "           1.1629],\n",
      "         [ 1.2745, -0.4294, -0.1449,  0.2828,  1.0326,  0.3992, -0.7202,\n",
      "          -1.5498]]], grad_fn=<AddBackward0>)\n",
      "Level 1 from range(2)\n",
      "Model 0 from range(2)\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "x in layer before MQA:\n",
      "tensor([[[-1.4372, -0.1648, -1.5215,  0.4213],\n",
      "         [ 1.0980, -1.3593, -1.2852,  1.4749],\n",
      "         [ 0.0072,  0.5485,  1.4280, -0.1434]]])\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[-1.4033, -0.2513, -1.3675,  0.4547],\n",
      "         [ 1.0140, -1.2682, -1.1584,  1.4253],\n",
      "         [-0.0341,  0.5946,  1.5082, -0.1655]]], grad_fn=<AddBackward0>)\n",
      "layer's final residual state:\n",
      "tensor([[[-1.4145, -0.3354, -1.3474,  0.5146],\n",
      "         [ 1.0544, -1.3979, -1.1244,  1.5831],\n",
      "         [-0.0266,  0.5258,  1.5616, -0.0875]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 4])\n",
      "tensor([[[-1.4145, -0.3354, -1.3474,  0.5146],\n",
      "         [ 1.0544, -1.3979, -1.1244,  1.5831],\n",
      "         [-0.0266,  0.5258,  1.5616, -0.0875]]], grad_fn=<AddBackward0>)\n",
      "Model 1 from range(2)\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "x in layer before MQA:\n",
      "tensor([[[ 0.4308, -0.8585, -1.3597,  1.5565],\n",
      "         [-2.2003, -2.0060,  0.0684, -1.6505],\n",
      "         [-1.8424, -1.8042, -0.7318, -1.7832]]])\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[ 0.3461, -0.8467, -1.4204,  1.5135],\n",
      "         [-2.3626, -1.9869, -0.0444, -1.7491],\n",
      "         [-2.0568, -1.7848, -0.8813, -1.9229]]], grad_fn=<AddBackward0>)\n",
      "layer's final residual state:\n",
      "tensor([[[ 0.2067, -0.6960, -1.3886,  1.4489],\n",
      "         [-2.5132, -1.8183, -0.0177, -1.7053],\n",
      "         [-2.2103, -1.6391, -0.8663, -1.9145]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.2067, -0.6960, -1.3886,  1.4489],\n",
      "         [-2.5132, -1.8183, -0.0177, -1.7053],\n",
      "         [-2.2103, -1.6391, -0.8663, -1.9145]]], grad_fn=<AddBackward0>)\n",
      "final output: ((tensor([[[-0.0799,  1.3123,  0.0284,  0.6482, -0.6832, -1.0945,  0.5701,\n",
      "           2.0812],\n",
      "         [ 0.6014,  2.1037,  0.3467, -0.3399,  0.4538, -0.1907,  0.5511,\n",
      "           1.1629],\n",
      "         [ 1.2745, -0.4294, -0.1449,  0.2828,  1.0326,  0.3992, -0.7202,\n",
      "          -1.5498]]], grad_fn=<AddBackward0>),), (tensor([[[-1.4145, -0.3354, -1.3474,  0.5146],\n",
      "         [ 1.0544, -1.3979, -1.1244,  1.5831],\n",
      "         [-0.0266,  0.5258,  1.5616, -0.0875]]], grad_fn=<AddBackward0>), tensor([[[ 0.2067, -0.6960, -1.3886,  1.4489],\n",
      "         [-2.5132, -1.8183, -0.0177, -1.7053],\n",
      "         [-2.2103, -1.6391, -0.8663, -1.9145]]], grad_fn=<AddBackward0>)))\n",
      "------------- END Layer.forwardTuple() ------------\n",
      "out: ((tensor([[[-0.0799,  1.3123,  0.0284,  0.6482, -0.6832, -1.0945,  0.5701,\n",
      "           2.0812],\n",
      "         [ 0.6014,  2.1037,  0.3467, -0.3399,  0.4538, -0.1907,  0.5511,\n",
      "           1.1629],\n",
      "         [ 1.2745, -0.4294, -0.1449,  0.2828,  1.0326,  0.3992, -0.7202,\n",
      "          -1.5498]]], grad_fn=<AddBackward0>),), (tensor([[[-1.4145, -0.3354, -1.3474,  0.5146],\n",
      "         [ 1.0544, -1.3979, -1.1244,  1.5831],\n",
      "         [-0.0266,  0.5258,  1.5616, -0.0875]]], grad_fn=<AddBackward0>), tensor([[[ 0.2067, -0.6960, -1.3886,  1.4489],\n",
      "         [-2.5132, -1.8183, -0.0177, -1.7053],\n",
      "         [-2.2103, -1.6391, -0.8663, -1.9145]]], grad_fn=<AddBackward0>)))\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [8, 4, 2]\n",
      "head_dim_list:  [4, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "# Testing our Layer's forwardTuple()\n",
    "config.verbose['Layer'] = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4, hold5 = config.hidden_size, config.num_attention_heads, config.head_dim, config.levels, config.max_position_embeddings\n",
    "config.hidden_size = 8\n",
    "config.num_attention_heads = 2\n",
    "config.head_dim = 4\n",
    "config.levels = 2\n",
    "config.max_position_embeddings = 3\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "layer = Layer(config)\n",
    "# we need to make sure to send in a tuple of the expected size. above we set hidden_size=4 and levels=2\n",
    "x = ((torch.randn((1,3,8)),),(torch.randn((1,3,4)),torch.randn((1,3,4))))\n",
    "print(f\"x: {x}\")\n",
    "out = layer(x)\n",
    "print(f\"out: {out}\")\n",
    "\n",
    "config.verbose['Layer'] = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.num_attention_heads = hold2\n",
    "config.head_dim = hold3\n",
    "config.levels = hold4\n",
    "config.max_position_embeddings = hold5\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "# clear up memory\n",
    "del hold1, hold2, hold3, hold4, hold5, x, layer, out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b302f731",
   "metadata": {},
   "source": [
    "# Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59b19f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayer(nn.Module):\n",
    "    def __init__(self, embedding: torch.Tensor, config: Config):\n",
    "        super().__init__()\n",
    "        self.verbose = config.verbose['OutputLayer']\n",
    "        \n",
    "        self.embedding = embedding\n",
    "        self.v = config.vocab_size\n",
    "        self.model_dim_list = config.model_dim_list\n",
    "\n",
    "        # applies RMSNorm to the embedding matrix\n",
    "        self.embedding_norm = RMSNorm(config.hidden_size,\n",
    "                                      eps = config.rms_norm_eps)\n",
    "        \n",
    "        # Applies RMSNorm to the model's final residual state before we use the embedding matrix to get logits\n",
    "        self.final_norm = RMSNorm(config.hidden_size,\n",
    "                                  eps = config.rms_norm_eps)\n",
    "\n",
    "    def forwardTensor(self, x, model=0):\n",
    "        if self.verbose: \n",
    "            print(\"------------- OutputLayer.forwardTensor() ------------\")\n",
    "            print(f\"x: {x.shape}\\n{x}\")\n",
    "\n",
    "        # setting up our splicing logic\n",
    "        d_i = x.shape[-1]\n",
    "        skip = model * d_i\n",
    "        if self.verbose:\n",
    "            print(f\"d_i: {d_i}\")\n",
    "            print(f\"skip: {skip}\")\n",
    "            print(f\"embedding: {self.embedding.shape}\\n{self.embedding}\")\n",
    "\n",
    "        # splice out our embedding matrix according to what model we're using\n",
    "        sliced_embed = self.embedding[:,skip:skip + d_i]\n",
    "        if self.verbose: print(f\"sliced_embed: {sliced_embed.shape}\\n{sliced_embed}\")\n",
    "\n",
    "        # normalize our sliced embedding matrix\n",
    "        normed_sliced_embed = self.embedding_norm(sliced_embed)\n",
    "        if self.verbose: print(f\"normed & sliced embedding: {normed_sliced_embed.shape}\\n{normed_sliced_embed}\")\n",
    "\n",
    "        # normalize the residual state before the final linear layer\n",
    "        x = self.final_norm(x, model)\n",
    "        if self.verbose: print(f\"normed x: {x.shape}\\n{x}\")\n",
    "\n",
    "        # calculating the final output logits of the model\n",
    "        logits = x @ normed_sliced_embed.t()\n",
    "        if self.verbose: \n",
    "            print(f\"final logits: {logits.shape}\\n{logits}\")\n",
    "            print(\"------------- END OutputLayer.forwardTensor() ------------\")\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def forwardTuple(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the final embedding classification layer during training.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tuple[Tuple[Tensor]]): \n",
    "                The input tuple of tuples of tensors \n",
    "                first tuple is of length config.levels and second layer of tuples have lengths of config.model_count\n",
    "                tensors are shape (batch size, sequence length, hidden dimension) where hidden dimension changes by which model was used\n",
    "\n",
    "        Returns:\n",
    "            output (Tuple[Tuple[Tensor]]): \n",
    "                The output tuple of tuples of tensors after applying the final embedding classification\n",
    "        \"\"\"\n",
    "        if self.verbose: \n",
    "            print(\"------------- OutputLayer.forwardTuple() ------------\")\n",
    "            print(f\"x:\\n{x}\")\n",
    "            \n",
    "        # forwardTuple() should only be used during training, so we assert input_len == max_position_embeddings\n",
    "        assert type(x) == tuple\n",
    "        input_len = x[0][0].shape[1]\n",
    "        if self.verbose: print(f\"input_len: {input_len}\")\n",
    "        assert input_len == config.max_position_embeddings\n",
    "\n",
    "        # we could define these from the config but this way the method is more flexible to testing\n",
    "        num_levels = len(x)\n",
    "        models_per_level = [len(x[i]) for i in range(num_levels)]\n",
    "        if self.verbose: \n",
    "            print(f\"num_levels: {num_levels}\")\n",
    "            print(f\"models_per_level: {models_per_level}\")\n",
    "\n",
    "        # the loop that iterates over levels, aka the different potential sizes of models\n",
    "        out = ()\n",
    "        for i in range(num_levels):\n",
    "            if self.verbose: print(f\"Level {i} from range({num_levels})\")\n",
    "\n",
    "            # now for the loop that iterates over models in this level\n",
    "            out_lvl = ()\n",
    "            for j in range(models_per_level[i]):\n",
    "                if verbose: print(f\"Model {j} from range({models_per_level[i]})\")\n",
    "\n",
    "                output = self.forwardTensor(x[i][j], model = j)\n",
    "                if verbose: print(f\"forwardTensor() output: {output.shape}\\n{output}\")\n",
    "                \n",
    "                out_lvl += (output,)\n",
    "            \n",
    "            out += (out_lvl,)\n",
    "        \n",
    "        if self.verbose:\n",
    "            print(f\"final output: {out}\")\n",
    "            print(\"------------- END Layer.forwardTuple() ------------\")\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def forward(self, x, model=0):\n",
    "        train = True if type(x) == tuple else False\n",
    "        if self.verbose: print(f\"---------- Layer Input: {'Tuple' if train else 'torch.Tensor'} ------------\")\n",
    "        return self.forwardTuple(x) if train else self.forwardTensor(x, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "feeb1bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [4, 2, 1]\n",
      "embedding: torch.Size([5, 4])\n",
      "tensor([[-1.0029, -0.6481, -0.1033,  0.0179],\n",
      "        [ 0.3040,  0.6162, -0.7662, -0.3065],\n",
      "        [-0.4503, -0.4188,  0.4614,  0.7140],\n",
      "        [-0.0873,  2.1468, -0.4038, -1.0478],\n",
      "        [-0.7905, -0.3966,  1.0696,  0.2321]])\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.2541, 0.1595, 0.1416, 0.9780],\n",
      "         [0.8886, 0.3968, 0.2795, 0.7710],\n",
      "         [0.2622, 0.9828, 0.5944, 0.6838]]])\n",
      "y: torch.Size([1, 3, 5])\n",
      "tensor([[[-1.1483, -0.8396,  2.1500, -1.2171,  0.3148],\n",
      "         [-3.0507,  0.1878,  0.3389, -0.1898, -0.8531],\n",
      "         [-2.3251,  0.0556,  0.6515,  1.3692,  0.4120]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.5048, 0.6667],\n",
      "         [0.2467, 0.7535],\n",
      "         [0.5575, 0.0836]]])\n",
      "y: torch.Size([1, 3, 5])\n",
      "tensor([[[-1.8794,  1.9641, -1.9700,  1.5441, -1.7941],\n",
      "         [-1.5543,  1.9799, -1.7502,  1.8739, -1.4085],\n",
      "         [-1.8222,  1.1410, -1.6503,  0.2161, -1.9009]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.7759, 0.1764],\n",
      "         [0.3252, 0.5368],\n",
      "         [0.5966, 0.5153]]])\n",
      "y: torch.Size([1, 3, 5])\n",
      "tensor([[[-1.8459, -1.9754,  1.4309, -1.1152,  1.9999],\n",
      "         [-0.7295, -1.5975,  1.9992, -1.9688,  1.3755],\n",
      "         [-1.2685, -1.8909,  1.9195, -1.7642,  1.7564]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [8, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "# Testing our OutputLayer's forwardTensor()\n",
    "config.verbose['OutputLayer'] = False\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2 = config.hidden_size, config.vocab_size\n",
    "config.hidden_size = 4\n",
    "config.vocab_size = 5\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "embedding = torch.randn(config.vocab_size, config.hidden_size)\n",
    "print(f\"embedding: {embedding.shape}\\n{embedding}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "layer = OutputLayer(embedding, config)\n",
    "y = layer(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size//config.split)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "y = layer(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size//config.split)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "y = layer(x, model=1)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "config.verbose['OutputLayer'] = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.vocab_size = hold2\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "# clear up memory\n",
    "del hold1, hold2, x, layer, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "87ca01f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [4, 2]\n",
      "embedding: torch.Size([5, 4])\n",
      "tensor([[-0.1378,  0.2186,  0.3803,  0.2366],\n",
      "        [ 0.0222,  1.4026, -0.7783, -0.8385],\n",
      "        [ 0.2198, -0.0664,  1.0288, -0.0800],\n",
      "        [-0.1992,  1.6327,  0.4469, -0.2077],\n",
      "        [-1.0631, -0.3205, -0.5173, -0.5734]])\n",
      "x: ((tensor([[[ 2.7732,  1.0811,  0.1061,  0.2997],\n",
      "         [-0.5242, -2.4195,  0.6849, -1.4000],\n",
      "         [-0.9715,  0.4694,  0.2765,  0.3599]]]),), (tensor([[[-0.8094, -0.7888],\n",
      "         [ 1.7284, -0.7206],\n",
      "         [-1.5999, -1.3520]]]), tensor([[[-0.5505, -1.4723],\n",
      "         [-0.8347, -0.1062],\n",
      "         [ 0.7869,  0.3531]]])))\n",
      "out: ((tensor([[[ 2.7552,  1.4414, -0.3336,  0.5405],\n",
      "         [-0.4132, -2.0876,  0.4866, -1.3920],\n",
      "         [-0.5444,  0.4848,  0.1880,  0.1476]]], grad_fn=<AddBackward0>),), (tensor([[[-0.6186, -0.7016],\n",
      "         [ 1.8485, -0.5166],\n",
      "         [-1.4759, -1.1790]]], grad_fn=<AddBackward0>), tensor([[[-0.5881, -1.4282],\n",
      "         [-0.7968, -0.0432],\n",
      "         [ 0.7436,  0.4093]]], grad_fn=<AddBackward0>)))\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [8, 4, 2]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([-0.0242,  0.1740,  0.0925,  0.2142], requires_grad=True)\n",
      "Bdown spliced: torch.Size([2])\n",
      "tensor([-0.0242,  0.1740], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.0218,  0.1717],\n",
      "         [-0.0091,  0.1531],\n",
      "         [-0.0124,  0.1585]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[ 0.8293,  0.5314],\n",
      "         [-0.4652,  1.3387],\n",
      "         [ 1.0220,  3.1334]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.8293,  0.5314],\n",
      "         [-0.4652,  1.3387],\n",
      "         [ 1.0220,  3.1334]]], grad_fn=<AddBackward0>)\n",
      "Model 1 from range(2)\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.4059, -0.8723],\n",
      "         [ 0.0212,  1.2475],\n",
      "         [ 0.7738,  0.3308]]])\n",
      "normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.5966, -1.2822],\n",
      "         [ 0.0241,  1.4140],\n",
      "         [ 1.3004,  0.5558]]])\n",
      "dim: 2\n",
      "skip: 2\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.5966, -1.2822],\n",
      "         [ 0.0241,  1.4140],\n",
      "         [ 1.3004,  0.5558]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 2])\n",
      "d_skip: 2\n",
      "models_in_this_level: 2\n",
      "h_dim: 16\n",
      "h_skip: 16\n",
      "self.Wqkv: torch.Size([4, 192])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3301,  0.3833,  0.4804,  0.3799, -0.3784,  0.0511,  0.3683,  0.1008,\n",
      "         -0.2285, -0.0197,  0.0348,  0.1120, -0.3621, -0.2547,  0.4441, -0.4699,\n",
      "         -0.3608, -0.4284,  0.1076,  0.4837, -0.0746,  0.1340, -0.4201, -0.0809,\n",
      "          0.1141, -0.1077,  0.0644,  0.3932,  0.0016,  0.0923, -0.3204, -0.1327,\n",
      "         -0.3313, -0.1889, -0.1339,  0.0845,  0.1315, -0.0367, -0.3632, -0.3186,\n",
      "         -0.0542, -0.2293, -0.4923, -0.4534, -0.3555, -0.4473, -0.2461,  0.3808,\n",
      "         -0.2491, -0.0109, -0.3062,  0.1362, -0.3017,  0.0435,  0.1770,  0.4863,\n",
      "         -0.2845, -0.0166,  0.4380,  0.1111,  0.3862,  0.1065,  0.2000, -0.2471,\n",
      "         -0.0877, -0.4372,  0.4612,  0.0399,  0.2884,  0.0085, -0.3506,  0.0517,\n",
      "          0.2164, -0.1687,  0.3631,  0.2543, -0.0103,  0.4510,  0.3025, -0.0815,\n",
      "         -0.0690,  0.4011, -0.0612, -0.3017, -0.4315, -0.0644, -0.0030,  0.1530,\n",
      "         -0.3891, -0.2778, -0.2134, -0.0432,  0.4629, -0.0109, -0.0543,  0.3298,\n",
      "          0.1822, -0.2275, -0.2900, -0.0932, -0.0859,  0.1376,  0.4553,  0.2669,\n",
      "         -0.1041, -0.4566,  0.4032,  0.4596,  0.0240,  0.2582,  0.4123,  0.0547,\n",
      "          0.2229,  0.0054, -0.2271,  0.2998,  0.3078,  0.1199,  0.0656, -0.4977,\n",
      "         -0.1922, -0.1638, -0.0580, -0.1722, -0.0207, -0.2017, -0.4377, -0.2301,\n",
      "          0.1247,  0.4458,  0.0637, -0.4035, -0.1888, -0.3908,  0.4974,  0.0636,\n",
      "         -0.3287,  0.3867, -0.3518, -0.0131,  0.0452, -0.4103, -0.0953,  0.3811,\n",
      "          0.1975,  0.4332,  0.1069,  0.0880, -0.3776,  0.1469, -0.3684,  0.2586,\n",
      "          0.0258,  0.2367, -0.4173, -0.4539,  0.0401,  0.3268,  0.4671,  0.3535,\n",
      "          0.3936,  0.4408,  0.4123,  0.1102, -0.4779,  0.2480,  0.2368, -0.1251,\n",
      "         -0.3720,  0.2231, -0.0660,  0.0564, -0.3418, -0.2669,  0.4652, -0.4518,\n",
      "         -0.3219,  0.2803, -0.4718, -0.3934, -0.3866, -0.2158,  0.4596,  0.4616,\n",
      "          0.3334,  0.4542, -0.2137, -0.0657, -0.2930,  0.2428,  0.0470, -0.3103],\n",
      "        [-0.3085,  0.2079, -0.0558,  0.4727, -0.0572,  0.4325, -0.4721, -0.3933,\n",
      "         -0.4037,  0.2223, -0.3074, -0.4287,  0.4854, -0.1563,  0.2378,  0.3082,\n",
      "          0.2669, -0.4955, -0.4475,  0.1592, -0.2937,  0.1021, -0.2588,  0.0658,\n",
      "          0.1464,  0.0969, -0.4747, -0.1736, -0.2305,  0.3631,  0.2602,  0.4455,\n",
      "         -0.4695, -0.4149,  0.2928, -0.2088, -0.2163,  0.2472,  0.2846, -0.3211,\n",
      "         -0.2504,  0.3590, -0.4542, -0.4578,  0.0834, -0.2651, -0.3716,  0.0151,\n",
      "         -0.0984,  0.0979,  0.0320,  0.0527, -0.4798, -0.0049,  0.3435, -0.2685,\n",
      "          0.0051, -0.3738, -0.0213,  0.3898,  0.4789,  0.2627, -0.2252,  0.1004,\n",
      "         -0.4895, -0.0907, -0.1520, -0.2988,  0.0696,  0.3762, -0.0568,  0.3029,\n",
      "          0.2704,  0.1733,  0.0607, -0.3022,  0.0958,  0.2246,  0.0864,  0.1957,\n",
      "         -0.1262,  0.0189, -0.3558,  0.3480, -0.4218,  0.3846,  0.2977, -0.2668,\n",
      "         -0.4719, -0.3136,  0.0346, -0.2221, -0.0843, -0.1978,  0.3260, -0.1030,\n",
      "          0.2973,  0.2319, -0.3665,  0.3388, -0.4193, -0.3237,  0.3003,  0.1727,\n",
      "          0.1312,  0.3687,  0.3775, -0.1518, -0.0341, -0.4558,  0.4252,  0.1452,\n",
      "          0.4464, -0.0587,  0.2010,  0.3056,  0.1037, -0.2781,  0.1119,  0.4075,\n",
      "          0.3024,  0.4253,  0.3235, -0.2391, -0.1235, -0.2415,  0.3376,  0.0750,\n",
      "          0.4636, -0.2067, -0.2724,  0.2813,  0.3083, -0.1645,  0.4072, -0.3367,\n",
      "         -0.4844,  0.4903, -0.2458,  0.3983,  0.4745,  0.1093,  0.3478, -0.3051,\n",
      "          0.2002, -0.0206,  0.2579, -0.2674, -0.3509,  0.4186, -0.1168,  0.3344,\n",
      "          0.4014, -0.1908,  0.1122, -0.2635, -0.2765, -0.4911,  0.3065,  0.2265,\n",
      "         -0.4627, -0.0526, -0.4488,  0.0989, -0.0009,  0.3884,  0.4852, -0.1694,\n",
      "         -0.1860, -0.0708, -0.4281,  0.1182, -0.2934,  0.0983, -0.2847,  0.1333,\n",
      "          0.3494,  0.2996, -0.4393,  0.2396,  0.0914, -0.1308,  0.4602,  0.1519,\n",
      "          0.0964,  0.2173, -0.3352,  0.2081, -0.3686,  0.4634,  0.1625,  0.1153],\n",
      "        [-0.4529, -0.1572, -0.0539,  0.3450,  0.4261,  0.2598,  0.2026, -0.0352,\n",
      "          0.2761, -0.3891,  0.3350, -0.2972, -0.0346, -0.0281, -0.1072,  0.4468,\n",
      "          0.0352,  0.2435, -0.1058,  0.3529,  0.0723,  0.2023,  0.3687, -0.4046,\n",
      "         -0.4893, -0.1057,  0.0078, -0.4062,  0.2501,  0.4469,  0.2588,  0.3636,\n",
      "         -0.4504, -0.4279, -0.2497, -0.4591, -0.4598, -0.1506,  0.0301,  0.1856,\n",
      "         -0.3996,  0.0477, -0.2218,  0.4408,  0.3175, -0.2819, -0.3891, -0.2715,\n",
      "         -0.0119, -0.1215,  0.1585,  0.4206, -0.1848,  0.1778,  0.4221, -0.1031,\n",
      "          0.1428,  0.2237, -0.3037, -0.0875,  0.3658,  0.3300,  0.1022, -0.1516,\n",
      "         -0.4768,  0.0470,  0.2139,  0.3245, -0.2838, -0.2326,  0.3793, -0.4275,\n",
      "          0.0624, -0.1127,  0.3912, -0.2025, -0.1955,  0.0486, -0.3604,  0.1436,\n",
      "         -0.2875, -0.3124, -0.1595, -0.0667, -0.0748, -0.3298, -0.3057,  0.2011,\n",
      "          0.1131, -0.0505, -0.2193,  0.2211,  0.3423, -0.1574, -0.4652,  0.2528,\n",
      "          0.1820,  0.1900, -0.2276, -0.0111, -0.4589, -0.4369,  0.2751,  0.2070,\n",
      "          0.2440, -0.3492,  0.1704, -0.0839, -0.4068,  0.1689, -0.2187, -0.2404,\n",
      "         -0.4594,  0.1789, -0.4587, -0.1004,  0.4327,  0.1572,  0.1019,  0.1318,\n",
      "          0.3062,  0.3306,  0.1911,  0.4272,  0.0892,  0.2553,  0.4253,  0.1659,\n",
      "         -0.0153, -0.3781,  0.4079,  0.0030,  0.4418, -0.4477,  0.2635, -0.3938,\n",
      "          0.0555, -0.3003, -0.2474, -0.3613,  0.0365,  0.0919,  0.0444, -0.0326,\n",
      "          0.1988, -0.0043,  0.4960, -0.2726,  0.2577, -0.1399,  0.0490, -0.2605,\n",
      "         -0.2334,  0.1966, -0.3514, -0.0269, -0.2524, -0.0522,  0.3142, -0.0619,\n",
      "         -0.2309,  0.3951, -0.2231,  0.4976, -0.0028, -0.4960, -0.3880,  0.3877,\n",
      "         -0.3430,  0.3044,  0.0261,  0.3276, -0.1741, -0.0334,  0.0502,  0.0558,\n",
      "         -0.2254,  0.2467,  0.3906, -0.3988, -0.2671,  0.3635, -0.2941, -0.4413,\n",
      "         -0.2717, -0.1500,  0.1175, -0.0211, -0.1650,  0.1372, -0.1467, -0.4682],\n",
      "        [ 0.1006,  0.1690, -0.2365, -0.2573, -0.3941, -0.1949,  0.1145,  0.4879,\n",
      "          0.3939, -0.4653,  0.3927, -0.3928, -0.1648, -0.3126, -0.2240, -0.2955,\n",
      "          0.4953,  0.0201, -0.4571, -0.3099,  0.1722, -0.3074, -0.2650, -0.0343,\n",
      "          0.2155,  0.1219,  0.4715,  0.1549,  0.0180, -0.3136, -0.1017,  0.0442,\n",
      "         -0.0807,  0.3326, -0.2793,  0.4693,  0.2938,  0.2537,  0.4737, -0.4222,\n",
      "         -0.1041,  0.0470, -0.1856,  0.1775, -0.1044,  0.3396,  0.3710,  0.1461,\n",
      "          0.3862,  0.0915,  0.0245, -0.4862,  0.2971, -0.4279, -0.0600, -0.3252,\n",
      "          0.0575,  0.0737, -0.1541,  0.0132, -0.3831,  0.1708,  0.1641,  0.3259,\n",
      "          0.2080, -0.2994,  0.3289, -0.1726,  0.1728, -0.4535,  0.2559,  0.0939,\n",
      "          0.1923, -0.4447,  0.4706,  0.1378,  0.3362,  0.3585, -0.4693,  0.0737,\n",
      "         -0.0180, -0.1533,  0.1602,  0.0328,  0.3808,  0.0183, -0.4225,  0.2609,\n",
      "         -0.2920, -0.3166,  0.0291,  0.4230,  0.3254, -0.1631,  0.0777,  0.0084,\n",
      "          0.3930,  0.0684,  0.3393, -0.4065,  0.0184,  0.1434, -0.4191, -0.2668,\n",
      "         -0.2583, -0.3938,  0.2692,  0.3379, -0.1610, -0.4425, -0.3757, -0.3587,\n",
      "         -0.4249,  0.2199,  0.3004,  0.0443,  0.3836,  0.0263,  0.2958, -0.1507,\n",
      "          0.0537, -0.0687,  0.1514, -0.4911,  0.0052,  0.0761, -0.1913,  0.1853,\n",
      "         -0.0124, -0.3541,  0.0277, -0.3630, -0.3104, -0.1769, -0.1923,  0.1557,\n",
      "          0.4131,  0.2811,  0.3978,  0.4198,  0.4174,  0.1364, -0.3672,  0.0243,\n",
      "         -0.0475, -0.2074, -0.3462,  0.3039,  0.0092, -0.2067,  0.4334, -0.2698,\n",
      "         -0.4175,  0.4882,  0.1428,  0.1868, -0.2379, -0.2610,  0.2044,  0.1492,\n",
      "         -0.4313, -0.2646,  0.0871, -0.2397,  0.1344,  0.3381, -0.0484,  0.2644,\n",
      "         -0.2736, -0.4856, -0.1567,  0.0605,  0.1655,  0.1458,  0.1922, -0.2275,\n",
      "          0.3972, -0.1485, -0.0114,  0.1299,  0.2137,  0.2654,  0.2137, -0.3936,\n",
      "         -0.4087,  0.1286,  0.3506, -0.1719, -0.0443, -0.4216, -0.0135,  0.2737]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([4, 128])\n",
      "tensor([[ 0.3301,  0.3833,  0.4804,  0.3799, -0.3784,  0.0511,  0.3683,  0.1008,\n",
      "         -0.2285, -0.0197,  0.0348,  0.1120, -0.3621, -0.2547,  0.4441, -0.4699,\n",
      "         -0.3608, -0.4284,  0.1076,  0.4837, -0.0746,  0.1340, -0.4201, -0.0809,\n",
      "          0.1141, -0.1077,  0.0644,  0.3932,  0.0016,  0.0923, -0.3204, -0.1327,\n",
      "         -0.3313, -0.1889, -0.1339,  0.0845,  0.1315, -0.0367, -0.3632, -0.3186,\n",
      "         -0.0542, -0.2293, -0.4923, -0.4534, -0.3555, -0.4473, -0.2461,  0.3808,\n",
      "         -0.2491, -0.0109, -0.3062,  0.1362, -0.3017,  0.0435,  0.1770,  0.4863,\n",
      "         -0.2845, -0.0166,  0.4380,  0.1111,  0.3862,  0.1065,  0.2000, -0.2471,\n",
      "         -0.0877, -0.4372,  0.4612,  0.0399,  0.2884,  0.0085, -0.3506,  0.0517,\n",
      "          0.2164, -0.1687,  0.3631,  0.2543, -0.0103,  0.4510,  0.3025, -0.0815,\n",
      "         -0.0690,  0.4011, -0.0612, -0.3017, -0.4315, -0.0644, -0.0030,  0.1530,\n",
      "         -0.3891, -0.2778, -0.2134, -0.0432,  0.4629, -0.0109, -0.0543,  0.3298,\n",
      "          0.1822, -0.2275, -0.2900, -0.0932, -0.0859,  0.1376,  0.4553,  0.2669,\n",
      "         -0.1041, -0.4566,  0.4032,  0.4596,  0.0240,  0.2582,  0.4123,  0.0547,\n",
      "          0.2229,  0.0054, -0.2271,  0.2998,  0.3078,  0.1199,  0.0656, -0.4977,\n",
      "         -0.1922, -0.1638, -0.0580, -0.1722, -0.0207, -0.2017, -0.4377, -0.2301],\n",
      "        [-0.3085,  0.2079, -0.0558,  0.4727, -0.0572,  0.4325, -0.4721, -0.3933,\n",
      "         -0.4037,  0.2223, -0.3074, -0.4287,  0.4854, -0.1563,  0.2378,  0.3082,\n",
      "          0.2669, -0.4955, -0.4475,  0.1592, -0.2937,  0.1021, -0.2588,  0.0658,\n",
      "          0.1464,  0.0969, -0.4747, -0.1736, -0.2305,  0.3631,  0.2602,  0.4455,\n",
      "         -0.4695, -0.4149,  0.2928, -0.2088, -0.2163,  0.2472,  0.2846, -0.3211,\n",
      "         -0.2504,  0.3590, -0.4542, -0.4578,  0.0834, -0.2651, -0.3716,  0.0151,\n",
      "         -0.0984,  0.0979,  0.0320,  0.0527, -0.4798, -0.0049,  0.3435, -0.2685,\n",
      "          0.0051, -0.3738, -0.0213,  0.3898,  0.4789,  0.2627, -0.2252,  0.1004,\n",
      "         -0.4895, -0.0907, -0.1520, -0.2988,  0.0696,  0.3762, -0.0568,  0.3029,\n",
      "          0.2704,  0.1733,  0.0607, -0.3022,  0.0958,  0.2246,  0.0864,  0.1957,\n",
      "         -0.1262,  0.0189, -0.3558,  0.3480, -0.4218,  0.3846,  0.2977, -0.2668,\n",
      "         -0.4719, -0.3136,  0.0346, -0.2221, -0.0843, -0.1978,  0.3260, -0.1030,\n",
      "          0.2973,  0.2319, -0.3665,  0.3388, -0.4193, -0.3237,  0.3003,  0.1727,\n",
      "          0.1312,  0.3687,  0.3775, -0.1518, -0.0341, -0.4558,  0.4252,  0.1452,\n",
      "          0.4464, -0.0587,  0.2010,  0.3056,  0.1037, -0.2781,  0.1119,  0.4075,\n",
      "          0.3024,  0.4253,  0.3235, -0.2391, -0.1235, -0.2415,  0.3376,  0.0750],\n",
      "        [-0.4529, -0.1572, -0.0539,  0.3450,  0.4261,  0.2598,  0.2026, -0.0352,\n",
      "          0.2761, -0.3891,  0.3350, -0.2972, -0.0346, -0.0281, -0.1072,  0.4468,\n",
      "          0.0352,  0.2435, -0.1058,  0.3529,  0.0723,  0.2023,  0.3687, -0.4046,\n",
      "         -0.4893, -0.1057,  0.0078, -0.4062,  0.2501,  0.4469,  0.2588,  0.3636,\n",
      "         -0.4504, -0.4279, -0.2497, -0.4591, -0.4598, -0.1506,  0.0301,  0.1856,\n",
      "         -0.3996,  0.0477, -0.2218,  0.4408,  0.3175, -0.2819, -0.3891, -0.2715,\n",
      "         -0.0119, -0.1215,  0.1585,  0.4206, -0.1848,  0.1778,  0.4221, -0.1031,\n",
      "          0.1428,  0.2237, -0.3037, -0.0875,  0.3658,  0.3300,  0.1022, -0.1516,\n",
      "         -0.4768,  0.0470,  0.2139,  0.3245, -0.2838, -0.2326,  0.3793, -0.4275,\n",
      "          0.0624, -0.1127,  0.3912, -0.2025, -0.1955,  0.0486, -0.3604,  0.1436,\n",
      "         -0.2875, -0.3124, -0.1595, -0.0667, -0.0748, -0.3298, -0.3057,  0.2011,\n",
      "          0.1131, -0.0505, -0.2193,  0.2211,  0.3423, -0.1574, -0.4652,  0.2528,\n",
      "          0.1820,  0.1900, -0.2276, -0.0111, -0.4589, -0.4369,  0.2751,  0.2070,\n",
      "          0.2440, -0.3492,  0.1704, -0.0839, -0.4068,  0.1689, -0.2187, -0.2404,\n",
      "         -0.4594,  0.1789, -0.4587, -0.1004,  0.4327,  0.1572,  0.1019,  0.1318,\n",
      "          0.3062,  0.3306,  0.1911,  0.4272,  0.0892,  0.2553,  0.4253,  0.1659],\n",
      "        [ 0.1006,  0.1690, -0.2365, -0.2573, -0.3941, -0.1949,  0.1145,  0.4879,\n",
      "          0.3939, -0.4653,  0.3927, -0.3928, -0.1648, -0.3126, -0.2240, -0.2955,\n",
      "          0.4953,  0.0201, -0.4571, -0.3099,  0.1722, -0.3074, -0.2650, -0.0343,\n",
      "          0.2155,  0.1219,  0.4715,  0.1549,  0.0180, -0.3136, -0.1017,  0.0442,\n",
      "         -0.0807,  0.3326, -0.2793,  0.4693,  0.2938,  0.2537,  0.4737, -0.4222,\n",
      "         -0.1041,  0.0470, -0.1856,  0.1775, -0.1044,  0.3396,  0.3710,  0.1461,\n",
      "          0.3862,  0.0915,  0.0245, -0.4862,  0.2971, -0.4279, -0.0600, -0.3252,\n",
      "          0.0575,  0.0737, -0.1541,  0.0132, -0.3831,  0.1708,  0.1641,  0.3259,\n",
      "          0.2080, -0.2994,  0.3289, -0.1726,  0.1728, -0.4535,  0.2559,  0.0939,\n",
      "          0.1923, -0.4447,  0.4706,  0.1378,  0.3362,  0.3585, -0.4693,  0.0737,\n",
      "         -0.0180, -0.1533,  0.1602,  0.0328,  0.3808,  0.0183, -0.4225,  0.2609,\n",
      "         -0.2920, -0.3166,  0.0291,  0.4230,  0.3254, -0.1631,  0.0777,  0.0084,\n",
      "          0.3930,  0.0684,  0.3393, -0.4065,  0.0184,  0.1434, -0.4191, -0.2668,\n",
      "         -0.2583, -0.3938,  0.2692,  0.3379, -0.1610, -0.4425, -0.3757, -0.3587,\n",
      "         -0.4249,  0.2199,  0.3004,  0.0443,  0.3836,  0.0263,  0.2958, -0.1507,\n",
      "          0.0537, -0.0687,  0.1514, -0.4911,  0.0052,  0.0761, -0.1913,  0.1853]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([4, 32])\n",
      "tensor([[ 0.1247,  0.4458,  0.0637, -0.4035, -0.1888, -0.3908,  0.4974,  0.0636,\n",
      "         -0.3287,  0.3867, -0.3518, -0.0131,  0.0452, -0.4103, -0.0953,  0.3811,\n",
      "          0.1975,  0.4332,  0.1069,  0.0880, -0.3776,  0.1469, -0.3684,  0.2586,\n",
      "          0.0258,  0.2367, -0.4173, -0.4539,  0.0401,  0.3268,  0.4671,  0.3535],\n",
      "        [ 0.4636, -0.2067, -0.2724,  0.2813,  0.3083, -0.1645,  0.4072, -0.3367,\n",
      "         -0.4844,  0.4903, -0.2458,  0.3983,  0.4745,  0.1093,  0.3478, -0.3051,\n",
      "          0.2002, -0.0206,  0.2579, -0.2674, -0.3509,  0.4186, -0.1168,  0.3344,\n",
      "          0.4014, -0.1908,  0.1122, -0.2635, -0.2765, -0.4911,  0.3065,  0.2265],\n",
      "        [-0.0153, -0.3781,  0.4079,  0.0030,  0.4418, -0.4477,  0.2635, -0.3938,\n",
      "          0.0555, -0.3003, -0.2474, -0.3613,  0.0365,  0.0919,  0.0444, -0.0326,\n",
      "          0.1988, -0.0043,  0.4960, -0.2726,  0.2577, -0.1399,  0.0490, -0.2605,\n",
      "         -0.2334,  0.1966, -0.3514, -0.0269, -0.2524, -0.0522,  0.3142, -0.0619],\n",
      "        [-0.0124, -0.3541,  0.0277, -0.3630, -0.3104, -0.1769, -0.1923,  0.1557,\n",
      "          0.4131,  0.2811,  0.3978,  0.4198,  0.4174,  0.1364, -0.3672,  0.0243,\n",
      "         -0.0475, -0.2074, -0.3462,  0.3039,  0.0092, -0.2067,  0.4334, -0.2698,\n",
      "         -0.4175,  0.4882,  0.1428,  0.1868, -0.2379, -0.2610,  0.2044,  0.1492]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([4, 32])\n",
      "tensor([[ 0.3936,  0.4408,  0.4123,  0.1102, -0.4779,  0.2480,  0.2368, -0.1251,\n",
      "         -0.3720,  0.2231, -0.0660,  0.0564, -0.3418, -0.2669,  0.4652, -0.4518,\n",
      "         -0.3219,  0.2803, -0.4718, -0.3934, -0.3866, -0.2158,  0.4596,  0.4616,\n",
      "          0.3334,  0.4542, -0.2137, -0.0657, -0.2930,  0.2428,  0.0470, -0.3103],\n",
      "        [-0.4627, -0.0526, -0.4488,  0.0989, -0.0009,  0.3884,  0.4852, -0.1694,\n",
      "         -0.1860, -0.0708, -0.4281,  0.1182, -0.2934,  0.0983, -0.2847,  0.1333,\n",
      "          0.3494,  0.2996, -0.4393,  0.2396,  0.0914, -0.1308,  0.4602,  0.1519,\n",
      "          0.0964,  0.2173, -0.3352,  0.2081, -0.3686,  0.4634,  0.1625,  0.1153],\n",
      "        [-0.2309,  0.3951, -0.2231,  0.4976, -0.0028, -0.4960, -0.3880,  0.3877,\n",
      "         -0.3430,  0.3044,  0.0261,  0.3276, -0.1741, -0.0334,  0.0502,  0.0558,\n",
      "         -0.2254,  0.2467,  0.3906, -0.3988, -0.2671,  0.3635, -0.2941, -0.4413,\n",
      "         -0.2717, -0.1500,  0.1175, -0.0211, -0.1650,  0.1372, -0.1467, -0.4682],\n",
      "        [-0.4313, -0.2646,  0.0871, -0.2397,  0.1344,  0.3381, -0.0484,  0.2644,\n",
      "         -0.2736, -0.4856, -0.1567,  0.0605,  0.1655,  0.1458,  0.1922, -0.2275,\n",
      "          0.3972, -0.1485, -0.0114,  0.1299,  0.2137,  0.2654,  0.2137, -0.3936,\n",
      "         -0.4087,  0.1286,  0.3506, -0.1719, -0.0443, -0.4216, -0.0135,  0.2737]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([2, 64])\n",
      "tensor([[ 0.0352,  0.2435, -0.1058,  0.3529,  0.0723,  0.2023,  0.3687, -0.4046,\n",
      "         -0.4893, -0.1057,  0.0078, -0.4062,  0.2501,  0.4469,  0.2588,  0.3636,\n",
      "         -0.0119, -0.1215,  0.1585,  0.4206, -0.1848,  0.1778,  0.4221, -0.1031,\n",
      "          0.1428,  0.2237, -0.3037, -0.0875,  0.3658,  0.3300,  0.1022, -0.1516,\n",
      "         -0.2875, -0.3124, -0.1595, -0.0667, -0.0748, -0.3298, -0.3057,  0.2011,\n",
      "          0.1131, -0.0505, -0.2193,  0.2211,  0.3423, -0.1574, -0.4652,  0.2528,\n",
      "         -0.4594,  0.1789, -0.4587, -0.1004,  0.4327,  0.1572,  0.1019,  0.1318,\n",
      "          0.3062,  0.3306,  0.1911,  0.4272,  0.0892,  0.2553,  0.4253,  0.1659],\n",
      "        [ 0.4953,  0.0201, -0.4571, -0.3099,  0.1722, -0.3074, -0.2650, -0.0343,\n",
      "          0.2155,  0.1219,  0.4715,  0.1549,  0.0180, -0.3136, -0.1017,  0.0442,\n",
      "          0.3862,  0.0915,  0.0245, -0.4862,  0.2971, -0.4279, -0.0600, -0.3252,\n",
      "          0.0575,  0.0737, -0.1541,  0.0132, -0.3831,  0.1708,  0.1641,  0.3259,\n",
      "         -0.0180, -0.1533,  0.1602,  0.0328,  0.3808,  0.0183, -0.4225,  0.2609,\n",
      "         -0.2920, -0.3166,  0.0291,  0.4230,  0.3254, -0.1631,  0.0777,  0.0084,\n",
      "         -0.4249,  0.2199,  0.3004,  0.0443,  0.3836,  0.0263,  0.2958, -0.1507,\n",
      "          0.0537, -0.0687,  0.1514, -0.4911,  0.0052,  0.0761, -0.1913,  0.1853]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([2, 16])\n",
      "tensor([[ 0.1988, -0.0043,  0.4960, -0.2726,  0.2577, -0.1399,  0.0490, -0.2605,\n",
      "         -0.2334,  0.1966, -0.3514, -0.0269, -0.2524, -0.0522,  0.3142, -0.0619],\n",
      "        [-0.0475, -0.2074, -0.3462,  0.3039,  0.0092, -0.2067,  0.4334, -0.2698,\n",
      "         -0.4175,  0.4882,  0.1428,  0.1868, -0.2379, -0.2610,  0.2044,  0.1492]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([2, 16])\n",
      "tensor([[-0.2254,  0.2467,  0.3906, -0.3988, -0.2671,  0.3635, -0.2941, -0.4413,\n",
      "         -0.2717, -0.1500,  0.1175, -0.0211, -0.1650,  0.1372, -0.1467, -0.4682],\n",
      "        [ 0.3972, -0.1485, -0.0114,  0.1299,  0.2137,  0.2654,  0.2137, -0.3936,\n",
      "         -0.4087,  0.1286,  0.3506, -0.1719, -0.0443, -0.4216, -0.0135,  0.2737]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([2, 96])\n",
      "tensor([[ 0.0352,  0.2435, -0.1058,  0.3529,  0.0723,  0.2023,  0.3687, -0.4046,\n",
      "         -0.4893, -0.1057,  0.0078, -0.4062,  0.2501,  0.4469,  0.2588,  0.3636,\n",
      "         -0.0119, -0.1215,  0.1585,  0.4206, -0.1848,  0.1778,  0.4221, -0.1031,\n",
      "          0.1428,  0.2237, -0.3037, -0.0875,  0.3658,  0.3300,  0.1022, -0.1516,\n",
      "         -0.2875, -0.3124, -0.1595, -0.0667, -0.0748, -0.3298, -0.3057,  0.2011,\n",
      "          0.1131, -0.0505, -0.2193,  0.2211,  0.3423, -0.1574, -0.4652,  0.2528,\n",
      "         -0.4594,  0.1789, -0.4587, -0.1004,  0.4327,  0.1572,  0.1019,  0.1318,\n",
      "          0.3062,  0.3306,  0.1911,  0.4272,  0.0892,  0.2553,  0.4253,  0.1659,\n",
      "          0.1988, -0.0043,  0.4960, -0.2726,  0.2577, -0.1399,  0.0490, -0.2605,\n",
      "         -0.2334,  0.1966, -0.3514, -0.0269, -0.2524, -0.0522,  0.3142, -0.0619,\n",
      "         -0.2254,  0.2467,  0.3906, -0.3988, -0.2671,  0.3635, -0.2941, -0.4413,\n",
      "         -0.2717, -0.1500,  0.1175, -0.0211, -0.1650,  0.1372, -0.1467, -0.4682],\n",
      "        [ 0.4953,  0.0201, -0.4571, -0.3099,  0.1722, -0.3074, -0.2650, -0.0343,\n",
      "          0.2155,  0.1219,  0.4715,  0.1549,  0.0180, -0.3136, -0.1017,  0.0442,\n",
      "          0.3862,  0.0915,  0.0245, -0.4862,  0.2971, -0.4279, -0.0600, -0.3252,\n",
      "          0.0575,  0.0737, -0.1541,  0.0132, -0.3831,  0.1708,  0.1641,  0.3259,\n",
      "         -0.0180, -0.1533,  0.1602,  0.0328,  0.3808,  0.0183, -0.4225,  0.2609,\n",
      "         -0.2920, -0.3166,  0.0291,  0.4230,  0.3254, -0.1631,  0.0777,  0.0084,\n",
      "         -0.4249,  0.2199,  0.3004,  0.0443,  0.3836,  0.0263,  0.2958, -0.1507,\n",
      "          0.0537, -0.0687,  0.1514, -0.4911,  0.0052,  0.0761, -0.1913,  0.1853,\n",
      "         -0.0475, -0.2074, -0.3462,  0.3039,  0.0092, -0.2067,  0.4334, -0.2698,\n",
      "         -0.4175,  0.4882,  0.1428,  0.1868, -0.2379, -0.2610,  0.2044,  0.1492,\n",
      "          0.3972, -0.1485, -0.0114,  0.1299,  0.2137,  0.2654,  0.2137, -0.3936,\n",
      "         -0.4087,  0.1286,  0.3506, -0.1719, -0.0443, -0.4216, -0.0135,  0.2737]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 96])\n",
      "tensor([[[-0.6561, -0.1711,  0.6492,  0.1869, -0.2639,  0.2735,  0.1198,\n",
      "           0.2854,  0.0155, -0.0932, -0.6092,  0.0437, -0.1723,  0.1355,\n",
      "          -0.0240, -0.2736, -0.4881, -0.0449, -0.1260,  0.3724, -0.2707,\n",
      "           0.4426, -0.1749,  0.4785, -0.1589, -0.2280,  0.3787,  0.0352,\n",
      "           0.2730, -0.4158, -0.2713, -0.3274,  0.1946,  0.3830, -0.1102,\n",
      "          -0.0023, -0.4437,  0.1732,  0.7241, -0.4545,  0.3070,  0.4362,\n",
      "           0.0935, -0.6742, -0.6215,  0.3030,  0.1779, -0.1615,  0.8189,\n",
      "          -0.3887, -0.1116,  0.0031, -0.7500, -0.1276, -0.4401,  0.1146,\n",
      "          -0.2514, -0.1091, -0.3081,  0.3748, -0.0599, -0.2499, -0.0084,\n",
      "          -0.3366, -0.0577,  0.2685,  0.1480, -0.2271, -0.1654,  0.3485,\n",
      "          -0.5850,  0.5014,  0.6746, -0.7432,  0.0265, -0.2235,  0.4556,\n",
      "           0.3658, -0.4496, -0.1544, -0.3748,  0.0432, -0.2184,  0.0714,\n",
      "          -0.1147, -0.5571, -0.0986,  0.7679,  0.6861, -0.0755, -0.5196,\n",
      "           0.2330,  0.1552,  0.4587,  0.1048, -0.0717],\n",
      "         [ 0.7012,  0.0343, -0.6489, -0.4298,  0.2452, -0.4298, -0.3658,\n",
      "          -0.0583,  0.2930,  0.1698,  0.6669,  0.2092,  0.0315, -0.4326,\n",
      "          -0.1376,  0.0713,  0.5458,  0.1265,  0.0385, -0.6773,  0.4157,\n",
      "          -0.6007, -0.0747, -0.4623,  0.0847,  0.1096, -0.2252,  0.0166,\n",
      "          -0.5329,  0.2494,  0.2344,  0.4571, -0.0324, -0.2243,  0.2227,\n",
      "           0.0448,  0.5367,  0.0180, -0.6047,  0.3738, -0.4102, -0.4490,\n",
      "           0.0358,  0.6034,  0.4684, -0.2344,  0.0987,  0.0179, -0.6119,\n",
      "           0.3153,  0.4138,  0.0602,  0.5528,  0.0410,  0.4208, -0.2099,\n",
      "           0.0832, -0.0892,  0.2187, -0.6841,  0.0095,  0.1138, -0.2603,\n",
      "           0.2660, -0.0624, -0.2933, -0.4776,  0.4232,  0.0191, -0.2956,\n",
      "           0.6140, -0.3878, -0.5960,  0.6950,  0.1935,  0.2635, -0.3425,\n",
      "          -0.3703,  0.2966,  0.2095,  0.5562, -0.2040, -0.0067,  0.1740,\n",
      "           0.2958,  0.3840,  0.2952, -0.5672, -0.5844,  0.1783,  0.4985,\n",
      "          -0.2436, -0.0666, -0.5928, -0.0226,  0.3758],\n",
      "         [ 0.3211,  0.3278, -0.3917,  0.2866,  0.1897,  0.0921,  0.3321,\n",
      "          -0.5452, -0.5165, -0.0697,  0.2722, -0.4421,  0.3353,  0.4068,\n",
      "           0.2800,  0.4974,  0.1992, -0.1071,  0.2198,  0.2768, -0.0752,\n",
      "          -0.0066,  0.5156, -0.3149,  0.2177,  0.3319, -0.4805, -0.1064,\n",
      "           0.2628,  0.5240,  0.2241, -0.0160, -0.3839, -0.4915, -0.1184,\n",
      "          -0.0685,  0.1144, -0.4187, -0.6324,  0.4065, -0.0153, -0.2417,\n",
      "          -0.2690,  0.5226,  0.6260, -0.2953, -0.5617,  0.3333, -0.8336,\n",
      "           0.3549, -0.4295, -0.1059,  0.7759,  0.2191,  0.2969,  0.0876,\n",
      "           0.4279,  0.3917,  0.3327,  0.2825,  0.1189,  0.3743,  0.4467,\n",
      "           0.3187,  0.2321, -0.1209,  0.4525, -0.1855,  0.3402, -0.2968,\n",
      "           0.3046, -0.4887, -0.5356,  0.5270, -0.3775,  0.0688, -0.4605,\n",
      "          -0.2129,  0.5223,  0.0025, -0.0724,  0.2383,  0.5016, -0.4464,\n",
      "          -0.2285,  0.6203, -0.2636, -0.7926, -0.5805, -0.1236,  0.3477,\n",
      "          -0.1230, -0.2392, -0.0559, -0.1982, -0.4567]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 64])\n",
      "tensor([[[-0.6561, -0.1711,  0.6492,  0.1869, -0.2639,  0.2735,  0.1198,\n",
      "           0.2854,  0.0155, -0.0932, -0.6092,  0.0437, -0.1723,  0.1355,\n",
      "          -0.0240, -0.2736, -0.4881, -0.0449, -0.1260,  0.3724, -0.2707,\n",
      "           0.4426, -0.1749,  0.4785, -0.1589, -0.2280,  0.3787,  0.0352,\n",
      "           0.2730, -0.4158, -0.2713, -0.3274,  0.1946,  0.3830, -0.1102,\n",
      "          -0.0023, -0.4437,  0.1732,  0.7241, -0.4545,  0.3070,  0.4362,\n",
      "           0.0935, -0.6742, -0.6215,  0.3030,  0.1779, -0.1615,  0.8189,\n",
      "          -0.3887, -0.1116,  0.0031, -0.7500, -0.1276, -0.4401,  0.1146,\n",
      "          -0.2514, -0.1091, -0.3081,  0.3748, -0.0599, -0.2499, -0.0084,\n",
      "          -0.3366],\n",
      "         [ 0.7012,  0.0343, -0.6489, -0.4298,  0.2452, -0.4298, -0.3658,\n",
      "          -0.0583,  0.2930,  0.1698,  0.6669,  0.2092,  0.0315, -0.4326,\n",
      "          -0.1376,  0.0713,  0.5458,  0.1265,  0.0385, -0.6773,  0.4157,\n",
      "          -0.6007, -0.0747, -0.4623,  0.0847,  0.1096, -0.2252,  0.0166,\n",
      "          -0.5329,  0.2494,  0.2344,  0.4571, -0.0324, -0.2243,  0.2227,\n",
      "           0.0448,  0.5367,  0.0180, -0.6047,  0.3738, -0.4102, -0.4490,\n",
      "           0.0358,  0.6034,  0.4684, -0.2344,  0.0987,  0.0179, -0.6119,\n",
      "           0.3153,  0.4138,  0.0602,  0.5528,  0.0410,  0.4208, -0.2099,\n",
      "           0.0832, -0.0892,  0.2187, -0.6841,  0.0095,  0.1138, -0.2603,\n",
      "           0.2660],\n",
      "         [ 0.3211,  0.3278, -0.3917,  0.2866,  0.1897,  0.0921,  0.3321,\n",
      "          -0.5452, -0.5165, -0.0697,  0.2722, -0.4421,  0.3353,  0.4068,\n",
      "           0.2800,  0.4974,  0.1992, -0.1071,  0.2198,  0.2768, -0.0752,\n",
      "          -0.0066,  0.5156, -0.3149,  0.2177,  0.3319, -0.4805, -0.1064,\n",
      "           0.2628,  0.5240,  0.2241, -0.0160, -0.3839, -0.4915, -0.1184,\n",
      "          -0.0685,  0.1144, -0.4187, -0.6324,  0.4065, -0.0153, -0.2417,\n",
      "          -0.2690,  0.5226,  0.6260, -0.2953, -0.5617,  0.3333, -0.8336,\n",
      "           0.3549, -0.4295, -0.1059,  0.7759,  0.2191,  0.2969,  0.0876,\n",
      "           0.4279,  0.3917,  0.3327,  0.2825,  0.1189,  0.3743,  0.4467,\n",
      "           0.3187]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.0577,  0.2685,  0.1480, -0.2271, -0.1654,  0.3485, -0.5850,\n",
      "           0.5014,  0.6746, -0.7432,  0.0265, -0.2235,  0.4556,  0.3658,\n",
      "          -0.4496, -0.1544],\n",
      "         [-0.0624, -0.2933, -0.4776,  0.4232,  0.0191, -0.2956,  0.6140,\n",
      "          -0.3878, -0.5960,  0.6950,  0.1935,  0.2635, -0.3425, -0.3703,\n",
      "           0.2966,  0.2095],\n",
      "         [ 0.2321, -0.1209,  0.4525, -0.1855,  0.3402, -0.2968,  0.3046,\n",
      "          -0.4887, -0.5356,  0.5270, -0.3775,  0.0688, -0.4605, -0.2129,\n",
      "           0.5223,  0.0025]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.3748,  0.0432, -0.2184,  0.0714, -0.1147, -0.5571, -0.0986,\n",
      "           0.7679,  0.6861, -0.0755, -0.5196,  0.2330,  0.1552,  0.4587,\n",
      "           0.1048, -0.0717],\n",
      "         [ 0.5562, -0.2040, -0.0067,  0.1740,  0.2958,  0.3840,  0.2952,\n",
      "          -0.5672, -0.5844,  0.1783,  0.4985, -0.2436, -0.0666, -0.5928,\n",
      "          -0.0226,  0.3758],\n",
      "         [-0.0724,  0.2383,  0.5016, -0.4464, -0.2285,  0.6203, -0.2636,\n",
      "          -0.7926, -0.5805, -0.1236,  0.3477, -0.1230, -0.2392, -0.0559,\n",
      "          -0.1982, -0.4567]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[-0.6561, -0.1711,  0.6492,  0.1869, -0.2639,  0.2735,  0.1198,\n",
      "            0.2854,  0.0155, -0.0932, -0.6092,  0.0437, -0.1723,  0.1355,\n",
      "           -0.0240, -0.2736],\n",
      "          [-0.4881, -0.0449, -0.1260,  0.3724, -0.2707,  0.4426, -0.1749,\n",
      "            0.4785, -0.1589, -0.2280,  0.3787,  0.0352,  0.2730, -0.4158,\n",
      "           -0.2713, -0.3274],\n",
      "          [ 0.1946,  0.3830, -0.1102, -0.0023, -0.4437,  0.1732,  0.7241,\n",
      "           -0.4545,  0.3070,  0.4362,  0.0935, -0.6742, -0.6215,  0.3030,\n",
      "            0.1779, -0.1615],\n",
      "          [ 0.8189, -0.3887, -0.1116,  0.0031, -0.7500, -0.1276, -0.4401,\n",
      "            0.1146, -0.2514, -0.1091, -0.3081,  0.3748, -0.0599, -0.2499,\n",
      "           -0.0084, -0.3366]],\n",
      "\n",
      "         [[ 0.7012,  0.0343, -0.6489, -0.4298,  0.2452, -0.4298, -0.3658,\n",
      "           -0.0583,  0.2930,  0.1698,  0.6669,  0.2092,  0.0315, -0.4326,\n",
      "           -0.1376,  0.0713],\n",
      "          [ 0.5458,  0.1265,  0.0385, -0.6773,  0.4157, -0.6007, -0.0747,\n",
      "           -0.4623,  0.0847,  0.1096, -0.2252,  0.0166, -0.5329,  0.2494,\n",
      "            0.2344,  0.4571],\n",
      "          [-0.0324, -0.2243,  0.2227,  0.0448,  0.5367,  0.0180, -0.6047,\n",
      "            0.3738, -0.4102, -0.4490,  0.0358,  0.6034,  0.4684, -0.2344,\n",
      "            0.0987,  0.0179],\n",
      "          [-0.6119,  0.3153,  0.4138,  0.0602,  0.5528,  0.0410,  0.4208,\n",
      "           -0.2099,  0.0832, -0.0892,  0.2187, -0.6841,  0.0095,  0.1138,\n",
      "           -0.2603,  0.2660]],\n",
      "\n",
      "         [[ 0.3211,  0.3278, -0.3917,  0.2866,  0.1897,  0.0921,  0.3321,\n",
      "           -0.5452, -0.5165, -0.0697,  0.2722, -0.4421,  0.3353,  0.4068,\n",
      "            0.2800,  0.4974],\n",
      "          [ 0.1992, -0.1071,  0.2198,  0.2768, -0.0752, -0.0066,  0.5156,\n",
      "           -0.3149,  0.2177,  0.3319, -0.4805, -0.1064,  0.2628,  0.5240,\n",
      "            0.2241, -0.0160],\n",
      "          [-0.3839, -0.4915, -0.1184, -0.0685,  0.1144, -0.4187, -0.6324,\n",
      "            0.4065, -0.0153, -0.2417, -0.2690,  0.5226,  0.6260, -0.2953,\n",
      "           -0.5617,  0.3333],\n",
      "          [-0.8336,  0.3549, -0.4295, -0.1059,  0.7759,  0.2191,  0.2969,\n",
      "            0.0876,  0.4279,  0.3917,  0.3327,  0.2825,  0.1189,  0.3743,\n",
      "            0.4467,  0.3187]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 16])\n",
      "tensor([[[[-0.0577,  0.2685,  0.1480, -0.2271, -0.1654,  0.3485, -0.5850,\n",
      "            0.5014,  0.6746, -0.7432,  0.0265, -0.2235,  0.4556,  0.3658,\n",
      "           -0.4496, -0.1544]],\n",
      "\n",
      "         [[-0.0624, -0.2933, -0.4776,  0.4232,  0.0191, -0.2956,  0.6140,\n",
      "           -0.3878, -0.5960,  0.6950,  0.1935,  0.2635, -0.3425, -0.3703,\n",
      "            0.2966,  0.2095]],\n",
      "\n",
      "         [[ 0.2321, -0.1209,  0.4525, -0.1855,  0.3402, -0.2968,  0.3046,\n",
      "           -0.4887, -0.5356,  0.5270, -0.3775,  0.0688, -0.4605, -0.2129,\n",
      "            0.5223,  0.0025]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 16])\n",
      "tensor([[[[-0.3748,  0.0432, -0.2184,  0.0714, -0.1147, -0.5571, -0.0986,\n",
      "            0.7679,  0.6861, -0.0755, -0.5196,  0.2330,  0.1552,  0.4587,\n",
      "            0.1048, -0.0717]],\n",
      "\n",
      "         [[ 0.5562, -0.2040, -0.0067,  0.1740,  0.2958,  0.3840,  0.2952,\n",
      "           -0.5672, -0.5844,  0.1783,  0.4985, -0.2436, -0.0666, -0.5928,\n",
      "           -0.0226,  0.3758]],\n",
      "\n",
      "         [[-0.0724,  0.2383,  0.5016, -0.4464, -0.2285,  0.6203, -0.2636,\n",
      "           -0.7926, -0.5805, -0.1236,  0.3477, -0.1230, -0.2392, -0.0559,\n",
      "           -0.1982, -0.4567]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[-0.6561, -0.1711,  0.6492,  0.1869, -0.2639,  0.2735,  0.1198,\n",
      "            0.2854,  0.0155, -0.0932, -0.6092,  0.0437, -0.1723,  0.1355,\n",
      "           -0.0240, -0.2736],\n",
      "          [-0.4881, -0.0449, -0.1260,  0.3724, -0.2707,  0.4426, -0.1749,\n",
      "            0.4785, -0.1589, -0.2280,  0.3787,  0.0352,  0.2730, -0.4158,\n",
      "           -0.2713, -0.3274],\n",
      "          [ 0.1946,  0.3830, -0.1102, -0.0023, -0.4437,  0.1732,  0.7241,\n",
      "           -0.4545,  0.3070,  0.4362,  0.0935, -0.6742, -0.6215,  0.3030,\n",
      "            0.1779, -0.1615],\n",
      "          [ 0.8189, -0.3887, -0.1116,  0.0031, -0.7500, -0.1276, -0.4401,\n",
      "            0.1146, -0.2514, -0.1091, -0.3081,  0.3748, -0.0599, -0.2499,\n",
      "           -0.0084, -0.3366]],\n",
      "\n",
      "         [[ 0.1324, -0.0615, -0.8241, -0.4600,  0.2409, -0.4048, -0.3613,\n",
      "           -0.0595,  0.7484,  0.1620,  0.4320,  0.1299,  0.0558, -0.4561,\n",
      "           -0.1491,  0.0702],\n",
      "          [ 0.2236,  0.0486,  0.1066, -0.6696,  0.4668, -0.6138, -0.0821,\n",
      "           -0.4703,  0.5051,  0.1602, -0.2020, -0.1035, -0.4887,  0.2153,\n",
      "            0.2320,  0.4488],\n",
      "          [ 0.3277,  0.0496,  0.2005, -0.0627,  0.4872,  0.0311, -0.6075,\n",
      "            0.3734, -0.2489, -0.4994,  0.1033,  0.6018,  0.5196, -0.2330,\n",
      "            0.0795,  0.0246],\n",
      "          [-0.4007,  0.3143,  0.3252,  0.1803,  0.5491,  0.0346,  0.4288,\n",
      "           -0.2146, -0.4699,  0.0927,  0.3365, -0.6626,  0.0646,  0.1159,\n",
      "           -0.2468,  0.2623]],\n",
      "\n",
      "         [[ 0.3360,  0.2043, -0.4768,  0.4226,  0.1193,  0.0459,  0.3138,\n",
      "           -0.5626,  0.5069,  0.2657, -0.0120, -0.3147,  0.3663,  0.4146,\n",
      "            0.3004,  0.4777],\n",
      "          [-0.2808, -0.3456,  0.4613,  0.2965, -0.1259, -0.0654,  0.5004,\n",
      "           -0.3141,  0.0905,  0.0466, -0.2576, -0.0034,  0.2426,  0.5200,\n",
      "            0.2563, -0.0272],\n",
      "          [ 0.1737,  0.0060,  0.0635, -0.2462, -0.0122, -0.3829, -0.5956,\n",
      "            0.3944, -0.3427, -0.5477, -0.2870,  0.4660,  0.6363, -0.3404,\n",
      "           -0.6005,  0.3476],\n",
      "          [-0.0422, -0.2002, -0.5431, -0.1976,  0.7368,  0.1757,  0.2681,\n",
      "            0.0762, -0.9361,  0.4891,  0.0145,  0.2280,  0.2707,  0.3965,\n",
      "            0.4646,  0.3217]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 16])\n",
      "tensor([[[[-5.7660e-02,  2.6845e-01,  1.4805e-01, -2.2712e-01, -1.6545e-01,\n",
      "            3.4847e-01, -5.8497e-01,  5.0135e-01,  6.7456e-01, -7.4323e-01,\n",
      "            2.6468e-02, -2.2347e-01,  4.5562e-01,  3.6577e-01, -4.4958e-01,\n",
      "           -1.5441e-01]],\n",
      "\n",
      "         [[ 4.6779e-01, -6.1870e-01, -5.1412e-01,  3.6992e-01,  5.3237e-02,\n",
      "           -2.7433e-01,  6.0436e-01, -3.9144e-01, -3.7450e-01,  4.3159e-01,\n",
      "            3.5398e-02,  3.3420e-01, -3.3884e-01, -3.8633e-01,  3.1588e-01,\n",
      "            2.0256e-01]],\n",
      "\n",
      "         [[ 3.9041e-01, -5.2760e-01,  5.8817e-01, -1.9785e-01,  4.2486e-01,\n",
      "           -2.7105e-01,  2.7100e-01, -4.8849e-01,  4.3387e-01,  1.1836e-01,\n",
      "           -3.7023e-02, -5.0612e-05, -3.8373e-01, -2.4487e-01,  5.4048e-01,\n",
      "           -1.4884e-02]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[-5.7660e-02,  2.6845e-01,  1.4805e-01, -2.2712e-01, -1.6545e-01,\n",
      "            3.4847e-01, -5.8497e-01,  5.0135e-01,  6.7456e-01, -7.4323e-01,\n",
      "            2.6468e-02, -2.2347e-01,  4.5562e-01,  3.6577e-01, -4.4958e-01,\n",
      "           -1.5441e-01],\n",
      "          [-5.7660e-02,  2.6845e-01,  1.4805e-01, -2.2712e-01, -1.6545e-01,\n",
      "            3.4847e-01, -5.8497e-01,  5.0135e-01,  6.7456e-01, -7.4323e-01,\n",
      "            2.6468e-02, -2.2347e-01,  4.5562e-01,  3.6577e-01, -4.4958e-01,\n",
      "           -1.5441e-01],\n",
      "          [-5.7660e-02,  2.6845e-01,  1.4805e-01, -2.2712e-01, -1.6545e-01,\n",
      "            3.4847e-01, -5.8497e-01,  5.0135e-01,  6.7456e-01, -7.4323e-01,\n",
      "            2.6468e-02, -2.2347e-01,  4.5562e-01,  3.6577e-01, -4.4958e-01,\n",
      "           -1.5441e-01],\n",
      "          [-5.7660e-02,  2.6845e-01,  1.4805e-01, -2.2712e-01, -1.6545e-01,\n",
      "            3.4847e-01, -5.8497e-01,  5.0135e-01,  6.7456e-01, -7.4323e-01,\n",
      "            2.6468e-02, -2.2347e-01,  4.5562e-01,  3.6577e-01, -4.4958e-01,\n",
      "           -1.5441e-01]],\n",
      "\n",
      "         [[ 4.6779e-01, -6.1870e-01, -5.1412e-01,  3.6992e-01,  5.3237e-02,\n",
      "           -2.7433e-01,  6.0436e-01, -3.9144e-01, -3.7450e-01,  4.3159e-01,\n",
      "            3.5398e-02,  3.3420e-01, -3.3884e-01, -3.8633e-01,  3.1588e-01,\n",
      "            2.0256e-01],\n",
      "          [ 4.6779e-01, -6.1870e-01, -5.1412e-01,  3.6992e-01,  5.3237e-02,\n",
      "           -2.7433e-01,  6.0436e-01, -3.9144e-01, -3.7450e-01,  4.3159e-01,\n",
      "            3.5398e-02,  3.3420e-01, -3.3884e-01, -3.8633e-01,  3.1588e-01,\n",
      "            2.0256e-01],\n",
      "          [ 4.6779e-01, -6.1870e-01, -5.1412e-01,  3.6992e-01,  5.3237e-02,\n",
      "           -2.7433e-01,  6.0436e-01, -3.9144e-01, -3.7450e-01,  4.3159e-01,\n",
      "            3.5398e-02,  3.3420e-01, -3.3884e-01, -3.8633e-01,  3.1588e-01,\n",
      "            2.0256e-01],\n",
      "          [ 4.6779e-01, -6.1870e-01, -5.1412e-01,  3.6992e-01,  5.3237e-02,\n",
      "           -2.7433e-01,  6.0436e-01, -3.9144e-01, -3.7450e-01,  4.3159e-01,\n",
      "            3.5398e-02,  3.3420e-01, -3.3884e-01, -3.8633e-01,  3.1588e-01,\n",
      "            2.0256e-01]],\n",
      "\n",
      "         [[ 3.9041e-01, -5.2760e-01,  5.8817e-01, -1.9785e-01,  4.2486e-01,\n",
      "           -2.7105e-01,  2.7100e-01, -4.8849e-01,  4.3387e-01,  1.1836e-01,\n",
      "           -3.7023e-02, -5.0612e-05, -3.8373e-01, -2.4487e-01,  5.4048e-01,\n",
      "           -1.4884e-02],\n",
      "          [ 3.9041e-01, -5.2760e-01,  5.8817e-01, -1.9785e-01,  4.2486e-01,\n",
      "           -2.7105e-01,  2.7100e-01, -4.8849e-01,  4.3387e-01,  1.1836e-01,\n",
      "           -3.7023e-02, -5.0612e-05, -3.8373e-01, -2.4487e-01,  5.4048e-01,\n",
      "           -1.4884e-02],\n",
      "          [ 3.9041e-01, -5.2760e-01,  5.8817e-01, -1.9785e-01,  4.2486e-01,\n",
      "           -2.7105e-01,  2.7100e-01, -4.8849e-01,  4.3387e-01,  1.1836e-01,\n",
      "           -3.7023e-02, -5.0612e-05, -3.8373e-01, -2.4487e-01,  5.4048e-01,\n",
      "           -1.4884e-02],\n",
      "          [ 3.9041e-01, -5.2760e-01,  5.8817e-01, -1.9785e-01,  4.2486e-01,\n",
      "           -2.7105e-01,  2.7100e-01, -4.8849e-01,  4.3387e-01,  1.1836e-01,\n",
      "           -3.7023e-02, -5.0612e-05, -3.8373e-01, -2.4487e-01,  5.4048e-01,\n",
      "           -1.4884e-02]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[-0.3748,  0.0432, -0.2184,  0.0714, -0.1147, -0.5571, -0.0986,\n",
      "            0.7679,  0.6861, -0.0755, -0.5196,  0.2330,  0.1552,  0.4587,\n",
      "            0.1048, -0.0717],\n",
      "          [-0.3748,  0.0432, -0.2184,  0.0714, -0.1147, -0.5571, -0.0986,\n",
      "            0.7679,  0.6861, -0.0755, -0.5196,  0.2330,  0.1552,  0.4587,\n",
      "            0.1048, -0.0717],\n",
      "          [-0.3748,  0.0432, -0.2184,  0.0714, -0.1147, -0.5571, -0.0986,\n",
      "            0.7679,  0.6861, -0.0755, -0.5196,  0.2330,  0.1552,  0.4587,\n",
      "            0.1048, -0.0717],\n",
      "          [-0.3748,  0.0432, -0.2184,  0.0714, -0.1147, -0.5571, -0.0986,\n",
      "            0.7679,  0.6861, -0.0755, -0.5196,  0.2330,  0.1552,  0.4587,\n",
      "            0.1048, -0.0717]],\n",
      "\n",
      "         [[ 0.5562, -0.2040, -0.0067,  0.1740,  0.2958,  0.3840,  0.2952,\n",
      "           -0.5672, -0.5844,  0.1783,  0.4985, -0.2436, -0.0666, -0.5928,\n",
      "           -0.0226,  0.3758],\n",
      "          [ 0.5562, -0.2040, -0.0067,  0.1740,  0.2958,  0.3840,  0.2952,\n",
      "           -0.5672, -0.5844,  0.1783,  0.4985, -0.2436, -0.0666, -0.5928,\n",
      "           -0.0226,  0.3758],\n",
      "          [ 0.5562, -0.2040, -0.0067,  0.1740,  0.2958,  0.3840,  0.2952,\n",
      "           -0.5672, -0.5844,  0.1783,  0.4985, -0.2436, -0.0666, -0.5928,\n",
      "           -0.0226,  0.3758],\n",
      "          [ 0.5562, -0.2040, -0.0067,  0.1740,  0.2958,  0.3840,  0.2952,\n",
      "           -0.5672, -0.5844,  0.1783,  0.4985, -0.2436, -0.0666, -0.5928,\n",
      "           -0.0226,  0.3758]],\n",
      "\n",
      "         [[-0.0724,  0.2383,  0.5016, -0.4464, -0.2285,  0.6203, -0.2636,\n",
      "           -0.7926, -0.5805, -0.1236,  0.3477, -0.1230, -0.2392, -0.0559,\n",
      "           -0.1982, -0.4567],\n",
      "          [-0.0724,  0.2383,  0.5016, -0.4464, -0.2285,  0.6203, -0.2636,\n",
      "           -0.7926, -0.5805, -0.1236,  0.3477, -0.1230, -0.2392, -0.0559,\n",
      "           -0.1982, -0.4567],\n",
      "          [-0.0724,  0.2383,  0.5016, -0.4464, -0.2285,  0.6203, -0.2636,\n",
      "           -0.7926, -0.5805, -0.1236,  0.3477, -0.1230, -0.2392, -0.0559,\n",
      "           -0.1982, -0.4567],\n",
      "          [-0.0724,  0.2383,  0.5016, -0.4464, -0.2285,  0.6203, -0.2636,\n",
      "           -0.7926, -0.5805, -0.1236,  0.3477, -0.1230, -0.2392, -0.0559,\n",
      "           -0.1982, -0.4567]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[-0.6561, -0.1711,  0.6492,  0.1869, -0.2639,  0.2735,  0.1198,\n",
      "            0.2854,  0.0155, -0.0932, -0.6092,  0.0437, -0.1723,  0.1355,\n",
      "           -0.0240, -0.2736],\n",
      "          [ 0.1324, -0.0615, -0.8241, -0.4600,  0.2409, -0.4048, -0.3613,\n",
      "           -0.0595,  0.7484,  0.1620,  0.4320,  0.1299,  0.0558, -0.4561,\n",
      "           -0.1491,  0.0702],\n",
      "          [ 0.3360,  0.2043, -0.4768,  0.4226,  0.1193,  0.0459,  0.3138,\n",
      "           -0.5626,  0.5069,  0.2657, -0.0120, -0.3147,  0.3663,  0.4146,\n",
      "            0.3004,  0.4777]],\n",
      "\n",
      "         [[-0.4881, -0.0449, -0.1260,  0.3724, -0.2707,  0.4426, -0.1749,\n",
      "            0.4785, -0.1589, -0.2280,  0.3787,  0.0352,  0.2730, -0.4158,\n",
      "           -0.2713, -0.3274],\n",
      "          [ 0.2236,  0.0486,  0.1066, -0.6696,  0.4668, -0.6138, -0.0821,\n",
      "           -0.4703,  0.5051,  0.1602, -0.2020, -0.1035, -0.4887,  0.2153,\n",
      "            0.2320,  0.4488],\n",
      "          [-0.2808, -0.3456,  0.4613,  0.2965, -0.1259, -0.0654,  0.5004,\n",
      "           -0.3141,  0.0905,  0.0466, -0.2576, -0.0034,  0.2426,  0.5200,\n",
      "            0.2563, -0.0272]],\n",
      "\n",
      "         [[ 0.1946,  0.3830, -0.1102, -0.0023, -0.4437,  0.1732,  0.7241,\n",
      "           -0.4545,  0.3070,  0.4362,  0.0935, -0.6742, -0.6215,  0.3030,\n",
      "            0.1779, -0.1615],\n",
      "          [ 0.3277,  0.0496,  0.2005, -0.0627,  0.4872,  0.0311, -0.6075,\n",
      "            0.3734, -0.2489, -0.4994,  0.1033,  0.6018,  0.5196, -0.2330,\n",
      "            0.0795,  0.0246],\n",
      "          [ 0.1737,  0.0060,  0.0635, -0.2462, -0.0122, -0.3829, -0.5956,\n",
      "            0.3944, -0.3427, -0.5477, -0.2870,  0.4660,  0.6363, -0.3404,\n",
      "           -0.6005,  0.3476]],\n",
      "\n",
      "         [[ 0.8189, -0.3887, -0.1116,  0.0031, -0.7500, -0.1276, -0.4401,\n",
      "            0.1146, -0.2514, -0.1091, -0.3081,  0.3748, -0.0599, -0.2499,\n",
      "           -0.0084, -0.3366],\n",
      "          [-0.4007,  0.3143,  0.3252,  0.1803,  0.5491,  0.0346,  0.4288,\n",
      "           -0.2146, -0.4699,  0.0927,  0.3365, -0.6626,  0.0646,  0.1159,\n",
      "           -0.2468,  0.2623],\n",
      "          [-0.0422, -0.2002, -0.5431, -0.1976,  0.7368,  0.1757,  0.2681,\n",
      "            0.0762, -0.9361,  0.4891,  0.0145,  0.2280,  0.2707,  0.3965,\n",
      "            0.4646,  0.3217]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[-5.7660e-02,  2.6845e-01,  1.4805e-01, -2.2712e-01, -1.6545e-01,\n",
      "            3.4847e-01, -5.8497e-01,  5.0135e-01,  6.7456e-01, -7.4323e-01,\n",
      "            2.6468e-02, -2.2347e-01,  4.5562e-01,  3.6577e-01, -4.4958e-01,\n",
      "           -1.5441e-01],\n",
      "          [ 4.6779e-01, -6.1870e-01, -5.1412e-01,  3.6992e-01,  5.3237e-02,\n",
      "           -2.7433e-01,  6.0436e-01, -3.9144e-01, -3.7450e-01,  4.3159e-01,\n",
      "            3.5398e-02,  3.3420e-01, -3.3884e-01, -3.8633e-01,  3.1588e-01,\n",
      "            2.0256e-01],\n",
      "          [ 3.9041e-01, -5.2760e-01,  5.8817e-01, -1.9785e-01,  4.2486e-01,\n",
      "           -2.7105e-01,  2.7100e-01, -4.8849e-01,  4.3387e-01,  1.1836e-01,\n",
      "           -3.7023e-02, -5.0612e-05, -3.8373e-01, -2.4487e-01,  5.4048e-01,\n",
      "           -1.4884e-02]],\n",
      "\n",
      "         [[-5.7660e-02,  2.6845e-01,  1.4805e-01, -2.2712e-01, -1.6545e-01,\n",
      "            3.4847e-01, -5.8497e-01,  5.0135e-01,  6.7456e-01, -7.4323e-01,\n",
      "            2.6468e-02, -2.2347e-01,  4.5562e-01,  3.6577e-01, -4.4958e-01,\n",
      "           -1.5441e-01],\n",
      "          [ 4.6779e-01, -6.1870e-01, -5.1412e-01,  3.6992e-01,  5.3237e-02,\n",
      "           -2.7433e-01,  6.0436e-01, -3.9144e-01, -3.7450e-01,  4.3159e-01,\n",
      "            3.5398e-02,  3.3420e-01, -3.3884e-01, -3.8633e-01,  3.1588e-01,\n",
      "            2.0256e-01],\n",
      "          [ 3.9041e-01, -5.2760e-01,  5.8817e-01, -1.9785e-01,  4.2486e-01,\n",
      "           -2.7105e-01,  2.7100e-01, -4.8849e-01,  4.3387e-01,  1.1836e-01,\n",
      "           -3.7023e-02, -5.0612e-05, -3.8373e-01, -2.4487e-01,  5.4048e-01,\n",
      "           -1.4884e-02]],\n",
      "\n",
      "         [[-5.7660e-02,  2.6845e-01,  1.4805e-01, -2.2712e-01, -1.6545e-01,\n",
      "            3.4847e-01, -5.8497e-01,  5.0135e-01,  6.7456e-01, -7.4323e-01,\n",
      "            2.6468e-02, -2.2347e-01,  4.5562e-01,  3.6577e-01, -4.4958e-01,\n",
      "           -1.5441e-01],\n",
      "          [ 4.6779e-01, -6.1870e-01, -5.1412e-01,  3.6992e-01,  5.3237e-02,\n",
      "           -2.7433e-01,  6.0436e-01, -3.9144e-01, -3.7450e-01,  4.3159e-01,\n",
      "            3.5398e-02,  3.3420e-01, -3.3884e-01, -3.8633e-01,  3.1588e-01,\n",
      "            2.0256e-01],\n",
      "          [ 3.9041e-01, -5.2760e-01,  5.8817e-01, -1.9785e-01,  4.2486e-01,\n",
      "           -2.7105e-01,  2.7100e-01, -4.8849e-01,  4.3387e-01,  1.1836e-01,\n",
      "           -3.7023e-02, -5.0612e-05, -3.8373e-01, -2.4487e-01,  5.4048e-01,\n",
      "           -1.4884e-02]],\n",
      "\n",
      "         [[-5.7660e-02,  2.6845e-01,  1.4805e-01, -2.2712e-01, -1.6545e-01,\n",
      "            3.4847e-01, -5.8497e-01,  5.0135e-01,  6.7456e-01, -7.4323e-01,\n",
      "            2.6468e-02, -2.2347e-01,  4.5562e-01,  3.6577e-01, -4.4958e-01,\n",
      "           -1.5441e-01],\n",
      "          [ 4.6779e-01, -6.1870e-01, -5.1412e-01,  3.6992e-01,  5.3237e-02,\n",
      "           -2.7433e-01,  6.0436e-01, -3.9144e-01, -3.7450e-01,  4.3159e-01,\n",
      "            3.5398e-02,  3.3420e-01, -3.3884e-01, -3.8633e-01,  3.1588e-01,\n",
      "            2.0256e-01],\n",
      "          [ 3.9041e-01, -5.2760e-01,  5.8817e-01, -1.9785e-01,  4.2486e-01,\n",
      "           -2.7105e-01,  2.7100e-01, -4.8849e-01,  4.3387e-01,  1.1836e-01,\n",
      "           -3.7023e-02, -5.0612e-05, -3.8373e-01, -2.4487e-01,  5.4048e-01,\n",
      "           -1.4884e-02]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[-0.3748,  0.0432, -0.2184,  0.0714, -0.1147, -0.5571, -0.0986,\n",
      "            0.7679,  0.6861, -0.0755, -0.5196,  0.2330,  0.1552,  0.4587,\n",
      "            0.1048, -0.0717],\n",
      "          [ 0.5562, -0.2040, -0.0067,  0.1740,  0.2958,  0.3840,  0.2952,\n",
      "           -0.5672, -0.5844,  0.1783,  0.4985, -0.2436, -0.0666, -0.5928,\n",
      "           -0.0226,  0.3758],\n",
      "          [-0.0724,  0.2383,  0.5016, -0.4464, -0.2285,  0.6203, -0.2636,\n",
      "           -0.7926, -0.5805, -0.1236,  0.3477, -0.1230, -0.2392, -0.0559,\n",
      "           -0.1982, -0.4567]],\n",
      "\n",
      "         [[-0.3748,  0.0432, -0.2184,  0.0714, -0.1147, -0.5571, -0.0986,\n",
      "            0.7679,  0.6861, -0.0755, -0.5196,  0.2330,  0.1552,  0.4587,\n",
      "            0.1048, -0.0717],\n",
      "          [ 0.5562, -0.2040, -0.0067,  0.1740,  0.2958,  0.3840,  0.2952,\n",
      "           -0.5672, -0.5844,  0.1783,  0.4985, -0.2436, -0.0666, -0.5928,\n",
      "           -0.0226,  0.3758],\n",
      "          [-0.0724,  0.2383,  0.5016, -0.4464, -0.2285,  0.6203, -0.2636,\n",
      "           -0.7926, -0.5805, -0.1236,  0.3477, -0.1230, -0.2392, -0.0559,\n",
      "           -0.1982, -0.4567]],\n",
      "\n",
      "         [[-0.3748,  0.0432, -0.2184,  0.0714, -0.1147, -0.5571, -0.0986,\n",
      "            0.7679,  0.6861, -0.0755, -0.5196,  0.2330,  0.1552,  0.4587,\n",
      "            0.1048, -0.0717],\n",
      "          [ 0.5562, -0.2040, -0.0067,  0.1740,  0.2958,  0.3840,  0.2952,\n",
      "           -0.5672, -0.5844,  0.1783,  0.4985, -0.2436, -0.0666, -0.5928,\n",
      "           -0.0226,  0.3758],\n",
      "          [-0.0724,  0.2383,  0.5016, -0.4464, -0.2285,  0.6203, -0.2636,\n",
      "           -0.7926, -0.5805, -0.1236,  0.3477, -0.1230, -0.2392, -0.0559,\n",
      "           -0.1982, -0.4567]],\n",
      "\n",
      "         [[-0.3748,  0.0432, -0.2184,  0.0714, -0.1147, -0.5571, -0.0986,\n",
      "            0.7679,  0.6861, -0.0755, -0.5196,  0.2330,  0.1552,  0.4587,\n",
      "            0.1048, -0.0717],\n",
      "          [ 0.5562, -0.2040, -0.0067,  0.1740,  0.2958,  0.3840,  0.2952,\n",
      "           -0.5672, -0.5844,  0.1783,  0.4985, -0.2436, -0.0666, -0.5928,\n",
      "           -0.0226,  0.3758],\n",
      "          [-0.0724,  0.2383,  0.5016, -0.4464, -0.2285,  0.6203, -0.2636,\n",
      "           -0.7926, -0.5805, -0.1236,  0.3477, -0.1230, -0.2392, -0.0559,\n",
      "           -0.1982, -0.4567]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[ 0.0839, -0.1760, -0.0180],\n",
      "          [ 0.0601,  0.0638,  0.0425],\n",
      "          [-0.0691,  0.1406,  0.0556]],\n",
      "\n",
      "         [[ 0.1658, -0.1311, -0.2714],\n",
      "          [-0.0973,  0.0462,  0.3320],\n",
      "          [-0.0794,  0.0400,  0.1279]],\n",
      "\n",
      "         [[-0.1583,  0.1056,  0.1091],\n",
      "          [ 0.1737, -0.1183, -0.0468],\n",
      "          [ 0.2294, -0.1555, -0.1996]],\n",
      "\n",
      "         [[-0.0044,  0.1434, -0.0067],\n",
      "          [-0.1023, -0.0511, -0.0294],\n",
      "          [-0.3255,  0.2597, -0.0484]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[ 8.3870e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 6.0132e-02,  6.3773e-02, -2.3820e+38],\n",
      "          [-6.9056e-02,  1.4061e-01,  5.5601e-02]],\n",
      "\n",
      "         [[ 1.6580e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [-9.7255e-02,  4.6160e-02, -2.3820e+38],\n",
      "          [-7.9415e-02,  3.9964e-02,  1.2786e-01]],\n",
      "\n",
      "         [[-1.5829e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 1.7367e-01, -1.1828e-01, -2.3820e+38],\n",
      "          [ 2.2939e-01, -1.5550e-01, -1.9957e-01]],\n",
      "\n",
      "         [[-4.4009e-03, -2.3820e+38, -2.3820e+38],\n",
      "          [-1.0228e-01, -5.1141e-02, -2.3820e+38],\n",
      "          [-3.2546e-01,  2.5970e-01, -4.8447e-02]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4991, 0.5009, 0.0000],\n",
      "          [0.2971, 0.3664, 0.3365]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4642, 0.5358, 0.0000],\n",
      "          [0.2979, 0.3356, 0.3665]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5725, 0.4275, 0.0000],\n",
      "          [0.4289, 0.2919, 0.2793]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4872, 0.5128, 0.0000],\n",
      "          [0.2430, 0.4363, 0.3206]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[-3.7476e-01,  4.3165e-02, -2.1839e-01,  7.1376e-02, -1.1471e-01,\n",
      "           -5.5712e-01, -9.8641e-02,  7.6793e-01,  6.8609e-01, -7.5455e-02,\n",
      "           -5.1961e-01,  2.3305e-01,  1.5524e-01,  4.5872e-01,  1.0478e-01,\n",
      "           -7.1665e-02],\n",
      "          [ 9.1548e-02, -8.0633e-02, -1.1237e-01,  1.2280e-01,  9.0913e-02,\n",
      "           -8.5723e-02,  9.8617e-02,  9.9157e-02,  4.9693e-02,  5.1646e-02,\n",
      "           -9.6046e-03, -5.7316e-03,  4.4123e-02, -6.8021e-02,  4.0977e-02,\n",
      "            1.5248e-01],\n",
      "          [ 6.8072e-02,  1.8290e-02,  1.0146e-01, -6.5248e-02, -2.6120e-03,\n",
      "            1.8390e-01, -9.8663e-03, -2.4640e-01, -2.0563e-01,  1.3225e-03,\n",
      "            1.4529e-01, -6.1419e-02, -5.8786e-02, -9.9740e-02, -4.3852e-02,\n",
      "           -3.7304e-02]],\n",
      "\n",
      "         [[-3.7476e-01,  4.3165e-02, -2.1839e-01,  7.1376e-02, -1.1471e-01,\n",
      "           -5.5712e-01, -9.8641e-02,  7.6793e-01,  6.8609e-01, -7.5455e-02,\n",
      "           -5.1961e-01,  2.3305e-01,  1.5524e-01,  4.5872e-01,  1.0478e-01,\n",
      "           -7.1665e-02],\n",
      "          [ 1.2402e-01, -8.9254e-02, -1.0499e-01,  1.2638e-01,  1.0523e-01,\n",
      "           -5.2896e-02,  1.1235e-01,  5.2586e-02,  5.3768e-03,  6.0497e-02,\n",
      "            2.5911e-02, -2.2360e-02,  3.6386e-02, -1.0470e-01,  3.6535e-02,\n",
      "            1.6809e-01],\n",
      "          [ 4.8509e-02,  3.1734e-02,  1.1652e-01, -8.3913e-02, -1.8641e-02,\n",
      "            1.9023e-01, -2.6914e-02, -2.5209e-01, -2.0452e-01, -7.9194e-03,\n",
      "            1.3996e-01, -5.7428e-02, -6.3783e-02, -8.2826e-02, -4.9012e-02,\n",
      "           -6.2595e-02]],\n",
      "\n",
      "         [[-3.7476e-01,  4.3165e-02, -2.1839e-01,  7.1376e-02, -1.1471e-01,\n",
      "           -5.5712e-01, -9.8641e-02,  7.6793e-01,  6.8609e-01, -7.5455e-02,\n",
      "           -5.1961e-01,  2.3305e-01,  1.5524e-01,  4.5872e-01,  1.0478e-01,\n",
      "           -7.1665e-02],\n",
      "          [ 2.3235e-02, -6.2497e-02, -1.2790e-01,  1.1526e-01,  6.0790e-02,\n",
      "           -1.5478e-01,  6.9719e-02,  1.9713e-01,  1.4292e-01,  3.3026e-02,\n",
      "           -8.4320e-02,  2.9249e-02,  6.0401e-02,  9.1451e-03,  5.0324e-02,\n",
      "            1.1964e-01],\n",
      "          [-1.8621e-02,  2.5537e-02,  4.4458e-02, -4.3256e-02, -2.6689e-02,\n",
      "            4.6350e-02, -2.9772e-02, -5.7546e-02, -3.8432e-02, -1.4834e-02,\n",
      "            1.9751e-02, -5.5079e-03, -1.9669e-02,  8.0953e-03, -1.7013e-02,\n",
      "           -4.8607e-02]],\n",
      "\n",
      "         [[-3.7476e-01,  4.3165e-02, -2.1839e-01,  7.1376e-02, -1.1471e-01,\n",
      "           -5.5712e-01, -9.8641e-02,  7.6793e-01,  6.8609e-01, -7.5455e-02,\n",
      "           -5.1961e-01,  2.3305e-01,  1.5524e-01,  4.5872e-01,  1.0478e-01,\n",
      "           -7.1665e-02],\n",
      "          [ 1.0260e-01, -8.3567e-02, -1.0986e-01,  1.2402e-01,  9.5787e-02,\n",
      "           -7.4550e-02,  1.0329e-01,  8.3306e-02,  3.4609e-02,  5.4659e-02,\n",
      "            2.4836e-03, -1.1391e-02,  4.1490e-02, -8.0506e-02,  3.9465e-02,\n",
      "            1.5779e-01],\n",
      "          [ 1.2838e-01, -2.1011e-03,  1.0481e-01, -4.9832e-02,  2.7912e-02,\n",
      "            2.3100e-01,  2.0302e-02, -3.1496e-01, -2.7435e-01,  1.9836e-02,\n",
      "            2.0271e-01, -8.9099e-02, -6.8028e-02, -1.6511e-01, -4.7942e-02,\n",
      "            1.2030e-04]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 64])\n",
      "tensor([[[-3.7476e-01,  4.3165e-02, -2.1839e-01,  7.1376e-02, -1.1471e-01,\n",
      "          -5.5712e-01, -9.8641e-02,  7.6793e-01,  6.8609e-01, -7.5455e-02,\n",
      "          -5.1961e-01,  2.3305e-01,  1.5524e-01,  4.5872e-01,  1.0478e-01,\n",
      "          -7.1665e-02, -3.7476e-01,  4.3165e-02, -2.1839e-01,  7.1376e-02,\n",
      "          -1.1471e-01, -5.5712e-01, -9.8641e-02,  7.6793e-01,  6.8609e-01,\n",
      "          -7.5455e-02, -5.1961e-01,  2.3305e-01,  1.5524e-01,  4.5872e-01,\n",
      "           1.0478e-01, -7.1665e-02, -3.7476e-01,  4.3165e-02, -2.1839e-01,\n",
      "           7.1376e-02, -1.1471e-01, -5.5712e-01, -9.8641e-02,  7.6793e-01,\n",
      "           6.8609e-01, -7.5455e-02, -5.1961e-01,  2.3305e-01,  1.5524e-01,\n",
      "           4.5872e-01,  1.0478e-01, -7.1665e-02, -3.7476e-01,  4.3165e-02,\n",
      "          -2.1839e-01,  7.1376e-02, -1.1471e-01, -5.5712e-01, -9.8641e-02,\n",
      "           7.6793e-01,  6.8609e-01, -7.5455e-02, -5.1961e-01,  2.3305e-01,\n",
      "           1.5524e-01,  4.5872e-01,  1.0478e-01, -7.1665e-02],\n",
      "         [ 9.1548e-02, -8.0633e-02, -1.1237e-01,  1.2280e-01,  9.0913e-02,\n",
      "          -8.5723e-02,  9.8617e-02,  9.9157e-02,  4.9693e-02,  5.1646e-02,\n",
      "          -9.6046e-03, -5.7316e-03,  4.4123e-02, -6.8021e-02,  4.0977e-02,\n",
      "           1.5248e-01,  1.2402e-01, -8.9254e-02, -1.0499e-01,  1.2638e-01,\n",
      "           1.0523e-01, -5.2896e-02,  1.1235e-01,  5.2586e-02,  5.3768e-03,\n",
      "           6.0497e-02,  2.5911e-02, -2.2360e-02,  3.6386e-02, -1.0470e-01,\n",
      "           3.6535e-02,  1.6809e-01,  2.3235e-02, -6.2497e-02, -1.2790e-01,\n",
      "           1.1526e-01,  6.0790e-02, -1.5478e-01,  6.9719e-02,  1.9713e-01,\n",
      "           1.4292e-01,  3.3026e-02, -8.4320e-02,  2.9249e-02,  6.0401e-02,\n",
      "           9.1451e-03,  5.0324e-02,  1.1964e-01,  1.0260e-01, -8.3567e-02,\n",
      "          -1.0986e-01,  1.2402e-01,  9.5787e-02, -7.4550e-02,  1.0329e-01,\n",
      "           8.3306e-02,  3.4609e-02,  5.4659e-02,  2.4836e-03, -1.1391e-02,\n",
      "           4.1490e-02, -8.0506e-02,  3.9465e-02,  1.5779e-01],\n",
      "         [ 6.8072e-02,  1.8290e-02,  1.0146e-01, -6.5248e-02, -2.6120e-03,\n",
      "           1.8390e-01, -9.8663e-03, -2.4640e-01, -2.0563e-01,  1.3225e-03,\n",
      "           1.4529e-01, -6.1419e-02, -5.8786e-02, -9.9740e-02, -4.3852e-02,\n",
      "          -3.7304e-02,  4.8509e-02,  3.1734e-02,  1.1652e-01, -8.3913e-02,\n",
      "          -1.8641e-02,  1.9023e-01, -2.6914e-02, -2.5209e-01, -2.0452e-01,\n",
      "          -7.9194e-03,  1.3996e-01, -5.7428e-02, -6.3783e-02, -8.2826e-02,\n",
      "          -4.9012e-02, -6.2595e-02, -1.8621e-02,  2.5537e-02,  4.4458e-02,\n",
      "          -4.3256e-02, -2.6689e-02,  4.6350e-02, -2.9772e-02, -5.7546e-02,\n",
      "          -3.8432e-02, -1.4834e-02,  1.9751e-02, -5.5079e-03, -1.9669e-02,\n",
      "           8.0953e-03, -1.7013e-02, -4.8607e-02,  1.2838e-01, -2.1011e-03,\n",
      "           1.0481e-01, -4.9832e-02,  2.7912e-02,  2.3100e-01,  2.0302e-02,\n",
      "          -3.1496e-01, -2.7435e-01,  1.9836e-02,  2.0271e-01, -8.9099e-02,\n",
      "          -6.8028e-02, -1.6511e-01, -4.7942e-02,  1.2030e-04]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([128, 4])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0139, -0.0787, -0.0272, -0.0417],\n",
      "        [ 0.0033,  0.0494, -0.0123,  0.0227],\n",
      "        [ 0.0840,  0.0823,  0.0379, -0.0151],\n",
      "        [-0.0278,  0.0584,  0.0317,  0.0743],\n",
      "        [ 0.0762, -0.0099, -0.0417,  0.0459],\n",
      "        [ 0.0649,  0.0091, -0.0589,  0.0485],\n",
      "        [ 0.0458, -0.0723,  0.0485, -0.0311],\n",
      "        [ 0.0279,  0.0563,  0.0791, -0.0248],\n",
      "        [-0.0813,  0.0755, -0.0165,  0.0383],\n",
      "        [-0.0721, -0.0364, -0.0574,  0.0814],\n",
      "        [-0.0438, -0.0211,  0.0055, -0.0661],\n",
      "        [ 0.0021,  0.0126, -0.0437, -0.0395],\n",
      "        [ 0.0412, -0.0169, -0.0531,  0.0762],\n",
      "        [ 0.0454,  0.0125,  0.0438,  0.0251],\n",
      "        [ 0.0287,  0.0397,  0.0766, -0.0685],\n",
      "        [-0.0439, -0.0862,  0.0521,  0.0354],\n",
      "        [ 0.0287, -0.0362,  0.0043, -0.0332],\n",
      "        [-0.0180,  0.0598, -0.0022,  0.0810],\n",
      "        [ 0.0312, -0.0346, -0.0249,  0.0128],\n",
      "        [ 0.0677,  0.0725,  0.0375, -0.0278],\n",
      "        [ 0.0479,  0.0114, -0.0526, -0.0683],\n",
      "        [-0.0165, -0.0462,  0.0869, -0.0703],\n",
      "        [ 0.0455,  0.0610,  0.0053,  0.0742],\n",
      "        [ 0.0757, -0.0677,  0.0271,  0.0347],\n",
      "        [-0.0526, -0.0091,  0.0032, -0.0064],\n",
      "        [-0.0776,  0.0104,  0.0064, -0.0518],\n",
      "        [-0.0017, -0.0029, -0.0549, -0.0834],\n",
      "        [-0.0227,  0.0364,  0.0527, -0.0069],\n",
      "        [ 0.0085,  0.0365, -0.0376, -0.0070],\n",
      "        [ 0.0820, -0.0390, -0.0115, -0.0306],\n",
      "        [ 0.0498, -0.0566,  0.0319, -0.0084],\n",
      "        [-0.0578, -0.0742,  0.0627, -0.0273],\n",
      "        [-0.0837, -0.0356, -0.0646,  0.0619],\n",
      "        [-0.0584, -0.0646,  0.0578, -0.0586],\n",
      "        [ 0.0294, -0.0348, -0.0340,  0.0118],\n",
      "        [-0.0396, -0.0184,  0.0042,  0.0138],\n",
      "        [ 0.0701, -0.0292,  0.0398, -0.0565],\n",
      "        [ 0.0478, -0.0783,  0.0089, -0.0451],\n",
      "        [ 0.0826, -0.0510,  0.0791,  0.0765],\n",
      "        [-0.0505, -0.0013, -0.0316, -0.0613],\n",
      "        [ 0.0244,  0.0527, -0.0418, -0.0360],\n",
      "        [ 0.0488,  0.0199,  0.0736,  0.0449],\n",
      "        [-0.0180,  0.0355, -0.0553,  0.0215],\n",
      "        [-0.0083,  0.0779, -0.0510, -0.0657],\n",
      "        [ 0.0020, -0.0527, -0.0741,  0.0346],\n",
      "        [ 0.0375,  0.0752, -0.0377, -0.0659],\n",
      "        [ 0.0219,  0.0304,  0.0374, -0.0870],\n",
      "        [-0.0281, -0.0418,  0.0484,  0.0157],\n",
      "        [-0.0478,  0.0550,  0.0397, -0.0394],\n",
      "        [ 0.0020, -0.0341,  0.0732, -0.0505],\n",
      "        [ 0.0496,  0.0150, -0.0263, -0.0473],\n",
      "        [-0.0515, -0.0541, -0.0173, -0.0739],\n",
      "        [ 0.0284,  0.0858,  0.0706,  0.0280],\n",
      "        [-0.0219,  0.0860,  0.0841, -0.0408],\n",
      "        [-0.0817, -0.0685,  0.0569,  0.0070],\n",
      "        [-0.0781,  0.0541,  0.0586, -0.0666],\n",
      "        [-0.0822, -0.0732,  0.0389, -0.0767],\n",
      "        [-0.0720, -0.0872,  0.0849, -0.0393],\n",
      "        [ 0.0702, -0.0119,  0.0197,  0.0544],\n",
      "        [-0.0763,  0.0492,  0.0762, -0.0804],\n",
      "        [-0.0458,  0.0242,  0.0344, -0.0160],\n",
      "        [ 0.0139, -0.0879,  0.0617,  0.0035],\n",
      "        [-0.0723, -0.0875, -0.0360, -0.0717],\n",
      "        [ 0.0703,  0.0624, -0.0128, -0.0182],\n",
      "        [-0.0372, -0.0452,  0.0723,  0.0477],\n",
      "        [-0.0867, -0.0091,  0.0572,  0.0341],\n",
      "        [-0.0241, -0.0047, -0.0325, -0.0600],\n",
      "        [ 0.0211,  0.0089, -0.0471,  0.0287],\n",
      "        [ 0.0486,  0.0764,  0.0096, -0.0662],\n",
      "        [ 0.0428, -0.0168, -0.0484,  0.0304],\n",
      "        [ 0.0791,  0.0326,  0.0070,  0.0849],\n",
      "        [ 0.0162,  0.0343,  0.0489,  0.0249],\n",
      "        [-0.0175,  0.0154, -0.0133, -0.0546],\n",
      "        [-0.0223, -0.0140,  0.0862,  0.0644],\n",
      "        [-0.0730, -0.0561, -0.0175,  0.0270],\n",
      "        [-0.0750,  0.0791,  0.0761,  0.0207],\n",
      "        [ 0.0588,  0.0386, -0.0708, -0.0120],\n",
      "        [-0.0486,  0.0058, -0.0176, -0.0872],\n",
      "        [ 0.0420,  0.0211, -0.0453, -0.0756],\n",
      "        [-0.0469,  0.0648,  0.0840,  0.0771],\n",
      "        [ 0.0167, -0.0466,  0.0435, -0.0558],\n",
      "        [-0.0180,  0.0093, -0.0242, -0.0033],\n",
      "        [ 0.0672, -0.0508,  0.0832, -0.0102],\n",
      "        [ 0.0063, -0.0791, -0.0377, -0.0634],\n",
      "        [ 0.0389,  0.0140, -0.0615,  0.0771],\n",
      "        [-0.0714,  0.0663, -0.0419,  0.0436],\n",
      "        [-0.0005, -0.0116,  0.0021, -0.0355],\n",
      "        [ 0.0094,  0.0812,  0.0483, -0.0468],\n",
      "        [-0.0453,  0.0196,  0.0874, -0.0572],\n",
      "        [ 0.0470,  0.0742, -0.0554,  0.0737],\n",
      "        [-0.0046, -0.0622, -0.0864, -0.0697],\n",
      "        [-0.0350,  0.0755, -0.0144, -0.0533],\n",
      "        [-0.0733, -0.0141, -0.0706,  0.0772],\n",
      "        [ 0.0318, -0.0718, -0.0039, -0.0808],\n",
      "        [ 0.0250, -0.0836, -0.0437, -0.0564],\n",
      "        [ 0.0294, -0.0549, -0.0642, -0.0373],\n",
      "        [-0.0052,  0.0771, -0.0031,  0.0305],\n",
      "        [-0.0191, -0.0395,  0.0577,  0.0202],\n",
      "        [-0.0204,  0.0647, -0.0778, -0.0585],\n",
      "        [ 0.0814,  0.0275, -0.0803,  0.0322],\n",
      "        [ 0.0391,  0.0696, -0.0112,  0.0406],\n",
      "        [-0.0811, -0.0443, -0.0128, -0.0556],\n",
      "        [ 0.0855,  0.0882,  0.0047, -0.0368],\n",
      "        [ 0.0583, -0.0531,  0.0071,  0.0782],\n",
      "        [-0.0406, -0.0030,  0.0457, -0.0841],\n",
      "        [ 0.0442, -0.0754, -0.0803, -0.0494],\n",
      "        [-0.0486, -0.0157,  0.0512,  0.0465],\n",
      "        [-0.0004, -0.0288, -0.0781,  0.0676],\n",
      "        [-0.0460, -0.0629, -0.0148,  0.0751],\n",
      "        [ 0.0104, -0.0334, -0.0388, -0.0608],\n",
      "        [-0.0189,  0.0272,  0.0264, -0.0059],\n",
      "        [-0.0798, -0.0168, -0.0396, -0.0421],\n",
      "        [-0.0075,  0.0069, -0.0432, -0.0495],\n",
      "        [-0.0047,  0.0769, -0.0368,  0.0721],\n",
      "        [-0.0162,  0.0626, -0.0485,  0.0189],\n",
      "        [ 0.0079,  0.0517, -0.0361, -0.0346],\n",
      "        [ 0.0057,  0.0109, -0.0509, -0.0678],\n",
      "        [ 0.0801, -0.0706,  0.0351, -0.0406],\n",
      "        [-0.0175,  0.0669,  0.0229,  0.0174],\n",
      "        [-0.0324,  0.0067, -0.0462, -0.0483],\n",
      "        [ 0.0860,  0.0160,  0.0122,  0.0359],\n",
      "        [ 0.0542, -0.0770, -0.0258,  0.0683],\n",
      "        [-0.0590, -0.0269,  0.0553,  0.0716],\n",
      "        [ 0.0781,  0.0203, -0.0462,  0.0629],\n",
      "        [ 0.0124, -0.0418,  0.0457,  0.0071],\n",
      "        [-0.0077,  0.0158,  0.0176, -0.0724],\n",
      "        [ 0.0464, -0.0335, -0.0344,  0.0326],\n",
      "        [-0.0237,  0.0807, -0.0869,  0.0878]], requires_grad=True)\n",
      "spliced Wo: torch.Size([64, 2])\n",
      "tensor([[ 0.0043, -0.0332],\n",
      "        [-0.0022,  0.0810],\n",
      "        [-0.0249,  0.0128],\n",
      "        [ 0.0375, -0.0278],\n",
      "        [-0.0526, -0.0683],\n",
      "        [ 0.0869, -0.0703],\n",
      "        [ 0.0053,  0.0742],\n",
      "        [ 0.0271,  0.0347],\n",
      "        [ 0.0032, -0.0064],\n",
      "        [ 0.0064, -0.0518],\n",
      "        [-0.0549, -0.0834],\n",
      "        [ 0.0527, -0.0069],\n",
      "        [-0.0376, -0.0070],\n",
      "        [-0.0115, -0.0306],\n",
      "        [ 0.0319, -0.0084],\n",
      "        [ 0.0627, -0.0273],\n",
      "        [ 0.0397, -0.0394],\n",
      "        [ 0.0732, -0.0505],\n",
      "        [-0.0263, -0.0473],\n",
      "        [-0.0173, -0.0739],\n",
      "        [ 0.0706,  0.0280],\n",
      "        [ 0.0841, -0.0408],\n",
      "        [ 0.0569,  0.0070],\n",
      "        [ 0.0586, -0.0666],\n",
      "        [ 0.0389, -0.0767],\n",
      "        [ 0.0849, -0.0393],\n",
      "        [ 0.0197,  0.0544],\n",
      "        [ 0.0762, -0.0804],\n",
      "        [ 0.0344, -0.0160],\n",
      "        [ 0.0617,  0.0035],\n",
      "        [-0.0360, -0.0717],\n",
      "        [-0.0128, -0.0182],\n",
      "        [ 0.0435, -0.0558],\n",
      "        [-0.0242, -0.0033],\n",
      "        [ 0.0832, -0.0102],\n",
      "        [-0.0377, -0.0634],\n",
      "        [-0.0615,  0.0771],\n",
      "        [-0.0419,  0.0436],\n",
      "        [ 0.0021, -0.0355],\n",
      "        [ 0.0483, -0.0468],\n",
      "        [ 0.0874, -0.0572],\n",
      "        [-0.0554,  0.0737],\n",
      "        [-0.0864, -0.0697],\n",
      "        [-0.0144, -0.0533],\n",
      "        [-0.0706,  0.0772],\n",
      "        [-0.0039, -0.0808],\n",
      "        [-0.0437, -0.0564],\n",
      "        [-0.0642, -0.0373],\n",
      "        [-0.0432, -0.0495],\n",
      "        [-0.0368,  0.0721],\n",
      "        [-0.0485,  0.0189],\n",
      "        [-0.0361, -0.0346],\n",
      "        [-0.0509, -0.0678],\n",
      "        [ 0.0351, -0.0406],\n",
      "        [ 0.0229,  0.0174],\n",
      "        [-0.0462, -0.0483],\n",
      "        [ 0.0122,  0.0359],\n",
      "        [-0.0258,  0.0683],\n",
      "        [ 0.0553,  0.0716],\n",
      "        [-0.0462,  0.0629],\n",
      "        [ 0.0457,  0.0071],\n",
      "        [ 0.0176, -0.0724],\n",
      "        [-0.0344,  0.0326],\n",
      "        [-0.0869,  0.0878]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.1324, -0.1417],\n",
      "         [-0.0045, -0.0405],\n",
      "         [ 0.0003,  0.0333]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[-0.2735, -1.0140],\n",
      "         [ 0.0167,  1.2070],\n",
      "         [ 0.7741,  0.3640]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.2735, -1.0140],\n",
      "         [ 0.0167,  1.2070],\n",
      "         [ 0.7741,  0.3640]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.3683, -1.3654],\n",
      "         [ 0.0196,  1.4141],\n",
      "         [ 1.2798,  0.6018]]], grad_fn=<MulBackward0>)\n",
      "dim: 2\n",
      "skip: 2\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.3683, -1.3654],\n",
      "         [ 0.0196,  1.4141],\n",
      "         [ 1.2798,  0.6018]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.3683, -1.3654],\n",
      "         [ 0.0196,  1.4141],\n",
      "         [ 1.2798,  0.6018]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 2\n",
      "d_skip: 2\n",
      "i_dim: 8\n",
      "i_skip: 8\n",
      "Wgate: torch.Size([4, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.4920,  0.3010,  0.1499, -0.3392,  0.4856, -0.4726,  0.0867,  0.2430,\n",
      "          0.3942,  0.4743, -0.2700, -0.3373,  0.3804, -0.2024, -0.3690, -0.1290],\n",
      "        [-0.4186, -0.3264, -0.4179,  0.2408, -0.1365,  0.1968,  0.0011, -0.4685,\n",
      "         -0.1507,  0.2918,  0.3232,  0.0377, -0.1145, -0.4494, -0.2852, -0.0918],\n",
      "        [-0.3881, -0.4971, -0.3846,  0.2184,  0.2203,  0.3713,  0.1256, -0.4725,\n",
      "          0.1326,  0.1626,  0.4547, -0.0986,  0.3189,  0.3653,  0.2537,  0.3719],\n",
      "        [ 0.2424, -0.1054,  0.2797, -0.0835, -0.3293,  0.3435,  0.2210,  0.4816,\n",
      "          0.2108,  0.2477, -0.1237, -0.2128,  0.3488, -0.4129, -0.1048,  0.3282]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([2, 8])\n",
      "tensor([[ 0.1326,  0.1626,  0.4547, -0.0986,  0.3189,  0.3653,  0.2537,  0.3719],\n",
      "        [ 0.2108,  0.2477, -0.1237, -0.2128,  0.3488, -0.4129, -0.1048,  0.3282]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([16])\n",
      "Parameter containing:\n",
      "tensor([-0.4296, -0.2656, -0.3730, -0.4881,  0.2735,  0.3810, -0.2995, -0.2457,\n",
      "         0.3296, -0.1207, -0.1850,  0.1151, -0.1815,  0.0565,  0.1133,  0.2197],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([8])\n",
      "tensor([ 0.3296, -0.1207, -0.1850,  0.1151, -0.1815,  0.0565,  0.1133,  0.2197],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.0071, -0.5188, -0.1835,  0.4421, -0.7751,  0.4857,  0.1629,\n",
      "          -0.3654],\n",
      "         [ 0.6302,  0.2328, -0.3510, -0.1878,  0.3179, -0.5202, -0.0300,\n",
      "           0.6910],\n",
      "         [ 0.6261,  0.2365,  0.3224, -0.1392,  0.4364,  0.2754,  0.3749,\n",
      "           0.8931]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.0035, -0.1567, -0.0784,  0.2965, -0.1699,  0.3334,  0.0920,\n",
      "          -0.1306],\n",
      "         [ 0.4637,  0.1378, -0.1273, -0.0799,  0.1986, -0.1568, -0.0146,\n",
      "           0.5219],\n",
      "         [ 0.4598,  0.1403,  0.2020, -0.0619,  0.2919,  0.1676,  0.2423,\n",
      "           0.7271]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.4397,  0.0639,  0.3251, -0.4417, -0.1198, -0.4560,  0.3213, -0.1483,\n",
      "         -0.0726, -0.1437, -0.1375,  0.2211,  0.4713, -0.1941,  0.2534, -0.1524],\n",
      "        [-0.1683,  0.0169, -0.3806, -0.3985,  0.0782, -0.4636, -0.4176, -0.0484,\n",
      "         -0.3432, -0.1917,  0.4930,  0.3492, -0.0744,  0.1426, -0.3753,  0.4084],\n",
      "        [-0.2476,  0.0072, -0.3127, -0.3605,  0.3117,  0.4990,  0.0193,  0.1382,\n",
      "          0.2498,  0.1919,  0.2682, -0.3924, -0.0709, -0.2175, -0.3473,  0.1599],\n",
      "        [ 0.4299,  0.3039,  0.2940, -0.4826,  0.1980, -0.0223, -0.0833, -0.0818,\n",
      "          0.1703, -0.4762, -0.0675,  0.1609, -0.0555, -0.1132, -0.4620,  0.0077]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([2, 8])\n",
      "tensor([[ 0.2498,  0.1919,  0.2682, -0.3924, -0.0709, -0.2175, -0.3473,  0.1599],\n",
      "        [ 0.1703, -0.4762, -0.0675,  0.1609, -0.0555, -0.1132, -0.4620,  0.0077]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([16])\n",
      "Parameter containing:\n",
      "tensor([ 0.3836,  0.0685, -0.3794,  0.2138,  0.0660,  0.0780, -0.2228,  0.0113,\n",
      "        -0.1320,  0.3062, -0.2345,  0.4163, -0.4161,  0.3209,  0.2370, -0.2584],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([8])\n",
      "tensor([-0.1320,  0.3062, -0.2345,  0.4163, -0.4161,  0.3209,  0.2370, -0.2584],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.4564,  0.8857, -0.2411,  0.3411, -0.3141,  0.5555,  0.9957,\n",
      "          -0.3278],\n",
      "         [ 0.1137, -0.3635, -0.3246,  0.6361, -0.4960,  0.1565, -0.4230,\n",
      "          -0.2444],\n",
      "         [ 0.2902,  0.2652,  0.0681,  0.0109, -0.5403, -0.0256, -0.4855,\n",
      "          -0.0491]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.0016, -0.1387,  0.0189,  0.1011,  0.0534,  0.1852,  0.0916,\n",
      "           0.0428],\n",
      "         [ 0.0527, -0.0501,  0.0413, -0.0508, -0.0985, -0.0245,  0.0062,\n",
      "          -0.1275],\n",
      "         [ 0.1334,  0.0372,  0.0138, -0.0007, -0.1577, -0.0043, -0.1176,\n",
      "          -0.0357]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([16, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.1754,  0.0761,  0.1232,  0.1366],\n",
      "        [-0.0635,  0.1460,  0.0160, -0.1254],\n",
      "        [ 0.0276, -0.1514, -0.0825, -0.1608],\n",
      "        [ 0.0913, -0.0020,  0.2442,  0.1662],\n",
      "        [ 0.1354,  0.1174,  0.2448, -0.2325],\n",
      "        [-0.0123,  0.0650,  0.1438, -0.1913],\n",
      "        [-0.0013,  0.2201,  0.1408,  0.0611],\n",
      "        [-0.1373,  0.1180,  0.1358, -0.1107],\n",
      "        [ 0.1830,  0.2221, -0.1693,  0.2425],\n",
      "        [-0.1213, -0.0698,  0.0168,  0.1718],\n",
      "        [-0.0677, -0.2102, -0.0295, -0.1922],\n",
      "        [ 0.0277,  0.0011,  0.1526, -0.0894],\n",
      "        [ 0.2469,  0.1777,  0.0815,  0.0353],\n",
      "        [-0.1205, -0.1902, -0.1469, -0.0820],\n",
      "        [ 0.0319, -0.0125,  0.0571, -0.1865],\n",
      "        [ 0.0971,  0.0427,  0.2102,  0.0165]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([8, 2])\n",
      "tensor([[-0.1693,  0.2425],\n",
      "        [ 0.0168,  0.1718],\n",
      "        [-0.0295, -0.1922],\n",
      "        [ 0.1526, -0.0894],\n",
      "        [ 0.0815,  0.0353],\n",
      "        [-0.1469, -0.0820],\n",
      "        [ 0.0571, -0.1865],\n",
      "        [ 0.2102,  0.0165]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([-0.0242,  0.1740,  0.0925,  0.2142], requires_grad=True)\n",
      "Bdown spliced: torch.Size([2])\n",
      "tensor([0.0925, 0.2142], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 2])\n",
      "tensor([[[0.0962, 0.1484],\n",
      "         [0.0429, 0.2103],\n",
      "         [0.0436, 0.2665]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[-0.1773, -0.8656],\n",
      "         [ 0.0596,  1.4173],\n",
      "         [ 0.8177,  0.6306]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.1773, -0.8656],\n",
      "         [ 0.0596,  1.4173],\n",
      "         [ 0.8177,  0.6306]]], grad_fn=<AddBackward0>)\n",
      "final output: ((tensor([[[ 1.2365,  0.0804,  0.6235,  1.7477],\n",
      "         [ 1.1942, -0.9424,  0.9134,  2.3550],\n",
      "         [ 1.6781,  1.0569, -1.5321,  0.0980]]], grad_fn=<AddBackward0>),), (tensor([[[ 0.8293,  0.5314],\n",
      "         [-0.4652,  1.3387],\n",
      "         [ 1.0220,  3.1334]]], grad_fn=<AddBackward0>), tensor([[[-0.1773, -0.8656],\n",
      "         [ 0.0596,  1.4173],\n",
      "         [ 0.8177,  0.6306]]], grad_fn=<AddBackward0>)))\n",
      "------------- END Layer.forwardTuple() ------------\n",
      "out: ((tensor([[[ 1.2365,  0.0804,  0.6235,  1.7477],\n",
      "         [ 1.1942, -0.9424,  0.9134,  2.3550],\n",
      "         [ 1.6781,  1.0569, -1.5321,  0.0980]]], grad_fn=<AddBackward0>),), (tensor([[[ 0.8293,  0.5314],\n",
      "         [-0.4652,  1.3387],\n",
      "         [ 1.0220,  3.1334]]], grad_fn=<AddBackward0>), tensor([[[-0.1773, -0.8656],\n",
      "         [ 0.0596,  1.4173],\n",
      "         [ 0.8177,  0.6306]]], grad_fn=<AddBackward0>)))\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n"
     ]
    }
   ],
   "source": [
    "# Testing our Layer's forwardTuple()\n",
    "config.verbose['OutputLayer'] = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4 = config.hidden_size, config.levels, config.max_position_embeddings, config.hidden_size\n",
    "config.hidden_size = 4\n",
    "config.levels = 2\n",
    "config.max_position_embeddings = 3\n",
    "config.vocab_size = 5\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "embedding = torch.randn(config.vocab_size, config.hidden_size)\n",
    "print(f\"embedding: {embedding.shape}\\n{embedding}\")\n",
    "\n",
    "layer = Layer(config)\n",
    "# we need to make sure to send in a tuple of the expected size. above we set hidden_size=4 and levels=2\n",
    "x = ((torch.randn((1,3,config.hidden_size)),),\n",
    "     (torch.randn((1,3,config.hidden_size//config.split)),torch.randn((1,3,config.hidden_size//config.split))))\n",
    "print(f\"x: {x}\")\n",
    "out = layer(x)\n",
    "print(f\"out: {out}\")\n",
    "\n",
    "config.verbose['OutputLayer'] = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.levels = hold2\n",
    "config.max_position_embeddings = hold3\n",
    "config.vocab_size = hold4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "# clear up memory\n",
    "del hold1, hold2, hold3, hold4, x, layer, out, embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9089c1",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e090675",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractalLoss(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.verbose = config.verbose['FractalLoss']\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        input: \n",
    "            - logits are a tuple of tuples of tensors each of shape [batch_size, max_seq_len, vocab_size]\n",
    "            - target is a shape [batch_size, max_seq_len] tensor of the integer indices of the correct tokens\n",
    "        output: a tensor containing a single float of the loss value\n",
    "        \"\"\"\n",
    "        if self.verbose: \n",
    "            print(\"------------- FractalLoss.forward() ------------\")\n",
    "            print(f\"logits:\\n{logits}\")\n",
    "            \n",
    "        assert type(logits) == tuple # since this function should only be used during training\n",
    "            \n",
    "        # should only be used during training, so we assert input_len == max_position_embeddings\n",
    "        b,t,v = logits[0][0].shape\n",
    "        if self.verbose: print(f\"b:{b}, t:{t}, v:{v}, b*t:{b*t}\")\n",
    "        assert t == config.max_position_embeddings\n",
    "        \n",
    "        # Calculate losses for each output and stack them. \n",
    "        # i apologize for the weird format instead of regular for loops, but it feels better in my head\n",
    "        loss = torch.stack([ # stacks across levels\n",
    "                            torch.stack( # stacks across models in level\n",
    "                                        [self.criterion(logits_ij.view(b*t, v), # reshapes for CELoss\n",
    "                                                        target.view(b*t)) \n",
    "                                         for logits_ij in logits[i]] # iterates across models in level\n",
    "                            ).sum() # sums across models in level\n",
    "                            for i in range(len(logits))] # iterates across levels\n",
    "                          ).sum() # sums across levels\n",
    "\n",
    "        if self.verbose:\n",
    "            print(f\"final loss: {loss}\")\n",
    "            print(\"------------- END FractalLoss.forward() ------------\")\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b4151cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [4, 2]\n",
      "embedding: torch.Size([5, 4])\n",
      "tensor([[ 0.4979,  0.5967, -1.1949,  0.2624],\n",
      "        [ 0.0638,  1.9315, -0.7009, -1.5998],\n",
      "        [-1.9362,  0.1396, -0.3804, -1.5269],\n",
      "        [-2.6050,  0.4274, -0.8691, -1.0572],\n",
      "        [ 0.7238, -1.8383,  0.3935, -0.8516]])\n",
      "logits: ((tensor([[[ 0.0831, -1.2784, -0.4596,  1.1504,  2.9164],\n",
      "         [-0.3105,  0.6825,  1.1086, -1.2702,  1.2147],\n",
      "         [-0.7330,  1.9513,  0.2516,  1.4598, -0.2380]],\n",
      "\n",
      "        [[ 0.7541, -0.2404, -0.7586,  0.6544, -0.8579],\n",
      "         [ 0.4715,  0.5374, -1.3068, -0.5886,  1.2973],\n",
      "         [-1.2384,  1.3170,  0.0318, -1.5689,  0.4312]]]),), (tensor([[[ 1.5523,  0.1870, -0.4250, -0.5468,  0.2964],\n",
      "         [ 1.5569, -0.4984,  0.9834,  0.2026,  0.5319],\n",
      "         [-0.2308,  1.8356, -0.2126,  1.0607,  1.2194]],\n",
      "\n",
      "        [[ 0.1093,  0.7257, -1.2447, -0.1513, -0.3139],\n",
      "         [-0.9834, -0.0567,  1.0787, -1.0214,  1.6036],\n",
      "         [-0.6170,  0.3794,  1.9120, -0.5171, -0.4947]]]), tensor([[[-0.2441,  0.6304,  0.1295, -0.8517,  2.2266],\n",
      "         [-0.0665,  0.2241,  0.3724,  0.1355, -0.8233],\n",
      "         [-0.1940,  1.4225,  0.3912,  0.2158, -0.1434]],\n",
      "\n",
      "        [[ 0.3388, -0.2840,  0.9358,  2.0212, -0.0475],\n",
      "         [-2.1114,  0.8911,  0.0376, -0.4147, -1.6053],\n",
      "         [-0.1795,  0.8832, -0.4532, -0.7544, -0.5065]]])))\n",
      "target: tensor([[[1, 4, 0],\n",
      "         [1, 4, 3]]])\n",
      "out: 6.872470855712891\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [8, 4, 2]\n"
     ]
    }
   ],
   "source": [
    "# Testing our FractalLoss\n",
    "config.verbose['OutputLayer'] = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4 = config.hidden_size, config.levels, config.max_position_embeddings, config.hidden_size\n",
    "config.hidden_size = 4\n",
    "config.levels = 2\n",
    "config.max_position_embeddings = 3\n",
    "config.vocab_size = 5\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "embedding = torch.randn(config.vocab_size, config.hidden_size)\n",
    "print(f\"embedding: {embedding.shape}\\n{embedding}\")\n",
    "\n",
    "loss = FractalLoss(config)\n",
    "# we need to make sure to send in a tuple of the expected size. above we set hidden_size=4 and levels=2\n",
    "logits = ((torch.randn((2,3,config.vocab_size)),),\n",
    "     (torch.randn((2,3,config.vocab_size)),torch.randn((2,3,config.vocab_size))))\n",
    "print(f\"logits: {logits}\")\n",
    "target = torch.randint(config.vocab_size, (2,3)).unsqueeze(0)\n",
    "print(f\"target: {target}\")\n",
    "out = loss(logits, target)\n",
    "print(f\"out: {out}\")\n",
    "\n",
    "config.verbose['OutputLayer'] = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.levels = hold2\n",
    "config.max_position_embeddings = hold3\n",
    "config.vocab_size = hold4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "# clear up memory\n",
    "del hold1, hold2, hold3, hold4, embedding, loss, logits, target, out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d511fec",
   "metadata": {},
   "source": [
    "# The Model itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9d0b47ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractalFormer_base(nn.Module):\n",
    "    def __init__(self, config: Config, tokenizer: tokenizer):\n",
    "        super().__init__()\n",
    "        self.verbose=config.verbose['Model']\n",
    "        self.config = config\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # hyperparameters\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.max_seq_len = config.max_position_embeddings\n",
    "        self.head_dim = config.head_dim\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        ### FractalFormer-specific hyperparameters\n",
    "        self.num_levels = config.levels # the number of levels for sub-models to exist on\n",
    "        self.split = config.split # the number of splits to make at a given level\n",
    "        self.model_count = config.model_count # list of number of models at a given level\n",
    "        self.model_dim_list = config.model_dim_list # list of hidden dimensions corresponding to each given level\n",
    "        self.head_dim_list = config.head_dim_list # list of attention head dimensions corresponding to each given level    \n",
    "\n",
    "        # the embedding matrix. for converting tokens to the first residual state, and the last residual state to logits\n",
    "        self.embedder = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "\n",
    "        # for normalizing the initial embeddings\n",
    "        self.embedder_norm = RMSNorm(config.hidden_size)\n",
    "\n",
    "        # Initialize a sequence of DecoderLayer instances as specified by the number of hidden layers in the config\n",
    "        self.layers = nn.ModuleList(Layer(config) for _ in range(config.num_hidden_layers))\n",
    "\n",
    "        # initializing output layer\n",
    "        self.output_layer = OutputLayer(self.embedder.weight, config)\n",
    "        # i think i need to do this bc in the above version you can't use `self.` inside the init\n",
    "        #@property \n",
    "        #def output_layer(self):\n",
    "            #return OutputLayer(self.embedder.weight, config)\n",
    "\n",
    "        # the loss function\n",
    "        self.criterion = FractalLoss(config)\n",
    "\n",
    "    def forwardTensor(self,\n",
    "                      input_token_ids: torch.Tensor,\n",
    "                      level: int = 0, # integer designating the level of model to use. 0 is largest model, -1 is smallest\n",
    "                      model: int = 0, # integer designating the model in that level to use. 0 is top-left, -1 is bottom right\n",
    "                     ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        inputs: \n",
    "            - input_token_ids (torch.Tensor): a tensor of integers size (batch_size, sequence_length)\n",
    "            - level: integer designating the level of model to use. 0 is largest model, -1 is smallest\n",
    "            - model: integer designating the model in that level to use. 0 is top-left, -1 is bottom right\n",
    "        output: a torch.Tensor shape (batch_size, sequence_length, vocab_size)\n",
    "        \"\"\"\n",
    "        if self.verbose: \n",
    "            print(\"------------- FractalFormer.forwardTensor() ------------\")\n",
    "            print(f\"input_token_ids: {input_token_ids.shape}\\n{input_token_ids}\")\n",
    "        \n",
    "        # adjusting everything to the specified level & model\n",
    "        d_dim = self.hidden_size // (2**level)\n",
    "        d_skip = model * d_dim\n",
    "        if self.verbose:\n",
    "            print(f\"d_dim: {d_dim}\")\n",
    "            print(f\"d_skip: {d_skip}\")\n",
    "        \n",
    "        # turn the input tokens into the first residual state using the embedding matrix\n",
    "        # (batch_size, input_len) & (vocab_size, hidden_size) -> (batch_size, input_len, hidden_size) -> (batch_size, input_len, d_dim)\n",
    "        x = self.embedder(input_token_ids)\n",
    "        if self.verbose: print(f\"x0: {x.shape}\\n{x}\")\n",
    "\n",
    "        x = x[:,:, d_skip:d_skip + d_dim]\n",
    "        if self.verbose: print(f\"spliced x0: {x0.shape}\\n{x0}\")\n",
    "        \n",
    "        # Gemma normalizes the embedding by sqrt(hidden_size)\n",
    "        # the question is, should I do this with the full sized hidden_size or do it at the splice size????\n",
    "        # imma do it at the splice size and change it later if i think the models aren't learning well\n",
    "        #x = x * (d_dim**0.5)\n",
    "        # alternatively i could just switch to doing a regular RMSNorm which would be more like me\n",
    "        # if i figure out this different sizes of hyperspheres thing it'd be more in line with that\n",
    "        x = self.embedder_norm(x, model)\n",
    "        if self.verbose: print(f\"normalized initial x: {x.shape}\\n{x}\")\n",
    "\n",
    "        # Iteratively process the input through each Layer\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if self.verbose: print(f\"begin layer {i}\")\n",
    "            x = layer(x, model)\n",
    "            if self.verbose: print(f\"output of layer {i}: {x.shape}\\n{x}\")\n",
    "\n",
    "        logits = self.output_layer(x, model)\n",
    "        if self.verbose: \n",
    "            print(f\"output logits: {logits.shape}\\n{logits}\")\n",
    "            print(\"------------- END FractalFormer.forwardTensor() ------------\")\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def forwardTuple(self,\n",
    "                     input_token_ids: torch.Tensor,\n",
    "                     target_token_ids: torch.Tensor,\n",
    "                    ) -> torch.Tensor:\n",
    "        if self.verbose: \n",
    "            print(\"------------- FractalFormer.forwardTuple() ------------\")\n",
    "            print(f\"input_token_ids: {input_token_ids.shape}\\n{input_token_ids}\")\n",
    "            print(f\"target_token_ids: {target_token_ids.shape}\\n{target_token_ids}\")\n",
    "        \n",
    "        # use the embedding matrix to turn the input tokens into the first residual state of the largest model\n",
    "        # (batch_size, input_len) & (vocab_size, hidden_size) -> (batch_size, input_len, hidden_size)\n",
    "        x0 = self.embedder(input_token_ids)\n",
    "        if self.verbose: print(f\"initial x: {x.shape}\\n{x}\")\n",
    "\n",
    "        # create the first fractal tuple of residual states\n",
    "        x = ()\n",
    "        for i, models_in_level in enumerate(config.model_count):\n",
    "            if self.verbose: print(f\"i: {i}, models_in_level: {models_in_level}, iterating over {config.model_count}\")\n",
    "            \n",
    "            x_lvl = ()\n",
    "            for j, d_dim in enumerate(config.model_dim_list):\n",
    "                if self.verbose: print(f\"j: {j}, d_dim: {d_dim}, iterating over {config.model_dim_list}\")\n",
    "\n",
    "                skip = j * d_dim\n",
    "                if self.verbose: print(f\"skip: {skip}\")\n",
    "                \n",
    "                x_ij_spliced = x0[:,:,skip:skip + d_dim]\n",
    "                if self.verbose: print(f\"initial x[{i}][{j}] spliced: {x_ij_spliced.shape}\\n{x_ij_spliced}\")\n",
    "                    \n",
    "                x_ij_spliced_normed = self.embedder_norm(x_ij_spliced, model=j) # * (d_dim**0.5) # if i want to do Gemma normalization instead\n",
    "                if self.verbose: print(f\"initial x[{i}][{j}] spliced & normed: {x_ij_spliced_normed.shape}\\n{x_ij_spliced_normed}\")\n",
    "                \n",
    "                x_lvl += (x_ij_spliced_normed,)  \n",
    "            x += (x_lvl,)\n",
    "        if self.verbose: print(f\"full tuple initial x: {x0}\")\n",
    "\n",
    "        # Iteratively process the input through each Layer\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if self.verbose: print(f\"begin layer {i}\")\n",
    "            \n",
    "            x = layer(x)\n",
    "            if self.verbose: print(f\"output of layer {i}: {x}\")\n",
    "\n",
    "        logits = self.output_layer(x)\n",
    "        if self.verbose: \n",
    "            print(f\"output logits: {logits}\")\n",
    "            print(\"------------- END FractalFormer.forwardTuple() ------------\")\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def forward(self,\n",
    "                input_token_ids: torch.Tensor, # a shape (batch_size, input_seq_len OR max_seq_len)list of integer token ids\n",
    "                target_token_ids: torch.Tensor = None, # a shape (batch_size, max_seq_len) list of token ids to train on\n",
    "                level: int = 0, # integer designating the level of model to use. 0 is largest model\n",
    "                model: int = 0, # integer designating the model in that level to use. 0 is top-left model in level\n",
    "                ):\n",
    "        if self.verbose: \n",
    "            print(\"------------- FractalFormer.forward() ------------\")\n",
    "            print(f\"input_token_ids: {input_token_ids.shape}\\n{input_token_ids}\")\n",
    "            print(f\"target_token_ids: {target_token_ids}\")\n",
    "            print(f\"level: {level}\")\n",
    "            print(f\"model: {model}\")\n",
    "        \n",
    "        if target_token_ids is None: # if we're not training, then we don't need to calculate loss\n",
    "            logits = self.forwardTensor(input_token_ids, level, model)\n",
    "            loss = None\n",
    "        else:\n",
    "            # if we are training\n",
    "            # training uses a tuple of tuples of tensors\n",
    "            logits = self.forwardTuple(input_token_ids, target_token_ids) # -> Tuple[Tuple[Tensor shape (batch_size, max_seq_len, vocab_size)]]\n",
    "            \n",
    "            # custom Fractal CELoss function\n",
    "            loss = self.criterion(logits, target_token_ids) \n",
    "        \n",
    "        if self.verbose: \n",
    "            print(f\"logits: {logits}\")\n",
    "            print(f\"loss: {loss}\")\n",
    "            print(\"------------- END FractalFormer.forward() ------------\")\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad() # no need to keep track of gradients during inference\n",
    "    def Sampler(\n",
    "        self,\n",
    "        logits: torch.Tensor, # shape (batch_size, input_len, vocab_size)\n",
    "        temperature: float, # controls how boring vs random the outputs should be\n",
    "        top_p: float, # the maximum cumulative probability of output options we're willing to consider\n",
    "        top_k: int, # the maximum number of output options we're willing to consider\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        The Sampler function is responsible for generating token predictions from Gemma's output.\n",
    "        It supports temperature scaling, top-p (nucleus) sampling, and top-k sampling \n",
    "        The class operates as follows:\n",
    "    \n",
    "        1. Selects the last hidden state for each sequence in the batch\n",
    "    \n",
    "        2. Computes logits by multiplying the selected hidden states with the transposed embedding matrix. \n",
    "    \n",
    "        3. Temperature is used to scale the logits, making the distribution over tokens sharper (lower temperature) \n",
    "        or flatter (higher temperature), which affects the randomness of the sampling (flatter -> more random)\n",
    "    \n",
    "        4. The softmax function is applied to the scaled logits to obtain a probability distribution over the vocabulary.\n",
    "    \n",
    "        5. For top-p sampling, the function computes the cumulative sum of the sorted probabilities and masks out tokens until the \n",
    "        cumulative probability exceeds the threshold defined by `top_ps`. This allows the model to focus on a subset of the most \n",
    "        probable tokens while ignoring the long tail of less likely tokens. \n",
    "        We to ignore long tail probabilities to avoid nonsensical output\n",
    "    \n",
    "        7. For top-k sampling, the function masks out all tokens except the `k` most likely ones, as specified by `top_ks`. \n",
    "        This ensures that the model only considers a fixed number of the most probable tokens for the next token prediction.\n",
    "    \n",
    "        8. After applying both the top-p and top-k masks, the probabilities are re-normalized so that they sum up to 1\n",
    "    \n",
    "        9. The function then samples from the re-normalized probability distribution to select the next token. \n",
    "        \"\"\"\n",
    "        if config.verbose['Sampler']:\n",
    "            print(\"----------------- FractalFormer.Sampler() --------------\")\n",
    "            print(f\"temperature: {temperature}, top_p: {top_p}, top_k: {top_k}\")\n",
    "            \n",
    "        # Select the last element for each sequence.\n",
    "        # (batch_size, input_len, vocab_size) -> (batch_size, vocab_size)\n",
    "        logits = logits[:,-1,:]\n",
    "        if config.verbose['Sampler']: print(f\"logits: {logits.shape}\\n{logits}\")\n",
    "        \n",
    "        # Apply temperature scaling\n",
    "        # (batch_size, vocab_size) / float -> (batch_size, vocab_size)\n",
    "        logits.clone().div_(temperature) # the clone() is because i didn't properly prevent gradient tracking and i'm too lazy to fix the issue at its cause\n",
    "        if config.verbose['Sampler']: print(f\"logits w temperature: {logits.shape}\\n{logits}\")\n",
    "\n",
    "        # Calculate probabilities with softmax.\n",
    "        probs = torch.softmax(logits, dim=-1, dtype=torch.float) # dim=-1 is the vocab_size dimension that we calculate along\n",
    "        if config.verbose['Sampler']: print(f\"probs: {probs.shape}\\n{probs}\")\n",
    "\n",
    "        # sort the probabilities to for use in top-p & top-k\n",
    "        # both are (batch_size, vocab_size)\n",
    "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "        # probs_sort contains float probabilities while probs_idx contains integer indices\n",
    "        if config.verbose['Sampler']: \n",
    "            print(f\"probs_sort: {probs_sort.shape}\\n{probs_sort}\")\n",
    "            print(f\"probs_idx: {probs_idx.shape}\\n{probs_idx}\")\n",
    "\n",
    "        # calculating top-p\n",
    "        # creates same-size tensor of cumulatve probabilities instead of indivdiual probs\n",
    "        probs_sum = torch.cumsum(probs_sort, dim=-1) \n",
    "        if config.verbose['Sampler']: print(f\"probs_sum: {probs_sum.shape}\\n{probs_sum}\")\n",
    "        # mask where 0's are top-p selections & 1's are to be excluded\n",
    "        top_ps_mask = (probs_sum - probs_sort) > top_p\n",
    "        if config.verbose['Sampler']: print(f\"top_ps_mask: {top_ps_mask.shape}\\n{top_ps_mask}\")\n",
    "        # the original probabilities with excluded tokens changed to 0.0\n",
    "        probs_sort = torch.where(top_ps_mask, 0, probs_sort) \n",
    "        if config.verbose['Sampler']: print(f\"probs_sort: {probs_sort.shape}\\n{probs_sort}\")\n",
    "\n",
    "        # calculating top_k\n",
    "        # create a shape (vocab_size) tensor that just iterates up by 1's\n",
    "        top_ks_mask = torch.arange(probs_idx.shape[-1], device=probs_idx.device) \n",
    "        if config.verbose['Sampler']: print(f\"top_ks_mask: {top_ks_mask.shape}\\n{top_ks_mask}\")\n",
    "        # expand our mask along the batch_size dimension to become size (batch_size, vocab_size)\n",
    "        # \"expand\" means copy the original into this new size, so each length vocab_size row is the same\n",
    "        top_ks_mask = top_ks_mask.expand(probs_idx.shape[0], -1)\n",
    "        if config.verbose['Sampler']: print(f\"top_ks_mask: {top_ks_mask.shape}\\n{top_ks_mask}\")\n",
    "        # top_ks is a list of integers. we keep whichever entries in top_ks_mask are greater than their corresponding entries in top_ks\n",
    "        top_ks_mask = top_ks_mask >= top_k\n",
    "        if config.verbose['Sampler']: print(f\"top_ks_mask: {top_ks_mask.shape}\\n{top_ks_mask}\")\n",
    "\n",
    "        # we'll be combining top-p with top-k and using whichever gives us fewer tokens. a very conservative approach\n",
    "        # this trims probs_sort to also fit within our top_k requirement\n",
    "        probs_sort = torch.where(top_ks_mask, 0, probs_sort)\n",
    "        if config.verbose['Sampler']: print(f\"probs_sort: {probs_sort.shape}\\n{probs_sort}\")\n",
    "\n",
    "        # Re-normalization so that total probabilities add up to 1\n",
    "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "        if config.verbose['Sampler']: print(f\"probs_sort: {probs_sort.shape}\\n{probs_sort}\")\n",
    "        \n",
    "        # now we rearrange the modified probabilities in probs_sort back to their original order according to probs_idx\n",
    "        probs = torch.gather(probs_sort,\n",
    "                             dim=-1,\n",
    "                             index=torch.argsort(probs_idx, dim=-1))\n",
    "        if config.verbose['Sampler']: print(f\"probs: {probs.shape}\\n{probs}\")\n",
    "        \n",
    "        # samples from the distribution\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "        if config.verbose['Sampler']: print(f\"next_token_id: {next_token_id.shape}\\n{next_token_id}\")\n",
    "        \n",
    "        return next_token_id # returns the predicted token\n",
    "        \n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        output_len: int = 100, # the model will output 100 tokens\n",
    "        temperature: float = 0.7, # 0.95 is pretty close to not even using temperature at all (1.0 would be no effect)\n",
    "        top_p: float = 1.0, # defaulting to 1 means we essentially don't use top-p\n",
    "        top_k: int = config.vocab_size, # setting top_k = vocab_size means we're effectively not using top_k at all\n",
    "        level: int = 0, # which size model we want to perform inference with\n",
    "        model: int = 0, # which model in that level we want to perform inference with\n",
    "    ) -> str: \n",
    "        \n",
    "        # encoding the prompt into token indices\n",
    "        tokens = self.tokenizer.encode(prompt)\n",
    "\n",
    "        # turning it into the right tensor shape\n",
    "        tokens = torch.tensor(tokens, device=config.device).unsqueeze(0)\n",
    "        \n",
    "        # we wouldn't want to go past the maximum context length we trained on\n",
    "        assert len(tokens) + output_len <= self.config.max_position_embeddings\n",
    "\n",
    "        for i in range(output_len):\n",
    "            # get the model's output logits and ignore the loss, which would be a NoneType object\n",
    "            logits, _ = self(tokens[:,:self.max_seq_len], level=level, model=model)\n",
    "            \n",
    "            next_token = self.Sampler(\n",
    "                logits = logits, # the actual output of the model\n",
    "                temperature = temperature,\n",
    "                top_p = top_p,\n",
    "                top_k = top_k\n",
    "            )\n",
    "            #print(next_token)\n",
    "\n",
    "            # add our new token to the sequence\n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "\n",
    "        # decode our list of tokens to an actual string\n",
    "        output = self.tokenizer.decode(tokens.squeeze(0).tolist())\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7641ae",
   "metadata": {},
   "source": [
    "# Training-related Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3b4604bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be our training dataset, the rest for validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d76bf9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading for training which generates a small batch of data of inputs x and targets y\n",
    "def get_batch(split, batch_size):\n",
    "    # whether we grab from our training or validation dataset\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - config.max_position_embeddings, (batch_size,))\n",
    "    x = torch.stack([data[i:i+config.max_position_embeddings] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+config.max_position_embeddings+1] for i in ix])\n",
    "    x, y = x.to(config.device), y.to(config.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "74f0d059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  1,  72,   1,  51,  73,  52,  97,   0,  31,  43,  43,   1,  65,  67,\n",
      "           1, 118, 117,  95,  56,   1,  96,   1,  84,   1, 101,   1,  50,  73,\n",
      "          42,   1,  86,   1,  44,  78,  87,  56,   8,   0,  19,  47,  95,   1,\n",
      "          83,   1,  72,   1, 117, 122,  58,  10,   1,  59,  54,  79,   1,  65,\n",
      "          63,   1, 117,  44,  43,  66,  21,   1, 108,  77,  45,  43,   1,  72,\n",
      "          43,  82,  35,  92,  58,  43,   5,  68,   1,  65,  67,   1,  87,  77,\n",
      "           5,  80,   1,  73,   1,  91,  89,  58,  66,  80,  86,   1,  39,  81,\n",
      "           1,  39,  50, 126,  44,  82, 123,   1,  42,  53,   1, 116,   1,  69,\n",
      "          58,  68,  56,  59,  54,  58,   1,  83,   1,  69,   1, 101,   1,  41,\n",
      "         127,  91,   8,   0,  35,  46,  63,   1,  21,   1, 118,  57, 104, 109,\n",
      "           1,  69,  84,   1,  65,  74,   1,  98,  42,   1,  94,   1, 118,  78,\n",
      "          46,  82,  21,  57,   1,  54,  77,  58,  50,  63,   1,  84,   1,  98,\n",
      "          46,  53, 111,   1, 101,   1,  50,  39,  42,  63,   5,  57,   1,  44,\n",
      "          39, 104,  11,   0,  14, 114,   1, 108,  47,  43,  44,  50,  63,   1,\n",
      "          84,   1,  58,  39, 124,   1,  72,  52, 104,   1,  44, 115,  51,   1,\n",
      "          87,  56,   1, 118,  39,  42,   1,  44,  97,  68,   0,  13,   1,  54,\n",
      "          93,  41,  47,  67,  57,   1,  56,  97,  66,  39,   1,  56,  97,   1,\n",
      "         121,   1,  21,   1,  51,  59,  80,   1,  59,  91,   0,  21,  52,   1,\n",
      "         118,  77,   1,  43]])\n",
      " the morning\n",
      "See thou deliver it to my lord and father.\n",
      "Give me the light: upon thy life, I charge thee,\n",
      "Whate'er thou hear'st or seest, stand all aloof,\n",
      "And do not interrupt me in my course.\n",
      "Why I descend into this bed of death,\n",
      "Is partly to behold my lady's face;\n",
      "But chiefly to take thence from her dead finger\n",
      "A precious ring, a ring that I must use\n",
      "In dear e\n",
      "-------\n",
      "tensor([[ 72,   1,  51,  73,  52,  97,   0,  31,  43,  43,   1,  65,  67,   1,\n",
      "         118, 117,  95,  56,   1,  96,   1,  84,   1, 101,   1,  50,  73,  42,\n",
      "           1,  86,   1,  44,  78,  87,  56,   8,   0,  19,  47,  95,   1,  83,\n",
      "           1,  72,   1, 117, 122,  58,  10,   1,  59,  54,  79,   1,  65,  63,\n",
      "           1, 117,  44,  43,  66,  21,   1, 108,  77,  45,  43,   1,  72,  43,\n",
      "          82,  35,  92,  58,  43,   5,  68,   1,  65,  67,   1,  87,  77,   5,\n",
      "          80,   1,  73,   1,  91,  89,  58,  66,  80,  86,   1,  39,  81,   1,\n",
      "          39,  50, 126,  44,  82, 123,   1,  42,  53,   1, 116,   1,  69,  58,\n",
      "          68,  56,  59,  54,  58,   1,  83,   1,  69,   1, 101,   1,  41, 127,\n",
      "          91,   8,   0,  35,  46,  63,   1,  21,   1, 118,  57, 104, 109,   1,\n",
      "          69,  84,   1,  65,  74,   1,  98,  42,   1,  94,   1, 118,  78,  46,\n",
      "          82,  21,  57,   1,  54,  77,  58,  50,  63,   1,  84,   1,  98,  46,\n",
      "          53, 111,   1, 101,   1,  50,  39,  42,  63,   5,  57,   1,  44,  39,\n",
      "         104,  11,   0,  14, 114,   1, 108,  47,  43,  44,  50,  63,   1,  84,\n",
      "           1,  58,  39, 124,   1,  72,  52, 104,   1,  44, 115,  51,   1,  87,\n",
      "          56,   1, 118,  39,  42,   1,  44,  97,  68,   0,  13,   1,  54,  93,\n",
      "          41,  47,  67,  57,   1,  56,  97,  66,  39,   1,  56,  97,   1, 121,\n",
      "           1,  21,   1,  51,  59,  80,   1,  59,  91,   0,  21,  52,   1, 118,\n",
      "          77,   1,  43,  51]])\n",
      "the morning\n",
      "See thou deliver it to my lord and father.\n",
      "Give me the light: upon thy life, I charge thee,\n",
      "Whate'er thou hear'st or seest, stand all aloof,\n",
      "And do not interrupt me in my course.\n",
      "Why I descend into this bed of death,\n",
      "Is partly to behold my lady's face;\n",
      "But chiefly to take thence from her dead finger\n",
      "A precious ring, a ring that I must use\n",
      "In dear em\n"
     ]
    }
   ],
   "source": [
    "# a demonstration of what a batch with batch_size=1 looks like. Notice the one-token offset in characters\n",
    "xb, yb = get_batch('train', 1)\n",
    "print(xb)\n",
    "print(tokenizer.decode(xb.squeeze(0).tolist()))\n",
    "print(\"-------\")\n",
    "print(yb)\n",
    "print(tokenizer.decode(yb.squeeze(0).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "78a7c1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, batch_size, eval_iters = 10): # to estimate loss during the training loop\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fc68c9",
   "metadata": {},
   "source": [
    "# Instantiating a brand new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06118186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to make sure nothing got messed up above. \n",
    "# if an error gets thrown in one of the test cells then the config values won't reset\n",
    "print(config)\n",
    "\n",
    "model = FractalFormer_base(config, tokenizer).to(config.device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3003f7",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "06f3d856",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3e-5\u001b[39m\n\u001b[1;32m      4\u001b[0m weight_decay \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.01\u001b[39m\n\u001b[0;32m----> 5\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlearning_rate, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# how long we want to train for\u001b[39;00m\n\u001b[1;32m      8\u001b[0m max_iters \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5000\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# create a PyTorch optimizer\n",
    "# this is not what they used, but this learning rate & weight decay work for our tiny minGemma\n",
    "learning_rate = 3e-5\n",
    "weight_decay = 0.01\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# how long we want to train for\n",
    "max_iters = 5000\n",
    "\n",
    "# how often we want to check & see how our loss is doing\n",
    "eval_interval = 250\n",
    "\n",
    "# batch size to use\n",
    "batch_size = 12\n",
    "\n",
    "# if you want to do debugging\n",
    "config.verbose['RMSNorm'] = False\n",
    "config.verbose['MLP'] = False\n",
    "config.verbose['MQA'] = False\n",
    "config.verbose['Layer'] = False\n",
    "config.verbose['OutputLayer'] = False\n",
    "config.verbose['FractalLoss'] = False\n",
    "config.verbose['FractalFormer'] = False\n",
    "config.verbose['Sampler'] = False\n",
    "config.verbose['Generate'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f76c3",
   "metadata": {},
   "source": [
    "# ------------ BOOKMARK ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ca3e922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 19.1140, val loss 22.1463, time elapsed: 1.54 seconds\n",
      "step 250: train loss 19.2682, val loss 22.2341, time elapsed: 354.09 seconds\n",
      "step 500: train loss 18.8027, val loss 22.1553, time elapsed: 688.91 seconds\n",
      "step 750: train loss 18.8095, val loss 21.7493, time elapsed: 1021.93 seconds\n",
      "step 1000: train loss 19.0256, val loss 22.6200, time elapsed: 1356.87 seconds\n",
      "step 1250: train loss 18.8943, val loss 22.5097, time elapsed: 1692.22 seconds\n",
      "step 1500: train loss 18.8110, val loss 22.2320, time elapsed: 2028.29 seconds\n",
      "step 1750: train loss 18.9138, val loss 21.8615, time elapsed: 2363.70 seconds\n",
      "step 2000: train loss 18.8240, val loss 22.2428, time elapsed: 2709.21 seconds\n",
      "step 2250: train loss 19.0780, val loss 21.9875, time elapsed: 3042.35 seconds\n",
      "step 2500: train loss 18.7679, val loss 22.1570, time elapsed: 3372.90 seconds\n",
      "step 2750: train loss 18.6269, val loss 22.4533, time elapsed: 3703.29 seconds\n",
      "step 3000: train loss 18.8560, val loss 21.8843, time elapsed: 4034.63 seconds\n",
      "step 3250: train loss 18.4526, val loss 22.2834, time elapsed: 4364.82 seconds\n",
      "step 3500: train loss 18.6428, val loss 22.4813, time elapsed: 4695.05 seconds\n",
      "step 3750: train loss 18.7680, val loss 22.3242, time elapsed: 5025.20 seconds\n",
      "step 4000: train loss 18.7381, val loss 22.0920, time elapsed: 5362.44 seconds\n",
      "step 4250: train loss 18.7113, val loss 21.8017, time elapsed: 5703.41 seconds\n",
      "step 4500: train loss 18.6947, val loss 22.1474, time elapsed: 6033.60 seconds\n",
      "step 4750: train loss 18.7029, val loss 22.4592, time elapsed: 6364.48 seconds\n",
      "step 4999: train loss 18.6812, val loss 22.1771, time elapsed: 6693.84 seconds\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "start_time = time.time()\n",
    "\n",
    "# Enable anomaly detection. uncomment these lines if you need to do extensive debugging\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size)\n",
    "    \n",
    "    # train\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss(model, batch_size)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Disable anomaly detection after the training loop\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac6744d",
   "metadata": {},
   "source": [
    "# Saving your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "090948b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model currently held in memory\n",
    "# the filename specifies the model's class, hyperparameters, and date/time it was saved\n",
    "import os\n",
    "\n",
    "# Ensure the directory exists\n",
    "model_dir = 'models'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Create a shorter, more concise filename\n",
    "filename = (f'{model.__class__.__name__}'\n",
    "           f'-v{config.vocab_size}'\n",
    "           f'-max_t{config.max_position_embeddings}'\n",
    "           f'-layers{config.num_hidden_layers}'\n",
    "           f'-heads{config.num_attention_heads}'\n",
    "           f'-kv_heads{config.num_key_value_heads}'\n",
    "           f'-hidden{config.hidden_size}'\n",
    "           f'-intermediate{config.intermediate_size}'\n",
    "           f'-head_dim{config.head_dim}'\n",
    "           f'-theta{config.rope_theta}'\n",
    "           f'-levels{config.levels}'\n",
    "           f'-split{config.split}'\n",
    "           f'-lr{learning_rate}'\n",
    "           f'-decay{weight_decay}'\n",
    "           f'-batch{batch_size}'\n",
    "            f'-train_iter{15000}'\n",
    "           f'--{time.strftime(\"%Y-%m-%d|%H-%M-%S\")}.pth')\n",
    "\n",
    "# Save the model\n",
    "model_path = os.path.join(model_dir, filename)\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859ce832",
   "metadata": {},
   "source": [
    "# Load a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "21eea23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972.672 K parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FractalFormer_base(\n",
       "  (embedder): Embedding(128, 128)\n",
       "  (embedder_norm): RMSNorm()\n",
       "  (layers): ModuleList(\n",
       "    (0-3): 4 x Layer(\n",
       "      (self_attn): MultiQueryAttention(\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (output_layer): OutputLayer(\n",
       "    (embedding_norm): RMSNorm()\n",
       "    (final_norm): RMSNorm()\n",
       "  )\n",
       "  (criterion): FractalLoss(\n",
       "    (criterion): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a blank model\n",
    "model = FractalFormer_base(config, tokenizer).to(config.device)  \n",
    "\n",
    "# here's the path to a minGemma model that i've trained with roughly 1m parameters\n",
    "path = 'models/FractalFormer_base-v128-max_t256-layers4-heads4-kv_heads1-hidden128-intermediate512-head_dim32-theta100.0-levels3-split2-lr3e-05-decay0.01-batch12-train_iter15000--2024-03-06|15-27-50.pth'\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(path))\n",
    "# REMEMBER TO CHANGE VALUES IN CONFIG TO MATCH THE MODEL YOU'VE LOADED\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# If you plan to continue training the model, switch to training mode\n",
    "#model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12b8c0b",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1923eecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level: 0, model: 0\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romeo can,\n",
      "And thou shalt a speech and the sun of the people.\n",
      "\n",
      "Second Murderer:\n",
      "Why, but I alone to-night.\n",
      "\n",
      "MENENIUS:\n",
      "What doubt the citizens, which you, gentlemen:\n",
      "Then shall be contented shall be so speak.\n",
      "This is the presence of their crown of a\n",
      "The people of a spirits of a speech of his \n",
      "level: 1, model: 0\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR\n",
      "level: 1, model: 1\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Rome.\n",
      "\n",
      "KING RICHARD III:\n",
      "Why, by the good call the comfort of this,\n",
      "The canst the fair such suppresion to so be so substance\n",
      "That should a sun a present the fear to be sposed\n",
      "I have the subts so some to be speed the people in the such princess to somet a man some all the such\n",
      "To \n",
      "level: 2, model: 0\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR\n",
      "level: 2, model: 1\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR\n",
      "level: 2, model: 2\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Rome, I have the peak and such all shal the compeak of the come spec I would so some a shall them the com thee all the come, and the priain of the prother such so condss of am the princes to thee a man; and shal the coms of so shall speak the peak the peot sumbles shall shame speace, the d\n",
      "level: 2, model: 3\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR\n"
     ]
    }
   ],
   "source": [
    "model.eval() # sets model to eval mode\n",
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou R\" # the classic line\n",
    "max_useable_output_len = config.max_position_embeddings - len(input_str)\n",
    "\n",
    "for i in range(config.levels):\n",
    "    for j in range(config.model_count[i]):\n",
    "        print(f\"level: {i}, model: {j}\")\n",
    "        output = model.generate(input_str, \n",
    "                                output_len = max_useable_output_len, \n",
    "                                temperature=0.7, \n",
    "                                top_k = 3, \n",
    "                                top_p = 0.95,\n",
    "                               level = i,\n",
    "                               model = j)\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a887a1",
   "metadata": {},
   "source": [
    "### so there's almost definitely something wrong happening here. \n",
    "i'm thinking it's just bad training dynamics given that we're trying to train them all simultaneously like in MatFormer when what we really should be doing is the idea from `FractalFormer_ModelMerging.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a9b78b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
