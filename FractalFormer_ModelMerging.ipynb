{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ccc40e3",
   "metadata": {},
   "source": [
    "# FractaFormer - Model Merging\n",
    "\n",
    "For this one i'm thinking i do the thing where i train tiny models and then combine them into a FractalFormer\n",
    "\n",
    "The question is, should i let the marger models have their own dense weight matrices or should they only get to use their upper-right and lower-left quadrants? gonna have to experiment with how matmul looks under the latter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95a204aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# used for the tokenizer\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# Imports used for the config\n",
    "import dataclasses \n",
    "from typing import Optional\n",
    "\n",
    "# Imports used for the model\n",
    "import re\n",
    "from typing import Any, List, Sequence, Tuple, Union\n",
    "import numpy as np\n",
    "\n",
    "# used in the training loop\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e172c181",
   "metadata": {},
   "source": [
    "# The Dataset\n",
    "\n",
    "the dataset we'll be using is just TinyShakespeare for sake of simplicity & ability to do run/train locally on any computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "49ca7881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "\n",
      " ['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] 65\n"
     ]
    }
   ],
   "source": [
    "# load the dataset\n",
    "with open('input.txt', 'r', encoding='utf-8') as f:\n",
    "    text = f.read()\n",
    "\n",
    "# the first 200 characters. It's just one continuous text document with all of the works of shakespeare back-to-back\n",
    "print(text[:200])\n",
    "\n",
    "# here are all the unique characters that occur in this text and how many there are\n",
    "chars = sorted(list(set(text)))\n",
    "v = len(chars)\n",
    "print('\\n', chars, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d23a7e",
   "metadata": {},
   "source": [
    "# The Tokenizer\n",
    "\n",
    "We'll be using a very simple tokenizer I previoiusly trained off of the TinyShakespeare dataset that has 128 total tokens and ignores stuff like special tokens & regex. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb1e7cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab length:  128\n",
      "Encoded: [22, 33, 24, 21, 17, 32, 71, 27, 1, 30, 53, 83, 53, 66, 30, 53, 83, 53, 2, 1, 61, 87, 93, 105, 43, 1, 77, 58, 1, 65, 67, 1, 30, 53, 83, 53, 12] 37\n",
      "Decoded: JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romeo? 49\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer data using pickle\n",
    "with open('./tokenizers/tokenizer.model', 'rb') as f:\n",
    "    loaded_tokenizer_data = pickle.load(f)\n",
    "\n",
    "# Extract the stoi mapping and merges from the loaded data\n",
    "loaded_stoi = loaded_tokenizer_data['stoi']\n",
    "loaded_merges = loaded_tokenizer_data['merges']\n",
    "\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, stoi, merges):\n",
    "        self.stoi = stoi\n",
    "        self.merges = merges\n",
    "        self.itos = {i: s for s, i in stoi.items()}  # Inverse mapping for decoding\n",
    "\n",
    "        self.vocab_len = len(stoi) + len(merges)\n",
    "\n",
    "    def encode(self, text):\n",
    "        # Convert the text to a list of token IDs, using space for unknown characters\n",
    "        tokens = [self.stoi.get(c, self.stoi[' ']) for c in text]\n",
    "\n",
    "        # Perform merging with the possibility of nested merges\n",
    "        i = 0\n",
    "        while i < len(tokens) - 1:\n",
    "            pair = (tokens[i], tokens[i + 1])\n",
    "            if pair in self.merges:\n",
    "                # Replace the current pair with its merged token\n",
    "                merged_token = self.merges[pair]\n",
    "                tokens[i] = merged_token\n",
    "                del tokens[i + 1]\n",
    "\n",
    "                # Move back to handle possible nested merges\n",
    "                if i > 0:\n",
    "                    i -= 1\n",
    "            else:\n",
    "                i += 1\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        def expand_token(token):\n",
    "            # Base case: if the token is a direct mapping, return its character\n",
    "            if token in self.itos:\n",
    "                return self.itos[token]\n",
    "            # Recursive case: if the token is a merged token, expand its constituents\n",
    "            elif token in self.merges.values():\n",
    "                pair = next(key for key, value in self.merges.items() if value == token)\n",
    "                return ''.join(expand_token(t) for t in pair)\n",
    "            # Fallback for unknown tokens\n",
    "            else:\n",
    "                return ''\n",
    "\n",
    "        # Decode each token in the list, handling nested merges recursively\n",
    "        return ''.join(expand_token(token) for token in tokens)\n",
    "        \n",
    "# Example usage\n",
    "# Assuming loaded_stoi and loaded_merges are already loaded from the tokenizer.model file\n",
    "\n",
    "tokenizer = SimpleTokenizer(loaded_stoi, loaded_merges)\n",
    "print(\"vocab length: \", tokenizer.vocab_len)\n",
    "\n",
    "# Encoding text\n",
    "encoded_text = tokenizer.encode(\"JULIET:\\nO Romeo, Romeo! wherefore art thou Romeo?\")\n",
    "print(\"Encoded:\", encoded_text, len(encoded_text))\n",
    "\n",
    "# Decoding back\n",
    "decoded_text = tokenizer.decode(encoded_text)\n",
    "print(\"Decoded:\", decoded_text, len(decoded_text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac092b1a",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dedbb53d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "single large model -> hierarchy of many smaller models inside\n",
      "model_count: [1, 2, 4]\n",
      "model_dim_list: [128, 64, 32]\n",
      "head_dim_list: [32, 16, 8]\n",
      "Config(vocab_size=128, max_position_embeddings=256, num_hidden_layers=4, num_attention_heads=4, num_key_value_heads=1, hidden_size=128, head_dim=32, rms_norm_eps=1e-06)\n"
     ]
    }
   ],
   "source": [
    "@dataclasses.dataclass # a class meant specifically to just hold data\n",
    "class Config:\n",
    "    \"\"\" \n",
    "    The default configuration & hyperparameters for FractalFormer\n",
    "    In this case, it's the hyperparameters of the largest model and the hyperparameters that define relative size of smaller models\n",
    "    \"\"\"\n",
    "    # The number of tokens in the vocabulary.\n",
    "    vocab_size: int = tokenizer.vocab_len\n",
    "    \n",
    "    # The maximum sequence length that this model might ever be used with.\n",
    "    max_position_embeddings: int = 256\n",
    "    \n",
    "    # The number of layers in the model.\n",
    "    num_hidden_layers: int = 4\n",
    "    \n",
    "    # The number of attention heads used in the attention layers of the model.\n",
    "    num_attention_heads: int = 4\n",
    "    \n",
    "    # The number of key-value heads for implementing multi-query attention.\n",
    "    num_key_value_heads: int = 1\n",
    "    # Ensures that the number of query heads is evenly divisible by the number of KV heads.\n",
    "    assert num_attention_heads % num_key_value_heads == 0\n",
    "    \n",
    "    # The hidden size of the model, AKA the embedding dimension\n",
    "    hidden_size: int = 128\n",
    "    # the attention heads need to cleanly divide up the hidden_size of the model for MQA\n",
    "    assert hidden_size % num_attention_heads == 0\n",
    "\n",
    "    # how much larger the inner dimension of the MLP should be than the hidden size of the model\n",
    "    intermediate_multiplier = 4\n",
    "    # The inner dimension of the MLP part of the decoder layer\n",
    "    @property\n",
    "    def intermediate_size(self):\n",
    "        return self.intermediate_multiplier * self.hidden_size\n",
    "    \n",
    "    # The number of head dimensions\n",
    "    head_dim: int = 32\n",
    "    \n",
    "    # The epsilon used by the rms normalization layers.\n",
    "    rms_norm_eps: float = 1e-6 # this is to promote numerical stability & prevent dividing by 0\n",
    "    \n",
    "    # the scaling factor that determines the frequencies for the rotary positional encodings\n",
    "    rope_theta = 100.0\n",
    "    # smaller models should use a smaller theta, but I'm just guessing here. 1000 might work too. 10,000 is the usual\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # the % of neurons to dropout in the MLP\n",
    "    dropout = 0.1\n",
    "\n",
    "    ####### for debugging & visualization\n",
    "    verbose = {\n",
    "    'RMSNorm': False,\n",
    "    'MLP': False,\n",
    "    'MQA': False,\n",
    "    'Layer': False,\n",
    "    'OutputLayer': False,\n",
    "    'FractalLoss': False,\n",
    "    'FractalFormer': False,\n",
    "    'Sampler': False,\n",
    "    'Generate': False\n",
    "    }\n",
    "\n",
    "    ####### FractalFormer-specific hyperparameters\n",
    "\n",
    "    # the number of levels for sub-models to exist on\n",
    "    levels = 3\n",
    "    \n",
    "    # the number of splits to make at a given level\n",
    "    split = 2 # i don't recommend choosing any value other than 2\n",
    "    # needs to be divisible by 2 in order to splice cleanly\n",
    "    assert split % 2 == 0\n",
    "    # RoPE requires a head dimension of length larger than 1 in order to work\n",
    "    assert head_dim // (split * (levels-1)) > 1\n",
    "    # really though you shouldn't be getting anywhere near that small of a head dimension even at the lowest level, that'd be useless\n",
    "\n",
    "    @property\n",
    "    def model_count(self):\n",
    "        return [self.split**i for i in range(self.levels)]\n",
    "\n",
    "    @property\n",
    "    def model_dim_list(self):\n",
    "        return [self.hidden_size // (self.split**i) for i in range(self.levels)]\n",
    "\n",
    "    @property\n",
    "    def head_dim_list(self):\n",
    "        return [self.head_dim // (self.split**i) for i in range(self.levels)]\n",
    "\n",
    "configs = {0: Config()}\n",
    "\n",
    "print(\"single large model -> hierarchy of many smaller models inside\")\n",
    "print(f\"model_count: {configs[0].model_count}\")\n",
    "print(f\"model_dim_list: {configs[0].model_dim_list}\")\n",
    "print(f\"head_dim_list: {configs[0].head_dim_list}\")\n",
    "print(configs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8453c44-ab57-4b42-a594-f304aceef633",
   "metadata": {},
   "source": [
    "### defining the smaller models' configs\n",
    "so that we can train the smaller models of that size first and then merge them all later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a664a78-d65b-403f-9445-6eadcae90180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(vocab_size=128, max_position_embeddings=256, num_hidden_layers=4, num_attention_heads=4, num_key_value_heads=1, hidden_size=64, head_dim=16, rms_norm_eps=1e-06)\n",
      "[1, 2]\n",
      "[64, 32]\n",
      "[16, 8]\n",
      "Config(vocab_size=128, max_position_embeddings=256, num_hidden_layers=4, num_attention_heads=4, num_key_value_heads=1, hidden_size=32, head_dim=8, rms_norm_eps=1e-06)\n",
      "[1]\n",
      "[32]\n",
      "[8]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, configs[0].levels):\n",
    "    # Create a new Config instance for the smaller configuration\n",
    "    configs[i] = dataclasses.replace(configs[i-1])\n",
    "\n",
    "    # adjust levels of new config accordingly\n",
    "    configs[i].levels = configs[i-1].levels - 1\n",
    "\n",
    "    # Update the hidden_size and other dependent properties\n",
    "    configs[i].hidden_size = configs[i-1].hidden_size // configs[i-1].split\n",
    "    configs[i].head_dim = configs[i-1].head_dim // configs[i-1].split\n",
    "\n",
    "    # Ensure hidden_size is divisible by the number of attention heads\n",
    "    assert configs[i].hidden_size % configs[i].num_attention_heads == 0, \"hidden_size must be divisible by num_attention_heads\"\n",
    "\n",
    "    # Ensure that the new head_dim values are valid\n",
    "    assert all(hd > 1 for hd in configs[i].head_dim_list), \"All head dimensions must be greater than 1\"\n",
    "\n",
    "    print(configs[i])\n",
    "    print(configs[i].model_count)\n",
    "    print(configs[i].model_dim_list)\n",
    "    print(configs[i].head_dim_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4d6eaf-9487-47de-9555-3787bb9eba8f",
   "metadata": {},
   "source": [
    "Now that i've got relevant configs for each model size, i need to redefine all my functions such that i can train the smaller models first, concatenate them, and then train the bigger models on top with the smaller models frozen. Let's start just by splitting up the dataset into relevant portions. In reality training a legit model i'd want to be using different datasets (or doing different finetunings of the same base model) but for our experiments here it'll be easier to just split up TinyShakespeare for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72a03cb0-14aa-439a-9a60-1fa563df6ac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 1115394 First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "---------\n",
      "1 0 557697 First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "---------\n",
      "1 1 557697  flattering truth of sleep,\n",
      "My dreams presage some joyful news at hand:\n",
      "My bosom's lord sits lightly in his throne;\n",
      "And all this day an unaccustom'd spirit\n",
      "Lifts me above the ground with cheerful thou\n",
      "---------\n",
      "2 0 278848 First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you\n",
      "---------\n",
      "2 1 278848  how to curse.\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "My words are dull; O, quicken them with thine!\n",
      "\n",
      "QUEEN MARGARET:\n",
      "Thy woes will make them sharp, and pierce like mine.\n",
      "\n",
      "DUCHESS OF YORK:\n",
      "Why should calamity be full of w\n",
      "---------\n",
      "2 2 278848 e flattering truth of sleep,\n",
      "My dreams presage some joyful news at hand:\n",
      "My bosom's lord sits lightly in his throne;\n",
      "And all this day an unaccustom'd spirit\n",
      "Lifts me above the ground with cheerful tho\n",
      "---------\n",
      "2 3 278848 ll one to me; for had I\n",
      "been the finder out of this secret, it would not\n",
      "have relished among my other discredits.\n",
      "Here come those I have done good to against my will,\n",
      "and already appearing in the blos\n"
     ]
    }
   ],
   "source": [
    "l = len(text)\n",
    "datasets = ((text,),)\n",
    "for i in range(1, configs[0].levels):\n",
    "    l = l // configs[i-1].split\n",
    "    datasets += (tuple([text[l*j:l*(j+1)] for j in range(configs[i-1].split**i)]),)\n",
    "\n",
    "print(0, 0, len(datasets[0][0]), datasets[0][0][:200])\n",
    "print(\"---------\")\n",
    "print(1, 0, len(datasets[1][0]), datasets[1][0][:200])\n",
    "print(\"---------\")\n",
    "print(1, 1, len(datasets[1][1]), datasets[1][1][:200])\n",
    "print(\"---------\")\n",
    "print(2, 0, len(datasets[2][0]), datasets[2][0][:200])\n",
    "print(\"---------\")\n",
    "print(2, 1, len(datasets[2][1]), datasets[2][1][:200])\n",
    "print(\"---------\")\n",
    "print(2, 2, len(datasets[2][2]), datasets[2][2][:200])\n",
    "print(\"---------\")\n",
    "print(2, 3, len(datasets[2][3]), datasets[2][3][:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17668b43-d8d3-4f60-afda-09c76dfea34a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight Matrix:\n",
      " Parameter containing:\n",
      "tensor([[ 0.0000,  0.0000, -0.3292,  0.1937],\n",
      "        [-0.9672, -0.2706, -0.0000,  0.0000]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# should the bigger models get their own full weight matrices or just use the quadrants?\n",
    "class CustomWeightedLayer(nn.Module):\n",
    "    def __init__(self, m, n):\n",
    "        super(CustomWeightedLayer, self).__init__()\n",
    "        self.m, self.n = m, n\n",
    "        self.weights = nn.Parameter(torch.Tensor(m, n))\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        # Initialize weights, for example, using Xavier initialization\n",
    "        nn.init.xavier_uniform_(self.weights)\n",
    "        \n",
    "        # Create and apply the mask\n",
    "        mask = self.create_mask(self.m, self.n)\n",
    "        self.weights.data *= mask\n",
    "\n",
    "        # Apply mask to zero out gradients where the mask is zero\n",
    "        self.weights.register_hook(lambda grad: grad * mask)\n",
    "\n",
    "    def create_mask(self, m, n):\n",
    "        mask = torch.ones(m, n)\n",
    "        m_mid = m // 2\n",
    "        n_mid = n // 2\n",
    "\n",
    "        # Adjust the mask based on the matrix dimensions\n",
    "        mask[:m_mid, :n_mid] = 0  # Upper-left quadrant\n",
    "        mask[-(m - m_mid):, -(n - n_mid):] = 0  # Lower-right quadrant\n",
    "        return mask\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.mm(x, self.weights)\n",
    "\n",
    "# Example Usage\n",
    "m, n = 2, 4  # Example dimensions of the weight matrix\n",
    "layer = CustomWeightedLayer(m, n)\n",
    "print(\"Weight Matrix:\\n\", layer.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad2e2482-e8bb-44f9-aec1-a756983aebb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4619, -2.6933],\n",
      "        [ 0.8262, -0.3242],\n",
      "        [-0.3523,  0.4842]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5595a5aa-8457-4a2e-8f91-f6d73332e654",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6050,  0.7288,  0.1521, -0.0895],\n",
       "        [ 0.3136,  0.0877, -0.2720,  0.1601],\n",
       "        [-0.4683, -0.1310,  0.1160, -0.0683]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3d5428",
   "metadata": {},
   "source": [
    "# Multi-Layer Perceptron\n",
    "\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/ffwd.jpeg\" width=\"512\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ba294f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements a multi-layer perceptron with a GeGLU gating mechanism. The GeGLU\n",
    "    activation combines a standard GeLU activation with a learned gating mechanism, enabling\n",
    "    the network to control the flow of information more dynamically.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        hidden_size: int,\n",
    "        intermediate_size: int,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initializes the GemmaMLP module.\n",
    "\n",
    "        Parameters:\n",
    "            hidden_size (int): The size of the input and output tensors.\n",
    "            intermediate_size (int): The size of the tensor after the initial transformation\n",
    "                                     and before the gating and final projection. This is typically\n",
    "                                     larger than the hidden size to allow for a richer representation.\n",
    "            dropout (float): the dropout rate to use during training in forwardTuple()\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        assert intermediate_size % hidden_size == 0\n",
    "        self.intermediate_multiplier = intermediate_size // hidden_size\n",
    "\n",
    "        # Linear transformation for the gating mechanism, projecting input to an intermediate size.\n",
    "        self.Wgate = nn.Parameter(torch.Tensor(hidden_size, intermediate_size))\n",
    "        self.Bgate = nn.Parameter(torch.Tensor(intermediate_size))\n",
    "\n",
    "        # Linear transformation for the input tensor, also projecting to the intermediate size but\n",
    "        # intended for element-wise multiplication with the gated output.\n",
    "        self.Wup = nn.Parameter(torch.Tensor(hidden_size, intermediate_size))\n",
    "        self.Bup = nn.Parameter(torch.Tensor(intermediate_size))\n",
    "\n",
    "        # Linear transformation to project the gated and combined tensor back to the original\n",
    "        # hidden size, completing the MLP structure.\n",
    "        self.Wdown = nn.Parameter(torch.Tensor(intermediate_size, hidden_size))\n",
    "        self.Bdown = nn.Parameter(torch.Tensor(hidden_size))\n",
    "\n",
    "        # Initialize weights with uniform distribution\n",
    "        # For gate & up, where in_features is hidden_size\n",
    "        limit_gateup = 1 / np.sqrt(hidden_size)\n",
    "        nn.init.uniform_(self.Wgate, -limit_gateup, limit_gateup)\n",
    "        nn.init.uniform_(self.Bgate, -limit_gateup, limit_gateup)\n",
    "        nn.init.uniform_(self.Wup, -limit_gateup, limit_gateup)\n",
    "        nn.init.uniform_(self.Bup, -limit_gateup, limit_gateup)\n",
    "        \n",
    "        # For down, where in_features is intermediate_size\n",
    "        limit_down = 1 / np.sqrt(intermediate_size)\n",
    "        nn.init.uniform_(self.Wdown, -limit_down, limit_down)\n",
    "        nn.init.uniform_(self.Bdown, -limit_down, limit_down)\n",
    "        \n",
    "        # defining our dropout for training in forwardTuple()\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "    def forwardTensor(self, x, model:int=0):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the MLP module during inference.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tensor): The input tensor to the MLP. \n",
    "                        shape (batch size, sequence length, hidden dimension) where hidden dimension changes by which model was used\n",
    "            model (int): the indicator of which model we're using. \n",
    "                        used in calculating our skip length for splicing. \n",
    "                        defaults to the equivalent of what's used in MatFormer+, meaning no skip, aka we use the top-left-most splice\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output tensor after applying the GeGLU gating mechanism and the MLP transformations.\n",
    "        \"\"\"\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- MLP.forwardTensor() ------------\")\n",
    "            print(f\"x: {x.shape}\\n{x}\")\n",
    "            \n",
    "        # figuring out how we should do our splicing\n",
    "        d_dim = x.shape[-1]\n",
    "        d_skip = model * d_dim\n",
    "        i_dim = d_dim * self.intermediate_multiplier\n",
    "        i_skip = model * i_dim\n",
    "        if verbose: \n",
    "            print(f\"d_dim: {d_dim}\")\n",
    "            print(f\"d_skip: {d_skip}\")\n",
    "            print(f\"i_dim: {i_dim}\")\n",
    "            print(f\"i_skip: {i_skip}\")\n",
    "        \n",
    "        # Applies linear transformation for gating.\n",
    "        Wgate = self.Wgate[d_skip:d_skip + d_dim, i_skip:i_skip + i_dim]\n",
    "        Bgate = self.Bgate[i_skip:i_skip + i_dim]\n",
    "        Xgate = x @ Wgate + Bgate\n",
    "        if verbose: \n",
    "            print(f\"Wgate: {self.Wgate.shape}\\n{self.Wgate}\")\n",
    "            print(f\"Wgate spliced: {Wgate.shape}\\n{Wgate}\")\n",
    "            print(f\"Bgate: {self.Bgate.shape}\\n{self.Bgate}\")\n",
    "            print(f\"Bgate spliced: {Bgate.shape}\\n{Bgate}\")\n",
    "            print(f\"Xgate: {Xgate.shape}\\n{Xgate}\")\n",
    "\n",
    "        # Applies GeLU activation to the gate, introducing non-linearity and enabling the gating mechanism.\n",
    "        Xgate = F.gelu(Xgate)\n",
    "        if verbose: print(f\"GeLU'ed Xgate: {Xgate.shape}\\n{Xgate}\")\n",
    "\n",
    "        # Applies another linear transformation to the input tensor for subsequent combination with the gate.\n",
    "        Wup = self.Wup[d_skip:d_skip + d_dim, i_skip:i_skip + i_dim]\n",
    "        Bup = self.Bup[i_skip:i_skip + i_dim]\n",
    "        Xup = x @ Wup + Bup\n",
    "        if verbose: \n",
    "            print(f\"Wup: {self.Wup.shape}\\n{self.Wup}\")\n",
    "            print(f\"Wup spliced: {Wup.shape}\\n{Wup}\")\n",
    "            print(f\"Bup: {self.Bup.shape}\\n{self.Bup}\")\n",
    "            print(f\"Bup spliced: {Bup.shape}\\n{Bup}\")\n",
    "            print(f\"Xup: {Xup.shape}\\n{Xup}\")\n",
    "\n",
    "        # Element-wise multiplication of the gated tensor with the transformed input tensor, modulating\n",
    "        # the input based on the gate's activation.\n",
    "        Xfuse = Xgate * Xup\n",
    "        if verbose: print(f\"Xfuse: {Xfuse.shape}\\n{Xfuse}\")\n",
    "\n",
    "        # Applies the final linear transformation to project the modulated tensor back to the hidden size.\n",
    "        Wdown = self.Wdown[i_skip:i_skip + i_dim, d_skip:d_skip + d_dim]\n",
    "        Bdown = self.Bdown[d_skip:d_skip + d_dim]\n",
    "        outputs = Xfuse @ Wdown + Bdown\n",
    "        if verbose: \n",
    "            print(f\"Wdown: {self.Wdown.shape}\\n{self.Wdown}\")\n",
    "            print(f\"Wdown spliced: {Wdown.shape}\\n{Wdown}\")\n",
    "            print(f\"Bdown: {self.Bdown.shape}\\n{self.Bdown}\")\n",
    "            print(f\"Bdown spliced: {Bdown.shape}\\n{Bdown}\")\n",
    "            print(f\"outputs: {outputs.shape}\\n{outputs}\") \n",
    "            print(\"------------- END MLP.forwardTensor() ------------\")\n",
    "\n",
    "        # Returns the final output tensor of the MLP, after gating and modulation.\n",
    "        return outputs\n",
    "\n",
    "    def forwardTuple(self, x, drop_bool: bool = True):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the MLP module during training.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tuple[Tuple[Tensor]]): \n",
    "                The input tuple of tuples of tensors to the MLP. \n",
    "                first tuple is of length config.levels and second layer of tuples have lengths of config.model_count\n",
    "                tensors are shape (batch size, sequence length, hidden dimension) where hidden dimension changes by which model was used\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tuple[Tensor]]: \n",
    "                The output tuple of tuples of tensors after applying the GeGLU gating mechanism and the MLP transformations.\n",
    "        \"\"\"\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- MLP.forwardTuple() ------------\")\n",
    "            print(f\"x: {x}\")\n",
    "\n",
    "        # if we had sent through the config we could've just grabbed these values from there but too late now\n",
    "        num_levels = len(x)\n",
    "        models_per_level = [len(x[i]) for i in range(num_levels)]\n",
    "        if verbose: \n",
    "            print(f\"num_levels: {num_levels}\")\n",
    "            print(f\"models_per_level: {models_per_level}\")\n",
    "        \n",
    "        out = ()\n",
    "        for i in range(num_levels):\n",
    "            if verbose: print(f\"i: {i}\")\n",
    "            \n",
    "            out_lvl = ()\n",
    "            for j in range(models_per_level[i]):\n",
    "                if verbose: print(f\"j: {j}\")\n",
    "\n",
    "                output = self.forwardTensor(x[i][j], model=j)\n",
    "                if verbose: print(f\"forwardTensor() output: {output.shape}\\n{output}\")\n",
    "                    \n",
    "                out_lvl += (self.drop(output),) if drop_bool else (output,)\n",
    "\n",
    "            # pretty sure i have to save & store everything without overwriting to prevent in-place arguments. so annoying\n",
    "            if verbose: print(f\"out_lvl: {out_lvl}\")\n",
    "            out += (out_lvl,)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"out: {out}\")\n",
    "            print(\"------------- END MLP.forwardTuple() ------------\")\n",
    "        return out\n",
    "        \n",
    "    def forward(self, x, model=0, drop_bool = True):\n",
    "        train = True if type(x) == tuple else False\n",
    "        if verbose: print(f\"---------- MLP Input: {'Tuple' if train else 'torch.Tensor'} ------------\")\n",
    "        return self.forwardTuple(x, drop_bool) if train else self.forwardTensor(x, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5cf879",
   "metadata": {},
   "source": [
    "The following two cells are designed to help you comprehend what's happening. If you walk through every single print statement and follow along even down to watching what happens to each weight, you'll be able to clearly see what's happening with the odd splicing behavior. In order to make this somewhat feasible, I've set very small matrices for these examples. However I will admit it is still inevitably a pain, which is why I included the drawings above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "949b1e2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [4, 2, 1]\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 4])\n",
      "tensor([[[0.9590, 0.9053, 0.5484, 0.0616],\n",
      "         [0.3631, 0.7774, 0.6103, 0.4077]]])\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 2, 4])\n",
      "tensor([[[0.9590, 0.9053, 0.5484, 0.0616],\n",
      "         [0.3631, 0.7774, 0.6103, 0.4077]]])\n",
      "d_dim: 4\n",
      "d_skip: 0\n",
      "i_dim: 8\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.3171,  0.0909,  0.0684, -0.1795, -0.0728, -0.2897,  0.2435,  0.3105],\n",
      "        [-0.3894, -0.1471,  0.2837,  0.4253,  0.4213,  0.4507,  0.3852,  0.1857],\n",
      "        [-0.4413,  0.4736,  0.2402, -0.0791,  0.2685, -0.2349,  0.1284,  0.1757],\n",
      "        [-0.3339,  0.3974,  0.1393, -0.0508,  0.0225, -0.4128, -0.1585, -0.2609]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([4, 8])\n",
      "tensor([[-0.3171,  0.0909,  0.0684, -0.1795, -0.0728, -0.2897,  0.2435,  0.3105],\n",
      "        [-0.3894, -0.1471,  0.2837,  0.4253,  0.4213,  0.4507,  0.3852,  0.1857],\n",
      "        [-0.4413,  0.4736,  0.2402, -0.0791,  0.2685, -0.2349,  0.1284,  0.1757],\n",
      "        [-0.3339,  0.3974,  0.1393, -0.0508,  0.0225, -0.4128, -0.1585, -0.2609]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bgate: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.2178,  0.4992, -0.1880, -0.3831, -0.4591, -0.3945,  0.3288, -0.2448],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([8])\n",
      "tensor([ 0.2178,  0.4992, -0.1880, -0.3831, -0.4591, -0.3945,  0.3288, -0.2448],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 2, 8])\n",
      "tensor([[[-0.7015,  0.7374,  0.2747, -0.2167,  0.0010, -0.4185,  0.9716,\n",
      "           0.3013],\n",
      "         [-0.6056,  0.8689,  0.2608, -0.1866,  0.0150, -0.4609,  0.7303,\n",
      "           0.0131]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 2, 8])\n",
      "tensor([[[-1.6941e-01,  5.6746e-01,  1.6711e-01, -8.9774e-02,  5.1905e-04,\n",
      "          -1.4137e-01,  8.1063e-01,  1.8635e-01],\n",
      "         [-1.6496e-01,  7.0167e-01,  1.5720e-01, -7.9495e-02,  7.5732e-03,\n",
      "          -1.4862e-01,  5.6047e-01,  6.6438e-03]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3408,  0.4799,  0.0785,  0.4262, -0.0696, -0.4963, -0.0069, -0.3831],\n",
      "        [ 0.0313,  0.1317, -0.0385,  0.1944,  0.0500,  0.3205,  0.0186,  0.3553],\n",
      "        [ 0.0207,  0.4250, -0.2098, -0.4162, -0.2649, -0.2880,  0.2186,  0.3439],\n",
      "        [-0.4636,  0.2728, -0.4901,  0.2212,  0.0104,  0.0842, -0.2296,  0.0206]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([4, 8])\n",
      "tensor([[ 0.3408,  0.4799,  0.0785,  0.4262, -0.0696, -0.4963, -0.0069, -0.3831],\n",
      "        [ 0.0313,  0.1317, -0.0385,  0.1944,  0.0500,  0.3205,  0.0186,  0.3553],\n",
      "        [ 0.0207,  0.4250, -0.2098, -0.4162, -0.2649, -0.2880,  0.2186,  0.3439],\n",
      "        [-0.4636,  0.2728, -0.4901,  0.2212,  0.0104,  0.0842, -0.2296,  0.0206]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bup: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.2605,  0.0264, -0.3558,  0.1979,  0.1892, -0.4093, -0.4754,  0.3830],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([8])\n",
      "tensor([-0.2605,  0.0264, -0.3558,  0.1979,  0.1892, -0.4093, -0.4754,  0.3830],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 2, 8])\n",
      "tensor([[[ 0.0775,  0.8557, -0.4607,  0.5680,  0.0232, -0.7479, -0.3594,\n",
      "           0.5271],\n",
      "         [-0.2888,  0.6736, -0.6851,  0.3400,  0.0454, -0.4818, -0.4237,\n",
      "           0.7384]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 2, 8])\n",
      "tensor([[[-1.3124e-02,  4.8556e-01, -7.6981e-02, -5.0990e-02,  1.2025e-05,\n",
      "           1.0573e-01, -2.9138e-01,  9.8228e-02],\n",
      "         [ 4.7640e-02,  4.7262e-01, -1.0770e-01, -2.7026e-02,  3.4417e-04,\n",
      "           7.1603e-02, -2.3746e-01,  4.9056e-03]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([8, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.0918, -0.0948, -0.0818, -0.1721],\n",
      "        [-0.2914, -0.1275, -0.1154,  0.1421],\n",
      "        [-0.1338,  0.2178,  0.2218, -0.3041],\n",
      "        [ 0.0629, -0.1122,  0.1579, -0.2674],\n",
      "        [-0.0542, -0.1065,  0.0567,  0.2069],\n",
      "        [ 0.3075, -0.3121, -0.2670, -0.1137],\n",
      "        [ 0.0380,  0.1773, -0.1895,  0.3024],\n",
      "        [ 0.0559,  0.0621, -0.2397,  0.2010]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([8, 4])\n",
      "tensor([[-0.0918, -0.0948, -0.0818, -0.1721],\n",
      "        [-0.2914, -0.1275, -0.1154,  0.1421],\n",
      "        [-0.1338,  0.2178,  0.2218, -0.3041],\n",
      "        [ 0.0629, -0.1122,  0.1579, -0.2674],\n",
      "        [-0.0542, -0.1065,  0.0567,  0.2069],\n",
      "        [ 0.3075, -0.3121, -0.2670, -0.1137],\n",
      "        [ 0.0380,  0.1773, -0.1895,  0.3024],\n",
      "        [ 0.0559,  0.0621, -0.2397,  0.2010]], grad_fn=<AliasBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([-0.2631, -0.1037,  0.2921, -0.3263], requires_grad=True)\n",
      "Bdown spliced: torch.Size([4])\n",
      "tensor([-0.2631, -0.1037,  0.2921, -0.3263], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.3694, -0.2539,  0.2155, -0.2984],\n",
      "         [-0.3792, -0.2531,  0.2303, -0.3063]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "y: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.3694, -0.2539,  0.2155, -0.2984],\n",
      "         [-0.3792, -0.2531,  0.2303, -0.3063]]], grad_fn=<AddBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.6903, 0.7198],\n",
      "         [0.7758, 0.3244]]])\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.6903, 0.7198],\n",
      "         [0.7758, 0.3244]]])\n",
      "d_dim: 2\n",
      "d_skip: 0\n",
      "i_dim: 4\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3721, -0.2653,  0.2543, -0.1729,  0.3946,  0.2006,  0.1398, -0.1370],\n",
      "        [-0.4820, -0.3294,  0.1541, -0.1470,  0.4915,  0.1973, -0.4628,  0.3831],\n",
      "        [ 0.4092,  0.2494,  0.1383, -0.4926,  0.4713,  0.2031, -0.2494, -0.3491],\n",
      "        [-0.2949,  0.2169,  0.1094, -0.2385,  0.4975, -0.1279,  0.2385, -0.2050]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([2, 4])\n",
      "tensor([[ 0.3721, -0.2653,  0.2543, -0.1729],\n",
      "        [-0.4820, -0.3294,  0.1541, -0.1470]], grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.3225, -0.0723,  0.1653,  0.2233,  0.3670,  0.2184, -0.2093, -0.4483],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([4])\n",
      "tensor([ 0.3225, -0.0723,  0.1653,  0.2233], grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.2324, -0.4925,  0.4518, -0.0019],\n",
      "         [ 0.4548, -0.3849,  0.4126,  0.0415]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.1376, -0.1533,  0.3046, -0.0009],\n",
      "         [ 0.3072, -0.1348,  0.2723,  0.0214]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1934, -0.0505, -0.1009,  0.3992, -0.2208,  0.4758, -0.3406, -0.4556],\n",
      "        [-0.0644, -0.1223, -0.4625,  0.0439, -0.2408, -0.2759, -0.2859, -0.2425],\n",
      "        [ 0.3346,  0.1107,  0.2777, -0.4382, -0.1617,  0.1851,  0.3420, -0.2783],\n",
      "        [-0.0735,  0.3174,  0.1874, -0.1128,  0.4047, -0.4875, -0.2295, -0.3056]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([2, 4])\n",
      "tensor([[ 0.1934, -0.0505, -0.1009,  0.3992],\n",
      "        [-0.0644, -0.1223, -0.4625,  0.0439]], grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.3781, -0.3239,  0.0993, -0.4160,  0.2348, -0.3190,  0.0547, -0.4645],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([4])\n",
      "tensor([ 0.3781, -0.3239,  0.0993, -0.4160], grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.4653, -0.4468, -0.3033, -0.1088],\n",
      "         [ 0.5073, -0.4028, -0.1290, -0.0920]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 2, 4])\n",
      "tensor([[[ 6.4006e-02,  6.8483e-02, -9.2383e-02,  1.0121e-04],\n",
      "         [ 1.5582e-01,  5.4289e-02, -3.5135e-02, -1.9715e-03]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([8, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.2937, -0.1417,  0.2159,  0.3497],\n",
      "        [-0.1476,  0.1759,  0.1707, -0.2557],\n",
      "        [ 0.1759, -0.0163, -0.1197, -0.0592],\n",
      "        [-0.2917,  0.3187, -0.2411, -0.2776],\n",
      "        [-0.2201,  0.0507,  0.2685, -0.3048],\n",
      "        [ 0.3404,  0.2153, -0.2846,  0.0942],\n",
      "        [-0.1362,  0.1204,  0.1580,  0.1315],\n",
      "        [ 0.2250, -0.2990,  0.1829,  0.0378]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([4, 2])\n",
      "tensor([[-0.2937, -0.1417],\n",
      "        [-0.1476,  0.1759],\n",
      "        [ 0.1759, -0.0163],\n",
      "        [-0.2917,  0.3187]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([ 0.2332, -0.0468,  0.3399,  0.0143], requires_grad=True)\n",
      "Bdown spliced: torch.Size([2])\n",
      "tensor([ 0.2332, -0.0468], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 2, 2])\n",
      "tensor([[[ 0.1880, -0.0423],\n",
      "         [ 0.1739, -0.0594]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "y: torch.Size([1, 2, 2])\n",
      "tensor([[[ 0.1880, -0.0423],\n",
      "         [ 0.1739, -0.0594]]], grad_fn=<AddBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.8677, 0.6273],\n",
      "         [0.7086, 0.1511]]])\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[0.8677, 0.6273],\n",
      "         [0.7086, 0.1511]]])\n",
      "d_dim: 2\n",
      "d_skip: 2\n",
      "i_dim: 4\n",
      "i_skip: 4\n",
      "Wgate: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1868, -0.4387,  0.1369,  0.1120, -0.4382, -0.3767, -0.2558,  0.1531],\n",
      "        [-0.0662, -0.2719, -0.2160, -0.0576,  0.1066, -0.0191, -0.3032,  0.3816],\n",
      "        [-0.2704, -0.3032,  0.0980, -0.4340, -0.3298, -0.2297, -0.2236, -0.2737],\n",
      "        [ 0.2648,  0.4766, -0.4654, -0.0138,  0.2561, -0.2541,  0.2586,  0.0256]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([2, 4])\n",
      "tensor([[-0.3298, -0.2297, -0.2236, -0.2737],\n",
      "        [ 0.2561, -0.2541,  0.2586,  0.0256]], grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.4192,  0.2134,  0.3531, -0.3293,  0.3661, -0.3184, -0.3966,  0.2974],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([4])\n",
      "tensor([ 0.3661, -0.3184, -0.3966,  0.2974], grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.2405, -0.6771, -0.4284,  0.0759],\n",
      "         [ 0.1711, -0.5196, -0.5160,  0.1073]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.1431, -0.1687, -0.1432,  0.0402],\n",
      "         [ 0.0971, -0.1567, -0.1563,  0.0582]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0100,  0.4017,  0.0283,  0.0137,  0.2652,  0.4562,  0.4357,  0.0927],\n",
      "        [ 0.0446, -0.1283,  0.4313, -0.0005, -0.1115, -0.4757, -0.1972,  0.0865],\n",
      "        [ 0.0713, -0.3977, -0.4773,  0.4891,  0.4132,  0.0448, -0.2791, -0.2230],\n",
      "        [-0.0879, -0.0834, -0.0674,  0.1820,  0.0427, -0.4441,  0.0070, -0.4121]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([2, 4])\n",
      "tensor([[ 0.4132,  0.0448, -0.2791, -0.2230],\n",
      "        [ 0.0427, -0.4441,  0.0070, -0.4121]], grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.1966, -0.0414, -0.0236,  0.1005,  0.4819,  0.0114, -0.2412,  0.0166],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([4])\n",
      "tensor([ 0.4819,  0.0114, -0.2412,  0.0166], grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.8672, -0.2283, -0.4789, -0.4354],\n",
      "         [ 0.7811, -0.0240, -0.4378, -0.2037]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.1241,  0.0385,  0.0686, -0.0175],\n",
      "         [ 0.0759,  0.0038,  0.0684, -0.0119]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([8, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.0520,  0.2317,  0.2748,  0.0215],\n",
      "        [-0.1287,  0.1932, -0.1813, -0.2229],\n",
      "        [ 0.3046,  0.0828, -0.2229, -0.2516],\n",
      "        [-0.3500,  0.1937,  0.3234, -0.3533],\n",
      "        [ 0.1030,  0.2242,  0.2532,  0.2763],\n",
      "        [ 0.2206, -0.3463, -0.0765, -0.2366],\n",
      "        [-0.0280, -0.1946, -0.1566,  0.2085],\n",
      "        [ 0.0915,  0.1842, -0.1441, -0.3313]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.2532,  0.2763],\n",
      "        [-0.0765, -0.2366],\n",
      "        [-0.1566,  0.2085],\n",
      "        [-0.1441, -0.3313]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([-0.3392,  0.0368,  0.0779,  0.1217], requires_grad=True)\n",
      "Bdown spliced: torch.Size([2])\n",
      "tensor([0.0779, 0.1217], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 2, 2])\n",
      "tensor([[[0.0982, 0.1670],\n",
      "         [0.0878, 0.1600]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "y: torch.Size([1, 2, 2])\n",
      "tensor([[[0.0982, 0.1670],\n",
      "         [0.0878, 0.1600]]], grad_fn=<AddBackward0>)\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "# Testing our MLP's forwardTensor()\n",
    "verbose = True\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold = config.hidden_size\n",
    "config.hidden_size = 4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,4)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "mlp = MLP(4,8)\n",
    "y = mlp(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,2)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "mlp = MLP(4,8)\n",
    "y = mlp(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,2,2)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "mlp = MLP(4,8)\n",
    "y = mlp(x, model=1)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold\n",
    "print(\"model_count: \", config.model_count)\n",
    "\n",
    "# clear up memory\n",
    "del hold, x, y, mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b0a4ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [4, 2]\n",
      "x: ((tensor([[[-1.2954, -0.2177,  0.5420,  0.8073],\n",
      "         [ 0.7121,  0.8843,  0.1992,  1.4276]]]),), (tensor([[[ 0.5660,  0.3989],\n",
      "         [ 0.2794, -0.2898]]]), tensor([[[-0.7002, -1.2412],\n",
      "         [-0.5178, -0.3491]]])))\n",
      "---------- MLP Input: Tuple ------------\n",
      "------------- MLP.forwardTuple() ------------\n",
      "x: ((tensor([[[-1.2954, -0.2177,  0.5420,  0.8073],\n",
      "         [ 0.7121,  0.8843,  0.1992,  1.4276]]]),), (tensor([[[ 0.5660,  0.3989],\n",
      "         [ 0.2794, -0.2898]]]), tensor([[[-0.7002, -1.2412],\n",
      "         [-0.5178, -0.3491]]])))\n",
      "num_levels: 2\n",
      "models_per_level: [1, 2]\n",
      "i: 0\n",
      "j: 0\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 2, 4])\n",
      "tensor([[[-1.2954, -0.2177,  0.5420,  0.8073],\n",
      "         [ 0.7121,  0.8843,  0.1992,  1.4276]]])\n",
      "d_dim: 4\n",
      "d_skip: 0\n",
      "i_dim: 8\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.0606, -0.1885,  0.0784, -0.0998, -0.4402, -0.0970,  0.0956, -0.4802],\n",
      "        [ 0.3875, -0.4589, -0.1352,  0.1169,  0.1690, -0.2213, -0.1950, -0.2144],\n",
      "        [ 0.4122,  0.1108, -0.0464,  0.3175, -0.0258, -0.3077,  0.4446, -0.3013],\n",
      "        [-0.2836, -0.0062,  0.4048, -0.3877,  0.4285, -0.4543,  0.4875,  0.1637]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([4, 8])\n",
      "tensor([[-0.0606, -0.1885,  0.0784, -0.0998, -0.4402, -0.0970,  0.0956, -0.4802],\n",
      "        [ 0.3875, -0.4589, -0.1352,  0.1169,  0.1690, -0.2213, -0.1950, -0.2144],\n",
      "        [ 0.4122,  0.1108, -0.0464,  0.3175, -0.0258, -0.3077,  0.4446, -0.3013],\n",
      "        [-0.2836, -0.0062,  0.4048, -0.3877,  0.4285, -0.4543,  0.4875,  0.1637]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bgate: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.2467,  0.2090,  0.2062,  0.2382,  0.3182,  0.0397,  0.4995, -0.3231],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([8])\n",
      "tensor([ 0.2467,  0.2090,  0.2062,  0.2382,  0.3182,  0.0397,  0.4995, -0.3231],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 2, 8])\n",
      "tensor([[[ 0.2353,  0.6082,  0.4358,  0.2012,  1.1836, -0.3200,  1.0526,\n",
      "           0.3145],\n",
      "         [ 0.2234, -0.3178,  0.7111, -0.2198,  0.7607, -0.9348,  1.1797,\n",
      "          -0.6809]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 2, 8])\n",
      "tensor([[[ 0.1396,  0.4430,  0.2913,  0.1166,  1.0437, -0.1198,  0.8987,\n",
      "           0.1961],\n",
      "         [ 0.1315, -0.1193,  0.5415, -0.0908,  0.5908, -0.1635,  1.0393,\n",
      "          -0.1688]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1189,  0.0801, -0.3857,  0.4545, -0.2728,  0.4734,  0.1506,  0.1763],\n",
      "        [-0.4573,  0.2401,  0.0509,  0.0939, -0.3356,  0.1727, -0.0005, -0.3044],\n",
      "        [ 0.3446, -0.4363, -0.2402, -0.0577, -0.1262, -0.1539, -0.3363,  0.3594],\n",
      "        [ 0.0626,  0.0808,  0.1597,  0.4585,  0.2732,  0.1700, -0.1052,  0.1336]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([4, 8])\n",
      "tensor([[ 0.1189,  0.0801, -0.3857,  0.4545, -0.2728,  0.4734,  0.1506,  0.1763],\n",
      "        [-0.4573,  0.2401,  0.0509,  0.0939, -0.3356,  0.1727, -0.0005, -0.3044],\n",
      "        [ 0.3446, -0.4363, -0.2402, -0.0577, -0.1262, -0.1539, -0.3363,  0.3594],\n",
      "        [ 0.0626,  0.0808,  0.1597,  0.4585,  0.2732,  0.1700, -0.1052,  0.1336]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bup: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.0255, -0.1659, -0.2057, -0.1109, -0.3902,  0.1022,  0.1655, -0.4352],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([8])\n",
      "tensor([-0.0255, -0.1659, -0.2057, -0.1109, -0.3902,  0.1022,  0.1655, -0.4352],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 2, 8])\n",
      "tensor([[[ 0.1574, -0.4932,  0.2817, -0.3812,  0.1883, -0.4948, -0.2968,\n",
      "          -0.2946],\n",
      "         [-0.1872,  0.1319, -0.2552,  0.9388, -0.5163,  0.8042,  0.0550,\n",
      "          -0.3165]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 2, 8])\n",
      "tensor([[[ 0.0220, -0.2185,  0.0821, -0.0445,  0.1965,  0.0593, -0.2667,\n",
      "          -0.0578],\n",
      "         [-0.0246, -0.0157, -0.1382, -0.0852, -0.3050, -0.1315,  0.0572,\n",
      "           0.0534]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([8, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.0954, -0.0599,  0.3390,  0.0801],\n",
      "        [ 0.1210,  0.1081,  0.2444, -0.2147],\n",
      "        [-0.0932,  0.1595,  0.0249,  0.3238],\n",
      "        [-0.0770,  0.1118,  0.2430,  0.2550],\n",
      "        [-0.1942,  0.1569, -0.1109,  0.1033],\n",
      "        [-0.2482, -0.0311, -0.2295, -0.2234],\n",
      "        [-0.1926, -0.0906, -0.0246, -0.2104],\n",
      "        [ 0.1760, -0.2417,  0.1809, -0.0161]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([8, 4])\n",
      "tensor([[-0.0954, -0.0599,  0.3390,  0.0801],\n",
      "        [ 0.1210,  0.1081,  0.2444, -0.2147],\n",
      "        [-0.0932,  0.1595,  0.0249,  0.3238],\n",
      "        [-0.0770,  0.1118,  0.2430,  0.2550],\n",
      "        [-0.1942,  0.1569, -0.1109,  0.1033],\n",
      "        [-0.2482, -0.0311, -0.2295, -0.2234],\n",
      "        [-0.1926, -0.0906, -0.0246, -0.2104],\n",
      "        [ 0.1760, -0.2417,  0.1809, -0.0161]], grad_fn=<AliasBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([-0.1292,  0.2481,  0.3290, -0.0053], requires_grad=True)\n",
      "Bdown spliced: torch.Size([4])\n",
      "tensor([-0.1292,  0.2481,  0.3290, -0.0053], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.1736,  0.2985,  0.2350,  0.1227],\n",
      "         [-0.0191,  0.1545,  0.3650, -0.0854]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "forwardTensor() output: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.1736,  0.2985,  0.2350,  0.1227],\n",
      "         [-0.0191,  0.1545,  0.3650, -0.0854]]], grad_fn=<AddBackward0>)\n",
      "out_lvl: (tensor([[[-0.1929,  0.3316,  0.2611,  0.1364],\n",
      "         [-0.0212,  0.1716,  0.4055, -0.0949]]], grad_fn=<MulBackward0>),)\n",
      "i: 1\n",
      "j: 0\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[ 0.5660,  0.3989],\n",
      "         [ 0.2794, -0.2898]]])\n",
      "d_dim: 2\n",
      "d_skip: 0\n",
      "i_dim: 4\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.0606, -0.1885,  0.0784, -0.0998, -0.4402, -0.0970,  0.0956, -0.4802],\n",
      "        [ 0.3875, -0.4589, -0.1352,  0.1169,  0.1690, -0.2213, -0.1950, -0.2144],\n",
      "        [ 0.4122,  0.1108, -0.0464,  0.3175, -0.0258, -0.3077,  0.4446, -0.3013],\n",
      "        [-0.2836, -0.0062,  0.4048, -0.3877,  0.4285, -0.4543,  0.4875,  0.1637]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([2, 4])\n",
      "tensor([[-0.0606, -0.1885,  0.0784, -0.0998],\n",
      "        [ 0.3875, -0.4589, -0.1352,  0.1169]], grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.2467,  0.2090,  0.2062,  0.2382,  0.3182,  0.0397,  0.4995, -0.3231],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([4])\n",
      "tensor([0.2467, 0.2090, 0.2062, 0.2382], grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.3669, -0.0807,  0.1966,  0.2283],\n",
      "         [ 0.1174,  0.2894,  0.2673,  0.1764]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.2360, -0.0378,  0.1136,  0.1348],\n",
      "         [ 0.0642,  0.1776,  0.1618,  0.1006]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1189,  0.0801, -0.3857,  0.4545, -0.2728,  0.4734,  0.1506,  0.1763],\n",
      "        [-0.4573,  0.2401,  0.0509,  0.0939, -0.3356,  0.1727, -0.0005, -0.3044],\n",
      "        [ 0.3446, -0.4363, -0.2402, -0.0577, -0.1262, -0.1539, -0.3363,  0.3594],\n",
      "        [ 0.0626,  0.0808,  0.1597,  0.4585,  0.2732,  0.1700, -0.1052,  0.1336]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([2, 4])\n",
      "tensor([[ 0.1189,  0.0801, -0.3857,  0.4545],\n",
      "        [-0.4573,  0.2401,  0.0509,  0.0939]], grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.0255, -0.1659, -0.2057, -0.1109, -0.3902,  0.1022,  0.1655, -0.4352],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([4])\n",
      "tensor([-0.0255, -0.1659, -0.2057, -0.1109], grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.1406, -0.0248, -0.4037,  0.1838],\n",
      "         [ 0.1403, -0.2131, -0.3282, -0.0111]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.0332,  0.0009, -0.0459,  0.0248],\n",
      "         [ 0.0090, -0.0378, -0.0531, -0.0011]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([8, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.0954, -0.0599,  0.3390,  0.0801],\n",
      "        [ 0.1210,  0.1081,  0.2444, -0.2147],\n",
      "        [-0.0932,  0.1595,  0.0249,  0.3238],\n",
      "        [-0.0770,  0.1118,  0.2430,  0.2550],\n",
      "        [-0.1942,  0.1569, -0.1109,  0.1033],\n",
      "        [-0.2482, -0.0311, -0.2295, -0.2234],\n",
      "        [-0.1926, -0.0906, -0.0246, -0.2104],\n",
      "        [ 0.1760, -0.2417,  0.1809, -0.0161]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([4, 2])\n",
      "tensor([[-0.0954, -0.0599],\n",
      "        [ 0.1210,  0.1081],\n",
      "        [-0.0932,  0.1595],\n",
      "        [-0.0770,  0.1118]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([-0.1292,  0.2481,  0.3290, -0.0053], requires_grad=True)\n",
      "Bdown spliced: torch.Size([2])\n",
      "tensor([-0.1292,  0.2481], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 2, 2])\n",
      "tensor([[[-0.1236,  0.2457],\n",
      "         [-0.1296,  0.2349]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "forwardTensor() output: torch.Size([1, 2, 2])\n",
      "tensor([[[-0.1236,  0.2457],\n",
      "         [-0.1296,  0.2349]]], grad_fn=<AddBackward0>)\n",
      "j: 1\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 2, 2])\n",
      "tensor([[[-0.7002, -1.2412],\n",
      "         [-0.5178, -0.3491]]])\n",
      "d_dim: 2\n",
      "d_skip: 2\n",
      "i_dim: 4\n",
      "i_skip: 4\n",
      "Wgate: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.0606, -0.1885,  0.0784, -0.0998, -0.4402, -0.0970,  0.0956, -0.4802],\n",
      "        [ 0.3875, -0.4589, -0.1352,  0.1169,  0.1690, -0.2213, -0.1950, -0.2144],\n",
      "        [ 0.4122,  0.1108, -0.0464,  0.3175, -0.0258, -0.3077,  0.4446, -0.3013],\n",
      "        [-0.2836, -0.0062,  0.4048, -0.3877,  0.4285, -0.4543,  0.4875,  0.1637]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([2, 4])\n",
      "tensor([[-0.0258, -0.3077,  0.4446, -0.3013],\n",
      "        [ 0.4285, -0.4543,  0.4875,  0.1637]], grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([ 0.2467,  0.2090,  0.2062,  0.2382,  0.3182,  0.0397,  0.4995, -0.3231],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([4])\n",
      "tensor([ 0.3182,  0.0397,  0.4995, -0.3231], grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.1955,  0.8190, -0.4170, -0.3154],\n",
      "         [ 0.1820,  0.3576,  0.0991, -0.2243]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.0826,  0.6500, -0.1411, -0.1187],\n",
      "         [ 0.1042,  0.2288,  0.0535, -0.0922]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1189,  0.0801, -0.3857,  0.4545, -0.2728,  0.4734,  0.1506,  0.1763],\n",
      "        [-0.4573,  0.2401,  0.0509,  0.0939, -0.3356,  0.1727, -0.0005, -0.3044],\n",
      "        [ 0.3446, -0.4363, -0.2402, -0.0577, -0.1262, -0.1539, -0.3363,  0.3594],\n",
      "        [ 0.0626,  0.0808,  0.1597,  0.4585,  0.2732,  0.1700, -0.1052,  0.1336]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([2, 4])\n",
      "tensor([[-0.1262, -0.1539, -0.3363,  0.3594],\n",
      "        [ 0.2732,  0.1700, -0.1052,  0.1336]], grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.0255, -0.1659, -0.2057, -0.1109, -0.3902,  0.1022,  0.1655, -0.4352],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([4])\n",
      "tensor([-0.3902,  0.1022,  0.1655, -0.4352], grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 2, 4])\n",
      "tensor([[[-0.6410, -0.0010,  0.5315, -0.8527],\n",
      "         [-0.4203,  0.1226,  0.3763, -0.6679]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 2, 4])\n",
      "tensor([[[ 0.0529, -0.0007, -0.0750,  0.1012],\n",
      "         [-0.0438,  0.0280,  0.0201,  0.0616]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([8, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.0954, -0.0599,  0.3390,  0.0801],\n",
      "        [ 0.1210,  0.1081,  0.2444, -0.2147],\n",
      "        [-0.0932,  0.1595,  0.0249,  0.3238],\n",
      "        [-0.0770,  0.1118,  0.2430,  0.2550],\n",
      "        [-0.1942,  0.1569, -0.1109,  0.1033],\n",
      "        [-0.2482, -0.0311, -0.2295, -0.2234],\n",
      "        [-0.1926, -0.0906, -0.0246, -0.2104],\n",
      "        [ 0.1760, -0.2417,  0.1809, -0.0161]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([4, 2])\n",
      "tensor([[-0.1109,  0.1033],\n",
      "        [-0.2295, -0.2234],\n",
      "        [-0.0246, -0.2104],\n",
      "        [ 0.1809, -0.0161]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([-0.1292,  0.2481,  0.3290, -0.0053], requires_grad=True)\n",
      "Bdown spliced: torch.Size([2])\n",
      "tensor([ 0.3290, -0.0053], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 2, 2])\n",
      "tensor([[[ 0.3435,  0.0145],\n",
      "         [ 0.3381, -0.0213]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "forwardTensor() output: torch.Size([1, 2, 2])\n",
      "tensor([[[ 0.3435,  0.0145],\n",
      "         [ 0.3381, -0.0213]]], grad_fn=<AddBackward0>)\n",
      "out_lvl: (tensor([[[-0.1373,  0.2730],\n",
      "         [-0.0000,  0.2610]]], grad_fn=<MulBackward0>), tensor([[[ 0.3816,  0.0000],\n",
      "         [ 0.3757, -0.0237]]], grad_fn=<MulBackward0>))\n",
      "out: ((tensor([[[-0.1929,  0.3316,  0.2611,  0.1364],\n",
      "         [-0.0212,  0.1716,  0.4055, -0.0949]]], grad_fn=<MulBackward0>),), (tensor([[[-0.1373,  0.2730],\n",
      "         [-0.0000,  0.2610]]], grad_fn=<MulBackward0>), tensor([[[ 0.3816,  0.0000],\n",
      "         [ 0.3757, -0.0237]]], grad_fn=<MulBackward0>)))\n",
      "------------- END MLP.forwardTuple() ------------\n",
      "out: ((tensor([[[-0.1929,  0.3316,  0.2611,  0.1364],\n",
      "         [-0.0212,  0.1716,  0.4055, -0.0949]]], grad_fn=<MulBackward0>),), (tensor([[[-0.1373,  0.2730],\n",
      "         [-0.0000,  0.2610]]], grad_fn=<MulBackward0>), tensor([[[ 0.3816,  0.0000],\n",
      "         [ 0.3757, -0.0237]]], grad_fn=<MulBackward0>)))\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n"
     ]
    }
   ],
   "source": [
    "# Testing our MLP's forwardTuple()\n",
    "verbose = True\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2 = config.hidden_size, config.levels\n",
    "config.hidden_size = 4\n",
    "config.levels = 2\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "mlp = MLP(4,8)\n",
    "x = ((torch.randn((1,2,4)),),\n",
    "     (torch.randn((1,2,2)),torch.randn((1,2,2)))\n",
    "    )\n",
    "print(f\"x: {x}\")\n",
    "out = mlp(x)\n",
    "print(f\"out: {out}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.levels = hold2\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "# clear up memory\n",
    "del hold1, hold2, x, out, mlp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14cf359",
   "metadata": {},
   "source": [
    "# Attention\n",
    "\n",
    "To subset the attention heads, we have to not only splice according to the model's embedding dimension but also take into account new smaller head sizes and how they're spaced throughout the matrix. I'm assuming you know how self-attention works well enough to look at this weight matrix and get the idea\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/sa.jpeg\" width=\"512\"/>\n",
    "</p>\n",
    "\n",
    "then we've gotta concatenate the outputs of each head\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/mha_concat.jpeg\" width=\"512\"/>\n",
    "</p>\n",
    "\n",
    "and after that linearly project them\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/mha_proj.jpeg\" width=\"512\"/>\n",
    "</p>\n",
    "\n",
    "this is the place where our splicing gets conceptually annoying. instead of just grabbing the matrix in the upper corner, because of the way attention head output concatenation works we actually need to skip over certain parts of the linear projection matrix and then concatenate them together in order to use them. Here's an example of what the matrix multiplication looks like. on the left is a simplified version of the concatenated attention heads where i just showed it as a matrix rather than a tensor, and then on the right is the actual projection matrix. notice how the numbers in the pink output matrix look similar to the first column of the purple output matrix with a positive number, its negative, and then a smaller positive number; that's the self-similarity in action. the yellow arrows point to the parts that get skipped over. obviously this would look a lot uglier with bigger matrices & incorporating the blue/green layer\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./images/mha_proj_matmul.jpeg\" width=\"512\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3f3dd6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiQueryAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Implements Multi-Query Attention which supports a distinct number of attention heads for queries and key-values (KV).\n",
    "    In the case where the same number of queries and key-values are used, this implemenation is equivalent to regular Multi-Head Attention.  \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.num_kv_heads = config.num_key_value_heads\n",
    "        \n",
    "        # Determines the number of query heads associated with each KV head.\n",
    "        self.num_queries_per_kv = self.num_heads // self.num_kv_heads\n",
    "\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.head_dim = config.head_dim\n",
    "        self.theta = config.rope_theta\n",
    "\n",
    "        # Calculates the total size for all query projections.\n",
    "        self.q_size = self.num_heads * self.head_dim\n",
    "        # Calculates the total size for all key and value projections.\n",
    "        self.kv_size = self.num_kv_heads * self.head_dim\n",
    "        \n",
    "        # Initialize our learnable matrices\n",
    "        # the linear projection layer for queries, keys, and values\n",
    "        # no real reason why we're creating one matrix instead of separate ones. cleaner model summary view?\n",
    "        self.Wqkv = nn.Parameter(torch.Tensor(self.hidden_size,\n",
    "                                              (self.num_heads + 2 * self.num_kv_heads) * self.head_dim))\n",
    "        # the output projection layer, mapping the concatenated attention outputs back to the hidden size.\n",
    "        self.Wo = nn.Parameter(torch.Tensor(self.num_heads * self.head_dim, self.hidden_size))\n",
    "        \n",
    "        # Initialize weights with uniform distribution\n",
    "        # For qkv_proj, where in_features is hidden_size\n",
    "        limit_Wqkv = 1 / np.sqrt(self.hidden_size)\n",
    "        nn.init.uniform_(self.Wqkv, -limit_Wqkv, limit_Wqkv)\n",
    "        # for o_proj, where in_features is self.num_heads * self.head_dim\n",
    "        limit_Wo = 1 / np.sqrt(self.num_heads * self.head_dim)\n",
    "        nn.init.uniform_(self.Wo, -limit_Wo, limit_Wo)\n",
    "        \n",
    "        # for our attention mask we'll use very large negative values to prevent attending to certain tokens\n",
    "        mask_negatives = torch.full((1, 1, config.max_position_embeddings, config.max_position_embeddings),\n",
    "                                 -2.3819763e38).to(torch.float)\n",
    "        # then we'll replace the lower triangular ones with 0's to allow attention to see past tokens\n",
    "        mask = torch.triu(mask_negatives, diagonal=1).to(config.device)\n",
    "        # to define self.mask as a tensor that shouldn't undergo gradient descent\n",
    "        self.register_buffer('mask', mask)\n",
    "        \n",
    "        # defining our dropout\n",
    "        self.drop = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forwardTensor(self,\n",
    "                      x: torch.Tensor,\n",
    "                      model: int = 0,\n",
    "                     ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            x (torch.Tensor): Te input tensor to the attention mechanism.\n",
    "                        shape (batch_size, input_len, hidden_size)\n",
    "            model (int): the indicator of which model we're using. \n",
    "                        used in calculating our skip length for splicing. \n",
    "                        defaults to the equivalent of what's used in MatFormer+, meaning no skip, aka we use the top-left-most splice\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: The output tensor after applying the attention mechanism\n",
    "        \"\"\"\n",
    "        global verbose\n",
    "        if verbose: print(\"----------------- MultiQueryAttention.forwardTensor() --------------------\")\n",
    "        \n",
    "        # Ensures the input tensor is 3-dimensional (batch_size, input_len, hidden_size).\n",
    "        x_shape = x.shape\n",
    "        assert len(x_shape) == 3\n",
    "        if verbose: print(f\"x shape: {x_shape}\")\n",
    "\n",
    "        # Extracts input sequence length and embedding dimension length from the hidden states tensor.\n",
    "        batch_size, input_len, d_dim = x_shape\n",
    "        \n",
    "        # figuring out how we should do our splicing\n",
    "        # first along the embedding dimension\n",
    "        d_skip = model * d_dim  # the size of our skip along the model's embedding dimension\n",
    "        if verbose: print(f\"d_skip: {d_skip}\")\n",
    "        \n",
    "        # then for splicing along the head sizes dimension\n",
    "        index = config.model_dim_list.index(d_dim)\n",
    "        models_in_this_level = config.model_count[index] # how many models are in this level\n",
    "        h_dim = config.head_dim_list[index] # the head dimension size of this model in this level\n",
    "        h_skip = model * h_dim # the size of our skip along the head dimension\n",
    "        if verbose: \n",
    "            print(f\"models_in_this_level: {models_in_this_level}\")\n",
    "            print(f\"h_dim: {h_dim}\")\n",
    "            print(f\"h_skip: {h_skip}\")\n",
    "\n",
    "        # Splits the Wqkv tensor into separate tensors for queries, keys, and values based on their respective sizes.\n",
    "        if verbose: print(f\"self.Wqkv: {self.Wqkv.shape}\\n{self.Wqkv}\")\n",
    "        Wq, Wk, Wv = self.Wqkv.split([self.q_size,\n",
    "                                      self.kv_size,\n",
    "                                      self.kv_size],dim=-1)\n",
    "        if verbose: \n",
    "            print(f\"Wq: {Wq.shape}\\n{Wq}\")\n",
    "            print(f\"Wk: {Wk.shape}\\n{Wk}\")\n",
    "            print(f\"Wv: {Wv.shape}\\n{Wv}\")\n",
    "        \n",
    "        # splicing to get our correct weight matrices for each respective head\n",
    "        # d_dim is relatively self-explanatory\n",
    "        # i*self.head_dim is bc we initialized one single q, k, and v matrix for all heads so we have to\n",
    "        # iterate through said matrix to get to the correct head\n",
    "        Wq = torch.cat([Wq[d_skip:d_skip + d_dim,\\\n",
    "                               i*self.head_dim + h_skip:i*self.head_dim + h_skip + h_dim] \\\n",
    "                               for i in range(self.num_heads)], dim=1)\n",
    "        Wk = torch.cat([Wk[d_skip:d_skip + d_dim,\\\n",
    "                               i*self.head_dim + h_skip:i*self.head_dim + h_skip + h_dim] \\\n",
    "                               for i in range(self.num_kv_heads)], dim=1)\n",
    "        Wv = torch.cat([Wv[d_skip:d_skip + d_dim,\\\n",
    "                               i*self.head_dim + h_skip:i*self.head_dim + h_skip + h_dim] \\\n",
    "                               for i in range(self.num_kv_heads)], dim=1)\n",
    "        if verbose:\n",
    "            print(f\"Wq spliced: {Wq.shape}\\n{Wq}\")\n",
    "            print(f\"Wk spliced: {Wk.shape}\\n{Wk}\")\n",
    "            print(f\"Wv spliced: {Wv.shape}\\n{Wv}\")\n",
    "        \n",
    "        # this needs to be size (d_dim, (self.num_heads + 2 * self.num_kv_heads) * h_dim) aka (32,24)\n",
    "        # recombine the spliced Wq Wk and Wv. Now they're the right size for matmul against x\n",
    "        Wqkv_spliced = torch.cat((Wq, Wk, Wv), dim=-1)\n",
    "        if verbose:\n",
    "            print(f\"Wqkv_spliced: {Wqkv_spliced.shape}\\n{Wqkv_spliced}\")\n",
    "        \n",
    "\n",
    "        # finally we can project x to get our queries, keys and values\n",
    "        xqkv = x @ Wqkv_spliced\n",
    "        if verbose: print(f\"xqkv: {xqkv.shape}\\n{xqkv}\")\n",
    "            \n",
    "        # Splits the combined Xqkv tensor into separate tensors for queries (xq), keys (xk), and values (xv) based on their respective sizes.\n",
    "        xq, xk, xv = xqkv.split([self.q_size // models_in_this_level,\n",
    "                                 self.kv_size // models_in_this_level,\n",
    "                                 self.kv_size // models_in_this_level],dim=-1)\n",
    "        if verbose:\n",
    "            print(f\"xq: {xq.shape}\\n{xq}\")\n",
    "            print(f\"xk: {xk.shape}\\n{xk}\")\n",
    "            print(f\"xv: {xv.shape}\\n{xv}\")\n",
    "\n",
    "        # Reshapes each of the Q, K, and V tensors to separate the heads and align the dimensions for attention operations.\n",
    "        xq = xq.view(batch_size, input_len, self.num_heads, h_dim)#, self.head_dim)\n",
    "        xk = xk.view(batch_size, input_len, self.num_kv_heads, h_dim)#, self.head_dim)\n",
    "        xv = xv.view(batch_size, input_len, self.num_kv_heads, h_dim)#, self.head_dim)\n",
    "        if verbose:\n",
    "            print(f\"xq reshaped: {xq.shape}\\n{xq}\")\n",
    "            print(f\"xk reshaped: {xk.shape}\\n{xk}\")\n",
    "            print(f\"xv reshaped: {xv.shape}\\n{xv}\")\n",
    "\n",
    "        # Applies rotary positional embeddings to queries and keys to incorporate positional information.\n",
    "        xq = apply_rotary_emb(xq, h_dim, self.theta)#self.head_dim\n",
    "        xk = apply_rotary_emb(xk, h_dim, self.theta)#self.head_dim\n",
    "        # is the differring head dimension going to mess with RoPE? Not sure\n",
    "        if verbose:\n",
    "            print(f\"rotated xq: {xq.shape}\\n{xq}\")\n",
    "            print(f\"rotated xk: {xk.shape}\\n{xk}\")\n",
    "\n",
    "        # If the number of KV heads is different from the number of query heads, adjusts keys and values to match the query heads count.\n",
    "        if self.num_kv_heads != self.num_heads:\n",
    "            # [batch_size, input_len, n_local_heads, head_dim]\n",
    "            xk = torch.repeat_interleave(xk, self.num_queries_per_kv, dim=2)\n",
    "            xv = torch.repeat_interleave(xv, self.num_queries_per_kv, dim=2)\n",
    "            if verbose:\n",
    "                print(f\"repeat_interleaved xk: {xk.shape}\\n{xk}\")\n",
    "                print(f\"repeat_interleaved xv: {xv.shape}\\n{xv}\")\n",
    "\n",
    "        # Transposes Q, K, and V tensors to align them for the batch matrix multiplication in attention calculation.\n",
    "        # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        q = xq.transpose(1, 2)\n",
    "        k = xk.transpose(1, 2)\n",
    "        v = xv.transpose(1, 2)\n",
    "        if verbose:\n",
    "            print(f\"transposed xq: {q.shape}\\n{q}\")\n",
    "            print(f\"transposed xk: {k.shape}\\n{k}\")\n",
    "            print(f\"transposed xv: {v.shape}\\n{v}\")\n",
    "\n",
    "        # Calculates attention scores by performing a batch matrix multiplication between queries and keys, followed by scaling.\n",
    "        # [batch_size, n_local_heads, input_len, input_len]\n",
    "        scores = torch.matmul(q, k.transpose(2, 3)) * h_dim**-0.5#self.scaling\n",
    "        if verbose: print(f\"scores: {scores.shape}\\n{scores}\")\n",
    "        \n",
    "        # Applies the lower-triangular mask to the attention scores\n",
    "        if verbose: print(f\"mask: {self.mask[...,:input_len, :input_len].shape}\\n{self.mask[...,:input_len, :input_len]}\")\n",
    "        scores = scores + self.mask[...,:input_len, :input_len] # make sure mask is the correct size. input_len <= max_seq_len\n",
    "        if verbose: print(f\"masked scores: {scores.shape}\\n{scores}\")\n",
    "\n",
    "        # Applies softmax to the scores to obtain attention probabilities\n",
    "        scores = F.softmax(scores, dim=-1)\n",
    "        if verbose: print(f\"softmaxed scores: {scores.shape}\\n{scores}\")\n",
    "        \n",
    "        # Computes the weighted sum of values based on the attention scores to obtain the output of the attention mechanism.\n",
    "        # [batch_size, n_local_heads, input_len, head_dim]\n",
    "        attention = torch.matmul(scores, v)\n",
    "        if verbose: print(f\"attention: {attention.shape}\\n{attention}\")\n",
    "\n",
    "        # Reshapes the attention output to match the expected output dimensions, combining the heads back into the hidden dimension.\n",
    "        # [batch_size, input_len, hidden_dim]\n",
    "        attention = attention.transpose(1, 2).contiguous().view(batch_size, input_len, -1)\n",
    "        if verbose: print(f\"reshaped attention: {attention.shape}\\n{attention}\")\n",
    "\n",
    "        # Splice the output projection\n",
    "        Wo = torch.cat([self.Wo[i*self.head_dim + h_skip:i*self.head_dim + h_skip + h_dim,\\\n",
    "                                d_skip:d_skip + d_dim,\\\n",
    "                               ] for i in range(self.num_heads)], dim=0)\n",
    "        if verbose: \n",
    "            print(f\"self.Wo: {self.Wo.shape}\\n{self.Wo}\")\n",
    "            print(f\"spliced Wo: {Wo.shape}\\n{Wo}\")\n",
    "            \n",
    "        # Applies the final linear projection to the attention output, mapping it back to the hidden size dimension.\n",
    "        output = attention @ Wo\n",
    "        if verbose: \n",
    "            print(f\"projected output: {output.shape}\\n{output}\")\n",
    "            print(\"----------------- END MultiQueryAttention.forwardTensor() --------------------\")\n",
    "            \n",
    "        return output\n",
    "\n",
    "    def forwardTuple(self,\n",
    "                     x: Tuple[Tuple[torch.Tensor]],\n",
    "                     drop_bool: bool = True\n",
    "                    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the Attention module during training.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tuple[Tuple[Tensor]]): \n",
    "                The input tuple of tuples of tensors \n",
    "                first tuple is of length config.levels and second layer of tuples have lengths of config.model_count\n",
    "                tensors are shape (batch size, sequence length, hidden dimension) where hidden dimension changes by which model was used\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tuple[Tensor]]: \n",
    "                The output tuple of tuples of tensors after applying the MQA mechanism\n",
    "        \"\"\"\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- MultiQueryAttention.forwardTuple() ------------\")\n",
    "            print(f\"x: {x}\")\n",
    "            \n",
    "        # forwardTuple() should only be used during training, so we assert input_len == max_position_embeddings\n",
    "        input_len = x[0][0].shape[1]\n",
    "        if verbose: print(f\"input_len: {input_len}\")\n",
    "        assert input_len == config.max_position_embeddings\n",
    "\n",
    "        # we could define these from the config but this way the method is more flexible to testing\n",
    "        num_levels = len(x)\n",
    "        models_per_level = [len(x[i]) for i in range(num_levels)]\n",
    "        if verbose: \n",
    "            print(f\"num_levels: {num_levels}\")\n",
    "            print(f\"models_per_level: {models_per_level}\")\n",
    "\n",
    "        # the loop that iterates over levels, aka the different potential sizes of models\n",
    "        out = ()\n",
    "        for i in range(num_levels):\n",
    "            if verbose: print(f\"Level {i} from range({num_levels})\")\n",
    "\n",
    "            # now for the loop that iterates over models in this level\n",
    "            out_lvl = ()\n",
    "            for j in range(models_per_level[i]):\n",
    "                if verbose: print(f\"Model {j} from range({models_per_level[i]})\")\n",
    "\n",
    "                output = self.forwardTensor(x[i][j], model=j)\n",
    "                if verbose: print(f\"forwardTensor() output: {output.shape}\\n{output}\")\n",
    "                \n",
    "                out_lvl += (self.drop(output),) if drop_bool else (output,)\n",
    "            \n",
    "            out += (out_lvl,)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"final output: {out}\")\n",
    "            print(\"------------- END MultiQueryAttention.forwardTuple() ------------\")\n",
    "\n",
    "        return out\n",
    "        \n",
    "    def forward(self, x, model=0, drop_bool = True):\n",
    "        train = True if type(x) == tuple else False\n",
    "        if verbose: print(f\"---------- Attention Input: {'Tuple' if train else 'torch.Tensor'} ------------\")\n",
    "        return self.forwardTuple(x, drop_bool) if train else self.forwardTensor(x, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c01ce03",
   "metadata": {},
   "source": [
    "And here are the detailed print statements for the attention mechanism"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aa877863",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [8, 4, 2]\n",
      "head_dim_list:  [4, 2, 1]\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[0.7055, 0.8466, 0.5308, 0.1871, 0.2744, 0.8346, 0.3970, 0.4018],\n",
      "         [0.1970, 0.3540, 0.4749, 0.4411, 0.6331, 0.9159, 0.9953, 0.9859],\n",
      "         [0.5595, 0.4854, 0.6228, 0.1502, 0.8666, 0.9870, 0.2195, 0.0108]]])\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 8])\n",
      "d_skip: 0\n",
      "models_in_this_level: 1\n",
      "h_dim: 4\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0084,  0.0866, -0.2544,  0.0649, -0.0546,  0.0121, -0.1667, -0.0016,\n",
      "         -0.3344, -0.3115, -0.0106,  0.3216, -0.2331,  0.0319,  0.0506,  0.3433],\n",
      "        [-0.2864,  0.2378,  0.3431, -0.3004,  0.0780,  0.3157, -0.1521,  0.1160,\n",
      "         -0.3229,  0.3109,  0.2964, -0.1863,  0.3386,  0.2874,  0.1643,  0.0029],\n",
      "        [-0.2331, -0.2533, -0.1527,  0.3497,  0.0862,  0.1933, -0.0548, -0.1622,\n",
      "         -0.2746, -0.2929,  0.3241,  0.0354, -0.0316,  0.2532, -0.3335,  0.2040],\n",
      "        [ 0.1016,  0.0741,  0.1273, -0.1665, -0.2856, -0.3145,  0.3292,  0.3235,\n",
      "          0.2705,  0.0480,  0.3108,  0.2241, -0.1163, -0.3025,  0.2067,  0.2108],\n",
      "        [-0.2789, -0.0457,  0.2792,  0.0371,  0.1832,  0.0741,  0.2172, -0.1994,\n",
      "         -0.0220, -0.2091,  0.2817, -0.1691, -0.2633, -0.3278,  0.0207,  0.2526],\n",
      "        [ 0.3077,  0.1695,  0.3477, -0.0902, -0.1547, -0.2350, -0.0558, -0.1805,\n",
      "         -0.2736,  0.1217, -0.3040, -0.2949, -0.2596,  0.0765, -0.3210, -0.2894],\n",
      "        [-0.1340,  0.0246,  0.1862,  0.2908,  0.2585,  0.3050, -0.3180,  0.0584,\n",
      "          0.3453, -0.0673, -0.0841, -0.2025,  0.0657, -0.1397,  0.2868,  0.3435],\n",
      "        [-0.1684,  0.2433, -0.0229,  0.1601, -0.0563,  0.2339, -0.3221, -0.1767,\n",
      "          0.0924, -0.1794, -0.1964, -0.0427, -0.0451,  0.2471,  0.0420,  0.0272]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[ 0.0084,  0.0866, -0.2544,  0.0649, -0.0546,  0.0121, -0.1667, -0.0016],\n",
      "        [-0.2864,  0.2378,  0.3431, -0.3004,  0.0780,  0.3157, -0.1521,  0.1160],\n",
      "        [-0.2331, -0.2533, -0.1527,  0.3497,  0.0862,  0.1933, -0.0548, -0.1622],\n",
      "        [ 0.1016,  0.0741,  0.1273, -0.1665, -0.2856, -0.3145,  0.3292,  0.3235],\n",
      "        [-0.2789, -0.0457,  0.2792,  0.0371,  0.1832,  0.0741,  0.2172, -0.1994],\n",
      "        [ 0.3077,  0.1695,  0.3477, -0.0902, -0.1547, -0.2350, -0.0558, -0.1805],\n",
      "        [-0.1340,  0.0246,  0.1862,  0.2908,  0.2585,  0.3050, -0.3180,  0.0584],\n",
      "        [-0.1684,  0.2433, -0.0229,  0.1601, -0.0563,  0.2339, -0.3221, -0.1767]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[-0.3344, -0.3115, -0.0106,  0.3216],\n",
      "        [-0.3229,  0.3109,  0.2964, -0.1863],\n",
      "        [-0.2746, -0.2929,  0.3241,  0.0354],\n",
      "        [ 0.2705,  0.0480,  0.3108,  0.2241],\n",
      "        [-0.0220, -0.2091,  0.2817, -0.1691],\n",
      "        [-0.2736,  0.1217, -0.3040, -0.2949],\n",
      "        [ 0.3453, -0.0673, -0.0841, -0.2025],\n",
      "        [ 0.0924, -0.1794, -0.1964, -0.0427]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[-0.2331,  0.0319,  0.0506,  0.3433],\n",
      "        [ 0.3386,  0.2874,  0.1643,  0.0029],\n",
      "        [-0.0316,  0.2532, -0.3335,  0.2040],\n",
      "        [-0.1163, -0.3025,  0.2067,  0.2108],\n",
      "        [-0.2633, -0.3278,  0.0207,  0.2526],\n",
      "        [-0.2596,  0.0765, -0.3210, -0.2894],\n",
      "        [ 0.0657, -0.1397,  0.2868,  0.3435],\n",
      "        [-0.0451,  0.2471,  0.0420,  0.0272]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([8, 8])\n",
      "tensor([[ 0.0084,  0.0866, -0.2544,  0.0649, -0.0546,  0.0121, -0.1667, -0.0016],\n",
      "        [-0.2864,  0.2378,  0.3431, -0.3004,  0.0780,  0.3157, -0.1521,  0.1160],\n",
      "        [-0.2331, -0.2533, -0.1527,  0.3497,  0.0862,  0.1933, -0.0548, -0.1622],\n",
      "        [ 0.1016,  0.0741,  0.1273, -0.1665, -0.2856, -0.3145,  0.3292,  0.3235],\n",
      "        [-0.2789, -0.0457,  0.2792,  0.0371,  0.1832,  0.0741,  0.2172, -0.1994],\n",
      "        [ 0.3077,  0.1695,  0.3477, -0.0902, -0.1547, -0.2350, -0.0558, -0.1805],\n",
      "        [-0.1340,  0.0246,  0.1862,  0.2908,  0.2585,  0.3050, -0.3180,  0.0584],\n",
      "        [-0.1684,  0.2433, -0.0229,  0.1601, -0.0563,  0.2339, -0.3221, -0.1767]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([8, 4])\n",
      "tensor([[-0.3344, -0.3115, -0.0106,  0.3216],\n",
      "        [-0.3229,  0.3109,  0.2964, -0.1863],\n",
      "        [-0.2746, -0.2929,  0.3241,  0.0354],\n",
      "        [ 0.2705,  0.0480,  0.3108,  0.2241],\n",
      "        [-0.0220, -0.2091,  0.2817, -0.1691],\n",
      "        [-0.2736,  0.1217, -0.3040, -0.2949],\n",
      "        [ 0.3453, -0.0673, -0.0841, -0.2025],\n",
      "        [ 0.0924, -0.1794, -0.1964, -0.0427]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([8, 4])\n",
      "tensor([[-0.2331,  0.0319,  0.0506,  0.3433],\n",
      "        [ 0.3386,  0.2874,  0.1643,  0.0029],\n",
      "        [-0.0316,  0.2532, -0.3335,  0.2040],\n",
      "        [-0.1163, -0.3025,  0.2067,  0.2108],\n",
      "        [-0.2633, -0.3278,  0.0207,  0.2526],\n",
      "        [-0.2596,  0.0765, -0.3210, -0.2894],\n",
      "        [ 0.0657, -0.1397,  0.2868,  0.3435],\n",
      "        [-0.0451,  0.2471,  0.0420,  0.0272]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([8, 16])\n",
      "tensor([[ 0.0084,  0.0866, -0.2544,  0.0649, -0.0546,  0.0121, -0.1667, -0.0016,\n",
      "         -0.3344, -0.3115, -0.0106,  0.3216, -0.2331,  0.0319,  0.0506,  0.3433],\n",
      "        [-0.2864,  0.2378,  0.3431, -0.3004,  0.0780,  0.3157, -0.1521,  0.1160,\n",
      "         -0.3229,  0.3109,  0.2964, -0.1863,  0.3386,  0.2874,  0.1643,  0.0029],\n",
      "        [-0.2331, -0.2533, -0.1527,  0.3497,  0.0862,  0.1933, -0.0548, -0.1622,\n",
      "         -0.2746, -0.2929,  0.3241,  0.0354, -0.0316,  0.2532, -0.3335,  0.2040],\n",
      "        [ 0.1016,  0.0741,  0.1273, -0.1665, -0.2856, -0.3145,  0.3292,  0.3235,\n",
      "          0.2705,  0.0480,  0.3108,  0.2241, -0.1163, -0.3025,  0.2067,  0.2108],\n",
      "        [-0.2789, -0.0457,  0.2792,  0.0371,  0.1832,  0.0741,  0.2172, -0.1994,\n",
      "         -0.0220, -0.2091,  0.2817, -0.1691, -0.2633, -0.3278,  0.0207,  0.2526],\n",
      "        [ 0.3077,  0.1695,  0.3477, -0.0902, -0.1547, -0.2350, -0.0558, -0.1805,\n",
      "         -0.2736,  0.1217, -0.3040, -0.2949, -0.2596,  0.0765, -0.3210, -0.2894],\n",
      "        [-0.1340,  0.0246,  0.1862,  0.2908,  0.2585,  0.3050, -0.3180,  0.0584,\n",
      "          0.3453, -0.0673, -0.0841, -0.2025,  0.0657, -0.1397,  0.2868,  0.3435],\n",
      "        [-0.1684,  0.2433, -0.0229,  0.1601, -0.0563,  0.2339, -0.3221, -0.1767,\n",
      "          0.0924, -0.1794, -0.1964, -0.0427, -0.0451,  0.2471,  0.0420,  0.0272]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.2818,  0.3783,  0.4853,  0.0606,  0.0210,  0.3587, -0.4565,\n",
      "          -0.1816, -0.6646, -0.1576,  0.1848, -0.2602, -0.1973,  0.3613,\n",
      "          -0.0951,  0.3675],\n",
      "         [-0.3599,  0.4043,  0.7129,  0.3872,  0.1079,  0.4330, -0.5151,\n",
      "          -0.3012, -0.0210, -0.3341,  0.0164, -0.5078, -0.3759,  0.0620,\n",
      "           0.0469,  0.5221],\n",
      "         [-0.2335,  0.1530,  0.5740,  0.0919,  0.0804,  0.1348, -0.0920,\n",
      "          -0.3370, -0.6865, -0.2764,  0.3098, -0.3373, -0.4737,  0.0331,\n",
      "          -0.3042,  0.3612]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.2818,  0.3783,  0.4853,  0.0606,  0.0210,  0.3587, -0.4565,\n",
      "          -0.1816],\n",
      "         [-0.3599,  0.4043,  0.7129,  0.3872,  0.1079,  0.4330, -0.5151,\n",
      "          -0.3012],\n",
      "         [-0.2335,  0.1530,  0.5740,  0.0919,  0.0804,  0.1348, -0.0920,\n",
      "          -0.3370]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.6646, -0.1576,  0.1848, -0.2602],\n",
      "         [-0.0210, -0.3341,  0.0164, -0.5078],\n",
      "         [-0.6865, -0.2764,  0.3098, -0.3373]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.1973,  0.3613, -0.0951,  0.3675],\n",
      "         [-0.3759,  0.0620,  0.0469,  0.5221],\n",
      "         [-0.4737,  0.0331, -0.3042,  0.3612]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.2818,  0.3783,  0.4853,  0.0606],\n",
      "          [ 0.0210,  0.3587, -0.4565, -0.1816]],\n",
      "\n",
      "         [[-0.3599,  0.4043,  0.7129,  0.3872],\n",
      "          [ 0.1079,  0.4330, -0.5151, -0.3012]],\n",
      "\n",
      "         [[-0.2335,  0.1530,  0.5740,  0.0919],\n",
      "          [ 0.0804,  0.1348, -0.0920, -0.3370]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[-0.6646, -0.1576,  0.1848, -0.2602]],\n",
      "\n",
      "         [[-0.0210, -0.3341,  0.0164, -0.5078]],\n",
      "\n",
      "         [[-0.6865, -0.2764,  0.3098, -0.3373]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[-0.1973,  0.3613, -0.0951,  0.3675]],\n",
      "\n",
      "         [[-0.3759,  0.0620,  0.0469,  0.5221]],\n",
      "\n",
      "         [[-0.4737,  0.0331, -0.3042,  0.3612]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.2818,  0.3783,  0.4853,  0.0606],\n",
      "          [ 0.0210,  0.3587, -0.4565, -0.1816]],\n",
      "\n",
      "         [[-0.7943,  0.3637,  0.0823,  0.4256],\n",
      "          [ 0.4918,  0.4609, -0.1875, -0.2564]],\n",
      "\n",
      "         [[-0.4247,  0.1317, -0.4512,  0.1205],\n",
      "          [ 0.0502,  0.1991,  0.1113, -0.3035]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[-0.6646, -0.1576,  0.1848, -0.2602]],\n",
      "\n",
      "         [[-0.0251, -0.2817, -0.0088, -0.5386]],\n",
      "\n",
      "         [[ 0.0040, -0.2038, -0.7532, -0.3855]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.6646, -0.1576,  0.1848, -0.2602],\n",
      "          [-0.6646, -0.1576,  0.1848, -0.2602]],\n",
      "\n",
      "         [[-0.0251, -0.2817, -0.0088, -0.5386],\n",
      "          [-0.0251, -0.2817, -0.0088, -0.5386]],\n",
      "\n",
      "         [[ 0.0040, -0.2038, -0.7532, -0.3855],\n",
      "          [ 0.0040, -0.2038, -0.7532, -0.3855]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.1973,  0.3613, -0.0951,  0.3675],\n",
      "          [-0.1973,  0.3613, -0.0951,  0.3675]],\n",
      "\n",
      "         [[-0.3759,  0.0620,  0.0469,  0.5221],\n",
      "          [-0.3759,  0.0620,  0.0469,  0.5221]],\n",
      "\n",
      "         [[-0.4737,  0.0331, -0.3042,  0.3612],\n",
      "          [-0.4737,  0.0331, -0.3042,  0.3612]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.2818,  0.3783,  0.4853,  0.0606],\n",
      "          [-0.7943,  0.3637,  0.0823,  0.4256],\n",
      "          [-0.4247,  0.1317, -0.4512,  0.1205]],\n",
      "\n",
      "         [[ 0.0210,  0.3587, -0.4565, -0.1816],\n",
      "          [ 0.4918,  0.4609, -0.1875, -0.2564],\n",
      "          [ 0.0502,  0.1991,  0.1113, -0.3035]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.6646, -0.1576,  0.1848, -0.2602],\n",
      "          [-0.0251, -0.2817, -0.0088, -0.5386],\n",
      "          [ 0.0040, -0.2038, -0.7532, -0.3855]],\n",
      "\n",
      "         [[-0.6646, -0.1576,  0.1848, -0.2602],\n",
      "          [-0.0251, -0.2817, -0.0088, -0.5386],\n",
      "          [ 0.0040, -0.2038, -0.7532, -0.3855]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.1973,  0.3613, -0.0951,  0.3675],\n",
      "          [-0.3759,  0.0620,  0.0469,  0.5221],\n",
      "          [-0.4737,  0.0331, -0.3042,  0.3612]],\n",
      "\n",
      "         [[-0.1973,  0.3613, -0.0951,  0.3675],\n",
      "          [-0.3759,  0.0620,  0.0469,  0.5221],\n",
      "          [-0.4737,  0.0331, -0.3042,  0.3612]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 1.0080e-01, -6.8199e-02, -2.3354e-01],\n",
      "          [ 1.8753e-01, -1.5623e-01, -1.5167e-01],\n",
      "          [ 7.3399e-02, -4.3672e-02,  1.3244e-01]],\n",
      "\n",
      "         [[-5.3815e-02,  1.2871e-04,  1.7041e-01],\n",
      "          [-1.8371e-01, -1.2001e-03,  7.4045e-02],\n",
      "          [ 1.7407e-02,  5.2571e-02, -3.6186e-03]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 1.0080e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 1.8753e-01, -1.5623e-01, -2.3820e+38],\n",
      "          [ 7.3399e-02, -4.3672e-02,  1.3244e-01]],\n",
      "\n",
      "         [[-5.3815e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [-1.8371e-01, -1.2001e-03, -2.3820e+38],\n",
      "          [ 1.7407e-02,  5.2571e-02, -3.6186e-03]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5851, 0.4149, 0.0000],\n",
      "          [0.3389, 0.3015, 0.3596]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4545, 0.5455, 0.0000],\n",
      "          [0.3317, 0.3435, 0.3248]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.1973,  0.3613, -0.0951,  0.3675],\n",
      "          [-0.2714,  0.2371, -0.0362,  0.4316],\n",
      "          [-0.3505,  0.1531, -0.1274,  0.4118]],\n",
      "\n",
      "         [[-0.1973,  0.3613, -0.0951,  0.3675],\n",
      "          [-0.2947,  0.1980, -0.0176,  0.4518],\n",
      "          [-0.3484,  0.1519, -0.1142,  0.4186]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.1973,  0.3613, -0.0951,  0.3675, -0.1973,  0.3613, -0.0951,\n",
      "           0.3675],\n",
      "         [-0.2714,  0.2371, -0.0362,  0.4316, -0.2947,  0.1980, -0.0176,\n",
      "           0.4518],\n",
      "         [-0.3505,  0.1531, -0.1274,  0.4118, -0.3484,  0.1519, -0.1142,\n",
      "           0.4186]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0597, -0.1373,  0.0479,  0.0980,  0.0058,  0.0190, -0.2007, -0.2555],\n",
      "        [ 0.1822, -0.2157,  0.2664, -0.0641, -0.0206,  0.1744, -0.2495, -0.1554],\n",
      "        [ 0.1675,  0.2889,  0.1855,  0.0850, -0.0499,  0.3136, -0.1914, -0.0892],\n",
      "        [ 0.3352, -0.3358, -0.2905,  0.0532, -0.0555, -0.2506,  0.2104, -0.2141],\n",
      "        [-0.3294, -0.0156,  0.0392, -0.0122,  0.2541, -0.1622, -0.2295,  0.1940],\n",
      "        [ 0.2476,  0.1660,  0.0828, -0.0334,  0.0612, -0.0539, -0.3360, -0.1502],\n",
      "        [ 0.1642,  0.1229,  0.2313, -0.1728,  0.1286,  0.1988,  0.2303,  0.2853],\n",
      "        [-0.1603,  0.3175, -0.1397,  0.1837, -0.1176,  0.1549, -0.2859,  0.0903]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([8, 8])\n",
      "tensor([[ 0.0597, -0.1373,  0.0479,  0.0980,  0.0058,  0.0190, -0.2007, -0.2555],\n",
      "        [ 0.1822, -0.2157,  0.2664, -0.0641, -0.0206,  0.1744, -0.2495, -0.1554],\n",
      "        [ 0.1675,  0.2889,  0.1855,  0.0850, -0.0499,  0.3136, -0.1914, -0.0892],\n",
      "        [ 0.3352, -0.3358, -0.2905,  0.0532, -0.0555, -0.2506,  0.2104, -0.2141],\n",
      "        [-0.3294, -0.0156,  0.0392, -0.0122,  0.2541, -0.1622, -0.2295,  0.1940],\n",
      "        [ 0.2476,  0.1660,  0.0828, -0.0334,  0.0612, -0.0539, -0.3360, -0.1502],\n",
      "        [ 0.1642,  0.1229,  0.2313, -0.1728,  0.1286,  0.1988,  0.2303,  0.2853],\n",
      "        [-0.1603,  0.3175, -0.1397,  0.1837, -0.1176,  0.1549, -0.2859,  0.0903]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.2413, -0.0337, -0.0887,  0.0432, -0.1077, -0.0121, -0.1581,\n",
      "          -0.1624],\n",
      "         [ 0.2365,  0.0095, -0.1443,  0.0611, -0.1468,  0.0203, -0.0391,\n",
      "          -0.1078],\n",
      "         [ 0.1902, -0.0105, -0.2053,  0.0627, -0.1648, -0.0327,  0.0262,\n",
      "          -0.0962]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "y: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.2413, -0.0337, -0.0887,  0.0432, -0.1077, -0.0121, -0.1581,\n",
      "          -0.1624],\n",
      "         [ 0.2365,  0.0095, -0.1443,  0.0611, -0.1468,  0.0203, -0.0391,\n",
      "          -0.1078],\n",
      "         [ 0.1902, -0.0105, -0.2053,  0.0627, -0.1648, -0.0327,  0.0262,\n",
      "          -0.0962]]], grad_fn=<UnsafeViewBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.7052, 0.9090, 0.2437, 0.8229],\n",
      "         [0.9149, 0.7958, 0.8123, 0.0176],\n",
      "         [0.3121, 0.5622, 0.3855, 0.6461]]])\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 0\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.1392,  0.3054,  0.2523, -0.2487, -0.1462,  0.1367,  0.1134,  0.0040,\n",
      "          0.2191,  0.2368, -0.1852, -0.0722,  0.2744,  0.1070,  0.1995,  0.0370],\n",
      "        [-0.0263,  0.3510, -0.2792,  0.0800,  0.2418,  0.0577,  0.0591, -0.2129,\n",
      "         -0.2521,  0.0748,  0.2188,  0.0645,  0.0890, -0.1207, -0.1185,  0.0143],\n",
      "        [-0.2224, -0.1190, -0.2701, -0.2932, -0.2479, -0.2400, -0.0516, -0.1673,\n",
      "          0.2079, -0.3043,  0.1995, -0.2971,  0.2414,  0.2849,  0.0042,  0.0236],\n",
      "        [-0.2004, -0.2620,  0.2192, -0.3240,  0.3258, -0.1425,  0.3060, -0.0306,\n",
      "         -0.0775,  0.3165,  0.0996, -0.2585, -0.1885, -0.1713, -0.1781, -0.3053],\n",
      "        [-0.1704, -0.3226,  0.2106,  0.0927,  0.0928,  0.2887,  0.2173,  0.3285,\n",
      "         -0.1991, -0.1266,  0.2658, -0.0461,  0.0969,  0.0444,  0.0833,  0.1097],\n",
      "        [-0.1935, -0.1724,  0.1966, -0.2535, -0.2465,  0.2696,  0.2114, -0.2955,\n",
      "         -0.0686,  0.2259,  0.3469, -0.0766, -0.3234,  0.3309, -0.0217, -0.2645],\n",
      "        [ 0.3271, -0.1387, -0.0247, -0.1288,  0.0223,  0.2145,  0.0927, -0.3050,\n",
      "          0.1359,  0.1475,  0.2708,  0.1470,  0.1229,  0.0825,  0.1072, -0.1320],\n",
      "        [ 0.1527, -0.2460, -0.1708, -0.1151, -0.1764, -0.3414,  0.2613,  0.1736,\n",
      "         -0.1436,  0.1861,  0.1008,  0.0334, -0.1075,  0.3492, -0.2751, -0.3293]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[-0.1392,  0.3054,  0.2523, -0.2487, -0.1462,  0.1367,  0.1134,  0.0040],\n",
      "        [-0.0263,  0.3510, -0.2792,  0.0800,  0.2418,  0.0577,  0.0591, -0.2129],\n",
      "        [-0.2224, -0.1190, -0.2701, -0.2932, -0.2479, -0.2400, -0.0516, -0.1673],\n",
      "        [-0.2004, -0.2620,  0.2192, -0.3240,  0.3258, -0.1425,  0.3060, -0.0306],\n",
      "        [-0.1704, -0.3226,  0.2106,  0.0927,  0.0928,  0.2887,  0.2173,  0.3285],\n",
      "        [-0.1935, -0.1724,  0.1966, -0.2535, -0.2465,  0.2696,  0.2114, -0.2955],\n",
      "        [ 0.3271, -0.1387, -0.0247, -0.1288,  0.0223,  0.2145,  0.0927, -0.3050],\n",
      "        [ 0.1527, -0.2460, -0.1708, -0.1151, -0.1764, -0.3414,  0.2613,  0.1736]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[ 0.2191,  0.2368, -0.1852, -0.0722],\n",
      "        [-0.2521,  0.0748,  0.2188,  0.0645],\n",
      "        [ 0.2079, -0.3043,  0.1995, -0.2971],\n",
      "        [-0.0775,  0.3165,  0.0996, -0.2585],\n",
      "        [-0.1991, -0.1266,  0.2658, -0.0461],\n",
      "        [-0.0686,  0.2259,  0.3469, -0.0766],\n",
      "        [ 0.1359,  0.1475,  0.2708,  0.1470],\n",
      "        [-0.1436,  0.1861,  0.1008,  0.0334]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.2744,  0.1070,  0.1995,  0.0370],\n",
      "        [ 0.0890, -0.1207, -0.1185,  0.0143],\n",
      "        [ 0.2414,  0.2849,  0.0042,  0.0236],\n",
      "        [-0.1885, -0.1713, -0.1781, -0.3053],\n",
      "        [ 0.0969,  0.0444,  0.0833,  0.1097],\n",
      "        [-0.3234,  0.3309, -0.0217, -0.2645],\n",
      "        [ 0.1229,  0.0825,  0.1072, -0.1320],\n",
      "        [-0.1075,  0.3492, -0.2751, -0.3293]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[-0.1392,  0.3054, -0.1462,  0.1367],\n",
      "        [-0.0263,  0.3510,  0.2418,  0.0577],\n",
      "        [-0.2224, -0.1190, -0.2479, -0.2400],\n",
      "        [-0.2004, -0.2620,  0.3258, -0.1425]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.2191,  0.2368],\n",
      "        [-0.2521,  0.0748],\n",
      "        [ 0.2079, -0.3043],\n",
      "        [-0.0775,  0.3165]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.2744,  0.1070],\n",
      "        [ 0.0890, -0.1207],\n",
      "        [ 0.2414,  0.2849],\n",
      "        [-0.1885, -0.1713]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[-0.1392,  0.3054, -0.1462,  0.1367,  0.2191,  0.2368,  0.2744,  0.1070],\n",
      "        [-0.0263,  0.3510,  0.2418,  0.0577, -0.2521,  0.0748,  0.0890, -0.1207],\n",
      "        [-0.2224, -0.1190, -0.2479, -0.2400,  0.2079, -0.3043,  0.2414,  0.2849],\n",
      "        [-0.2004, -0.2620,  0.3258, -0.1425, -0.0775,  0.3165, -0.1885, -0.1713]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.3412,  0.2898,  0.3243, -0.0269, -0.0877,  0.4212,  0.1782,\n",
      "          -0.1058],\n",
      "         [-0.3325,  0.4575, -0.1370, -0.0265,  0.1674,  0.0345,  0.5146,\n",
      "           0.2301],\n",
      "         [-0.2735,  0.0775,  0.2052, -0.1095, -0.0432,  0.2031,  0.1070,\n",
      "          -0.0353]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.3412,  0.2898,  0.3243, -0.0269],\n",
      "         [-0.3325,  0.4575, -0.1370, -0.0265],\n",
      "         [-0.2735,  0.0775,  0.2052, -0.1095]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.0877,  0.4212],\n",
      "         [ 0.1674,  0.0345],\n",
      "         [-0.0432,  0.2031]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.1782, -0.1058],\n",
      "         [ 0.5146,  0.2301],\n",
      "         [ 0.1070, -0.0353]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.3412,  0.2898],\n",
      "          [ 0.3243, -0.0269]],\n",
      "\n",
      "         [[-0.3325,  0.4575],\n",
      "          [-0.1370, -0.0265]],\n",
      "\n",
      "         [[-0.2735,  0.0775],\n",
      "          [ 0.2052, -0.1095]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.0877,  0.4212]],\n",
      "\n",
      "         [[ 0.1674,  0.0345]],\n",
      "\n",
      "         [[-0.0432,  0.2031]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.1782, -0.1058]],\n",
      "\n",
      "         [[ 0.5146,  0.2301]],\n",
      "\n",
      "         [[ 0.1070, -0.0353]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.3412,  0.2898],\n",
      "          [ 0.3243, -0.0269]],\n",
      "\n",
      "         [[-0.5646, -0.0326],\n",
      "          [-0.0517, -0.1296]],\n",
      "\n",
      "         [[ 0.0433, -0.2809],\n",
      "          [ 0.0142,  0.2321]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.0877,  0.4212]],\n",
      "\n",
      "         [[ 0.0614,  0.1595]],\n",
      "\n",
      "         [[-0.1667, -0.1238]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.0877,  0.4212],\n",
      "          [-0.0877,  0.4212]],\n",
      "\n",
      "         [[ 0.0614,  0.1595],\n",
      "          [ 0.0614,  0.1595]],\n",
      "\n",
      "         [[-0.1667, -0.1238],\n",
      "          [-0.1667, -0.1238]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.1782, -0.1058],\n",
      "          [ 0.1782, -0.1058]],\n",
      "\n",
      "         [[ 0.5146,  0.2301],\n",
      "          [ 0.5146,  0.2301]],\n",
      "\n",
      "         [[ 0.1070, -0.0353],\n",
      "          [ 0.1070, -0.0353]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.3412,  0.2898],\n",
      "          [-0.5646, -0.0326],\n",
      "          [ 0.0433, -0.2809]],\n",
      "\n",
      "         [[ 0.3243, -0.0269],\n",
      "          [-0.0517, -0.1296],\n",
      "          [ 0.0142,  0.2321]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.0877,  0.4212],\n",
      "          [ 0.0614,  0.1595],\n",
      "          [-0.1667, -0.1238]],\n",
      "\n",
      "         [[-0.0877,  0.4212],\n",
      "          [ 0.0614,  0.1595],\n",
      "          [-0.1667, -0.1238]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.1782, -0.1058],\n",
      "          [ 0.5146,  0.2301],\n",
      "          [ 0.1070, -0.0353]],\n",
      "\n",
      "         [[ 0.1782, -0.1058],\n",
      "          [ 0.5146,  0.2301],\n",
      "          [ 0.1070, -0.0353]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 0.1075,  0.0179,  0.0148],\n",
      "          [ 0.0253, -0.0282,  0.0694],\n",
      "          [-0.0863, -0.0298,  0.0195]],\n",
      "\n",
      "         [[-0.0281,  0.0111, -0.0359],\n",
      "          [-0.0354, -0.0169,  0.0174],\n",
      "          [ 0.0683,  0.0268, -0.0220]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 1.0747e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 2.5288e-02, -2.8195e-02, -2.3820e+38],\n",
      "          [-8.6349e-02, -2.9800e-02,  1.9485e-02]],\n",
      "\n",
      "         [[-2.8116e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [-3.5389e-02, -1.6863e-02, -2.3820e+38],\n",
      "          [ 6.8264e-02,  2.6798e-02, -2.1993e-02]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5134, 0.4866, 0.0000],\n",
      "          [0.3155, 0.3338, 0.3507]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4954, 0.5046, 0.0000],\n",
      "          [0.3481, 0.3339, 0.3180]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.1782, -0.1058],\n",
      "          [ 0.3419,  0.0577],\n",
      "          [ 0.2655,  0.0310]],\n",
      "\n",
      "         [[ 0.1782, -0.1058],\n",
      "          [ 0.3480,  0.0637],\n",
      "          [ 0.2679,  0.0288]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.1782, -0.1058,  0.1782, -0.1058],\n",
      "         [ 0.3419,  0.0577,  0.3480,  0.0637],\n",
      "         [ 0.2655,  0.0310,  0.2679,  0.0288]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3253,  0.2820,  0.0314,  0.0528,  0.2784,  0.1995, -0.1068, -0.3164],\n",
      "        [ 0.2007, -0.3053, -0.2800, -0.1283, -0.1645,  0.1116, -0.0165,  0.3122],\n",
      "        [-0.1902, -0.2614, -0.0180,  0.1553,  0.0878,  0.1289,  0.1809,  0.0632],\n",
      "        [-0.2043, -0.2110,  0.0896,  0.3074,  0.0191,  0.0422,  0.2051,  0.1111],\n",
      "        [-0.1785,  0.0575, -0.1395, -0.1240, -0.1703, -0.0314, -0.1743, -0.2787],\n",
      "        [ 0.2241, -0.2988,  0.3527, -0.3427,  0.3077,  0.3395,  0.3310, -0.0336],\n",
      "        [-0.2834, -0.2118, -0.0265, -0.0344, -0.3104, -0.1379, -0.1909,  0.3092],\n",
      "        [ 0.2960, -0.2871, -0.1685,  0.3353, -0.0085, -0.1714,  0.2479, -0.3514]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[ 0.3253,  0.2820,  0.0314,  0.0528],\n",
      "        [ 0.2007, -0.3053, -0.2800, -0.1283],\n",
      "        [-0.1785,  0.0575, -0.1395, -0.1240],\n",
      "        [ 0.2241, -0.2988,  0.3527, -0.3427]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0188,  0.1244, -0.0269,  0.0372],\n",
      "         [ 0.0749,  0.0798, -0.0315, -0.0543],\n",
      "         [ 0.0512,  0.0722, -0.0276, -0.0330]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "y: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0188,  0.1244, -0.0269,  0.0372],\n",
      "         [ 0.0749,  0.0798, -0.0315, -0.0543],\n",
      "         [ 0.0512,  0.0722, -0.0276, -0.0330]]], grad_fn=<UnsafeViewBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.2934, 0.9206, 0.0997, 0.2433],\n",
      "         [0.4900, 0.1746, 0.8388, 0.3176],\n",
      "         [0.6291, 0.2122, 0.1289, 0.2937]]])\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 4\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 2\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3092, -0.3057, -0.3403,  0.2563,  0.0584, -0.3289,  0.1528,  0.2759,\n",
      "          0.1744, -0.0354, -0.0263,  0.2060, -0.2975, -0.3090, -0.0182, -0.0562],\n",
      "        [ 0.0211, -0.3000, -0.0785,  0.2177,  0.0108,  0.2197,  0.2262, -0.3319,\n",
      "         -0.0449, -0.1821, -0.1487, -0.1496, -0.3321, -0.2146, -0.1623,  0.1949],\n",
      "        [ 0.1986,  0.0445, -0.0672,  0.2575, -0.0633, -0.2952, -0.1527,  0.3023,\n",
      "          0.1360, -0.1018,  0.0026,  0.1262, -0.2841, -0.2604, -0.2644,  0.1195],\n",
      "        [ 0.1059,  0.3175,  0.1181,  0.0045, -0.1922, -0.1806, -0.1427,  0.1904,\n",
      "          0.0911, -0.2641, -0.2635, -0.0513,  0.0612,  0.1808,  0.2505, -0.3287],\n",
      "        [ 0.1349, -0.3458,  0.0659, -0.3057,  0.1061,  0.0321,  0.1263, -0.2914,\n",
      "          0.0745, -0.0631,  0.2144,  0.1335,  0.0767,  0.1453,  0.1280,  0.2751],\n",
      "        [ 0.1553, -0.3180, -0.3337,  0.1825,  0.2597,  0.3227,  0.1897, -0.0163,\n",
      "         -0.0425,  0.0037,  0.1656, -0.2128,  0.3376,  0.2258, -0.1788, -0.0042],\n",
      "        [ 0.2529, -0.1979, -0.0050,  0.2298, -0.3524, -0.1205, -0.0511, -0.2180,\n",
      "         -0.1319, -0.2314,  0.2347, -0.1719,  0.0943, -0.2905,  0.0424, -0.0478],\n",
      "        [-0.0162, -0.1725,  0.0095, -0.2457,  0.2765,  0.3416, -0.0654, -0.0246,\n",
      "          0.0703, -0.1804, -0.3329, -0.1517,  0.1092,  0.0530,  0.0227,  0.0694]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[ 0.3092, -0.3057, -0.3403,  0.2563,  0.0584, -0.3289,  0.1528,  0.2759],\n",
      "        [ 0.0211, -0.3000, -0.0785,  0.2177,  0.0108,  0.2197,  0.2262, -0.3319],\n",
      "        [ 0.1986,  0.0445, -0.0672,  0.2575, -0.0633, -0.2952, -0.1527,  0.3023],\n",
      "        [ 0.1059,  0.3175,  0.1181,  0.0045, -0.1922, -0.1806, -0.1427,  0.1904],\n",
      "        [ 0.1349, -0.3458,  0.0659, -0.3057,  0.1061,  0.0321,  0.1263, -0.2914],\n",
      "        [ 0.1553, -0.3180, -0.3337,  0.1825,  0.2597,  0.3227,  0.1897, -0.0163],\n",
      "        [ 0.2529, -0.1979, -0.0050,  0.2298, -0.3524, -0.1205, -0.0511, -0.2180],\n",
      "        [-0.0162, -0.1725,  0.0095, -0.2457,  0.2765,  0.3416, -0.0654, -0.0246]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[ 0.1744, -0.0354, -0.0263,  0.2060],\n",
      "        [-0.0449, -0.1821, -0.1487, -0.1496],\n",
      "        [ 0.1360, -0.1018,  0.0026,  0.1262],\n",
      "        [ 0.0911, -0.2641, -0.2635, -0.0513],\n",
      "        [ 0.0745, -0.0631,  0.2144,  0.1335],\n",
      "        [-0.0425,  0.0037,  0.1656, -0.2128],\n",
      "        [-0.1319, -0.2314,  0.2347, -0.1719],\n",
      "        [ 0.0703, -0.1804, -0.3329, -0.1517]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[-0.2975, -0.3090, -0.0182, -0.0562],\n",
      "        [-0.3321, -0.2146, -0.1623,  0.1949],\n",
      "        [-0.2841, -0.2604, -0.2644,  0.1195],\n",
      "        [ 0.0612,  0.1808,  0.2505, -0.3287],\n",
      "        [ 0.0767,  0.1453,  0.1280,  0.2751],\n",
      "        [ 0.3376,  0.2258, -0.1788, -0.0042],\n",
      "        [ 0.0943, -0.2905,  0.0424, -0.0478],\n",
      "        [ 0.1092,  0.0530,  0.0227,  0.0694]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[ 0.0659, -0.3057,  0.1263, -0.2914],\n",
      "        [-0.3337,  0.1825,  0.1897, -0.0163],\n",
      "        [-0.0050,  0.2298, -0.0511, -0.2180],\n",
      "        [ 0.0095, -0.2457, -0.0654, -0.0246]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.2144,  0.1335],\n",
      "        [ 0.1656, -0.2128],\n",
      "        [ 0.2347, -0.1719],\n",
      "        [-0.3329, -0.1517]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.1280,  0.2751],\n",
      "        [-0.1788, -0.0042],\n",
      "        [ 0.0424, -0.0478],\n",
      "        [ 0.0227,  0.0694]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[ 0.0659, -0.3057,  0.1263, -0.2914,  0.2144,  0.1335,  0.1280,  0.2751],\n",
      "        [-0.3337,  0.1825,  0.1897, -0.0163,  0.1656, -0.2128, -0.1788, -0.0042],\n",
      "        [-0.0050,  0.2298, -0.0511, -0.2180,  0.2347, -0.1719,  0.0424, -0.0478],\n",
      "        [ 0.0095, -0.2457, -0.0654, -0.0246, -0.3329, -0.1517,  0.0227,  0.0694]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.2860,  0.0414,  0.1907, -0.1282,  0.1578, -0.2108, -0.1173,\n",
      "           0.0890],\n",
      "         [-0.0272, -0.0032,  0.0314, -0.3363,  0.2251, -0.1642,  0.0742,\n",
      "           0.1160],\n",
      "         [-0.0272, -0.1962,  0.0939, -0.2221,  0.1025, -0.0279,  0.0547,\n",
      "           0.1864]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.2860,  0.0414,  0.1907, -0.1282],\n",
      "         [-0.0272, -0.0032,  0.0314, -0.3363],\n",
      "         [-0.0272, -0.1962,  0.0939, -0.2221]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.1578, -0.2108],\n",
      "         [ 0.2251, -0.1642],\n",
      "         [ 0.1025, -0.0279]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.1173,  0.0890],\n",
      "         [ 0.0742,  0.1160],\n",
      "         [ 0.0547,  0.1864]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.2860,  0.0414],\n",
      "          [ 0.1907, -0.1282]],\n",
      "\n",
      "         [[-0.0272, -0.0032],\n",
      "          [ 0.0314, -0.3363]],\n",
      "\n",
      "         [[-0.0272, -0.1962],\n",
      "          [ 0.0939, -0.2221]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.1578, -0.2108]],\n",
      "\n",
      "         [[ 0.2251, -0.1642]],\n",
      "\n",
      "         [[ 0.1025, -0.0279]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.1173,  0.0890]],\n",
      "\n",
      "         [[ 0.0742,  0.1160]],\n",
      "\n",
      "         [[ 0.0547,  0.1864]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.2860,  0.0414],\n",
      "          [ 0.1907, -0.1282]],\n",
      "\n",
      "         [[-0.0120, -0.0246],\n",
      "          [ 0.3000, -0.1553]],\n",
      "\n",
      "         [[ 0.1897,  0.0569],\n",
      "          [ 0.1628,  0.1778]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.1578, -0.2108]],\n",
      "\n",
      "         [[ 0.2598,  0.1007]],\n",
      "\n",
      "         [[-0.0173,  0.1048]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.1578, -0.2108],\n",
      "          [ 0.1578, -0.2108]],\n",
      "\n",
      "         [[ 0.2598,  0.1007],\n",
      "          [ 0.2598,  0.1007]],\n",
      "\n",
      "         [[-0.0173,  0.1048],\n",
      "          [-0.0173,  0.1048]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.1173,  0.0890],\n",
      "          [-0.1173,  0.0890]],\n",
      "\n",
      "         [[ 0.0742,  0.1160],\n",
      "          [ 0.0742,  0.1160]],\n",
      "\n",
      "         [[ 0.0547,  0.1864],\n",
      "          [ 0.0547,  0.1864]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.2860,  0.0414],\n",
      "          [-0.0120, -0.0246],\n",
      "          [ 0.1897,  0.0569]],\n",
      "\n",
      "         [[ 0.1907, -0.1282],\n",
      "          [ 0.3000, -0.1553],\n",
      "          [ 0.1628,  0.1778]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.1578, -0.2108],\n",
      "          [ 0.2598,  0.1007],\n",
      "          [-0.0173,  0.1048]],\n",
      "\n",
      "         [[ 0.1578, -0.2108],\n",
      "          [ 0.2598,  0.1007],\n",
      "          [-0.0173,  0.1048]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.1173,  0.0890],\n",
      "          [ 0.0742,  0.1160],\n",
      "          [ 0.0547,  0.1864]],\n",
      "\n",
      "         [[-0.1173,  0.0890],\n",
      "          [ 0.0742,  0.1160],\n",
      "          [ 0.0547,  0.1864]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-0.0381, -0.0496,  0.0066],\n",
      "          [ 0.0023, -0.0040, -0.0017],\n",
      "          [ 0.0127,  0.0389,  0.0019]],\n",
      "\n",
      "         [[ 0.0404,  0.0259, -0.0118],\n",
      "          [ 0.0566,  0.0440, -0.0152],\n",
      "          [-0.0083,  0.0426,  0.0112]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-3.8089e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 2.3340e-03, -3.9515e-03, -2.3820e+38],\n",
      "          [ 1.2683e-02,  3.8897e-02,  1.8996e-03]],\n",
      "\n",
      "         [[ 4.0391e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 5.6613e-02,  4.4036e-02, -2.3820e+38],\n",
      "          [-8.3368e-03,  4.2578e-02,  1.1188e-02]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5016, 0.4984, 0.0000],\n",
      "          [0.3316, 0.3404, 0.3280]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5031, 0.4969, 0.0000],\n",
      "          [0.3255, 0.3425, 0.3319]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.1173,  0.0890],\n",
      "          [-0.0218,  0.1024],\n",
      "          [ 0.0043,  0.1301]],\n",
      "\n",
      "         [[-0.1173,  0.0890],\n",
      "          [-0.0221,  0.1024],\n",
      "          [ 0.0054,  0.1306]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.1173,  0.0890, -0.1173,  0.0890],\n",
      "         [-0.0218,  0.1024, -0.0221,  0.1024],\n",
      "         [ 0.0043,  0.1301,  0.0054,  0.1306]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0620, -0.2358, -0.1826, -0.2235, -0.1885,  0.0890, -0.2769, -0.1678],\n",
      "        [ 0.1249, -0.0510, -0.2737, -0.1929, -0.1012, -0.0346,  0.0996, -0.2800],\n",
      "        [-0.1597, -0.0478, -0.0584,  0.0079, -0.0456, -0.1100,  0.0573,  0.2482],\n",
      "        [ 0.3317,  0.1106,  0.1076, -0.0556, -0.3495, -0.2939, -0.1827, -0.3208],\n",
      "        [-0.3276,  0.1607,  0.2807,  0.2326,  0.2705,  0.0879,  0.0086, -0.2637],\n",
      "        [-0.2024, -0.0697, -0.3320,  0.1896, -0.3392, -0.0869,  0.0998, -0.0135],\n",
      "        [-0.2300, -0.1932, -0.3502, -0.2809, -0.1434,  0.2118, -0.2066, -0.1408],\n",
      "        [-0.1598, -0.0303, -0.0083, -0.1411,  0.2360,  0.3222,  0.0395, -0.2297]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[-0.0456, -0.1100,  0.0573,  0.2482],\n",
      "        [-0.3495, -0.2939, -0.1827, -0.3208],\n",
      "        [-0.1434,  0.2118, -0.2066, -0.1408],\n",
      "        [ 0.2360,  0.3222,  0.0395, -0.2297]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0121, -0.0094,  0.0048, -0.0616],\n",
      "         [-0.0075,  0.0006, -0.0114, -0.0587],\n",
      "         [-0.0156,  0.0045, -0.0195, -0.0714]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "y: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0121, -0.0094,  0.0048, -0.0616],\n",
      "         [-0.0075,  0.0006, -0.0114, -0.0587],\n",
      "         [-0.0156,  0.0045, -0.0195, -0.0714]]], grad_fn=<UnsafeViewBackward0>)\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n",
      "head_dim_list:  [32, 16, 8]\n"
     ]
    }
   ],
   "source": [
    "# Testing our Attention's forwardTensor()\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4 = config.hidden_size, config.num_attention_heads, config.head_dim, config.max_position_embeddings\n",
    "config.hidden_size = 8\n",
    "config.num_attention_heads = 2\n",
    "config.head_dim = 4\n",
    "config.max_position_embeddings = 3\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,8)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "att = MultiQueryAttention(config)\n",
    "y = att(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,4)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "att = MultiQueryAttention(config)\n",
    "y = att(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,4)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "att = MultiQueryAttention(config)\n",
    "y = att(x, model=1)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.num_attention_heads = hold2\n",
    "config.head_dim = hold3\n",
    "config.max_position_embeddings = hold4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "# clear up memory\n",
    "del hold1, hold2, hold3, hold4, x, att, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c22983d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [8, 4]\n",
      "head_dim_list:  [4, 2]\n",
      "x: ((tensor([[[ 1.2369,  0.7309,  2.6780, -1.0167, -0.7010,  0.2938, -0.0129,\n",
      "           0.9855],\n",
      "         [ 0.9342,  0.2813, -1.8521,  1.3869, -0.4117, -0.8592, -0.1207,\n",
      "           1.4316],\n",
      "         [ 0.2269,  1.2348,  0.3012,  1.9918,  0.5064,  0.1692, -0.0296,\n",
      "          -0.5073]]]),), (tensor([[[-0.2603, -0.4997, -0.3798,  1.5055],\n",
      "         [ 0.4200, -0.1175,  0.0111, -2.7566],\n",
      "         [ 0.5740, -0.1371, -0.4050,  0.2510]]]), tensor([[[ 0.2608, -2.1316, -0.3448,  1.6580],\n",
      "         [-1.6757, -1.1787, -0.4115,  0.2764],\n",
      "         [ 0.9920,  0.2771, -0.2763, -0.0181]]])))\n",
      "---------- Attention Input: Tuple ------------\n",
      "------------- MultiQueryAttention.forwardTuple() ------------\n",
      "x: ((tensor([[[ 1.2369,  0.7309,  2.6780, -1.0167, -0.7010,  0.2938, -0.0129,\n",
      "           0.9855],\n",
      "         [ 0.9342,  0.2813, -1.8521,  1.3869, -0.4117, -0.8592, -0.1207,\n",
      "           1.4316],\n",
      "         [ 0.2269,  1.2348,  0.3012,  1.9918,  0.5064,  0.1692, -0.0296,\n",
      "          -0.5073]]]),), (tensor([[[-0.2603, -0.4997, -0.3798,  1.5055],\n",
      "         [ 0.4200, -0.1175,  0.0111, -2.7566],\n",
      "         [ 0.5740, -0.1371, -0.4050,  0.2510]]]), tensor([[[ 0.2608, -2.1316, -0.3448,  1.6580],\n",
      "         [-1.6757, -1.1787, -0.4115,  0.2764],\n",
      "         [ 0.9920,  0.2771, -0.2763, -0.0181]]])))\n",
      "input_len: 3\n",
      "num_levels: 2\n",
      "models_per_level: [1, 2]\n",
      "Level 0 from range(2)\n",
      "Model 0 from range(1)\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 8])\n",
      "d_skip: 0\n",
      "models_in_this_level: 1\n",
      "h_dim: 4\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2527, -0.2941, -0.0126, -0.2985, -0.0427, -0.3478, -0.2334, -0.0192,\n",
      "          0.2420, -0.1638, -0.3066,  0.1400,  0.2726,  0.0764,  0.3247, -0.3018],\n",
      "        [ 0.3438,  0.2528,  0.2946,  0.0792, -0.0657, -0.0009, -0.2824, -0.0137,\n",
      "          0.0628, -0.0496,  0.2931,  0.2992, -0.2457,  0.3075,  0.2589,  0.2065],\n",
      "        [ 0.2754,  0.0175, -0.0211,  0.2802,  0.0330,  0.1987, -0.3279,  0.0989,\n",
      "         -0.0106,  0.1647, -0.0737,  0.2523,  0.0943,  0.0897,  0.1326, -0.2075],\n",
      "        [-0.3143,  0.0516,  0.1768,  0.2486, -0.2421,  0.1039,  0.2997,  0.1393,\n",
      "          0.0380,  0.1159, -0.2172, -0.1441,  0.3341, -0.1543,  0.2468, -0.3261],\n",
      "        [-0.2658, -0.2658, -0.2242,  0.3027,  0.0054, -0.1129,  0.2226,  0.3525,\n",
      "         -0.2329, -0.1480, -0.2079, -0.0429, -0.3268,  0.3424, -0.2798, -0.1772],\n",
      "        [-0.0191,  0.2375,  0.0977,  0.1757, -0.1853,  0.1919, -0.2512,  0.2967,\n",
      "         -0.0593,  0.0041,  0.1711, -0.1255, -0.0402, -0.1718, -0.0019, -0.0910],\n",
      "        [-0.0536,  0.1956, -0.0229,  0.2552, -0.2896, -0.0240,  0.2950, -0.2744,\n",
      "          0.0919, -0.1019,  0.1401,  0.3534,  0.1884, -0.2107, -0.3409, -0.2447],\n",
      "        [ 0.2660,  0.0382,  0.1990,  0.2905,  0.2758, -0.1844, -0.1002,  0.2525,\n",
      "          0.2465,  0.2066,  0.3480, -0.2099, -0.1332, -0.1288,  0.2787, -0.1696]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[ 0.2527, -0.2941, -0.0126, -0.2985, -0.0427, -0.3478, -0.2334, -0.0192],\n",
      "        [ 0.3438,  0.2528,  0.2946,  0.0792, -0.0657, -0.0009, -0.2824, -0.0137],\n",
      "        [ 0.2754,  0.0175, -0.0211,  0.2802,  0.0330,  0.1987, -0.3279,  0.0989],\n",
      "        [-0.3143,  0.0516,  0.1768,  0.2486, -0.2421,  0.1039,  0.2997,  0.1393],\n",
      "        [-0.2658, -0.2658, -0.2242,  0.3027,  0.0054, -0.1129,  0.2226,  0.3525],\n",
      "        [-0.0191,  0.2375,  0.0977,  0.1757, -0.1853,  0.1919, -0.2512,  0.2967],\n",
      "        [-0.0536,  0.1956, -0.0229,  0.2552, -0.2896, -0.0240,  0.2950, -0.2744],\n",
      "        [ 0.2660,  0.0382,  0.1990,  0.2905,  0.2758, -0.1844, -0.1002,  0.2525]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[ 0.2420, -0.1638, -0.3066,  0.1400],\n",
      "        [ 0.0628, -0.0496,  0.2931,  0.2992],\n",
      "        [-0.0106,  0.1647, -0.0737,  0.2523],\n",
      "        [ 0.0380,  0.1159, -0.2172, -0.1441],\n",
      "        [-0.2329, -0.1480, -0.2079, -0.0429],\n",
      "        [-0.0593,  0.0041,  0.1711, -0.1255],\n",
      "        [ 0.0919, -0.1019,  0.1401,  0.3534],\n",
      "        [ 0.2465,  0.2066,  0.3480, -0.2099]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.2726,  0.0764,  0.3247, -0.3018],\n",
      "        [-0.2457,  0.3075,  0.2589,  0.2065],\n",
      "        [ 0.0943,  0.0897,  0.1326, -0.2075],\n",
      "        [ 0.3341, -0.1543,  0.2468, -0.3261],\n",
      "        [-0.3268,  0.3424, -0.2798, -0.1772],\n",
      "        [-0.0402, -0.1718, -0.0019, -0.0910],\n",
      "        [ 0.1884, -0.2107, -0.3409, -0.2447],\n",
      "        [-0.1332, -0.1288,  0.2787, -0.1696]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([8, 8])\n",
      "tensor([[ 0.2527, -0.2941, -0.0126, -0.2985, -0.0427, -0.3478, -0.2334, -0.0192],\n",
      "        [ 0.3438,  0.2528,  0.2946,  0.0792, -0.0657, -0.0009, -0.2824, -0.0137],\n",
      "        [ 0.2754,  0.0175, -0.0211,  0.2802,  0.0330,  0.1987, -0.3279,  0.0989],\n",
      "        [-0.3143,  0.0516,  0.1768,  0.2486, -0.2421,  0.1039,  0.2997,  0.1393],\n",
      "        [-0.2658, -0.2658, -0.2242,  0.3027,  0.0054, -0.1129,  0.2226,  0.3525],\n",
      "        [-0.0191,  0.2375,  0.0977,  0.1757, -0.1853,  0.1919, -0.2512,  0.2967],\n",
      "        [-0.0536,  0.1956, -0.0229,  0.2552, -0.2896, -0.0240,  0.2950, -0.2744],\n",
      "        [ 0.2660,  0.0382,  0.1990,  0.2905,  0.2758, -0.1844, -0.1002,  0.2525]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([8, 4])\n",
      "tensor([[ 0.2420, -0.1638, -0.3066,  0.1400],\n",
      "        [ 0.0628, -0.0496,  0.2931,  0.2992],\n",
      "        [-0.0106,  0.1647, -0.0737,  0.2523],\n",
      "        [ 0.0380,  0.1159, -0.2172, -0.1441],\n",
      "        [-0.2329, -0.1480, -0.2079, -0.0429],\n",
      "        [-0.0593,  0.0041,  0.1711, -0.1255],\n",
      "        [ 0.0919, -0.1019,  0.1401,  0.3534],\n",
      "        [ 0.2465,  0.2066,  0.3480, -0.2099]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([8, 4])\n",
      "tensor([[ 0.2726,  0.0764,  0.3247, -0.3018],\n",
      "        [-0.2457,  0.3075,  0.2589,  0.2065],\n",
      "        [ 0.0943,  0.0897,  0.1326, -0.2075],\n",
      "        [ 0.3341, -0.1543,  0.2468, -0.3261],\n",
      "        [-0.3268,  0.3424, -0.2798, -0.1772],\n",
      "        [-0.0402, -0.1718, -0.0019, -0.0910],\n",
      "        [ 0.1884, -0.2107, -0.3409, -0.2447],\n",
      "        [-0.1332, -0.1288,  0.2787, -0.1696]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([8, 16])\n",
      "tensor([[ 0.2527, -0.2941, -0.0126, -0.2985, -0.0427, -0.3478, -0.2334, -0.0192,\n",
      "          0.2420, -0.1638, -0.3066,  0.1400,  0.2726,  0.0764,  0.3247, -0.3018],\n",
      "        [ 0.3438,  0.2528,  0.2946,  0.0792, -0.0657, -0.0009, -0.2824, -0.0137,\n",
      "          0.0628, -0.0496,  0.2931,  0.2992, -0.2457,  0.3075,  0.2589,  0.2065],\n",
      "        [ 0.2754,  0.0175, -0.0211,  0.2802,  0.0330,  0.1987, -0.3279,  0.0989,\n",
      "         -0.0106,  0.1647, -0.0737,  0.2523,  0.0943,  0.0897,  0.1326, -0.2075],\n",
      "        [-0.3143,  0.0516,  0.1768,  0.2486, -0.2421,  0.1039,  0.2997,  0.1393,\n",
      "          0.0380,  0.1159, -0.2172, -0.1441,  0.3341, -0.1543,  0.2468, -0.3261],\n",
      "        [-0.2658, -0.2658, -0.2242,  0.3027,  0.0054, -0.1129,  0.2226,  0.3525,\n",
      "         -0.2329, -0.1480, -0.2079, -0.0429, -0.3268,  0.3424, -0.2798, -0.1772],\n",
      "        [-0.0191,  0.2375,  0.0977,  0.1757, -0.1853,  0.1919, -0.2512,  0.2967,\n",
      "         -0.0593,  0.0041,  0.1711, -0.1255, -0.0402, -0.1718, -0.0019, -0.0910],\n",
      "        [-0.0536,  0.1956, -0.0229,  0.2552, -0.2896, -0.0240,  0.2950, -0.2744,\n",
      "          0.0919, -0.1019,  0.1401,  0.3534,  0.1884, -0.2107, -0.3409, -0.2447],\n",
      "        [ 0.2660,  0.0382,  0.1990,  0.2905,  0.2758, -0.1844, -0.1002,  0.2525,\n",
      "          0.2465,  0.2066,  0.3480, -0.2099, -0.1332, -0.1288,  0.2787, -0.1696]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 16])\n",
      "tensor([[[ 2.0645,  0.1068,  0.3456,  0.3086,  0.4510, -0.0501, -2.0102,\n",
      "           0.1820,  0.6659,  0.3943,  0.3957,  0.9958,  0.1540,  0.3015,\n",
      "           1.1697, -0.5129],\n",
      "         [-0.1001, -0.2280,  0.6514, -0.3211,  0.1317, -0.9288,  0.6706,\n",
      "          -0.0172,  0.8047,  0.0542,  0.0511, -0.6700,  0.4297, -0.3744,\n",
      "           1.0298, -0.3539],\n",
      "         [-0.3325,  0.2339,  0.5095,  0.6377, -0.7230,  0.2563,  0.2089,\n",
      "           0.3947, -0.0509,  0.0060, -0.4195,  0.2432,  0.3419,  0.3326,\n",
      "           0.6515, -0.5374]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 8])\n",
      "tensor([[[ 2.0645,  0.1068,  0.3456,  0.3086,  0.4510, -0.0501, -2.0102,\n",
      "           0.1820],\n",
      "         [-0.1001, -0.2280,  0.6514, -0.3211,  0.1317, -0.9288,  0.6706,\n",
      "          -0.0172],\n",
      "         [-0.3325,  0.2339,  0.5095,  0.6377, -0.7230,  0.2563,  0.2089,\n",
      "           0.3947]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.6659,  0.3943,  0.3957,  0.9958],\n",
      "         [ 0.8047,  0.0542,  0.0511, -0.6700],\n",
      "         [-0.0509,  0.0060, -0.4195,  0.2432]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.1540,  0.3015,  1.1697, -0.5129],\n",
      "         [ 0.4297, -0.3744,  1.0298, -0.3539],\n",
      "         [ 0.3419,  0.3326,  0.6515, -0.5374]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 2.0645,  0.1068,  0.3456,  0.3086],\n",
      "          [ 0.4510, -0.0501, -2.0102,  0.1820]],\n",
      "\n",
      "         [[-0.1001, -0.2280,  0.6514, -0.3211],\n",
      "          [ 0.1317, -0.9288,  0.6706, -0.0172]],\n",
      "\n",
      "         [[-0.3325,  0.2339,  0.5095,  0.6377],\n",
      "          [-0.7230,  0.2563,  0.2089,  0.3947]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[ 0.6659,  0.3943,  0.3957,  0.9958]],\n",
      "\n",
      "         [[ 0.8047,  0.0542,  0.0511, -0.6700]],\n",
      "\n",
      "         [[-0.0509,  0.0060, -0.4195,  0.2432]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[ 0.1540,  0.3015,  1.1697, -0.5129]],\n",
      "\n",
      "         [[ 0.4297, -0.3744,  1.0298, -0.3539]],\n",
      "\n",
      "         [[ 0.3419,  0.3326,  0.6515, -0.5374]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 2.0645,  0.1068,  0.3456,  0.3086],\n",
      "          [ 0.4510, -0.0501, -2.0102,  0.1820]],\n",
      "\n",
      "         [[-0.6022, -0.1948,  0.2677, -0.3423],\n",
      "          [-0.4931, -0.9224,  0.4731, -0.1099]],\n",
      "\n",
      "         [[-0.3249,  0.1026, -0.5143,  0.6715],\n",
      "          [ 0.1110,  0.1728, -0.7443,  0.4377]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[ 0.6659,  0.3943,  0.3957,  0.9958]],\n",
      "\n",
      "         [[ 0.3918,  0.1208,  0.7047, -0.6612]],\n",
      "\n",
      "         [[ 0.4026, -0.0425,  0.1283,  0.2395]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 0.6659,  0.3943,  0.3957,  0.9958],\n",
      "          [ 0.6659,  0.3943,  0.3957,  0.9958]],\n",
      "\n",
      "         [[ 0.3918,  0.1208,  0.7047, -0.6612],\n",
      "          [ 0.3918,  0.1208,  0.7047, -0.6612]],\n",
      "\n",
      "         [[ 0.4026, -0.0425,  0.1283,  0.2395],\n",
      "          [ 0.4026, -0.0425,  0.1283,  0.2395]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 0.1540,  0.3015,  1.1697, -0.5129],\n",
      "          [ 0.1540,  0.3015,  1.1697, -0.5129]],\n",
      "\n",
      "         [[ 0.4297, -0.3744,  1.0298, -0.3539],\n",
      "          [ 0.4297, -0.3744,  1.0298, -0.3539]],\n",
      "\n",
      "         [[ 0.3419,  0.3326,  0.6515, -0.5374],\n",
      "          [ 0.3419,  0.3326,  0.6515, -0.5374]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[ 2.0645,  0.1068,  0.3456,  0.3086],\n",
      "          [-0.6022, -0.1948,  0.2677, -0.3423],\n",
      "          [-0.3249,  0.1026, -0.5143,  0.6715]],\n",
      "\n",
      "         [[ 0.4510, -0.0501, -2.0102,  0.1820],\n",
      "          [-0.4931, -0.9224,  0.4731, -0.1099],\n",
      "          [ 0.1110,  0.1728, -0.7443,  0.4377]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[ 0.6659,  0.3943,  0.3957,  0.9958],\n",
      "          [ 0.3918,  0.1208,  0.7047, -0.6612],\n",
      "          [ 0.4026, -0.0425,  0.1283,  0.2395]],\n",
      "\n",
      "         [[ 0.6659,  0.3943,  0.3957,  0.9958],\n",
      "          [ 0.3918,  0.1208,  0.7047, -0.6612],\n",
      "          [ 0.4026, -0.0425,  0.1283,  0.2395]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[ 0.1540,  0.3015,  1.1697, -0.5129],\n",
      "          [ 0.4297, -0.3744,  1.0298, -0.3539],\n",
      "          [ 0.3419,  0.3326,  0.6515, -0.5374]],\n",
      "\n",
      "         [[ 0.1540,  0.3015,  1.1697, -0.5129],\n",
      "          [ 0.4297, -0.3744,  1.0298, -0.3539],\n",
      "          [ 0.3419,  0.3326,  0.6515, -0.5374]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 0.9305,  0.4306,  0.4725],\n",
      "          [-0.3564,  0.0777, -0.1409],\n",
      "          [ 0.1446, -0.4607, -0.0202]],\n",
      "\n",
      "         [[-0.1669, -0.6832, -0.0153],\n",
      "          [-0.3071,  0.0507, -0.0625],\n",
      "          [ 0.1417, -0.3748,  0.0233]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 9.3050e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [-3.5638e-01,  7.7738e-02, -2.3820e+38],\n",
      "          [ 1.4459e-01, -4.6068e-01, -2.0164e-02]],\n",
      "\n",
      "         [[-1.6689e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [-3.0712e-01,  5.0698e-02, -2.3820e+38],\n",
      "          [ 1.4168e-01, -3.7483e-01,  2.3335e-02]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.3931, 0.6069, 0.0000],\n",
      "          [0.4177, 0.2280, 0.3543]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4115, 0.5885, 0.0000],\n",
      "          [0.4024, 0.2401, 0.3575]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[ 0.1540,  0.3015,  1.1697, -0.5129],\n",
      "          [ 0.3213, -0.1087,  1.0848, -0.4164],\n",
      "          [ 0.2835,  0.1584,  0.9542, -0.4853]],\n",
      "\n",
      "         [[ 0.1540,  0.3015,  1.1697, -0.5129],\n",
      "          [ 0.3163, -0.0963,  1.0874, -0.4193],\n",
      "          [ 0.2874,  0.1503,  0.9509, -0.4835]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.1540,  0.3015,  1.1697, -0.5129,  0.1540,  0.3015,  1.1697,\n",
      "          -0.5129],\n",
      "         [ 0.3213, -0.1087,  1.0848, -0.4164,  0.3163, -0.0963,  1.0874,\n",
      "          -0.4193],\n",
      "         [ 0.2835,  0.1584,  0.9542, -0.4853,  0.2874,  0.1503,  0.9509,\n",
      "          -0.4835]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0797, -0.3179,  0.2176, -0.1303,  0.3455,  0.0768, -0.3501,  0.2563],\n",
      "        [ 0.0039, -0.3138,  0.0895, -0.0515,  0.1497, -0.1153,  0.1291, -0.1334],\n",
      "        [ 0.0436, -0.3222,  0.0706,  0.0727, -0.0675, -0.3082, -0.2491, -0.0591],\n",
      "        [ 0.1419, -0.0945,  0.3236, -0.0595, -0.2488, -0.2802,  0.2984,  0.0396],\n",
      "        [ 0.1404,  0.2242,  0.1001, -0.0388,  0.3207, -0.0486, -0.0110, -0.2863],\n",
      "        [ 0.1177, -0.2225,  0.1194, -0.1069, -0.2539,  0.2671,  0.0137, -0.1900],\n",
      "        [-0.0802, -0.2820,  0.0472,  0.1354,  0.2724, -0.0781,  0.2385,  0.1434],\n",
      "        [-0.0654,  0.0631, -0.2266,  0.1222, -0.1359, -0.1320,  0.3293, -0.2921]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([8, 8])\n",
      "tensor([[ 0.0797, -0.3179,  0.2176, -0.1303,  0.3455,  0.0768, -0.3501,  0.2563],\n",
      "        [ 0.0039, -0.3138,  0.0895, -0.0515,  0.1497, -0.1153,  0.1291, -0.1334],\n",
      "        [ 0.0436, -0.3222,  0.0706,  0.0727, -0.0675, -0.3082, -0.2491, -0.0591],\n",
      "        [ 0.1419, -0.0945,  0.3236, -0.0595, -0.2488, -0.2802,  0.2984,  0.0396],\n",
      "        [ 0.1404,  0.2242,  0.1001, -0.0388,  0.3207, -0.0486, -0.0110, -0.2863],\n",
      "        [ 0.1177, -0.2225,  0.1194, -0.1069, -0.2539,  0.2671,  0.0137, -0.1900],\n",
      "        [-0.0802, -0.2820,  0.0472,  0.1354,  0.2724, -0.0781,  0.2385,  0.1434],\n",
      "        [-0.0654,  0.0631, -0.2266,  0.1222, -0.1359, -0.1320,  0.3293, -0.2921]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.0115, -0.8668,  0.1999,  0.1375,  0.5081, -0.1904, -0.3468,\n",
      "           0.1260],\n",
      "         [-0.0133, -0.6191,  0.1685,  0.1614,  0.6041, -0.2512, -0.4045,\n",
      "           0.2224],\n",
      "         [ 0.0093, -0.6692,  0.1873,  0.0956,  0.5566, -0.1390, -0.3947,\n",
      "           0.1427]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.0115, -0.8668,  0.1999,  0.1375,  0.5081, -0.1904, -0.3468,\n",
      "           0.1260],\n",
      "         [-0.0133, -0.6191,  0.1685,  0.1614,  0.6041, -0.2512, -0.4045,\n",
      "           0.2224],\n",
      "         [ 0.0093, -0.6692,  0.1873,  0.0956,  0.5566, -0.1390, -0.3947,\n",
      "           0.1427]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Level 1 from range(2)\n",
      "Model 0 from range(2)\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 0\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2527, -0.2941, -0.0126, -0.2985, -0.0427, -0.3478, -0.2334, -0.0192,\n",
      "          0.2420, -0.1638, -0.3066,  0.1400,  0.2726,  0.0764,  0.3247, -0.3018],\n",
      "        [ 0.3438,  0.2528,  0.2946,  0.0792, -0.0657, -0.0009, -0.2824, -0.0137,\n",
      "          0.0628, -0.0496,  0.2931,  0.2992, -0.2457,  0.3075,  0.2589,  0.2065],\n",
      "        [ 0.2754,  0.0175, -0.0211,  0.2802,  0.0330,  0.1987, -0.3279,  0.0989,\n",
      "         -0.0106,  0.1647, -0.0737,  0.2523,  0.0943,  0.0897,  0.1326, -0.2075],\n",
      "        [-0.3143,  0.0516,  0.1768,  0.2486, -0.2421,  0.1039,  0.2997,  0.1393,\n",
      "          0.0380,  0.1159, -0.2172, -0.1441,  0.3341, -0.1543,  0.2468, -0.3261],\n",
      "        [-0.2658, -0.2658, -0.2242,  0.3027,  0.0054, -0.1129,  0.2226,  0.3525,\n",
      "         -0.2329, -0.1480, -0.2079, -0.0429, -0.3268,  0.3424, -0.2798, -0.1772],\n",
      "        [-0.0191,  0.2375,  0.0977,  0.1757, -0.1853,  0.1919, -0.2512,  0.2967,\n",
      "         -0.0593,  0.0041,  0.1711, -0.1255, -0.0402, -0.1718, -0.0019, -0.0910],\n",
      "        [-0.0536,  0.1956, -0.0229,  0.2552, -0.2896, -0.0240,  0.2950, -0.2744,\n",
      "          0.0919, -0.1019,  0.1401,  0.3534,  0.1884, -0.2107, -0.3409, -0.2447],\n",
      "        [ 0.2660,  0.0382,  0.1990,  0.2905,  0.2758, -0.1844, -0.1002,  0.2525,\n",
      "          0.2465,  0.2066,  0.3480, -0.2099, -0.1332, -0.1288,  0.2787, -0.1696]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[ 0.2527, -0.2941, -0.0126, -0.2985, -0.0427, -0.3478, -0.2334, -0.0192],\n",
      "        [ 0.3438,  0.2528,  0.2946,  0.0792, -0.0657, -0.0009, -0.2824, -0.0137],\n",
      "        [ 0.2754,  0.0175, -0.0211,  0.2802,  0.0330,  0.1987, -0.3279,  0.0989],\n",
      "        [-0.3143,  0.0516,  0.1768,  0.2486, -0.2421,  0.1039,  0.2997,  0.1393],\n",
      "        [-0.2658, -0.2658, -0.2242,  0.3027,  0.0054, -0.1129,  0.2226,  0.3525],\n",
      "        [-0.0191,  0.2375,  0.0977,  0.1757, -0.1853,  0.1919, -0.2512,  0.2967],\n",
      "        [-0.0536,  0.1956, -0.0229,  0.2552, -0.2896, -0.0240,  0.2950, -0.2744],\n",
      "        [ 0.2660,  0.0382,  0.1990,  0.2905,  0.2758, -0.1844, -0.1002,  0.2525]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[ 0.2420, -0.1638, -0.3066,  0.1400],\n",
      "        [ 0.0628, -0.0496,  0.2931,  0.2992],\n",
      "        [-0.0106,  0.1647, -0.0737,  0.2523],\n",
      "        [ 0.0380,  0.1159, -0.2172, -0.1441],\n",
      "        [-0.2329, -0.1480, -0.2079, -0.0429],\n",
      "        [-0.0593,  0.0041,  0.1711, -0.1255],\n",
      "        [ 0.0919, -0.1019,  0.1401,  0.3534],\n",
      "        [ 0.2465,  0.2066,  0.3480, -0.2099]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.2726,  0.0764,  0.3247, -0.3018],\n",
      "        [-0.2457,  0.3075,  0.2589,  0.2065],\n",
      "        [ 0.0943,  0.0897,  0.1326, -0.2075],\n",
      "        [ 0.3341, -0.1543,  0.2468, -0.3261],\n",
      "        [-0.3268,  0.3424, -0.2798, -0.1772],\n",
      "        [-0.0402, -0.1718, -0.0019, -0.0910],\n",
      "        [ 0.1884, -0.2107, -0.3409, -0.2447],\n",
      "        [-0.1332, -0.1288,  0.2787, -0.1696]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[ 0.2527, -0.2941, -0.0427, -0.3478],\n",
      "        [ 0.3438,  0.2528, -0.0657, -0.0009],\n",
      "        [ 0.2754,  0.0175,  0.0330,  0.1987],\n",
      "        [-0.3143,  0.0516, -0.2421,  0.1039]], grad_fn=<CatBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.2420, -0.1638],\n",
      "        [ 0.0628, -0.0496],\n",
      "        [-0.0106,  0.1647],\n",
      "        [ 0.0380,  0.1159]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.2726,  0.0764],\n",
      "        [-0.2457,  0.3075],\n",
      "        [ 0.0943,  0.0897],\n",
      "        [ 0.3341, -0.1543]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[ 0.2527, -0.2941, -0.0427, -0.3478,  0.2420, -0.1638,  0.2726,  0.0764],\n",
      "        [ 0.3438,  0.2528, -0.0657, -0.0009,  0.0628, -0.0496, -0.2457,  0.3075],\n",
      "        [ 0.2754,  0.0175,  0.0330,  0.1987, -0.0106,  0.1647,  0.0943,  0.0897],\n",
      "        [-0.3143,  0.0516, -0.2421,  0.1039,  0.0380,  0.1159,  0.3341, -0.1543]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.8154,  0.0212, -0.3331,  0.1719, -0.0332,  0.1793,  0.5189,\n",
      "          -0.4398],\n",
      "         [ 0.9353, -0.2952,  0.6575, -0.4301, -0.0105, -0.3805, -0.7765,\n",
      "           0.4222],\n",
      "         [-0.0926, -0.1976, -0.0896, -0.2539,  0.1441, -0.1248,  0.2358,\n",
      "          -0.0733]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.8154,  0.0212, -0.3331,  0.1719],\n",
      "         [ 0.9353, -0.2952,  0.6575, -0.4301],\n",
      "         [-0.0926, -0.1976, -0.0896, -0.2539]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.0332,  0.1793],\n",
      "         [-0.0105, -0.3805],\n",
      "         [ 0.1441, -0.1248]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.5189, -0.4398],\n",
      "         [-0.7765,  0.4222],\n",
      "         [ 0.2358, -0.0733]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.8154,  0.0212],\n",
      "          [-0.3331,  0.1719]],\n",
      "\n",
      "         [[ 0.9353, -0.2952],\n",
      "          [ 0.6575, -0.4301]],\n",
      "\n",
      "         [[-0.0926, -0.1976],\n",
      "          [-0.0896, -0.2539]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.0332,  0.1793]],\n",
      "\n",
      "         [[-0.0105, -0.3805]],\n",
      "\n",
      "         [[ 0.1441, -0.1248]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.5189, -0.4398]],\n",
      "\n",
      "         [[-0.7765,  0.4222]],\n",
      "\n",
      "         [[ 0.2358, -0.0733]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.8154,  0.0212],\n",
      "          [-0.3331,  0.1719]],\n",
      "\n",
      "         [[ 0.7537,  0.6275],\n",
      "          [ 0.7171,  0.3209]],\n",
      "\n",
      "         [[ 0.2182, -0.0019],\n",
      "          [ 0.2682,  0.0242]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.0332,  0.1793]],\n",
      "\n",
      "         [[ 0.3145, -0.2145]],\n",
      "\n",
      "         [[ 0.0535,  0.1830]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.0332,  0.1793],\n",
      "          [-0.0332,  0.1793]],\n",
      "\n",
      "         [[ 0.3145, -0.2145],\n",
      "          [ 0.3145, -0.2145]],\n",
      "\n",
      "         [[ 0.0535,  0.1830],\n",
      "          [ 0.0535,  0.1830]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.5189, -0.4398],\n",
      "          [ 0.5189, -0.4398]],\n",
      "\n",
      "         [[-0.7765,  0.4222],\n",
      "          [-0.7765,  0.4222]],\n",
      "\n",
      "         [[ 0.2358, -0.0733],\n",
      "          [ 0.2358, -0.0733]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.8154,  0.0212],\n",
      "          [ 0.7537,  0.6275],\n",
      "          [ 0.2182, -0.0019]],\n",
      "\n",
      "         [[-0.3331,  0.1719],\n",
      "          [ 0.7171,  0.3209],\n",
      "          [ 0.2682,  0.0242]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.0332,  0.1793],\n",
      "          [ 0.3145, -0.2145],\n",
      "          [ 0.0535,  0.1830]],\n",
      "\n",
      "         [[-0.0332,  0.1793],\n",
      "          [ 0.3145, -0.2145],\n",
      "          [ 0.0535,  0.1830]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.5189, -0.4398],\n",
      "          [-0.7765,  0.4222],\n",
      "          [ 0.2358, -0.0733]],\n",
      "\n",
      "         [[ 0.5189, -0.4398],\n",
      "          [-0.7765,  0.4222],\n",
      "          [ 0.2358, -0.0733]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 0.0218, -0.1845, -0.0281],\n",
      "          [ 0.0619,  0.0724,  0.1097],\n",
      "          [-0.0054,  0.0488,  0.0080]],\n",
      "\n",
      "         [[ 0.0296, -0.1001,  0.0096],\n",
      "          [ 0.0239,  0.1108,  0.0687],\n",
      "          [-0.0032,  0.0560,  0.0133]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 2.1808e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 6.1887e-02,  7.2446e-02, -2.3820e+38],\n",
      "          [-5.3622e-03,  4.8817e-02,  8.0092e-03]],\n",
      "\n",
      "         [[ 2.9600e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 2.3868e-02,  1.1082e-01, -2.3820e+38],\n",
      "          [-3.2221e-03,  5.5972e-02,  1.3282e-02]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4974, 0.5026, 0.0000],\n",
      "          [0.3258, 0.3440, 0.3302]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4783, 0.5217, 0.0000],\n",
      "          [0.3249, 0.3447, 0.3303]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.5189, -0.4398],\n",
      "          [-0.1322, -0.0065],\n",
      "          [-0.0201, -0.0223]],\n",
      "\n",
      "         [[ 0.5189, -0.4398],\n",
      "          [-0.1569,  0.0099],\n",
      "          [-0.0212, -0.0216]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.5189, -0.4398,  0.5189, -0.4398],\n",
      "         [-0.1322, -0.0065, -0.1569,  0.0099],\n",
      "         [-0.0201, -0.0223, -0.0212, -0.0216]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0797, -0.3179,  0.2176, -0.1303,  0.3455,  0.0768, -0.3501,  0.2563],\n",
      "        [ 0.0039, -0.3138,  0.0895, -0.0515,  0.1497, -0.1153,  0.1291, -0.1334],\n",
      "        [ 0.0436, -0.3222,  0.0706,  0.0727, -0.0675, -0.3082, -0.2491, -0.0591],\n",
      "        [ 0.1419, -0.0945,  0.3236, -0.0595, -0.2488, -0.2802,  0.2984,  0.0396],\n",
      "        [ 0.1404,  0.2242,  0.1001, -0.0388,  0.3207, -0.0486, -0.0110, -0.2863],\n",
      "        [ 0.1177, -0.2225,  0.1194, -0.1069, -0.2539,  0.2671,  0.0137, -0.1900],\n",
      "        [-0.0802, -0.2820,  0.0472,  0.1354,  0.2724, -0.0781,  0.2385,  0.1434],\n",
      "        [-0.0654,  0.0631, -0.2266,  0.1222, -0.1359, -0.1320,  0.3293, -0.2921]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[ 0.0797, -0.3179,  0.2176, -0.1303],\n",
      "        [ 0.0039, -0.3138,  0.0895, -0.0515],\n",
      "        [ 0.1404,  0.2242,  0.1001, -0.0388],\n",
      "        [ 0.1177, -0.2225,  0.1194, -0.1069]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0607,  0.1873,  0.0730, -0.0181],\n",
      "         [-0.0314,  0.0067, -0.0439,  0.0226],\n",
      "         [-0.0072,  0.0135, -0.0111,  0.0069]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0607,  0.1873,  0.0730, -0.0181],\n",
      "         [-0.0314,  0.0067, -0.0439,  0.0226],\n",
      "         [-0.0072,  0.0135, -0.0111,  0.0069]]], grad_fn=<UnsafeViewBackward0>)\n",
      "Model 1 from range(2)\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 4\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 2\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2527, -0.2941, -0.0126, -0.2985, -0.0427, -0.3478, -0.2334, -0.0192,\n",
      "          0.2420, -0.1638, -0.3066,  0.1400,  0.2726,  0.0764,  0.3247, -0.3018],\n",
      "        [ 0.3438,  0.2528,  0.2946,  0.0792, -0.0657, -0.0009, -0.2824, -0.0137,\n",
      "          0.0628, -0.0496,  0.2931,  0.2992, -0.2457,  0.3075,  0.2589,  0.2065],\n",
      "        [ 0.2754,  0.0175, -0.0211,  0.2802,  0.0330,  0.1987, -0.3279,  0.0989,\n",
      "         -0.0106,  0.1647, -0.0737,  0.2523,  0.0943,  0.0897,  0.1326, -0.2075],\n",
      "        [-0.3143,  0.0516,  0.1768,  0.2486, -0.2421,  0.1039,  0.2997,  0.1393,\n",
      "          0.0380,  0.1159, -0.2172, -0.1441,  0.3341, -0.1543,  0.2468, -0.3261],\n",
      "        [-0.2658, -0.2658, -0.2242,  0.3027,  0.0054, -0.1129,  0.2226,  0.3525,\n",
      "         -0.2329, -0.1480, -0.2079, -0.0429, -0.3268,  0.3424, -0.2798, -0.1772],\n",
      "        [-0.0191,  0.2375,  0.0977,  0.1757, -0.1853,  0.1919, -0.2512,  0.2967,\n",
      "         -0.0593,  0.0041,  0.1711, -0.1255, -0.0402, -0.1718, -0.0019, -0.0910],\n",
      "        [-0.0536,  0.1956, -0.0229,  0.2552, -0.2896, -0.0240,  0.2950, -0.2744,\n",
      "          0.0919, -0.1019,  0.1401,  0.3534,  0.1884, -0.2107, -0.3409, -0.2447],\n",
      "        [ 0.2660,  0.0382,  0.1990,  0.2905,  0.2758, -0.1844, -0.1002,  0.2525,\n",
      "          0.2465,  0.2066,  0.3480, -0.2099, -0.1332, -0.1288,  0.2787, -0.1696]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[ 0.2527, -0.2941, -0.0126, -0.2985, -0.0427, -0.3478, -0.2334, -0.0192],\n",
      "        [ 0.3438,  0.2528,  0.2946,  0.0792, -0.0657, -0.0009, -0.2824, -0.0137],\n",
      "        [ 0.2754,  0.0175, -0.0211,  0.2802,  0.0330,  0.1987, -0.3279,  0.0989],\n",
      "        [-0.3143,  0.0516,  0.1768,  0.2486, -0.2421,  0.1039,  0.2997,  0.1393],\n",
      "        [-0.2658, -0.2658, -0.2242,  0.3027,  0.0054, -0.1129,  0.2226,  0.3525],\n",
      "        [-0.0191,  0.2375,  0.0977,  0.1757, -0.1853,  0.1919, -0.2512,  0.2967],\n",
      "        [-0.0536,  0.1956, -0.0229,  0.2552, -0.2896, -0.0240,  0.2950, -0.2744],\n",
      "        [ 0.2660,  0.0382,  0.1990,  0.2905,  0.2758, -0.1844, -0.1002,  0.2525]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[ 0.2420, -0.1638, -0.3066,  0.1400],\n",
      "        [ 0.0628, -0.0496,  0.2931,  0.2992],\n",
      "        [-0.0106,  0.1647, -0.0737,  0.2523],\n",
      "        [ 0.0380,  0.1159, -0.2172, -0.1441],\n",
      "        [-0.2329, -0.1480, -0.2079, -0.0429],\n",
      "        [-0.0593,  0.0041,  0.1711, -0.1255],\n",
      "        [ 0.0919, -0.1019,  0.1401,  0.3534],\n",
      "        [ 0.2465,  0.2066,  0.3480, -0.2099]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.2726,  0.0764,  0.3247, -0.3018],\n",
      "        [-0.2457,  0.3075,  0.2589,  0.2065],\n",
      "        [ 0.0943,  0.0897,  0.1326, -0.2075],\n",
      "        [ 0.3341, -0.1543,  0.2468, -0.3261],\n",
      "        [-0.3268,  0.3424, -0.2798, -0.1772],\n",
      "        [-0.0402, -0.1718, -0.0019, -0.0910],\n",
      "        [ 0.1884, -0.2107, -0.3409, -0.2447],\n",
      "        [-0.1332, -0.1288,  0.2787, -0.1696]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[-0.2242,  0.3027,  0.2226,  0.3525],\n",
      "        [ 0.0977,  0.1757, -0.2512,  0.2967],\n",
      "        [-0.0229,  0.2552,  0.2950, -0.2744],\n",
      "        [ 0.1990,  0.2905, -0.1002,  0.2525]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[-0.2079, -0.0429],\n",
      "        [ 0.1711, -0.1255],\n",
      "        [ 0.1401,  0.3534],\n",
      "        [ 0.3480, -0.2099]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[-0.2798, -0.1772],\n",
      "        [-0.0019, -0.0910],\n",
      "        [-0.3409, -0.2447],\n",
      "        [ 0.2787, -0.1696]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[-0.2242,  0.3027,  0.2226,  0.3525, -0.2079, -0.0429, -0.2798, -0.1772],\n",
      "        [ 0.0977,  0.1757, -0.2512,  0.2967,  0.1711, -0.1255, -0.0019, -0.0910],\n",
      "        [-0.0229,  0.2552,  0.2950, -0.2744,  0.1401,  0.3534, -0.3409, -0.2447],\n",
      "        [ 0.1990,  0.2905, -0.1002,  0.2525,  0.3480, -0.2099,  0.2787, -0.1696]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.0710,  0.0981,  0.3257, -0.0272,  0.1097, -0.2137,  0.5108,\n",
      "          -0.0491],\n",
      "         [ 0.3248, -0.7391, -0.2260, -0.7577,  0.1852,  0.0164,  0.6884,\n",
      "           0.4581],\n",
      "         [-0.1925,  0.2732,  0.0715,  0.5031, -0.2039, -0.1712, -0.1889,\n",
      "          -0.1303]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0710,  0.0981,  0.3257, -0.0272],\n",
      "         [ 0.3248, -0.7391, -0.2260, -0.7577],\n",
      "         [-0.1925,  0.2732,  0.0715,  0.5031]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.1097, -0.2137],\n",
      "         [ 0.1852,  0.0164],\n",
      "         [-0.2039, -0.1712]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.5108, -0.0491],\n",
      "         [ 0.6884,  0.4581],\n",
      "         [-0.1889, -0.1303]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.0710,  0.0981],\n",
      "          [ 0.3257, -0.0272]],\n",
      "\n",
      "         [[ 0.3248, -0.7391],\n",
      "          [-0.2260, -0.7577]],\n",
      "\n",
      "         [[-0.1925,  0.2732],\n",
      "          [ 0.0715,  0.5031]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.1097, -0.2137]],\n",
      "\n",
      "         [[ 0.1852,  0.0164]],\n",
      "\n",
      "         [[-0.2039, -0.1712]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.5108, -0.0491]],\n",
      "\n",
      "         [[ 0.6884,  0.4581]],\n",
      "\n",
      "         [[-0.1889, -0.1303]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.0710,  0.0981],\n",
      "          [ 0.3257, -0.0272]],\n",
      "\n",
      "         [[ 0.7974, -0.1260],\n",
      "          [ 0.5155, -0.5995]],\n",
      "\n",
      "         [[-0.1683, -0.2888],\n",
      "          [-0.4872, -0.1444]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.1097, -0.2137]],\n",
      "\n",
      "         [[ 0.0863,  0.1647]],\n",
      "\n",
      "         [[ 0.2405, -0.1141]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.1097, -0.2137],\n",
      "          [ 0.1097, -0.2137]],\n",
      "\n",
      "         [[ 0.0863,  0.1647],\n",
      "          [ 0.0863,  0.1647]],\n",
      "\n",
      "         [[ 0.2405, -0.1141],\n",
      "          [ 0.2405, -0.1141]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.5108, -0.0491],\n",
      "          [ 0.5108, -0.0491]],\n",
      "\n",
      "         [[ 0.6884,  0.4581],\n",
      "          [ 0.6884,  0.4581]],\n",
      "\n",
      "         [[-0.1889, -0.1303],\n",
      "          [-0.1889, -0.1303]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.0710,  0.0981],\n",
      "          [ 0.7974, -0.1260],\n",
      "          [-0.1683, -0.2888]],\n",
      "\n",
      "         [[ 0.3257, -0.0272],\n",
      "          [ 0.5155, -0.5995],\n",
      "          [-0.4872, -0.1444]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.1097, -0.2137],\n",
      "          [ 0.0863,  0.1647],\n",
      "          [ 0.2405, -0.1141]],\n",
      "\n",
      "         [[ 0.1097, -0.2137],\n",
      "          [ 0.0863,  0.1647],\n",
      "          [ 0.2405, -0.1141]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.5108, -0.0491],\n",
      "          [ 0.6884,  0.4581],\n",
      "          [-0.1889, -0.1303]],\n",
      "\n",
      "         [[ 0.5108, -0.0491],\n",
      "          [ 0.6884,  0.4581],\n",
      "          [-0.1889, -0.1303]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-0.0093,  0.0158,  0.0042],\n",
      "          [ 0.0809,  0.0340,  0.1458],\n",
      "          [ 0.0306, -0.0439, -0.0053]],\n",
      "\n",
      "         [[ 0.0294,  0.0167,  0.0576],\n",
      "          [ 0.1306, -0.0384,  0.1360],\n",
      "          [-0.0160, -0.0465, -0.0712]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-9.3165e-03, -2.3820e+38, -2.3820e+38],\n",
      "          [ 8.0904e-02,  3.3986e-02, -2.3820e+38],\n",
      "          [ 3.0570e-02, -4.3901e-02, -5.3139e-03]],\n",
      "\n",
      "         [[ 2.9373e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 1.3057e-01, -3.8374e-02, -2.3820e+38],\n",
      "          [-1.5993e-02, -4.6546e-02, -7.1211e-02]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5117, 0.4883, 0.0000],\n",
      "          [0.3457, 0.3209, 0.3335]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5421, 0.4579, 0.0000],\n",
      "          [0.3429, 0.3326, 0.3245]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.5108, -0.0491],\n",
      "          [ 0.5975,  0.1986],\n",
      "          [ 0.3344,  0.0866]],\n",
      "\n",
      "         [[ 0.5108, -0.0491],\n",
      "          [ 0.5921,  0.1831],\n",
      "          [ 0.3428,  0.0933]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.5108, -0.0491,  0.5108, -0.0491],\n",
      "         [ 0.5975,  0.1986,  0.5921,  0.1831],\n",
      "         [ 0.3344,  0.0866,  0.3428,  0.0933]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0797, -0.3179,  0.2176, -0.1303,  0.3455,  0.0768, -0.3501,  0.2563],\n",
      "        [ 0.0039, -0.3138,  0.0895, -0.0515,  0.1497, -0.1153,  0.1291, -0.1334],\n",
      "        [ 0.0436, -0.3222,  0.0706,  0.0727, -0.0675, -0.3082, -0.2491, -0.0591],\n",
      "        [ 0.1419, -0.0945,  0.3236, -0.0595, -0.2488, -0.2802,  0.2984,  0.0396],\n",
      "        [ 0.1404,  0.2242,  0.1001, -0.0388,  0.3207, -0.0486, -0.0110, -0.2863],\n",
      "        [ 0.1177, -0.2225,  0.1194, -0.1069, -0.2539,  0.2671,  0.0137, -0.1900],\n",
      "        [-0.0802, -0.2820,  0.0472,  0.1354,  0.2724, -0.0781,  0.2385,  0.1434],\n",
      "        [-0.0654,  0.0631, -0.2266,  0.1222, -0.1359, -0.1320,  0.3293, -0.2921]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[-0.0675, -0.3082, -0.2491, -0.0591],\n",
      "        [-0.2488, -0.2802,  0.2984,  0.0396],\n",
      "        [ 0.2724, -0.0781,  0.2385,  0.1434],\n",
      "        [-0.1359, -0.1320,  0.3293, -0.2921]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.1235, -0.1771, -0.0362,  0.0554],\n",
      "         [ 0.0467, -0.3103,  0.1120,  0.0039],\n",
      "         [ 0.0366, -0.1664,  0.0550,  0.0056]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.1235, -0.1771, -0.0362,  0.0554],\n",
      "         [ 0.0467, -0.3103,  0.1120,  0.0039],\n",
      "         [ 0.0366, -0.1664,  0.0550,  0.0056]]], grad_fn=<UnsafeViewBackward0>)\n",
      "final output: ((tensor([[[-0.0128, -0.9632,  0.0000,  0.1527,  0.5646, -0.2116, -0.3853,\n",
      "           0.1400],\n",
      "         [-0.0148, -0.6878,  0.1872,  0.1793,  0.6712, -0.2791, -0.4494,\n",
      "           0.2471],\n",
      "         [ 0.0104, -0.7435,  0.2081,  0.1062,  0.6185, -0.1544, -0.4386,\n",
      "           0.1585]]], grad_fn=<MulBackward0>),), (tensor([[[ 0.0675,  0.2081,  0.0811, -0.0201],\n",
      "         [-0.0349,  0.0074, -0.0000,  0.0251],\n",
      "         [-0.0080,  0.0000, -0.0123,  0.0000]]], grad_fn=<MulBackward0>), tensor([[[ 0.1372, -0.1968, -0.0000,  0.0616],\n",
      "         [ 0.0518, -0.3447,  0.1244,  0.0044],\n",
      "         [ 0.0407, -0.1849,  0.0611,  0.0062]]], grad_fn=<MulBackward0>)))\n",
      "------------- END MultiQueryAttention.forwardTuple() ------------\n",
      "out: ((tensor([[[-0.0128, -0.9632,  0.0000,  0.1527,  0.5646, -0.2116, -0.3853,\n",
      "           0.1400],\n",
      "         [-0.0148, -0.6878,  0.1872,  0.1793,  0.6712, -0.2791, -0.4494,\n",
      "           0.2471],\n",
      "         [ 0.0104, -0.7435,  0.2081,  0.1062,  0.6185, -0.1544, -0.4386,\n",
      "           0.1585]]], grad_fn=<MulBackward0>),), (tensor([[[ 0.0675,  0.2081,  0.0811, -0.0201],\n",
      "         [-0.0349,  0.0074, -0.0000,  0.0251],\n",
      "         [-0.0080,  0.0000, -0.0123,  0.0000]]], grad_fn=<MulBackward0>), tensor([[[ 0.1372, -0.1968, -0.0000,  0.0616],\n",
      "         [ 0.0518, -0.3447,  0.1244,  0.0044],\n",
      "         [ 0.0407, -0.1849,  0.0611,  0.0062]]], grad_fn=<MulBackward0>)))\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n",
      "head_dim_list:  [32, 16, 8]\n"
     ]
    }
   ],
   "source": [
    "# Testing our Attention's forwardTuple()\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4, hold5 = config.hidden_size, config.num_attention_heads, config.head_dim, config.levels, config.max_position_embeddings\n",
    "config.hidden_size = 8\n",
    "config.num_attention_heads = 2\n",
    "config.head_dim = 4\n",
    "config.levels = 2\n",
    "config.max_position_embeddings = 3\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "att = MultiQueryAttention(config)\n",
    "# we need to make sure to send in a tuple of the expected size. above we set hidden_size=4 and levels=2\n",
    "x = ((torch.randn((1,3,8)),),(torch.randn((1,3,4)),torch.randn((1,3,4))))\n",
    "print(f\"x: {x}\")\n",
    "out = att(x)\n",
    "print(f\"out: {out}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.num_attention_heads = hold2\n",
    "config.head_dim = hold3\n",
    "config.levels = hold4\n",
    "config.max_position_embeddings = hold5\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "# clear up memory\n",
    "del hold1, hold2, hold3, hold4, hold5, x, att, out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67bb47d",
   "metadata": {},
   "source": [
    "# Layer\n",
    "\n",
    "nothing too interesting here besides the absurd amount of memory we're probably taking up with these tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d191c172",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(nn.Module):\n",
    "    \"\"\"\n",
    "    A decoder layer that integrates the MultiQueryAttention and MLP. It includes\n",
    "    normalization steps both before and after the attention mechanism to stabilize and accelerate training.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initializes the GemmaAttention mechanism with parameters from the config, enabling self-attention within the decoder layer.\n",
    "        self.self_attn = MultiQueryAttention(config)\n",
    "        \n",
    "        # Initializes the GemmaMLP module, providing a non-linear transformation after the attention mechanism.\n",
    "        self.mlp = MLP(\n",
    "            # the hidden dimension of the model\n",
    "            hidden_size = config.hidden_size,\n",
    "            # the number of nodes in the center of the two feedforward layers\n",
    "            intermediate_size = config.intermediate_size,\n",
    "            # the % of neurons to set to 0 during training\n",
    "            dropout = config.dropout,\n",
    "        )\n",
    "        \n",
    "        # Applies RMSNorm normalization to the input of the decoder layer for stable training dynamics.\n",
    "        self.input_layernorm = RMSNorm(config.hidden_size,\n",
    "                                       eps = config.rms_norm_eps)\n",
    "        \n",
    "        # Applies RMSNorm after the attention mechanism and before the MLP to ensure the output is well-conditioned for further processing.\n",
    "        self.post_attention_layernorm = RMSNorm(config.hidden_size,\n",
    "                                                eps = config.rms_norm_eps)\n",
    "\n",
    "    def forwardTensor(self,\n",
    "                # The input tensor to the decoder layer. shape (batch_size, input_len, hidden_size)\n",
    "                x: torch.Tensor,\n",
    "                model: int = 0,\n",
    "                drop_bool: bool = False\n",
    "                ) -> torch.Tensor:\n",
    "        global verbose\n",
    "        if verbose: print(\"----------------- Layer.forwardTensor() --------------------\")\n",
    "        \n",
    "        # Self Attention Block\n",
    "        # Stores the original input for use as a residual connection, aiding in mitigating the vanishing gradient problem\n",
    "        residual_connection = x\n",
    "        # Normalizes the input before processing by the attention mechanism.\n",
    "        x = self.input_layernorm(x, model)\n",
    "        # Processes the normalized input through the GemmaAttention mechanism\n",
    "        x = self.self_attn(x, model, drop_bool)\n",
    "        # The aforementioned residual connection\n",
    "        x = residual_connection + x\n",
    "        if verbose: print(f\"x in layer after MQA & resid connection and before MLP:\\n{x}\")\n",
    "\n",
    "        # MLP Block\n",
    "        # Again, stores the output of the attention block for use as a residual connection before processing by the MLP.\n",
    "        residual_connection = x\n",
    "        # Normalizes the output of the attention block before passing it to the MLP, ensuring a stable input distribution.\n",
    "        x = self.post_attention_layernorm(x, model)\n",
    "        # Transforms the normalized attention output through the MLP, introducing additional non-linearity and capacity to the model.\n",
    "        x = self.mlp(x, model, drop_bool)\n",
    "        # Another residual connection\n",
    "        x = residual_connection + x\n",
    "        if verbose: \n",
    "            print(f\"layer's final residual state:\\n{x}\")\n",
    "            print(\"----------------- END Layer.forwardTensor() --------------------\")\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forwardTuple(self,\n",
    "                     x: Tuple[Tuple[torch.Tensor]],\n",
    "                    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Defines the forward pass of a decoder layer during training.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tuple[Tuple[Tensor]]): \n",
    "                The input tuple of tuples of tensors \n",
    "                first tuple is of length config.levels and second layer of tuples have lengths of config.model_count\n",
    "                tensors are shape (batch size, sequence length, hidden dimension) where hidden dimension changes by which model was used\n",
    "\n",
    "        Returns:\n",
    "            Tuple[Tuple[Tensor]]: \n",
    "                The output tuple of tuples of tensors after applying the decoder layer\n",
    "        \"\"\"\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- Layer.forwardTuple() ------------\")\n",
    "            print(f\"x:\\n{x}\")\n",
    "            \n",
    "        # forwardTuple() should only be used during training, so we assert input_len == max_position_embeddings\n",
    "        input_len = x[0][0].shape[1]\n",
    "        if verbose: print(f\"input_len: {input_len}\")\n",
    "        assert input_len == config.max_position_embeddings\n",
    "\n",
    "        # we could define these from the config but this way the method is more flexible to testing\n",
    "        num_levels = len(x)\n",
    "        models_per_level = [len(x[i]) for i in range(num_levels)]\n",
    "        if verbose: \n",
    "            print(f\"num_levels: {num_levels}\")\n",
    "            print(f\"models_per_level: {models_per_level}\")\n",
    "\n",
    "        # the loop that iterates over levels, aka the different potential sizes of models\n",
    "        out = ()\n",
    "        for i in range(num_levels):\n",
    "            if verbose: print(f\"Level {i} from range({num_levels})\")\n",
    "\n",
    "            # now for the loop that iterates over models in this level\n",
    "            out_lvl = ()\n",
    "            for j in range(models_per_level[i]):\n",
    "                if verbose: print(f\"Model {j} from range({models_per_level[i]})\")\n",
    "\n",
    "                output = self.forwardTensor(x[i][j], model = j, drop_bool = True)\n",
    "                if verbose: print(f\"forwardTensor() output: {output.shape}\\n{output}\")\n",
    "                \n",
    "                out_lvl += (output,)\n",
    "            \n",
    "            out += (out_lvl,)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"final output: {out}\")\n",
    "            print(\"------------- END Layer.forwardTuple() ------------\")\n",
    "\n",
    "        return out\n",
    "        \n",
    "    def forward(self, x, model=0):\n",
    "        train = True if type(x) == tuple else False\n",
    "        if verbose: print(f\"---------- Layer Input: {'Tuple' if train else 'torch.Tensor'} ------------\")\n",
    "        return self.forwardTuple(x) if train else self.forwardTensor(x, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5b27eb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [8, 4, 2]\n",
      "head_dim_list:  [4, 2, 1]\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[0.3649, 0.7453, 0.5253, 0.7963, 0.7138, 0.5880, 0.2640, 0.4247],\n",
      "         [0.2535, 0.5296, 0.5828, 0.2522, 0.4695, 0.7472, 0.0419, 0.7992],\n",
      "         [0.4768, 0.7702, 0.6017, 0.1678, 0.9640, 0.4123, 0.5156, 0.4232]]])\n",
      "---------- Layer Input: torch.Tensor ------------\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[0.3649, 0.7453, 0.5253, 0.7963, 0.7138, 0.5880, 0.2640, 0.4247],\n",
      "         [0.2535, 0.5296, 0.5828, 0.2522, 0.4695, 0.7472, 0.0419, 0.7992],\n",
      "         [0.4768, 0.7702, 0.6017, 0.1678, 0.9640, 0.4123, 0.5156, 0.4232]]])\n",
      "normed x: torch.Size([1, 3, 8])\n",
      "tensor([[[0.6276, 1.2819, 0.9036, 1.3697, 1.2278, 1.0113, 0.4542, 0.7305],\n",
      "         [0.4870, 1.0175, 1.1197, 0.4846, 0.9020, 1.4357, 0.0805, 1.5355],\n",
      "         [0.8125, 1.3126, 1.0254, 0.2859, 1.6427, 0.7026, 0.8786, 0.7211]]])\n",
      "dim: 8\n",
      "skip: 0\n",
      "spliced scale: torch.Size([8])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 8])\n",
      "tensor([[[0.6276, 1.2819, 0.9036, 1.3697, 1.2278, 1.0113, 0.4542, 0.7305],\n",
      "         [0.4870, 1.0175, 1.1197, 0.4846, 0.9020, 1.4357, 0.0805, 1.5355],\n",
      "         [0.8125, 1.3126, 1.0254, 0.2859, 1.6427, 0.7026, 0.8786, 0.7211]]],\n",
      "       grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 8])\n",
      "d_skip: 0\n",
      "models_in_this_level: 1\n",
      "h_dim: 4\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1502,  0.1798, -0.0850, -0.1503, -0.1788, -0.2761, -0.2895,  0.2239,\n",
      "         -0.1242, -0.3357, -0.1516,  0.1445, -0.3465, -0.0087,  0.1680, -0.3096],\n",
      "        [-0.2497,  0.2637,  0.0318,  0.2843,  0.3023, -0.0147, -0.1036,  0.0282,\n",
      "          0.1418,  0.0838, -0.0708, -0.0738, -0.3388,  0.3156, -0.0171, -0.3506],\n",
      "        [-0.1851,  0.2389,  0.3192, -0.2411, -0.3012,  0.0172,  0.2669,  0.2849,\n",
      "          0.2737,  0.2778, -0.1082,  0.0954,  0.2263,  0.0940, -0.1288,  0.3529],\n",
      "        [-0.3062, -0.0375,  0.0070, -0.2296, -0.0683,  0.2136,  0.1168,  0.1170,\n",
      "         -0.2637, -0.1249, -0.1836, -0.1935,  0.0628,  0.2906, -0.3528,  0.2893],\n",
      "        [ 0.2859,  0.2414, -0.3532, -0.3180,  0.0624,  0.3174, -0.3462, -0.1620,\n",
      "         -0.0744, -0.1810,  0.0381,  0.3187, -0.1418, -0.2615, -0.0950,  0.2029],\n",
      "        [-0.1285,  0.0549, -0.3518,  0.0362,  0.3494,  0.1207,  0.1131, -0.1509,\n",
      "         -0.0154,  0.2893,  0.1281,  0.0523, -0.2585,  0.3296,  0.3162,  0.3222],\n",
      "        [ 0.1372, -0.3255, -0.0823,  0.3374, -0.1852, -0.0021,  0.0576,  0.1676,\n",
      "         -0.2661,  0.0652, -0.0189,  0.2911,  0.2301,  0.3267, -0.0217,  0.2536],\n",
      "        [-0.0199,  0.2874,  0.2247, -0.2074,  0.1151, -0.1673,  0.1663, -0.1422,\n",
      "         -0.1487,  0.0820,  0.2069,  0.0380, -0.0596, -0.2654,  0.2306,  0.1583]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[ 0.1502,  0.1798, -0.0850, -0.1503, -0.1788, -0.2761, -0.2895,  0.2239],\n",
      "        [-0.2497,  0.2637,  0.0318,  0.2843,  0.3023, -0.0147, -0.1036,  0.0282],\n",
      "        [-0.1851,  0.2389,  0.3192, -0.2411, -0.3012,  0.0172,  0.2669,  0.2849],\n",
      "        [-0.3062, -0.0375,  0.0070, -0.2296, -0.0683,  0.2136,  0.1168,  0.1170],\n",
      "        [ 0.2859,  0.2414, -0.3532, -0.3180,  0.0624,  0.3174, -0.3462, -0.1620],\n",
      "        [-0.1285,  0.0549, -0.3518,  0.0362,  0.3494,  0.1207,  0.1131, -0.1509],\n",
      "        [ 0.1372, -0.3255, -0.0823,  0.3374, -0.1852, -0.0021,  0.0576,  0.1676],\n",
      "        [-0.0199,  0.2874,  0.2247, -0.2074,  0.1151, -0.1673,  0.1663, -0.1422]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[-0.1242, -0.3357, -0.1516,  0.1445],\n",
      "        [ 0.1418,  0.0838, -0.0708, -0.0738],\n",
      "        [ 0.2737,  0.2778, -0.1082,  0.0954],\n",
      "        [-0.2637, -0.1249, -0.1836, -0.1935],\n",
      "        [-0.0744, -0.1810,  0.0381,  0.3187],\n",
      "        [-0.0154,  0.2893,  0.1281,  0.0523],\n",
      "        [-0.2661,  0.0652, -0.0189,  0.2911],\n",
      "        [-0.1487,  0.0820,  0.2069,  0.0380]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[-0.3465, -0.0087,  0.1680, -0.3096],\n",
      "        [-0.3388,  0.3156, -0.0171, -0.3506],\n",
      "        [ 0.2263,  0.0940, -0.1288,  0.3529],\n",
      "        [ 0.0628,  0.2906, -0.3528,  0.2893],\n",
      "        [-0.1418, -0.2615, -0.0950,  0.2029],\n",
      "        [-0.2585,  0.3296,  0.3162,  0.3222],\n",
      "        [ 0.2301,  0.3267, -0.0217,  0.2536],\n",
      "        [-0.0596, -0.2654,  0.2306,  0.1583]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([8, 8])\n",
      "tensor([[ 0.1502,  0.1798, -0.0850, -0.1503, -0.1788, -0.2761, -0.2895,  0.2239],\n",
      "        [-0.2497,  0.2637,  0.0318,  0.2843,  0.3023, -0.0147, -0.1036,  0.0282],\n",
      "        [-0.1851,  0.2389,  0.3192, -0.2411, -0.3012,  0.0172,  0.2669,  0.2849],\n",
      "        [-0.3062, -0.0375,  0.0070, -0.2296, -0.0683,  0.2136,  0.1168,  0.1170],\n",
      "        [ 0.2859,  0.2414, -0.3532, -0.3180,  0.0624,  0.3174, -0.3462, -0.1620],\n",
      "        [-0.1285,  0.0549, -0.3518,  0.0362,  0.3494,  0.1207,  0.1131, -0.1509],\n",
      "        [ 0.1372, -0.3255, -0.0823,  0.3374, -0.1852, -0.0021,  0.0576,  0.1676],\n",
      "        [-0.0199,  0.2874,  0.2247, -0.2074,  0.1151, -0.1673,  0.1663, -0.1422]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([8, 4])\n",
      "tensor([[-0.1242, -0.3357, -0.1516,  0.1445],\n",
      "        [ 0.1418,  0.0838, -0.0708, -0.0738],\n",
      "        [ 0.2737,  0.2778, -0.1082,  0.0954],\n",
      "        [-0.2637, -0.1249, -0.1836, -0.1935],\n",
      "        [-0.0744, -0.1810,  0.0381,  0.3187],\n",
      "        [-0.0154,  0.2893,  0.1281,  0.0523],\n",
      "        [-0.2661,  0.0652, -0.0189,  0.2911],\n",
      "        [-0.1487,  0.0820,  0.2069,  0.0380]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([8, 4])\n",
      "tensor([[-0.3465, -0.0087,  0.1680, -0.3096],\n",
      "        [-0.3388,  0.3156, -0.0171, -0.3506],\n",
      "        [ 0.2263,  0.0940, -0.1288,  0.3529],\n",
      "        [ 0.0628,  0.2906, -0.3528,  0.2893],\n",
      "        [-0.1418, -0.2615, -0.0950,  0.2029],\n",
      "        [-0.2585,  0.3296,  0.3162,  0.3222],\n",
      "        [ 0.2301,  0.3267, -0.0217,  0.2536],\n",
      "        [-0.0596, -0.2654,  0.2306,  0.1583]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([8, 16])\n",
      "tensor([[ 0.1502,  0.1798, -0.0850, -0.1503, -0.1788, -0.2761, -0.2895,  0.2239,\n",
      "         -0.1242, -0.3357, -0.1516,  0.1445, -0.3465, -0.0087,  0.1680, -0.3096],\n",
      "        [-0.2497,  0.2637,  0.0318,  0.2843,  0.3023, -0.0147, -0.1036,  0.0282,\n",
      "          0.1418,  0.0838, -0.0708, -0.0738, -0.3388,  0.3156, -0.0171, -0.3506],\n",
      "        [-0.1851,  0.2389,  0.3192, -0.2411, -0.3012,  0.0172,  0.2669,  0.2849,\n",
      "          0.2737,  0.2778, -0.1082,  0.0954,  0.2263,  0.0940, -0.1288,  0.3529],\n",
      "        [-0.3062, -0.0375,  0.0070, -0.2296, -0.0683,  0.2136,  0.1168,  0.1170,\n",
      "         -0.2637, -0.1249, -0.1836, -0.1935,  0.0628,  0.2906, -0.3528,  0.2893],\n",
      "        [ 0.2859,  0.2414, -0.3532, -0.3180,  0.0624,  0.3174, -0.3462, -0.1620,\n",
      "         -0.0744, -0.1810,  0.0381,  0.3187, -0.1418, -0.2615, -0.0950,  0.2029],\n",
      "        [-0.1285,  0.0549, -0.3518,  0.0362,  0.3494,  0.1207,  0.1131, -0.1509,\n",
      "         -0.0154,  0.2893,  0.1281,  0.0523, -0.2585,  0.3296,  0.3162,  0.3222],\n",
      "        [ 0.1372, -0.3255, -0.0823,  0.3374, -0.1852, -0.0021,  0.0576,  0.1676,\n",
      "         -0.2661,  0.0652, -0.0189,  0.2911,  0.2301,  0.3267, -0.0217,  0.2536],\n",
      "        [-0.0199,  0.2874,  0.2247, -0.2074,  0.1151, -0.1673,  0.1663, -0.1422,\n",
      "         -0.1487,  0.0820,  0.2069,  0.0380, -0.0596, -0.2654,  0.2306,  0.1583]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.5435,  1.0294, -0.3773, -0.6143,  0.3396,  0.5044, -0.0765,\n",
      "           0.2151, -0.3465,  0.1366, -0.2164,  0.4212, -0.7359,  0.8489,\n",
      "          -0.1543,  0.8772],\n",
      "         [-0.4826,  1.3169, -0.1335, -0.6914,  0.5700,  0.1757,  0.2190,\n",
      "          -0.0542, -0.0765,  0.5556,  0.1783,  0.4525, -0.8017,  0.4191,\n",
      "           0.4698,  0.9369],\n",
      "         [ 0.0026,  1.0828, -0.4357, -0.4119,  0.1914,  0.3187, -0.3830,\n",
      "           0.2171, -0.1836,  0.1088, -0.0945,  0.9063, -0.7317,  0.4843,\n",
      "           0.0945,  0.6295]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.5435,  1.0294, -0.3773, -0.6143,  0.3396,  0.5044, -0.0765,\n",
      "           0.2151],\n",
      "         [-0.4826,  1.3169, -0.1335, -0.6914,  0.5700,  0.1757,  0.2190,\n",
      "          -0.0542],\n",
      "         [ 0.0026,  1.0828, -0.4357, -0.4119,  0.1914,  0.3187, -0.3830,\n",
      "           0.2171]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.3465,  0.1366, -0.2164,  0.4212],\n",
      "         [-0.0765,  0.5556,  0.1783,  0.4525],\n",
      "         [-0.1836,  0.1088, -0.0945,  0.9063]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.7359,  0.8489, -0.1543,  0.8772],\n",
      "         [-0.8017,  0.4191,  0.4698,  0.9369],\n",
      "         [-0.7317,  0.4843,  0.0945,  0.6295]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.5435,  1.0294, -0.3773, -0.6143],\n",
      "          [ 0.3396,  0.5044, -0.0765,  0.2151]],\n",
      "\n",
      "         [[-0.4826,  1.3169, -0.1335, -0.6914],\n",
      "          [ 0.5700,  0.1757,  0.2190, -0.0542]],\n",
      "\n",
      "         [[ 0.0026,  1.0828, -0.4357, -0.4119],\n",
      "          [ 0.1914,  0.3187, -0.3830,  0.2171]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[-0.3465,  0.1366, -0.2164,  0.4212]],\n",
      "\n",
      "         [[-0.0765,  0.5556,  0.1783,  0.4525]],\n",
      "\n",
      "         [[-0.1836,  0.1088, -0.0945,  0.9063]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[-0.7359,  0.8489, -0.1543,  0.8772]],\n",
      "\n",
      "         [[-0.8017,  0.4191,  0.4698,  0.9369]],\n",
      "\n",
      "         [[-0.7317,  0.4843,  0.0945,  0.6295]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.5435,  1.0294, -0.3773, -0.6143],\n",
      "          [ 0.3396,  0.5044, -0.0765,  0.2151]],\n",
      "\n",
      "         [[-0.1484,  1.3793, -0.4783, -0.5564],\n",
      "          [ 0.1237,  0.1803,  0.5980, -0.0364]],\n",
      "\n",
      "         [[ 0.3951,  1.1431,  0.1837, -0.1885],\n",
      "          [ 0.2686,  0.2692,  0.3334,  0.2761]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[-0.3465,  0.1366, -0.2164,  0.4212]],\n",
      "\n",
      "         [[-0.1914,  0.5076,  0.0320,  0.5057]],\n",
      "\n",
      "         [[ 0.1623, -0.0734, -0.1276,  0.9098]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.3465,  0.1366, -0.2164,  0.4212],\n",
      "          [-0.3465,  0.1366, -0.2164,  0.4212]],\n",
      "\n",
      "         [[-0.1914,  0.5076,  0.0320,  0.5057],\n",
      "          [-0.1914,  0.5076,  0.0320,  0.5057]],\n",
      "\n",
      "         [[ 0.1623, -0.0734, -0.1276,  0.9098],\n",
      "          [ 0.1623, -0.0734, -0.1276,  0.9098]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.7359,  0.8489, -0.1543,  0.8772],\n",
      "          [-0.7359,  0.8489, -0.1543,  0.8772]],\n",
      "\n",
      "         [[-0.8017,  0.4191,  0.4698,  0.9369],\n",
      "          [-0.8017,  0.4191,  0.4698,  0.9369]],\n",
      "\n",
      "         [[-0.7317,  0.4843,  0.0945,  0.6295],\n",
      "          [-0.7317,  0.4843,  0.0945,  0.6295]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.5435,  1.0294, -0.3773, -0.6143],\n",
      "          [-0.1484,  1.3793, -0.4783, -0.5564],\n",
      "          [ 0.3951,  1.1431,  0.1837, -0.1885]],\n",
      "\n",
      "         [[ 0.3396,  0.5044, -0.0765,  0.2151],\n",
      "          [ 0.1237,  0.1803,  0.5980, -0.0364],\n",
      "          [ 0.2686,  0.2692,  0.3334,  0.2761]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.3465,  0.1366, -0.2164,  0.4212],\n",
      "          [-0.1914,  0.5076,  0.0320,  0.5057],\n",
      "          [ 0.1623, -0.0734, -0.1276,  0.9098]],\n",
      "\n",
      "         [[-0.3465,  0.1366, -0.2164,  0.4212],\n",
      "          [-0.1914,  0.5076,  0.0320,  0.5057],\n",
      "          [ 0.1623, -0.0734, -0.1276,  0.9098]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.7359,  0.8489, -0.1543,  0.8772],\n",
      "          [-0.8017,  0.4191,  0.4698,  0.9369],\n",
      "          [-0.7317,  0.4843,  0.0945,  0.6295]],\n",
      "\n",
      "         [[-0.7359,  0.8489, -0.1543,  0.8772],\n",
      "          [-0.8017,  0.4191,  0.4698,  0.9369],\n",
      "          [-0.7317,  0.4843,  0.0945,  0.6295]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 0.0759,  0.1519, -0.3373],\n",
      "          [ 0.0544,  0.2159, -0.2853],\n",
      "          [-0.0500,  0.2076, -0.1074]],\n",
      "\n",
      "         [[ 0.0292,  0.1487,  0.1118],\n",
      "          [-0.0815,  0.0343, -0.0513],\n",
      "          [-0.0061,  0.1177,  0.1162]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 7.5903e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 5.4443e-02,  2.1595e-01, -2.3820e+38],\n",
      "          [-4.9991e-02,  2.0758e-01, -1.0738e-01]],\n",
      "\n",
      "         [[ 2.9193e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [-8.1485e-02,  3.4279e-02, -2.3820e+38],\n",
      "          [-6.0912e-03,  1.1775e-01,  1.1622e-01]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4597, 0.5403, 0.0000],\n",
      "          [0.3088, 0.3996, 0.2916]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4711, 0.5289, 0.0000],\n",
      "          [0.3066, 0.3470, 0.3465]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.7359,  0.8489, -0.1543,  0.8772],\n",
      "          [-0.7715,  0.6167,  0.1829,  0.9094],\n",
      "          [-0.7610,  0.5708,  0.1676,  0.8288]],\n",
      "\n",
      "         [[-0.7359,  0.8489, -0.1543,  0.8772],\n",
      "          [-0.7707,  0.6215,  0.1758,  0.9088],\n",
      "          [-0.7573,  0.5734,  0.1484,  0.8121]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.7359,  0.8489, -0.1543,  0.8772, -0.7359,  0.8489, -0.1543,\n",
      "           0.8772],\n",
      "         [-0.7715,  0.6167,  0.1829,  0.9094, -0.7707,  0.6215,  0.1758,\n",
      "           0.9088],\n",
      "         [-0.7610,  0.5708,  0.1676,  0.8288, -0.7573,  0.5734,  0.1484,\n",
      "           0.8121]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.2359, -0.3445, -0.3074,  0.0164, -0.2715,  0.3457,  0.3244, -0.0531],\n",
      "        [ 0.0739, -0.0161,  0.0873, -0.1938,  0.0804,  0.0464, -0.1923,  0.2232],\n",
      "        [-0.2372, -0.1164,  0.1841, -0.1554,  0.0972,  0.1430,  0.1592,  0.1974],\n",
      "        [-0.1572,  0.2313, -0.3087,  0.3028, -0.1791,  0.2931, -0.3133,  0.2541],\n",
      "        [ 0.1753,  0.0905,  0.2442,  0.2022,  0.2775, -0.0964, -0.1781,  0.1421],\n",
      "        [-0.1795, -0.2951,  0.0751,  0.0858, -0.3370,  0.1048,  0.1379,  0.1900],\n",
      "        [-0.0702,  0.2856,  0.1283,  0.1785, -0.2395,  0.3145,  0.0259,  0.1171],\n",
      "        [-0.2090,  0.1637,  0.1075, -0.1175, -0.3137,  0.0028,  0.1549, -0.1207]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([8, 8])\n",
      "tensor([[-0.2359, -0.3445, -0.3074,  0.0164, -0.2715,  0.3457,  0.3244, -0.0531],\n",
      "        [ 0.0739, -0.0161,  0.0873, -0.1938,  0.0804,  0.0464, -0.1923,  0.2232],\n",
      "        [-0.2372, -0.1164,  0.1841, -0.1554,  0.0972,  0.1430,  0.1592,  0.1974],\n",
      "        [-0.1572,  0.2313, -0.3087,  0.3028, -0.1791,  0.2931, -0.3133,  0.2541],\n",
      "        [ 0.1753,  0.0905,  0.2442,  0.2022,  0.2775, -0.0964, -0.1781,  0.1421],\n",
      "        [-0.1795, -0.2951,  0.0751,  0.0858, -0.3370,  0.1048,  0.1379,  0.1900],\n",
      "        [-0.0702,  0.2856,  0.1283,  0.1785, -0.2395,  0.3145,  0.0259,  0.1171],\n",
      "        [-0.2090,  0.1637,  0.1075, -0.1175, -0.3137,  0.0028,  0.1549, -0.1207]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.3189,  0.2431, -0.0403, -0.0936, -0.6325,  0.1339, -0.3212,\n",
      "           0.3538],\n",
      "         [-0.4078,  0.3907,  0.0226, -0.0631, -0.6366,  0.2519, -0.2562,\n",
      "           0.3654],\n",
      "         [-0.3642,  0.3627,  0.0233, -0.0710, -0.5733,  0.2124, -0.2459,\n",
      "           0.3323]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[ 0.0460,  0.9884,  0.4850,  0.7028,  0.0813,  0.7218, -0.0572,\n",
      "           0.7785],\n",
      "         [-0.1543,  0.9203,  0.6054,  0.1891, -0.1671,  0.9992, -0.2144,\n",
      "           1.1645],\n",
      "         [ 0.1126,  1.1330,  0.6250,  0.0967,  0.3906,  0.6247,  0.2697,\n",
      "           0.7555]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.0460,  0.9884,  0.4850,  0.7028,  0.0813,  0.7218, -0.0572,\n",
      "           0.7785],\n",
      "         [-0.1543,  0.9203,  0.6054,  0.1891, -0.1671,  0.9992, -0.2144,\n",
      "           1.1645],\n",
      "         [ 0.1126,  1.1330,  0.6250,  0.0967,  0.3906,  0.6247,  0.2697,\n",
      "           0.7555]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.0771,  1.6574,  0.8133,  1.1784,  0.1364,  1.2104, -0.0959,\n",
      "           1.3054],\n",
      "         [-0.2268,  1.3530,  0.8901,  0.2780, -0.2456,  1.4689, -0.3151,\n",
      "           1.7120],\n",
      "         [ 0.1875,  1.8875,  1.0412,  0.1612,  0.6508,  1.0407,  0.4492,\n",
      "           1.2585]]], grad_fn=<MulBackward0>)\n",
      "dim: 8\n",
      "skip: 0\n",
      "spliced scale: torch.Size([8])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.0771,  1.6574,  0.8133,  1.1784,  0.1364,  1.2104, -0.0959,\n",
      "           1.3054],\n",
      "         [-0.2268,  1.3530,  0.8901,  0.2780, -0.2456,  1.4689, -0.3151,\n",
      "           1.7120],\n",
      "         [ 0.1875,  1.8875,  1.0412,  0.1612,  0.6508,  1.0407,  0.4492,\n",
      "           1.2585]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.0771,  1.6574,  0.8133,  1.1784,  0.1364,  1.2104, -0.0959,\n",
      "           1.3054],\n",
      "         [-0.2268,  1.3530,  0.8901,  0.2780, -0.2456,  1.4689, -0.3151,\n",
      "           1.7120],\n",
      "         [ 0.1875,  1.8875,  1.0412,  0.1612,  0.6508,  1.0407,  0.4492,\n",
      "           1.2585]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 8\n",
      "d_skip: 0\n",
      "i_dim: 32\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1760,  0.1617,  0.2807,  0.2303,  0.2907,  0.2507,  0.2461, -0.2108,\n",
      "         -0.1825, -0.1839,  0.0289, -0.3351,  0.0184, -0.3150,  0.2407, -0.2283,\n",
      "          0.3006,  0.1116, -0.1873, -0.0189,  0.2760,  0.1654, -0.3429, -0.2691,\n",
      "         -0.0934, -0.2141,  0.1404,  0.2679,  0.1097, -0.1541, -0.0320, -0.0230],\n",
      "        [-0.2891,  0.2322, -0.2442, -0.2503,  0.3135,  0.1324,  0.0568,  0.0847,\n",
      "         -0.2429,  0.3088, -0.0057,  0.0883, -0.3341,  0.1589,  0.3079, -0.1255,\n",
      "          0.2716,  0.1963, -0.2388,  0.3028, -0.2125, -0.1950,  0.0877, -0.3028,\n",
      "          0.2173, -0.2335, -0.0532, -0.0752,  0.2516,  0.2521,  0.0364, -0.1494],\n",
      "        [ 0.2527, -0.0988,  0.1256,  0.0559, -0.2523,  0.2330,  0.1019,  0.1221,\n",
      "         -0.2311, -0.3405,  0.0849, -0.2275, -0.2592, -0.3223,  0.1535, -0.2199,\n",
      "          0.2980,  0.2386,  0.2861, -0.1045,  0.3380,  0.1834,  0.3148, -0.2105,\n",
      "         -0.1854,  0.3025,  0.2121,  0.0624,  0.0137, -0.0299,  0.1318, -0.1496],\n",
      "        [-0.1045,  0.0137, -0.0626,  0.0391,  0.1527, -0.2717,  0.3227,  0.1922,\n",
      "         -0.0718,  0.0257,  0.2180, -0.0458,  0.1683,  0.1679,  0.3125,  0.0181,\n",
      "          0.2943,  0.2844,  0.0350,  0.0496,  0.2577, -0.2275,  0.0966, -0.2033,\n",
      "          0.2770,  0.1742, -0.2462,  0.2048,  0.1520, -0.1591,  0.0702, -0.0728],\n",
      "        [ 0.2875,  0.1488,  0.0089, -0.3011, -0.0944, -0.2060,  0.3534,  0.0776,\n",
      "          0.0262,  0.1434, -0.2318,  0.1620,  0.3342, -0.3227,  0.2201,  0.2323,\n",
      "         -0.0310, -0.2903,  0.1467, -0.3150,  0.2249, -0.0581, -0.0376,  0.3307,\n",
      "         -0.3353,  0.1747, -0.0357,  0.2419,  0.3352, -0.1686,  0.2382,  0.0151],\n",
      "        [-0.2751, -0.2114, -0.0754,  0.0091, -0.2831,  0.0468,  0.0837,  0.3076,\n",
      "         -0.2431,  0.3523,  0.0598,  0.1531, -0.2942,  0.1197,  0.0374,  0.2863,\n",
      "          0.1343, -0.1499,  0.1784, -0.2798, -0.3256,  0.2548, -0.2344,  0.2835,\n",
      "          0.1349,  0.0557,  0.0505,  0.2245, -0.3234,  0.1323,  0.1249,  0.1029],\n",
      "        [-0.2134,  0.0634,  0.0990, -0.3326,  0.2671, -0.0749,  0.1779, -0.0982,\n",
      "          0.1681, -0.2408, -0.1863,  0.1112,  0.0328, -0.3133,  0.2289,  0.2664,\n",
      "         -0.3323, -0.0777,  0.1480, -0.0807, -0.1593,  0.3455, -0.1876, -0.0862,\n",
      "         -0.2677,  0.0882,  0.2261, -0.3422, -0.0410,  0.0843,  0.1618, -0.1125],\n",
      "        [ 0.3267, -0.2617, -0.3281,  0.0869,  0.1342,  0.1968,  0.3423,  0.3504,\n",
      "         -0.1780, -0.0036, -0.1627,  0.0889,  0.0707,  0.0613, -0.2297, -0.2203,\n",
      "         -0.2489, -0.1318,  0.2520,  0.1960,  0.3480,  0.2359, -0.0771, -0.2166,\n",
      "          0.3351, -0.0614,  0.3102,  0.2966, -0.0357, -0.3250,  0.0561,  0.2476]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([8, 32])\n",
      "tensor([[ 0.1760,  0.1617,  0.2807,  0.2303,  0.2907,  0.2507,  0.2461, -0.2108,\n",
      "         -0.1825, -0.1839,  0.0289, -0.3351,  0.0184, -0.3150,  0.2407, -0.2283,\n",
      "          0.3006,  0.1116, -0.1873, -0.0189,  0.2760,  0.1654, -0.3429, -0.2691,\n",
      "         -0.0934, -0.2141,  0.1404,  0.2679,  0.1097, -0.1541, -0.0320, -0.0230],\n",
      "        [-0.2891,  0.2322, -0.2442, -0.2503,  0.3135,  0.1324,  0.0568,  0.0847,\n",
      "         -0.2429,  0.3088, -0.0057,  0.0883, -0.3341,  0.1589,  0.3079, -0.1255,\n",
      "          0.2716,  0.1963, -0.2388,  0.3028, -0.2125, -0.1950,  0.0877, -0.3028,\n",
      "          0.2173, -0.2335, -0.0532, -0.0752,  0.2516,  0.2521,  0.0364, -0.1494],\n",
      "        [ 0.2527, -0.0988,  0.1256,  0.0559, -0.2523,  0.2330,  0.1019,  0.1221,\n",
      "         -0.2311, -0.3405,  0.0849, -0.2275, -0.2592, -0.3223,  0.1535, -0.2199,\n",
      "          0.2980,  0.2386,  0.2861, -0.1045,  0.3380,  0.1834,  0.3148, -0.2105,\n",
      "         -0.1854,  0.3025,  0.2121,  0.0624,  0.0137, -0.0299,  0.1318, -0.1496],\n",
      "        [-0.1045,  0.0137, -0.0626,  0.0391,  0.1527, -0.2717,  0.3227,  0.1922,\n",
      "         -0.0718,  0.0257,  0.2180, -0.0458,  0.1683,  0.1679,  0.3125,  0.0181,\n",
      "          0.2943,  0.2844,  0.0350,  0.0496,  0.2577, -0.2275,  0.0966, -0.2033,\n",
      "          0.2770,  0.1742, -0.2462,  0.2048,  0.1520, -0.1591,  0.0702, -0.0728],\n",
      "        [ 0.2875,  0.1488,  0.0089, -0.3011, -0.0944, -0.2060,  0.3534,  0.0776,\n",
      "          0.0262,  0.1434, -0.2318,  0.1620,  0.3342, -0.3227,  0.2201,  0.2323,\n",
      "         -0.0310, -0.2903,  0.1467, -0.3150,  0.2249, -0.0581, -0.0376,  0.3307,\n",
      "         -0.3353,  0.1747, -0.0357,  0.2419,  0.3352, -0.1686,  0.2382,  0.0151],\n",
      "        [-0.2751, -0.2114, -0.0754,  0.0091, -0.2831,  0.0468,  0.0837,  0.3076,\n",
      "         -0.2431,  0.3523,  0.0598,  0.1531, -0.2942,  0.1197,  0.0374,  0.2863,\n",
      "          0.1343, -0.1499,  0.1784, -0.2798, -0.3256,  0.2548, -0.2344,  0.2835,\n",
      "          0.1349,  0.0557,  0.0505,  0.2245, -0.3234,  0.1323,  0.1249,  0.1029],\n",
      "        [-0.2134,  0.0634,  0.0990, -0.3326,  0.2671, -0.0749,  0.1779, -0.0982,\n",
      "          0.1681, -0.2408, -0.1863,  0.1112,  0.0328, -0.3133,  0.2289,  0.2664,\n",
      "         -0.3323, -0.0777,  0.1480, -0.0807, -0.1593,  0.3455, -0.1876, -0.0862,\n",
      "         -0.2677,  0.0882,  0.2261, -0.3422, -0.0410,  0.0843,  0.1618, -0.1125],\n",
      "        [ 0.3267, -0.2617, -0.3281,  0.0869,  0.1342,  0.1968,  0.3423,  0.3504,\n",
      "         -0.1780, -0.0036, -0.1627,  0.0889,  0.0707,  0.0613, -0.2297, -0.2203,\n",
      "         -0.2489, -0.1318,  0.2520,  0.1960,  0.3480,  0.2359, -0.0771, -0.2166,\n",
      "          0.3351, -0.0614,  0.3102,  0.2966, -0.0357, -0.3250,  0.0561,  0.2476]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bgate: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([ 0.1882,  0.3071, -0.3309, -0.1394, -0.2129,  0.2383, -0.0447, -0.2578,\n",
      "         0.1179, -0.1039,  0.0776,  0.0136, -0.2235,  0.0889,  0.2874,  0.2905,\n",
      "         0.0105, -0.1938, -0.2764, -0.3181,  0.0679, -0.1205,  0.3002, -0.2597,\n",
      "        -0.2345,  0.3042, -0.0831,  0.1504, -0.2135, -0.0298, -0.1462,  0.2660],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([32])\n",
      "tensor([ 0.1882,  0.3071, -0.3309, -0.1394, -0.2129,  0.2383, -0.0447, -0.2578,\n",
      "         0.1179, -0.1039,  0.0776,  0.0136, -0.2235,  0.0889,  0.2874,  0.2905,\n",
      "         0.0105, -0.1938, -0.2764, -0.3181,  0.0679, -0.1205,  0.3002, -0.2597,\n",
      "        -0.2345,  0.3042, -0.0831,  0.1504, -0.2135, -0.0298, -0.1462,  0.2660],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 32])\n",
      "tensor([[[-0.0419,  0.0568, -1.2134, -0.3298,  0.0979,  0.6390,  1.1111,\n",
      "           1.0418, -1.1106,  0.6116,  0.2427,  0.2080, -1.0097,  0.4747,\n",
      "           1.0629, -0.0275,  0.9382,  0.2837,  0.1380,  0.0379,  0.4216,\n",
      "           0.0256,  0.4175, -1.0792,  0.8747,  0.3546,  0.1615,  1.0633,\n",
      "           0.0140, -0.1309,  0.3431,  0.2698],\n",
      "         [ 0.1048, -0.3147, -1.3364, -0.1288, -0.2839,  0.9723,  0.7230,\n",
      "           1.1304, -1.1163,  0.6118,  0.1245,  0.2962, -1.2672,  0.5940,\n",
      "           0.4084, -0.1158,  0.5402, -0.0121,  0.3181,  0.0442,  0.2028,\n",
      "           0.3619,  0.3957, -0.8606,  0.9313,  0.2605,  0.4761,  0.9864,\n",
      "          -0.4490, -0.0718,  0.2172,  0.5225],\n",
      "         [ 0.1379,  0.2508, -1.0595, -0.7307,  0.1280,  0.8627,  1.0947,\n",
      "           0.7881, -1.0115,  0.4414, -0.1812,  0.2998, -1.0784, -0.1277,\n",
      "           1.1198,  0.0764,  0.5942, -0.0537,  0.2060, -0.1365,  0.2859,\n",
      "           0.3763,  0.2950, -0.9346,  0.2334,  0.3004,  0.5453,  0.7674,\n",
      "           0.1391,  0.0172,  0.4935,  0.1901]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 32])\n",
      "tensor([[[-0.0203,  0.0297, -0.1365, -0.1223,  0.0528,  0.4719,  0.9630,\n",
      "           0.8869, -0.1481,  0.4462,  0.1446,  0.1211, -0.1578,  0.3240,\n",
      "           0.9100, -0.0134,  0.7748,  0.1735,  0.0766,  0.0195,  0.2797,\n",
      "           0.0130,  0.2763, -0.1514,  0.7078,  0.2264,  0.0911,  0.9104,\n",
      "           0.0071, -0.0586,  0.2176,  0.1636],\n",
      "         [ 0.0568, -0.1185, -0.1212, -0.0578, -0.1102,  0.8114,  0.5532,\n",
      "           0.9844, -0.1475,  0.4464,  0.0684,  0.1826, -0.1299,  0.4299,\n",
      "           0.2690, -0.0526,  0.3811, -0.0060,  0.1988,  0.0229,  0.1177,\n",
      "           0.2321,  0.2587, -0.1676,  0.7675,  0.1570,  0.3252,  0.8267,\n",
      "          -0.1467, -0.0339,  0.1273,  0.3654],\n",
      "         [ 0.0765,  0.1503, -0.1533, -0.1699,  0.0705,  0.6952,  0.9449,\n",
      "           0.6184, -0.1577,  0.2960, -0.0776,  0.1852, -0.1514, -0.0574,\n",
      "           0.9727,  0.0405,  0.4301, -0.0257,  0.1198, -0.0609,  0.1751,\n",
      "           0.2433,  0.1817, -0.1636,  0.1382,  0.1856,  0.3856,  0.5975,\n",
      "           0.0773,  0.0087,  0.3401,  0.1094]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[-0.0345, -0.1969, -0.1190,  0.1863, -0.2238,  0.2097, -0.2660,  0.2972,\n",
      "          0.0656,  0.1339, -0.2194,  0.2952, -0.2318,  0.3463,  0.1745,  0.3532,\n",
      "          0.3499,  0.3015, -0.1515, -0.3300, -0.2480,  0.1244,  0.2610, -0.0567,\n",
      "         -0.1432,  0.0933, -0.1775, -0.1142,  0.0685, -0.1558, -0.0243, -0.2271],\n",
      "        [-0.2902,  0.2369,  0.2011,  0.2889,  0.2330,  0.2568,  0.2464,  0.0580,\n",
      "          0.0351,  0.0331,  0.1303,  0.1047, -0.3347, -0.2566, -0.3468,  0.0391,\n",
      "          0.2075, -0.2814, -0.0450, -0.1566,  0.0538,  0.2321, -0.2026, -0.0330,\n",
      "          0.1764,  0.2381,  0.1418,  0.0668,  0.0517, -0.2228, -0.2373,  0.2438],\n",
      "        [ 0.2093, -0.0292, -0.2097, -0.1470,  0.1214, -0.0553, -0.1848, -0.1491,\n",
      "         -0.2325, -0.2515, -0.1211, -0.2482, -0.1080,  0.1149,  0.3401,  0.0577,\n",
      "          0.3318, -0.2809,  0.3446, -0.2718,  0.2066,  0.0388,  0.0363, -0.1684,\n",
      "         -0.2782, -0.0058, -0.2276,  0.0017,  0.2462, -0.2645, -0.3242, -0.2317],\n",
      "        [ 0.2072,  0.0229, -0.2189,  0.1211,  0.0018,  0.1960,  0.1039, -0.1614,\n",
      "         -0.2486, -0.1426,  0.0934,  0.2223,  0.1704,  0.1920, -0.2447, -0.1783,\n",
      "          0.1922,  0.2109,  0.2685,  0.0757,  0.1809, -0.1553, -0.0856, -0.1202,\n",
      "          0.2088,  0.3433,  0.1272,  0.0832, -0.0377, -0.0443,  0.1917, -0.1048],\n",
      "        [ 0.2383,  0.1409,  0.2620,  0.2927, -0.0319,  0.2527,  0.1483, -0.2413,\n",
      "         -0.1935, -0.0693, -0.3446,  0.2758, -0.0276, -0.0283,  0.0510, -0.0187,\n",
      "         -0.2073, -0.0565,  0.2295, -0.1010, -0.0683,  0.2095, -0.2510, -0.1570,\n",
      "         -0.1460, -0.2980, -0.2999, -0.1810,  0.3325, -0.0536,  0.1931, -0.2711],\n",
      "        [-0.0434, -0.0916,  0.1320,  0.1927, -0.0154,  0.1202,  0.3082, -0.0792,\n",
      "          0.0547, -0.3517,  0.1226, -0.0848,  0.2627,  0.0718,  0.0380,  0.1332,\n",
      "         -0.0371,  0.2526, -0.3197, -0.3464, -0.1235,  0.2974, -0.2889,  0.1585,\n",
      "          0.0261,  0.3018,  0.2250,  0.0010, -0.1260, -0.0969,  0.2368,  0.1239],\n",
      "        [-0.3124, -0.0888, -0.1800, -0.2414,  0.0562, -0.1828,  0.1735,  0.1378,\n",
      "         -0.2697, -0.2483,  0.3533, -0.0007,  0.1085,  0.3183,  0.2935,  0.3070,\n",
      "          0.2565,  0.1640,  0.1111, -0.3058, -0.2252,  0.3500,  0.0774,  0.2653,\n",
      "          0.0897,  0.0913, -0.2136, -0.2988, -0.2393, -0.2459, -0.2337, -0.1627],\n",
      "        [ 0.0855, -0.2387,  0.1997,  0.2633, -0.1513, -0.1762,  0.1027,  0.1329,\n",
      "          0.2105, -0.1041, -0.0207,  0.3455, -0.1155, -0.1458, -0.0798, -0.1300,\n",
      "          0.0774, -0.0783, -0.0123,  0.2760, -0.1969, -0.3498, -0.1978,  0.3129,\n",
      "          0.0143, -0.0914, -0.0456,  0.1058, -0.0915,  0.2500, -0.0875, -0.2095]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([8, 32])\n",
      "tensor([[-0.0345, -0.1969, -0.1190,  0.1863, -0.2238,  0.2097, -0.2660,  0.2972,\n",
      "          0.0656,  0.1339, -0.2194,  0.2952, -0.2318,  0.3463,  0.1745,  0.3532,\n",
      "          0.3499,  0.3015, -0.1515, -0.3300, -0.2480,  0.1244,  0.2610, -0.0567,\n",
      "         -0.1432,  0.0933, -0.1775, -0.1142,  0.0685, -0.1558, -0.0243, -0.2271],\n",
      "        [-0.2902,  0.2369,  0.2011,  0.2889,  0.2330,  0.2568,  0.2464,  0.0580,\n",
      "          0.0351,  0.0331,  0.1303,  0.1047, -0.3347, -0.2566, -0.3468,  0.0391,\n",
      "          0.2075, -0.2814, -0.0450, -0.1566,  0.0538,  0.2321, -0.2026, -0.0330,\n",
      "          0.1764,  0.2381,  0.1418,  0.0668,  0.0517, -0.2228, -0.2373,  0.2438],\n",
      "        [ 0.2093, -0.0292, -0.2097, -0.1470,  0.1214, -0.0553, -0.1848, -0.1491,\n",
      "         -0.2325, -0.2515, -0.1211, -0.2482, -0.1080,  0.1149,  0.3401,  0.0577,\n",
      "          0.3318, -0.2809,  0.3446, -0.2718,  0.2066,  0.0388,  0.0363, -0.1684,\n",
      "         -0.2782, -0.0058, -0.2276,  0.0017,  0.2462, -0.2645, -0.3242, -0.2317],\n",
      "        [ 0.2072,  0.0229, -0.2189,  0.1211,  0.0018,  0.1960,  0.1039, -0.1614,\n",
      "         -0.2486, -0.1426,  0.0934,  0.2223,  0.1704,  0.1920, -0.2447, -0.1783,\n",
      "          0.1922,  0.2109,  0.2685,  0.0757,  0.1809, -0.1553, -0.0856, -0.1202,\n",
      "          0.2088,  0.3433,  0.1272,  0.0832, -0.0377, -0.0443,  0.1917, -0.1048],\n",
      "        [ 0.2383,  0.1409,  0.2620,  0.2927, -0.0319,  0.2527,  0.1483, -0.2413,\n",
      "         -0.1935, -0.0693, -0.3446,  0.2758, -0.0276, -0.0283,  0.0510, -0.0187,\n",
      "         -0.2073, -0.0565,  0.2295, -0.1010, -0.0683,  0.2095, -0.2510, -0.1570,\n",
      "         -0.1460, -0.2980, -0.2999, -0.1810,  0.3325, -0.0536,  0.1931, -0.2711],\n",
      "        [-0.0434, -0.0916,  0.1320,  0.1927, -0.0154,  0.1202,  0.3082, -0.0792,\n",
      "          0.0547, -0.3517,  0.1226, -0.0848,  0.2627,  0.0718,  0.0380,  0.1332,\n",
      "         -0.0371,  0.2526, -0.3197, -0.3464, -0.1235,  0.2974, -0.2889,  0.1585,\n",
      "          0.0261,  0.3018,  0.2250,  0.0010, -0.1260, -0.0969,  0.2368,  0.1239],\n",
      "        [-0.3124, -0.0888, -0.1800, -0.2414,  0.0562, -0.1828,  0.1735,  0.1378,\n",
      "         -0.2697, -0.2483,  0.3533, -0.0007,  0.1085,  0.3183,  0.2935,  0.3070,\n",
      "          0.2565,  0.1640,  0.1111, -0.3058, -0.2252,  0.3500,  0.0774,  0.2653,\n",
      "          0.0897,  0.0913, -0.2136, -0.2988, -0.2393, -0.2459, -0.2337, -0.1627],\n",
      "        [ 0.0855, -0.2387,  0.1997,  0.2633, -0.1513, -0.1762,  0.1027,  0.1329,\n",
      "          0.2105, -0.1041, -0.0207,  0.3455, -0.1155, -0.1458, -0.0798, -0.1300,\n",
      "          0.0774, -0.0783, -0.0123,  0.2760, -0.1969, -0.3498, -0.1978,  0.3129,\n",
      "          0.0143, -0.0914, -0.0456,  0.1058, -0.0915,  0.2500, -0.0875, -0.2095]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bup: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([-0.3434,  0.0199, -0.1054,  0.1085, -0.3156,  0.0699,  0.0418, -0.0772,\n",
      "         0.3115, -0.1414, -0.2681,  0.1874, -0.3144,  0.3272, -0.3007,  0.0272,\n",
      "        -0.1218,  0.3070,  0.3219,  0.2014,  0.1873,  0.3412,  0.1516,  0.1287,\n",
      "        -0.2760, -0.0995,  0.3407, -0.1615, -0.1807,  0.0896,  0.0587,  0.0463],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([32])\n",
      "tensor([-0.3434,  0.0199, -0.1054,  0.1085, -0.3156,  0.0699,  0.0418, -0.0772,\n",
      "         0.3115, -0.1414, -0.2681,  0.1874, -0.3144,  0.3272, -0.3007,  0.0272,\n",
      "        -0.1218,  0.3070,  0.3219,  0.2014,  0.1873,  0.3412,  0.1516,  0.1287,\n",
      "        -0.2760, -0.0995,  0.3407, -0.1615, -0.1807,  0.0896,  0.0587,  0.0463],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 32])\n",
      "tensor([[[-0.2913,  0.0057,  0.2637,  1.2648, -0.0718,  0.6652,  0.9125,\n",
      "          -0.2380,  0.2332, -0.9959, -0.0171,  0.8298, -0.6210,  0.1105,\n",
      "          -0.9531, -0.0845,  0.7487,  0.0640,  0.4499, -0.2591,  0.2445,\n",
      "           0.4824, -0.8849,  0.3444,  0.0468,  0.8986,  0.7193,  0.1832,\n",
      "          -0.1375, -0.3336, -0.1532, -0.0240],\n",
      "         [-0.3619, -0.1845,  0.4744,  1.0979, -0.1326,  0.2455,  0.8374,\n",
      "          -0.1166,  0.6413, -0.9899, -0.0060,  0.5024, -0.6027, -0.1804,\n",
      "          -0.7606, -0.1174,  0.4763, -0.1343,  0.0947, -0.0718,  0.1200,\n",
      "           0.2946, -0.8988,  0.6370, -0.1240,  0.6230,  0.7991,  0.3007,\n",
      "          -0.2656, -0.0479, -0.2680,  0.1335],\n",
      "         [-0.5693,  0.0594,  0.4766,  1.1691,  0.0069,  0.5536,  0.9057,\n",
      "          -0.1035,  0.1827, -0.9922, -0.1385,  0.7435, -0.9157,  0.0742,\n",
      "          -0.5037,  0.2993,  0.7510, -0.2248,  0.4618, -0.6433, -0.0352,\n",
      "           0.9808, -0.8359,  0.4368, -0.2355,  0.4627,  0.2444, -0.1595,\n",
      "           0.0427, -0.5742, -0.5435, -0.1785]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 32])\n",
      "tensor([[[ 5.9037e-03,  1.7048e-04, -3.5999e-02, -1.5466e-01, -3.7903e-03,\n",
      "           3.1392e-01,  8.7875e-01, -2.1111e-01, -3.4544e-02, -4.4440e-01,\n",
      "          -2.4746e-03,  1.0050e-01,  9.8021e-02,  3.5789e-02, -8.6732e-01,\n",
      "           1.1362e-03,  5.8010e-01,  1.1105e-02,  3.4447e-02, -5.0567e-03,\n",
      "           6.8382e-02,  6.2909e-03, -2.4449e-01, -5.2134e-02,  3.3118e-02,\n",
      "           2.0347e-01,  6.5518e-02,  1.6678e-01, -9.7413e-04,  1.9556e-02,\n",
      "          -3.3336e-02, -3.9194e-03],\n",
      "         [-2.0547e-02,  2.1862e-02, -5.7507e-02, -6.3464e-02,  1.4617e-02,\n",
      "           1.9921e-01,  4.6325e-01, -1.1478e-01, -9.4601e-02, -4.4190e-01,\n",
      "          -4.1112e-04,  9.1727e-02,  7.8310e-02, -7.7538e-02, -2.0459e-01,\n",
      "           6.1693e-03,  1.8155e-01,  8.0684e-04,  1.8826e-02, -1.6411e-03,\n",
      "           1.4115e-02,  6.8354e-02, -2.3256e-01, -1.0675e-01, -9.5201e-02,\n",
      "           9.7802e-02,  2.5988e-01,  2.4860e-01,  3.8958e-02,  1.6207e-03,\n",
      "          -3.4112e-02,  4.8792e-02],\n",
      "         [-4.3538e-02,  8.9290e-03, -7.3067e-02, -1.9861e-01,  4.8765e-04,\n",
      "           3.8487e-01,  8.5582e-01, -6.3991e-02, -2.8813e-02, -2.9371e-01,\n",
      "           1.0744e-02,  1.3770e-01,  1.3868e-01, -4.2557e-03, -4.8998e-01,\n",
      "           1.2124e-02,  3.2299e-01,  5.7804e-03,  5.5334e-02,  3.9148e-02,\n",
      "          -6.1667e-03,  2.3868e-01, -1.5189e-01, -7.1433e-02, -3.2551e-02,\n",
      "           8.5890e-02,  9.4228e-02, -9.5275e-02,  3.2991e-03, -5.0009e-03,\n",
      "          -1.8482e-01, -1.9526e-02]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([32, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.0850,  0.1153,  0.1581,  0.1439,  0.0503, -0.1568,  0.1261,  0.1764],\n",
      "        [ 0.0569, -0.0104,  0.1082, -0.1281, -0.1668,  0.1488,  0.1475, -0.0986],\n",
      "        [-0.1387, -0.1750, -0.1347, -0.0340, -0.1098,  0.1695, -0.0129, -0.1740],\n",
      "        [ 0.0198,  0.0893, -0.0751,  0.1264,  0.1262,  0.0622,  0.1746, -0.0427],\n",
      "        [ 0.1481, -0.1442, -0.0994,  0.1560, -0.0679, -0.0892,  0.0014, -0.0896],\n",
      "        [ 0.1427, -0.0932,  0.0327,  0.0317,  0.1637, -0.1696, -0.1525, -0.0409],\n",
      "        [ 0.0567, -0.0500, -0.1247, -0.0583,  0.0905, -0.0617,  0.1151, -0.0341],\n",
      "        [-0.0079, -0.1559, -0.1584, -0.0852,  0.1012,  0.0860, -0.1019, -0.0717],\n",
      "        [-0.0102,  0.0128, -0.0318, -0.0946, -0.1649, -0.0288,  0.0004,  0.1157],\n",
      "        [ 0.0497,  0.0184, -0.1080,  0.0066,  0.0440,  0.1178, -0.1676,  0.1211],\n",
      "        [ 0.0306,  0.1080,  0.1427,  0.0672, -0.0033, -0.0247,  0.0484, -0.1652],\n",
      "        [-0.0394,  0.1319,  0.0149, -0.0446,  0.1518,  0.0712,  0.0635, -0.0918],\n",
      "        [ 0.1189,  0.0833, -0.1566,  0.0450, -0.1586,  0.0322, -0.1621,  0.1114],\n",
      "        [ 0.1214,  0.0473, -0.1191, -0.0658, -0.1713, -0.0582,  0.1037, -0.1043],\n",
      "        [ 0.1116, -0.0727, -0.1730, -0.0014,  0.0986, -0.0410,  0.1223, -0.1329],\n",
      "        [ 0.0650,  0.1622, -0.1729,  0.0334,  0.0247, -0.1465, -0.0048,  0.1438],\n",
      "        [-0.0399,  0.1089,  0.0979, -0.0003, -0.0692, -0.1613,  0.0215,  0.1067],\n",
      "        [-0.0260, -0.0632,  0.1637, -0.0296, -0.0696,  0.0458, -0.1039,  0.0498],\n",
      "        [ 0.0445,  0.1640, -0.1516,  0.0887, -0.1539,  0.0745, -0.0450,  0.1303],\n",
      "        [ 0.1084,  0.1720,  0.0198,  0.1429, -0.0110, -0.0350, -0.1264, -0.1567],\n",
      "        [ 0.1279, -0.0088,  0.0901,  0.1618, -0.0099, -0.0392,  0.1676,  0.0993],\n",
      "        [-0.0247,  0.0870, -0.0077,  0.1028, -0.1554, -0.0746,  0.0758, -0.0241],\n",
      "        [-0.0216,  0.1248, -0.0355, -0.0416, -0.0078, -0.0313,  0.0194,  0.1227],\n",
      "        [ 0.0831, -0.0002, -0.0564,  0.0960, -0.0173, -0.0525,  0.0770,  0.0778],\n",
      "        [-0.0745, -0.0166,  0.1395, -0.0058,  0.1547,  0.0345, -0.1068,  0.1317],\n",
      "        [ 0.0487, -0.1538,  0.0905, -0.1369, -0.0311,  0.0144,  0.1008, -0.1313],\n",
      "        [-0.0726,  0.0640, -0.0446, -0.0528,  0.1134, -0.0013, -0.0278,  0.0920],\n",
      "        [ 0.0832, -0.0555,  0.0341, -0.0281,  0.0301,  0.0623, -0.1592,  0.1382],\n",
      "        [ 0.1725,  0.0517, -0.1166,  0.1355, -0.1035, -0.0208,  0.1060, -0.0896],\n",
      "        [ 0.0697, -0.0854,  0.0301, -0.0606,  0.0912,  0.1578,  0.0880, -0.0073],\n",
      "        [-0.1715,  0.0655, -0.0829, -0.1684,  0.0671,  0.1171, -0.1254, -0.1159],\n",
      "        [ 0.0771, -0.1681, -0.0308,  0.1270,  0.0781, -0.0324,  0.0923, -0.0970]],\n",
      "       requires_grad=True)\n",
      "Wdown spliced: torch.Size([32, 8])\n",
      "tensor([[-0.0850,  0.1153,  0.1581,  0.1439,  0.0503, -0.1568,  0.1261,  0.1764],\n",
      "        [ 0.0569, -0.0104,  0.1082, -0.1281, -0.1668,  0.1488,  0.1475, -0.0986],\n",
      "        [-0.1387, -0.1750, -0.1347, -0.0340, -0.1098,  0.1695, -0.0129, -0.1740],\n",
      "        [ 0.0198,  0.0893, -0.0751,  0.1264,  0.1262,  0.0622,  0.1746, -0.0427],\n",
      "        [ 0.1481, -0.1442, -0.0994,  0.1560, -0.0679, -0.0892,  0.0014, -0.0896],\n",
      "        [ 0.1427, -0.0932,  0.0327,  0.0317,  0.1637, -0.1696, -0.1525, -0.0409],\n",
      "        [ 0.0567, -0.0500, -0.1247, -0.0583,  0.0905, -0.0617,  0.1151, -0.0341],\n",
      "        [-0.0079, -0.1559, -0.1584, -0.0852,  0.1012,  0.0860, -0.1019, -0.0717],\n",
      "        [-0.0102,  0.0128, -0.0318, -0.0946, -0.1649, -0.0288,  0.0004,  0.1157],\n",
      "        [ 0.0497,  0.0184, -0.1080,  0.0066,  0.0440,  0.1178, -0.1676,  0.1211],\n",
      "        [ 0.0306,  0.1080,  0.1427,  0.0672, -0.0033, -0.0247,  0.0484, -0.1652],\n",
      "        [-0.0394,  0.1319,  0.0149, -0.0446,  0.1518,  0.0712,  0.0635, -0.0918],\n",
      "        [ 0.1189,  0.0833, -0.1566,  0.0450, -0.1586,  0.0322, -0.1621,  0.1114],\n",
      "        [ 0.1214,  0.0473, -0.1191, -0.0658, -0.1713, -0.0582,  0.1037, -0.1043],\n",
      "        [ 0.1116, -0.0727, -0.1730, -0.0014,  0.0986, -0.0410,  0.1223, -0.1329],\n",
      "        [ 0.0650,  0.1622, -0.1729,  0.0334,  0.0247, -0.1465, -0.0048,  0.1438],\n",
      "        [-0.0399,  0.1089,  0.0979, -0.0003, -0.0692, -0.1613,  0.0215,  0.1067],\n",
      "        [-0.0260, -0.0632,  0.1637, -0.0296, -0.0696,  0.0458, -0.1039,  0.0498],\n",
      "        [ 0.0445,  0.1640, -0.1516,  0.0887, -0.1539,  0.0745, -0.0450,  0.1303],\n",
      "        [ 0.1084,  0.1720,  0.0198,  0.1429, -0.0110, -0.0350, -0.1264, -0.1567],\n",
      "        [ 0.1279, -0.0088,  0.0901,  0.1618, -0.0099, -0.0392,  0.1676,  0.0993],\n",
      "        [-0.0247,  0.0870, -0.0077,  0.1028, -0.1554, -0.0746,  0.0758, -0.0241],\n",
      "        [-0.0216,  0.1248, -0.0355, -0.0416, -0.0078, -0.0313,  0.0194,  0.1227],\n",
      "        [ 0.0831, -0.0002, -0.0564,  0.0960, -0.0173, -0.0525,  0.0770,  0.0778],\n",
      "        [-0.0745, -0.0166,  0.1395, -0.0058,  0.1547,  0.0345, -0.1068,  0.1317],\n",
      "        [ 0.0487, -0.1538,  0.0905, -0.1369, -0.0311,  0.0144,  0.1008, -0.1313],\n",
      "        [-0.0726,  0.0640, -0.0446, -0.0528,  0.1134, -0.0013, -0.0278,  0.0920],\n",
      "        [ 0.0832, -0.0555,  0.0341, -0.0281,  0.0301,  0.0623, -0.1592,  0.1382],\n",
      "        [ 0.1725,  0.0517, -0.1166,  0.1355, -0.1035, -0.0208,  0.1060, -0.0896],\n",
      "        [ 0.0697, -0.0854,  0.0301, -0.0606,  0.0912,  0.1578,  0.0880, -0.0073],\n",
      "        [-0.1715,  0.0655, -0.0829, -0.1684,  0.0671,  0.1171, -0.1254, -0.1159],\n",
      "        [ 0.0771, -0.1681, -0.0308,  0.1270,  0.0781, -0.0324,  0.0923, -0.0970]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bdown: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.0952,  0.0296,  0.1446, -0.0618, -0.1172,  0.1323,  0.0880,  0.1261],\n",
      "       requires_grad=True)\n",
      "Bdown spliced: torch.Size([8])\n",
      "tensor([-0.0952,  0.0296,  0.1446, -0.0618, -0.1172,  0.1323,  0.0880,  0.1261],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.0941,  0.0570,  0.3775, -0.1179, -0.1628, -0.0866,  0.1072,\n",
      "           0.2199],\n",
      "         [-0.0550,  0.0070,  0.2244, -0.0776, -0.0765,  0.0099,  0.1313,\n",
      "           0.1017],\n",
      "         [-0.0314,  0.0561,  0.2203, -0.0723, -0.1415, -0.0950,  0.1256,\n",
      "           0.1360]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[-0.0481,  1.0454,  0.8625,  0.5849, -0.0815,  0.6353,  0.0500,\n",
      "           0.9984],\n",
      "         [-0.2093,  0.9273,  0.8299,  0.1116, -0.2436,  1.0091, -0.0830,\n",
      "           1.2662],\n",
      "         [ 0.0812,  1.1891,  0.8453,  0.0245,  0.2492,  0.5297,  0.3952,\n",
      "           0.8915]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "y: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.0481,  1.0454,  0.8625,  0.5849, -0.0815,  0.6353,  0.0500,\n",
      "           0.9984],\n",
      "         [-0.2093,  0.9273,  0.8299,  0.1116, -0.2436,  1.0091, -0.0830,\n",
      "           1.2662],\n",
      "         [ 0.0812,  1.1891,  0.8453,  0.0245,  0.2492,  0.5297,  0.3952,\n",
      "           0.8915]]], grad_fn=<AddBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.7040, 0.0184, 0.5238, 0.0015],\n",
      "         [0.7563, 0.7421, 0.3337, 0.1040],\n",
      "         [0.1373, 0.8824, 0.4077, 0.0667]]])\n",
      "---------- Layer Input: torch.Tensor ------------\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.7040, 0.0184, 0.5238, 0.0015],\n",
      "         [0.7563, 0.7421, 0.3337, 0.1040],\n",
      "         [0.1373, 0.8824, 0.4077, 0.0667]]])\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[1.6042, 0.0418, 1.1937, 0.0033],\n",
      "         [1.3557, 1.3303, 0.5982, 0.1865],\n",
      "         [0.2791, 1.7936, 0.8286, 0.1355]]])\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[1.6042, 0.0418, 1.1937, 0.0033],\n",
      "         [1.3557, 1.3303, 0.5982, 0.1865],\n",
      "         [0.2791, 1.7936, 0.8286, 0.1355]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 0\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1502,  0.1798, -0.0850, -0.1503, -0.1788, -0.2761, -0.2895,  0.2239,\n",
      "         -0.1242, -0.3357, -0.1516,  0.1445, -0.3465, -0.0087,  0.1680, -0.3096],\n",
      "        [-0.2497,  0.2637,  0.0318,  0.2843,  0.3023, -0.0147, -0.1036,  0.0282,\n",
      "          0.1418,  0.0838, -0.0708, -0.0738, -0.3388,  0.3156, -0.0171, -0.3506],\n",
      "        [-0.1851,  0.2389,  0.3192, -0.2411, -0.3012,  0.0172,  0.2669,  0.2849,\n",
      "          0.2737,  0.2778, -0.1082,  0.0954,  0.2263,  0.0940, -0.1288,  0.3529],\n",
      "        [-0.3062, -0.0375,  0.0070, -0.2296, -0.0683,  0.2136,  0.1168,  0.1170,\n",
      "         -0.2637, -0.1249, -0.1836, -0.1935,  0.0628,  0.2906, -0.3528,  0.2893],\n",
      "        [ 0.2859,  0.2414, -0.3532, -0.3180,  0.0624,  0.3174, -0.3462, -0.1620,\n",
      "         -0.0744, -0.1810,  0.0381,  0.3187, -0.1418, -0.2615, -0.0950,  0.2029],\n",
      "        [-0.1285,  0.0549, -0.3518,  0.0362,  0.3494,  0.1207,  0.1131, -0.1509,\n",
      "         -0.0154,  0.2893,  0.1281,  0.0523, -0.2585,  0.3296,  0.3162,  0.3222],\n",
      "        [ 0.1372, -0.3255, -0.0823,  0.3374, -0.1852, -0.0021,  0.0576,  0.1676,\n",
      "         -0.2661,  0.0652, -0.0189,  0.2911,  0.2301,  0.3267, -0.0217,  0.2536],\n",
      "        [-0.0199,  0.2874,  0.2247, -0.2074,  0.1151, -0.1673,  0.1663, -0.1422,\n",
      "         -0.1487,  0.0820,  0.2069,  0.0380, -0.0596, -0.2654,  0.2306,  0.1583]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[ 0.1502,  0.1798, -0.0850, -0.1503, -0.1788, -0.2761, -0.2895,  0.2239],\n",
      "        [-0.2497,  0.2637,  0.0318,  0.2843,  0.3023, -0.0147, -0.1036,  0.0282],\n",
      "        [-0.1851,  0.2389,  0.3192, -0.2411, -0.3012,  0.0172,  0.2669,  0.2849],\n",
      "        [-0.3062, -0.0375,  0.0070, -0.2296, -0.0683,  0.2136,  0.1168,  0.1170],\n",
      "        [ 0.2859,  0.2414, -0.3532, -0.3180,  0.0624,  0.3174, -0.3462, -0.1620],\n",
      "        [-0.1285,  0.0549, -0.3518,  0.0362,  0.3494,  0.1207,  0.1131, -0.1509],\n",
      "        [ 0.1372, -0.3255, -0.0823,  0.3374, -0.1852, -0.0021,  0.0576,  0.1676],\n",
      "        [-0.0199,  0.2874,  0.2247, -0.2074,  0.1151, -0.1673,  0.1663, -0.1422]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[-0.1242, -0.3357, -0.1516,  0.1445],\n",
      "        [ 0.1418,  0.0838, -0.0708, -0.0738],\n",
      "        [ 0.2737,  0.2778, -0.1082,  0.0954],\n",
      "        [-0.2637, -0.1249, -0.1836, -0.1935],\n",
      "        [-0.0744, -0.1810,  0.0381,  0.3187],\n",
      "        [-0.0154,  0.2893,  0.1281,  0.0523],\n",
      "        [-0.2661,  0.0652, -0.0189,  0.2911],\n",
      "        [-0.1487,  0.0820,  0.2069,  0.0380]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[-0.3465, -0.0087,  0.1680, -0.3096],\n",
      "        [-0.3388,  0.3156, -0.0171, -0.3506],\n",
      "        [ 0.2263,  0.0940, -0.1288,  0.3529],\n",
      "        [ 0.0628,  0.2906, -0.3528,  0.2893],\n",
      "        [-0.1418, -0.2615, -0.0950,  0.2029],\n",
      "        [-0.2585,  0.3296,  0.3162,  0.3222],\n",
      "        [ 0.2301,  0.3267, -0.0217,  0.2536],\n",
      "        [-0.0596, -0.2654,  0.2306,  0.1583]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[ 0.1502,  0.1798, -0.1788, -0.2761],\n",
      "        [-0.2497,  0.2637,  0.3023, -0.0147],\n",
      "        [-0.1851,  0.2389, -0.3012,  0.0172],\n",
      "        [-0.3062, -0.0375, -0.0683,  0.2136]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[-0.1242, -0.3357],\n",
      "        [ 0.1418,  0.0838],\n",
      "        [ 0.2737,  0.2778],\n",
      "        [-0.2637, -0.1249]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[-0.3465, -0.0087],\n",
      "        [-0.3388,  0.3156],\n",
      "        [ 0.2263,  0.0940],\n",
      "        [ 0.0628,  0.2906]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[ 0.1502,  0.1798, -0.1788, -0.2761, -0.1242, -0.3357, -0.3465, -0.0087],\n",
      "        [-0.2497,  0.2637,  0.3023, -0.0147,  0.1418,  0.0838, -0.3388,  0.3156],\n",
      "        [-0.1851,  0.2389, -0.3012,  0.0172,  0.2737,  0.2778,  0.2263,  0.0940],\n",
      "        [-0.3062, -0.0375, -0.0683,  0.2136, -0.2637, -0.1249,  0.0628,  0.2906]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.0086,  0.5844, -0.6338, -0.4224,  0.1325, -0.2038, -0.2998,\n",
      "           0.1125],\n",
      "         [-0.2963,  0.7304, -0.0331, -0.3439,  0.1348, -0.2008, -0.7735,\n",
      "           0.5185],\n",
      "         [-0.6009,  0.7160,  0.2334, -0.0604,  0.4106,  0.2698, -0.5084,\n",
      "           0.6809]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0086,  0.5844, -0.6338, -0.4224],\n",
      "         [-0.2963,  0.7304, -0.0331, -0.3439],\n",
      "         [-0.6009,  0.7160,  0.2334, -0.0604]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.1325, -0.2038],\n",
      "         [ 0.1348, -0.2008],\n",
      "         [ 0.4106,  0.2698]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.2998,  0.1125],\n",
      "         [-0.7735,  0.5185],\n",
      "         [-0.5084,  0.6809]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.0086,  0.5844],\n",
      "          [-0.6338, -0.4224]],\n",
      "\n",
      "         [[-0.2963,  0.7304],\n",
      "          [-0.0331, -0.3439]],\n",
      "\n",
      "         [[-0.6009,  0.7160],\n",
      "          [ 0.2334, -0.0604]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.1325, -0.2038]],\n",
      "\n",
      "         [[ 0.1348, -0.2008]],\n",
      "\n",
      "         [[ 0.4106,  0.2698]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.2998,  0.1125]],\n",
      "\n",
      "         [[-0.7735,  0.5185]],\n",
      "\n",
      "         [[-0.5084,  0.6809]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.0086,  0.5844],\n",
      "          [-0.6338, -0.4224]],\n",
      "\n",
      "         [[-0.7747,  0.1453],\n",
      "          [ 0.2715, -0.2137]],\n",
      "\n",
      "         [[-0.4010, -0.8443],\n",
      "          [-0.0423,  0.2374]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.1325, -0.2038]],\n",
      "\n",
      "         [[ 0.2418,  0.0049]],\n",
      "\n",
      "         [[-0.4163,  0.2611]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.1325, -0.2038],\n",
      "          [ 0.1325, -0.2038]],\n",
      "\n",
      "         [[ 0.2418,  0.0049],\n",
      "          [ 0.2418,  0.0049]],\n",
      "\n",
      "         [[-0.4163,  0.2611],\n",
      "          [-0.4163,  0.2611]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.2998,  0.1125],\n",
      "          [-0.2998,  0.1125]],\n",
      "\n",
      "         [[-0.7735,  0.5185],\n",
      "          [-0.7735,  0.5185]],\n",
      "\n",
      "         [[-0.5084,  0.6809],\n",
      "          [-0.5084,  0.6809]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.0086,  0.5844],\n",
      "          [-0.7747,  0.1453],\n",
      "          [-0.4010, -0.8443]],\n",
      "\n",
      "         [[-0.6338, -0.4224],\n",
      "          [ 0.2715, -0.2137],\n",
      "          [-0.0423,  0.2374]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.1325, -0.2038],\n",
      "          [ 0.2418,  0.0049],\n",
      "          [-0.4163,  0.2611]],\n",
      "\n",
      "         [[ 0.1325, -0.2038],\n",
      "          [ 0.2418,  0.0049],\n",
      "          [-0.4163,  0.2611]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.2998,  0.1125],\n",
      "          [-0.7735,  0.5185],\n",
      "          [-0.5084,  0.6809]],\n",
      "\n",
      "         [[-0.2998,  0.1125],\n",
      "          [-0.7735,  0.5185],\n",
      "          [-0.5084,  0.6809]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-0.0834,  0.0035,  0.1054],\n",
      "          [-0.0935, -0.1319,  0.2548],\n",
      "          [ 0.0841, -0.0715, -0.0378]],\n",
      "\n",
      "         [[ 0.0015, -0.1098,  0.1086],\n",
      "          [ 0.0562,  0.0457, -0.1194],\n",
      "          [-0.0382, -0.0064,  0.0563]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-8.3426e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [-9.3533e-02, -1.3193e-01, -2.3820e+38],\n",
      "          [ 8.4118e-02, -7.1486e-02, -3.7844e-02]],\n",
      "\n",
      "         [[ 1.4883e-03, -2.3820e+38, -2.3820e+38],\n",
      "          [ 5.6238e-02,  4.5665e-02, -2.3820e+38],\n",
      "          [-3.8175e-02, -6.3985e-03,  5.6261e-02]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5096, 0.4904, 0.0000],\n",
      "          [0.3648, 0.3122, 0.3229]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5026, 0.4974, 0.0000],\n",
      "          [0.3194, 0.3297, 0.3510]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.2998,  0.1125],\n",
      "          [-0.5321,  0.3116],\n",
      "          [-0.5151,  0.4228]],\n",
      "\n",
      "         [[-0.2998,  0.1125],\n",
      "          [-0.5354,  0.3144],\n",
      "          [-0.5292,  0.4458]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.2998,  0.1125, -0.2998,  0.1125],\n",
      "         [-0.5321,  0.3116, -0.5354,  0.3144],\n",
      "         [-0.5151,  0.4228, -0.5292,  0.4458]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.2359, -0.3445, -0.3074,  0.0164, -0.2715,  0.3457,  0.3244, -0.0531],\n",
      "        [ 0.0739, -0.0161,  0.0873, -0.1938,  0.0804,  0.0464, -0.1923,  0.2232],\n",
      "        [-0.2372, -0.1164,  0.1841, -0.1554,  0.0972,  0.1430,  0.1592,  0.1974],\n",
      "        [-0.1572,  0.2313, -0.3087,  0.3028, -0.1791,  0.2931, -0.3133,  0.2541],\n",
      "        [ 0.1753,  0.0905,  0.2442,  0.2022,  0.2775, -0.0964, -0.1781,  0.1421],\n",
      "        [-0.1795, -0.2951,  0.0751,  0.0858, -0.3370,  0.1048,  0.1379,  0.1900],\n",
      "        [-0.0702,  0.2856,  0.1283,  0.1785, -0.2395,  0.3145,  0.0259,  0.1171],\n",
      "        [-0.2090,  0.1637,  0.1075, -0.1175, -0.3137,  0.0028,  0.1549, -0.1207]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[-0.2359, -0.3445, -0.3074,  0.0164],\n",
      "        [ 0.0739, -0.0161,  0.0873, -0.1938],\n",
      "        [ 0.1753,  0.0905,  0.2442,  0.2022],\n",
      "        [-0.1795, -0.2951,  0.0751,  0.0858]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0063,  0.0411,  0.0372, -0.0777],\n",
      "         [-0.0017,  0.0370,  0.0836, -0.1504],\n",
      "         [-0.0200, -0.0088,  0.0995, -0.1592]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[ 0.7103,  0.0595,  0.5611, -0.0762],\n",
      "         [ 0.7546,  0.7792,  0.4174, -0.0464],\n",
      "         [ 0.1173,  0.8736,  0.5072, -0.0925]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.7103,  0.0595,  0.5611, -0.0762],\n",
      "         [ 0.7546,  0.7792,  0.4174, -0.0464],\n",
      "         [ 0.1173,  0.8736,  0.5072, -0.0925]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 1.5605,  0.1307,  1.2327, -0.1675],\n",
      "         [ 1.2975,  1.3398,  0.7177, -0.0798],\n",
      "         [ 0.2297,  1.7110,  0.9934, -0.1811]]], grad_fn=<MulBackward0>)\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 1.5605,  0.1307,  1.2327, -0.1675],\n",
      "         [ 1.2975,  1.3398,  0.7177, -0.0798],\n",
      "         [ 0.2297,  1.7110,  0.9934, -0.1811]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 1.5605,  0.1307,  1.2327, -0.1675],\n",
      "         [ 1.2975,  1.3398,  0.7177, -0.0798],\n",
      "         [ 0.2297,  1.7110,  0.9934, -0.1811]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 4\n",
      "d_skip: 0\n",
      "i_dim: 16\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1760,  0.1617,  0.2807,  0.2303,  0.2907,  0.2507,  0.2461, -0.2108,\n",
      "         -0.1825, -0.1839,  0.0289, -0.3351,  0.0184, -0.3150,  0.2407, -0.2283,\n",
      "          0.3006,  0.1116, -0.1873, -0.0189,  0.2760,  0.1654, -0.3429, -0.2691,\n",
      "         -0.0934, -0.2141,  0.1404,  0.2679,  0.1097, -0.1541, -0.0320, -0.0230],\n",
      "        [-0.2891,  0.2322, -0.2442, -0.2503,  0.3135,  0.1324,  0.0568,  0.0847,\n",
      "         -0.2429,  0.3088, -0.0057,  0.0883, -0.3341,  0.1589,  0.3079, -0.1255,\n",
      "          0.2716,  0.1963, -0.2388,  0.3028, -0.2125, -0.1950,  0.0877, -0.3028,\n",
      "          0.2173, -0.2335, -0.0532, -0.0752,  0.2516,  0.2521,  0.0364, -0.1494],\n",
      "        [ 0.2527, -0.0988,  0.1256,  0.0559, -0.2523,  0.2330,  0.1019,  0.1221,\n",
      "         -0.2311, -0.3405,  0.0849, -0.2275, -0.2592, -0.3223,  0.1535, -0.2199,\n",
      "          0.2980,  0.2386,  0.2861, -0.1045,  0.3380,  0.1834,  0.3148, -0.2105,\n",
      "         -0.1854,  0.3025,  0.2121,  0.0624,  0.0137, -0.0299,  0.1318, -0.1496],\n",
      "        [-0.1045,  0.0137, -0.0626,  0.0391,  0.1527, -0.2717,  0.3227,  0.1922,\n",
      "         -0.0718,  0.0257,  0.2180, -0.0458,  0.1683,  0.1679,  0.3125,  0.0181,\n",
      "          0.2943,  0.2844,  0.0350,  0.0496,  0.2577, -0.2275,  0.0966, -0.2033,\n",
      "          0.2770,  0.1742, -0.2462,  0.2048,  0.1520, -0.1591,  0.0702, -0.0728],\n",
      "        [ 0.2875,  0.1488,  0.0089, -0.3011, -0.0944, -0.2060,  0.3534,  0.0776,\n",
      "          0.0262,  0.1434, -0.2318,  0.1620,  0.3342, -0.3227,  0.2201,  0.2323,\n",
      "         -0.0310, -0.2903,  0.1467, -0.3150,  0.2249, -0.0581, -0.0376,  0.3307,\n",
      "         -0.3353,  0.1747, -0.0357,  0.2419,  0.3352, -0.1686,  0.2382,  0.0151],\n",
      "        [-0.2751, -0.2114, -0.0754,  0.0091, -0.2831,  0.0468,  0.0837,  0.3076,\n",
      "         -0.2431,  0.3523,  0.0598,  0.1531, -0.2942,  0.1197,  0.0374,  0.2863,\n",
      "          0.1343, -0.1499,  0.1784, -0.2798, -0.3256,  0.2548, -0.2344,  0.2835,\n",
      "          0.1349,  0.0557,  0.0505,  0.2245, -0.3234,  0.1323,  0.1249,  0.1029],\n",
      "        [-0.2134,  0.0634,  0.0990, -0.3326,  0.2671, -0.0749,  0.1779, -0.0982,\n",
      "          0.1681, -0.2408, -0.1863,  0.1112,  0.0328, -0.3133,  0.2289,  0.2664,\n",
      "         -0.3323, -0.0777,  0.1480, -0.0807, -0.1593,  0.3455, -0.1876, -0.0862,\n",
      "         -0.2677,  0.0882,  0.2261, -0.3422, -0.0410,  0.0843,  0.1618, -0.1125],\n",
      "        [ 0.3267, -0.2617, -0.3281,  0.0869,  0.1342,  0.1968,  0.3423,  0.3504,\n",
      "         -0.1780, -0.0036, -0.1627,  0.0889,  0.0707,  0.0613, -0.2297, -0.2203,\n",
      "         -0.2489, -0.1318,  0.2520,  0.1960,  0.3480,  0.2359, -0.0771, -0.2166,\n",
      "          0.3351, -0.0614,  0.3102,  0.2966, -0.0357, -0.3250,  0.0561,  0.2476]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([4, 16])\n",
      "tensor([[ 0.1760,  0.1617,  0.2807,  0.2303,  0.2907,  0.2507,  0.2461, -0.2108,\n",
      "         -0.1825, -0.1839,  0.0289, -0.3351,  0.0184, -0.3150,  0.2407, -0.2283],\n",
      "        [-0.2891,  0.2322, -0.2442, -0.2503,  0.3135,  0.1324,  0.0568,  0.0847,\n",
      "         -0.2429,  0.3088, -0.0057,  0.0883, -0.3341,  0.1589,  0.3079, -0.1255],\n",
      "        [ 0.2527, -0.0988,  0.1256,  0.0559, -0.2523,  0.2330,  0.1019,  0.1221,\n",
      "         -0.2311, -0.3405,  0.0849, -0.2275, -0.2592, -0.3223,  0.1535, -0.2199],\n",
      "        [-0.1045,  0.0137, -0.0626,  0.0391,  0.1527, -0.2717,  0.3227,  0.1922,\n",
      "         -0.0718,  0.0257,  0.2180, -0.0458,  0.1683,  0.1679,  0.3125,  0.0181]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([ 0.1882,  0.3071, -0.3309, -0.1394, -0.2129,  0.2383, -0.0447, -0.2578,\n",
      "         0.1179, -0.1039,  0.0776,  0.0136, -0.2235,  0.0889,  0.2874,  0.2905,\n",
      "         0.0105, -0.1938, -0.2764, -0.3181,  0.0679, -0.1205,  0.3002, -0.2597,\n",
      "        -0.2345,  0.3042, -0.0831,  0.1504, -0.2135, -0.0298, -0.1462,  0.2660],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([16])\n",
      "tensor([ 0.1882,  0.3071, -0.3309, -0.1394, -0.2129,  0.2383, -0.0447, -0.2578,\n",
      "         0.1179, -0.1039,  0.0776,  0.0136, -0.2235,  0.0889,  0.2874,  0.2905],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.7541,  0.4657,  0.2406,  0.2497, -0.0550,  0.9796,  0.4184,\n",
      "          -0.4573, -0.4715, -0.7746,  0.1901, -0.7706, -0.5861, -0.8074,\n",
      "           0.8403, -0.3562],\n",
      "         [ 0.2189,  0.7560, -0.1986, -0.1389,  0.3910,  0.9298,  0.3982,\n",
      "          -0.3456, -0.6045, -0.1751,  0.1510, -0.4626, -0.8466, -0.3517,\n",
      "           1.0976, -0.3331],\n",
      "         [ 0.0039,  0.6408, -0.5480, -0.4663,  0.1119,  0.8031,  0.1519,\n",
      "          -0.0749, -0.5562,  0.0394,  0.1193, -0.1300, -1.0789, -0.0622,\n",
      "           0.9654, -0.1983]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.5841,  0.3163,  0.1432,  0.1494, -0.0263,  0.8193,  0.2771,\n",
      "          -0.1480, -0.1502, -0.1699,  0.1094, -0.1699, -0.1635, -0.1693,\n",
      "           0.6719, -0.1285],\n",
      "         [ 0.1284,  0.5860, -0.0837, -0.0618,  0.2549,  0.7660,  0.2607,\n",
      "          -0.1261, -0.1649, -0.0754,  0.0846, -0.1489, -0.1681, -0.1275,\n",
      "           0.9481, -0.1231],\n",
      "         [ 0.0020,  0.4737, -0.1599, -0.1494,  0.0609,  0.6337,  0.0851,\n",
      "          -0.0352, -0.1608,  0.0203,  0.0653, -0.0583, -0.1514, -0.0296,\n",
      "           0.8040, -0.0836]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[-0.0345, -0.1969, -0.1190,  0.1863, -0.2238,  0.2097, -0.2660,  0.2972,\n",
      "          0.0656,  0.1339, -0.2194,  0.2952, -0.2318,  0.3463,  0.1745,  0.3532,\n",
      "          0.3499,  0.3015, -0.1515, -0.3300, -0.2480,  0.1244,  0.2610, -0.0567,\n",
      "         -0.1432,  0.0933, -0.1775, -0.1142,  0.0685, -0.1558, -0.0243, -0.2271],\n",
      "        [-0.2902,  0.2369,  0.2011,  0.2889,  0.2330,  0.2568,  0.2464,  0.0580,\n",
      "          0.0351,  0.0331,  0.1303,  0.1047, -0.3347, -0.2566, -0.3468,  0.0391,\n",
      "          0.2075, -0.2814, -0.0450, -0.1566,  0.0538,  0.2321, -0.2026, -0.0330,\n",
      "          0.1764,  0.2381,  0.1418,  0.0668,  0.0517, -0.2228, -0.2373,  0.2438],\n",
      "        [ 0.2093, -0.0292, -0.2097, -0.1470,  0.1214, -0.0553, -0.1848, -0.1491,\n",
      "         -0.2325, -0.2515, -0.1211, -0.2482, -0.1080,  0.1149,  0.3401,  0.0577,\n",
      "          0.3318, -0.2809,  0.3446, -0.2718,  0.2066,  0.0388,  0.0363, -0.1684,\n",
      "         -0.2782, -0.0058, -0.2276,  0.0017,  0.2462, -0.2645, -0.3242, -0.2317],\n",
      "        [ 0.2072,  0.0229, -0.2189,  0.1211,  0.0018,  0.1960,  0.1039, -0.1614,\n",
      "         -0.2486, -0.1426,  0.0934,  0.2223,  0.1704,  0.1920, -0.2447, -0.1783,\n",
      "          0.1922,  0.2109,  0.2685,  0.0757,  0.1809, -0.1553, -0.0856, -0.1202,\n",
      "          0.2088,  0.3433,  0.1272,  0.0832, -0.0377, -0.0443,  0.1917, -0.1048],\n",
      "        [ 0.2383,  0.1409,  0.2620,  0.2927, -0.0319,  0.2527,  0.1483, -0.2413,\n",
      "         -0.1935, -0.0693, -0.3446,  0.2758, -0.0276, -0.0283,  0.0510, -0.0187,\n",
      "         -0.2073, -0.0565,  0.2295, -0.1010, -0.0683,  0.2095, -0.2510, -0.1570,\n",
      "         -0.1460, -0.2980, -0.2999, -0.1810,  0.3325, -0.0536,  0.1931, -0.2711],\n",
      "        [-0.0434, -0.0916,  0.1320,  0.1927, -0.0154,  0.1202,  0.3082, -0.0792,\n",
      "          0.0547, -0.3517,  0.1226, -0.0848,  0.2627,  0.0718,  0.0380,  0.1332,\n",
      "         -0.0371,  0.2526, -0.3197, -0.3464, -0.1235,  0.2974, -0.2889,  0.1585,\n",
      "          0.0261,  0.3018,  0.2250,  0.0010, -0.1260, -0.0969,  0.2368,  0.1239],\n",
      "        [-0.3124, -0.0888, -0.1800, -0.2414,  0.0562, -0.1828,  0.1735,  0.1378,\n",
      "         -0.2697, -0.2483,  0.3533, -0.0007,  0.1085,  0.3183,  0.2935,  0.3070,\n",
      "          0.2565,  0.1640,  0.1111, -0.3058, -0.2252,  0.3500,  0.0774,  0.2653,\n",
      "          0.0897,  0.0913, -0.2136, -0.2988, -0.2393, -0.2459, -0.2337, -0.1627],\n",
      "        [ 0.0855, -0.2387,  0.1997,  0.2633, -0.1513, -0.1762,  0.1027,  0.1329,\n",
      "          0.2105, -0.1041, -0.0207,  0.3455, -0.1155, -0.1458, -0.0798, -0.1300,\n",
      "          0.0774, -0.0783, -0.0123,  0.2760, -0.1969, -0.3498, -0.1978,  0.3129,\n",
      "          0.0143, -0.0914, -0.0456,  0.1058, -0.0915,  0.2500, -0.0875, -0.2095]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([4, 16])\n",
      "tensor([[-0.0345, -0.1969, -0.1190,  0.1863, -0.2238,  0.2097, -0.2660,  0.2972,\n",
      "          0.0656,  0.1339, -0.2194,  0.2952, -0.2318,  0.3463,  0.1745,  0.3532],\n",
      "        [-0.2902,  0.2369,  0.2011,  0.2889,  0.2330,  0.2568,  0.2464,  0.0580,\n",
      "          0.0351,  0.0331,  0.1303,  0.1047, -0.3347, -0.2566, -0.3468,  0.0391],\n",
      "        [ 0.2093, -0.0292, -0.2097, -0.1470,  0.1214, -0.0553, -0.1848, -0.1491,\n",
      "         -0.2325, -0.2515, -0.1211, -0.2482, -0.1080,  0.1149,  0.3401,  0.0577],\n",
      "        [ 0.2072,  0.0229, -0.2189,  0.1211,  0.0018,  0.1960,  0.1039, -0.1614,\n",
      "         -0.2486, -0.1426,  0.0934,  0.2223,  0.1704,  0.1920, -0.2447, -0.1783]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([-0.3434,  0.0199, -0.1054,  0.1085, -0.3156,  0.0699,  0.0418, -0.0772,\n",
      "         0.3115, -0.1414, -0.2681,  0.1874, -0.3144,  0.3272, -0.3007,  0.0272,\n",
      "        -0.1218,  0.3070,  0.3219,  0.2014,  0.1873,  0.3412,  0.1516,  0.1287,\n",
      "        -0.2760, -0.0995,  0.3407, -0.1615, -0.1807,  0.0896,  0.0587,  0.0463],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([16])\n",
      "tensor([-0.3434,  0.0199, -0.1054,  0.1085, -0.3156,  0.0699,  0.0418, -0.0772,\n",
      "         0.3115, -0.1414, -0.2681,  0.1874, -0.3144,  0.3272, -0.3007,  0.0272],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.2119, -0.2963, -0.4867,  0.2356, -0.4849,  0.3299, -0.5863,\n",
      "           0.2374,  0.1736, -0.2144, -0.7584,  0.3185, -0.8816,  0.9436,\n",
      "           0.3865,  0.6845],\n",
      "         [-0.6434,  0.0590, -0.1234,  0.6222, -0.2068,  0.6309, -0.1142,\n",
      "           0.2921,  0.2967, -0.0925, -0.4726,  0.5148, -1.1547,  0.4999,\n",
      "          -0.2753,  0.5935],\n",
      "         [-0.6776,  0.3469,  0.0427,  0.4777,  0.1519,  0.4671,  0.1999,\n",
      "          -0.0285,  0.2007, -0.2780, -0.2328,  0.1474, -1.0784,  0.0470,\n",
      "          -0.4718,  0.2649]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.1238, -0.0937, -0.0697,  0.0352,  0.0127,  0.2703, -0.1625,\n",
      "          -0.0351, -0.0261,  0.0364, -0.0829, -0.0541,  0.1441, -0.1598,\n",
      "           0.2597, -0.0880],\n",
      "         [-0.0826,  0.0346,  0.0103, -0.0384, -0.0527,  0.4832, -0.0298,\n",
      "          -0.0368, -0.0489,  0.0070, -0.0400, -0.0766,  0.1942, -0.0637,\n",
      "          -0.2610, -0.0731],\n",
      "         [-0.0013,  0.1643, -0.0068, -0.0714,  0.0093,  0.2960,  0.0170,\n",
      "           0.0010, -0.0323, -0.0056, -0.0152, -0.0086,  0.1633, -0.0014,\n",
      "          -0.3793, -0.0221]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([32, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.0850,  0.1153,  0.1581,  0.1439,  0.0503, -0.1568,  0.1261,  0.1764],\n",
      "        [ 0.0569, -0.0104,  0.1082, -0.1281, -0.1668,  0.1488,  0.1475, -0.0986],\n",
      "        [-0.1387, -0.1750, -0.1347, -0.0340, -0.1098,  0.1695, -0.0129, -0.1740],\n",
      "        [ 0.0198,  0.0893, -0.0751,  0.1264,  0.1262,  0.0622,  0.1746, -0.0427],\n",
      "        [ 0.1481, -0.1442, -0.0994,  0.1560, -0.0679, -0.0892,  0.0014, -0.0896],\n",
      "        [ 0.1427, -0.0932,  0.0327,  0.0317,  0.1637, -0.1696, -0.1525, -0.0409],\n",
      "        [ 0.0567, -0.0500, -0.1247, -0.0583,  0.0905, -0.0617,  0.1151, -0.0341],\n",
      "        [-0.0079, -0.1559, -0.1584, -0.0852,  0.1012,  0.0860, -0.1019, -0.0717],\n",
      "        [-0.0102,  0.0128, -0.0318, -0.0946, -0.1649, -0.0288,  0.0004,  0.1157],\n",
      "        [ 0.0497,  0.0184, -0.1080,  0.0066,  0.0440,  0.1178, -0.1676,  0.1211],\n",
      "        [ 0.0306,  0.1080,  0.1427,  0.0672, -0.0033, -0.0247,  0.0484, -0.1652],\n",
      "        [-0.0394,  0.1319,  0.0149, -0.0446,  0.1518,  0.0712,  0.0635, -0.0918],\n",
      "        [ 0.1189,  0.0833, -0.1566,  0.0450, -0.1586,  0.0322, -0.1621,  0.1114],\n",
      "        [ 0.1214,  0.0473, -0.1191, -0.0658, -0.1713, -0.0582,  0.1037, -0.1043],\n",
      "        [ 0.1116, -0.0727, -0.1730, -0.0014,  0.0986, -0.0410,  0.1223, -0.1329],\n",
      "        [ 0.0650,  0.1622, -0.1729,  0.0334,  0.0247, -0.1465, -0.0048,  0.1438],\n",
      "        [-0.0399,  0.1089,  0.0979, -0.0003, -0.0692, -0.1613,  0.0215,  0.1067],\n",
      "        [-0.0260, -0.0632,  0.1637, -0.0296, -0.0696,  0.0458, -0.1039,  0.0498],\n",
      "        [ 0.0445,  0.1640, -0.1516,  0.0887, -0.1539,  0.0745, -0.0450,  0.1303],\n",
      "        [ 0.1084,  0.1720,  0.0198,  0.1429, -0.0110, -0.0350, -0.1264, -0.1567],\n",
      "        [ 0.1279, -0.0088,  0.0901,  0.1618, -0.0099, -0.0392,  0.1676,  0.0993],\n",
      "        [-0.0247,  0.0870, -0.0077,  0.1028, -0.1554, -0.0746,  0.0758, -0.0241],\n",
      "        [-0.0216,  0.1248, -0.0355, -0.0416, -0.0078, -0.0313,  0.0194,  0.1227],\n",
      "        [ 0.0831, -0.0002, -0.0564,  0.0960, -0.0173, -0.0525,  0.0770,  0.0778],\n",
      "        [-0.0745, -0.0166,  0.1395, -0.0058,  0.1547,  0.0345, -0.1068,  0.1317],\n",
      "        [ 0.0487, -0.1538,  0.0905, -0.1369, -0.0311,  0.0144,  0.1008, -0.1313],\n",
      "        [-0.0726,  0.0640, -0.0446, -0.0528,  0.1134, -0.0013, -0.0278,  0.0920],\n",
      "        [ 0.0832, -0.0555,  0.0341, -0.0281,  0.0301,  0.0623, -0.1592,  0.1382],\n",
      "        [ 0.1725,  0.0517, -0.1166,  0.1355, -0.1035, -0.0208,  0.1060, -0.0896],\n",
      "        [ 0.0697, -0.0854,  0.0301, -0.0606,  0.0912,  0.1578,  0.0880, -0.0073],\n",
      "        [-0.1715,  0.0655, -0.0829, -0.1684,  0.0671,  0.1171, -0.1254, -0.1159],\n",
      "        [ 0.0771, -0.1681, -0.0308,  0.1270,  0.0781, -0.0324,  0.0923, -0.0970]],\n",
      "       requires_grad=True)\n",
      "Wdown spliced: torch.Size([16, 4])\n",
      "tensor([[-0.0850,  0.1153,  0.1581,  0.1439],\n",
      "        [ 0.0569, -0.0104,  0.1082, -0.1281],\n",
      "        [-0.1387, -0.1750, -0.1347, -0.0340],\n",
      "        [ 0.0198,  0.0893, -0.0751,  0.1264],\n",
      "        [ 0.1481, -0.1442, -0.0994,  0.1560],\n",
      "        [ 0.1427, -0.0932,  0.0327,  0.0317],\n",
      "        [ 0.0567, -0.0500, -0.1247, -0.0583],\n",
      "        [-0.0079, -0.1559, -0.1584, -0.0852],\n",
      "        [-0.0102,  0.0128, -0.0318, -0.0946],\n",
      "        [ 0.0497,  0.0184, -0.1080,  0.0066],\n",
      "        [ 0.0306,  0.1080,  0.1427,  0.0672],\n",
      "        [-0.0394,  0.1319,  0.0149, -0.0446],\n",
      "        [ 0.1189,  0.0833, -0.1566,  0.0450],\n",
      "        [ 0.1214,  0.0473, -0.1191, -0.0658],\n",
      "        [ 0.1116, -0.0727, -0.1730, -0.0014],\n",
      "        [ 0.0650,  0.1622, -0.1729,  0.0334]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.0952,  0.0296,  0.1446, -0.0618, -0.1172,  0.1323,  0.0880,  0.1261],\n",
      "       requires_grad=True)\n",
      "Bdown spliced: torch.Size([4])\n",
      "tensor([-0.0952,  0.0296,  0.1446, -0.0618], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0254, -0.0263,  0.1061, -0.0245],\n",
      "         [-0.0445, -0.0104,  0.1963, -0.0551],\n",
      "         [-0.0662,  0.0268,  0.2184, -0.0725]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[ 0.6848,  0.0332,  0.6671, -0.1008],\n",
      "         [ 0.7101,  0.7688,  0.6137, -0.1015],\n",
      "         [ 0.0511,  0.9004,  0.7255, -0.1650]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "y: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.6848,  0.0332,  0.6671, -0.1008],\n",
      "         [ 0.7101,  0.7688,  0.6137, -0.1015],\n",
      "         [ 0.0511,  0.9004,  0.7255, -0.1650]]], grad_fn=<AddBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.2929, 0.6882, 0.8413, 0.4270],\n",
      "         [0.1778, 0.4286, 0.0205, 0.7897],\n",
      "         [0.6864, 0.3731, 0.0200, 0.3366]]])\n",
      "---------- Layer Input: torch.Tensor ------------\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.2929, 0.6882, 0.8413, 0.4270],\n",
      "         [0.1778, 0.4286, 0.0205, 0.7897],\n",
      "         [0.6864, 0.3731, 0.0200, 0.3366]]])\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.4866, 1.1433, 1.3975, 0.7092],\n",
      "         [0.3882, 0.9357, 0.0447, 1.7239],\n",
      "         [1.6134, 0.8770, 0.0469, 0.7911]]])\n",
      "dim: 4\n",
      "skip: 4\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.4866, 1.1433, 1.3975, 0.7092],\n",
      "         [0.3882, 0.9357, 0.0447, 1.7239],\n",
      "         [1.6134, 0.8770, 0.0469, 0.7911]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 4\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 2\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1502,  0.1798, -0.0850, -0.1503, -0.1788, -0.2761, -0.2895,  0.2239,\n",
      "         -0.1242, -0.3357, -0.1516,  0.1445, -0.3465, -0.0087,  0.1680, -0.3096],\n",
      "        [-0.2497,  0.2637,  0.0318,  0.2843,  0.3023, -0.0147, -0.1036,  0.0282,\n",
      "          0.1418,  0.0838, -0.0708, -0.0738, -0.3388,  0.3156, -0.0171, -0.3506],\n",
      "        [-0.1851,  0.2389,  0.3192, -0.2411, -0.3012,  0.0172,  0.2669,  0.2849,\n",
      "          0.2737,  0.2778, -0.1082,  0.0954,  0.2263,  0.0940, -0.1288,  0.3529],\n",
      "        [-0.3062, -0.0375,  0.0070, -0.2296, -0.0683,  0.2136,  0.1168,  0.1170,\n",
      "         -0.2637, -0.1249, -0.1836, -0.1935,  0.0628,  0.2906, -0.3528,  0.2893],\n",
      "        [ 0.2859,  0.2414, -0.3532, -0.3180,  0.0624,  0.3174, -0.3462, -0.1620,\n",
      "         -0.0744, -0.1810,  0.0381,  0.3187, -0.1418, -0.2615, -0.0950,  0.2029],\n",
      "        [-0.1285,  0.0549, -0.3518,  0.0362,  0.3494,  0.1207,  0.1131, -0.1509,\n",
      "         -0.0154,  0.2893,  0.1281,  0.0523, -0.2585,  0.3296,  0.3162,  0.3222],\n",
      "        [ 0.1372, -0.3255, -0.0823,  0.3374, -0.1852, -0.0021,  0.0576,  0.1676,\n",
      "         -0.2661,  0.0652, -0.0189,  0.2911,  0.2301,  0.3267, -0.0217,  0.2536],\n",
      "        [-0.0199,  0.2874,  0.2247, -0.2074,  0.1151, -0.1673,  0.1663, -0.1422,\n",
      "         -0.1487,  0.0820,  0.2069,  0.0380, -0.0596, -0.2654,  0.2306,  0.1583]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[ 0.1502,  0.1798, -0.0850, -0.1503, -0.1788, -0.2761, -0.2895,  0.2239],\n",
      "        [-0.2497,  0.2637,  0.0318,  0.2843,  0.3023, -0.0147, -0.1036,  0.0282],\n",
      "        [-0.1851,  0.2389,  0.3192, -0.2411, -0.3012,  0.0172,  0.2669,  0.2849],\n",
      "        [-0.3062, -0.0375,  0.0070, -0.2296, -0.0683,  0.2136,  0.1168,  0.1170],\n",
      "        [ 0.2859,  0.2414, -0.3532, -0.3180,  0.0624,  0.3174, -0.3462, -0.1620],\n",
      "        [-0.1285,  0.0549, -0.3518,  0.0362,  0.3494,  0.1207,  0.1131, -0.1509],\n",
      "        [ 0.1372, -0.3255, -0.0823,  0.3374, -0.1852, -0.0021,  0.0576,  0.1676],\n",
      "        [-0.0199,  0.2874,  0.2247, -0.2074,  0.1151, -0.1673,  0.1663, -0.1422]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[-0.1242, -0.3357, -0.1516,  0.1445],\n",
      "        [ 0.1418,  0.0838, -0.0708, -0.0738],\n",
      "        [ 0.2737,  0.2778, -0.1082,  0.0954],\n",
      "        [-0.2637, -0.1249, -0.1836, -0.1935],\n",
      "        [-0.0744, -0.1810,  0.0381,  0.3187],\n",
      "        [-0.0154,  0.2893,  0.1281,  0.0523],\n",
      "        [-0.2661,  0.0652, -0.0189,  0.2911],\n",
      "        [-0.1487,  0.0820,  0.2069,  0.0380]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[-0.3465, -0.0087,  0.1680, -0.3096],\n",
      "        [-0.3388,  0.3156, -0.0171, -0.3506],\n",
      "        [ 0.2263,  0.0940, -0.1288,  0.3529],\n",
      "        [ 0.0628,  0.2906, -0.3528,  0.2893],\n",
      "        [-0.1418, -0.2615, -0.0950,  0.2029],\n",
      "        [-0.2585,  0.3296,  0.3162,  0.3222],\n",
      "        [ 0.2301,  0.3267, -0.0217,  0.2536],\n",
      "        [-0.0596, -0.2654,  0.2306,  0.1583]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[-0.3532, -0.3180, -0.3462, -0.1620],\n",
      "        [-0.3518,  0.0362,  0.1131, -0.1509],\n",
      "        [-0.0823,  0.3374,  0.0576,  0.1676],\n",
      "        [ 0.2247, -0.2074,  0.1663, -0.1422]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.0381,  0.3187],\n",
      "        [ 0.1281,  0.0523],\n",
      "        [-0.0189,  0.2911],\n",
      "        [ 0.2069,  0.0380]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[-0.0950,  0.2029],\n",
      "        [ 0.3162,  0.3222],\n",
      "        [-0.0217,  0.2536],\n",
      "        [ 0.2306,  0.1583]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[-0.3532, -0.3180, -0.3462, -0.1620,  0.0381,  0.3187, -0.0950,  0.2029],\n",
      "        [-0.3518,  0.0362,  0.1131, -0.1509,  0.1281,  0.0523,  0.3162,  0.3222],\n",
      "        [-0.0823,  0.3374,  0.0576,  0.1676, -0.0189,  0.2911, -0.0217,  0.2536],\n",
      "        [ 0.2247, -0.2074,  0.1663, -0.1422,  0.2069,  0.0380,  0.2306,  0.1583]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.5298,  0.2111,  0.1592, -0.1180,  0.2853,  0.6486,  0.4485,\n",
      "           0.9337],\n",
      "         [-0.0825, -0.4321,  0.2607, -0.4418,  0.4904,  0.2511,  0.6555,\n",
      "           0.6645],\n",
      "         [-0.7044, -0.6296, -0.3252, -0.4984,  0.3365,  0.6037,  0.3054,\n",
      "           0.7471]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.5298,  0.2111,  0.1592, -0.1180],\n",
      "         [-0.0825, -0.4321,  0.2607, -0.4418],\n",
      "         [-0.7044, -0.6296, -0.3252, -0.4984]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[0.2853, 0.6486],\n",
      "         [0.4904, 0.2511],\n",
      "         [0.3365, 0.6037]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[0.4485, 0.9337],\n",
      "         [0.6555, 0.6645],\n",
      "         [0.3054, 0.7471]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.5298,  0.2111],\n",
      "          [ 0.1592, -0.1180]],\n",
      "\n",
      "         [[-0.0825, -0.4321],\n",
      "          [ 0.2607, -0.4418]],\n",
      "\n",
      "         [[-0.7044, -0.6296],\n",
      "          [-0.3252, -0.4984]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[0.2853, 0.6486]],\n",
      "\n",
      "         [[0.4904, 0.2511]],\n",
      "\n",
      "         [[0.3365, 0.6037]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[0.4485, 0.9337]],\n",
      "\n",
      "         [[0.6555, 0.6645]],\n",
      "\n",
      "         [[0.3054, 0.7471]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.5298,  0.2111],\n",
      "          [ 0.1592, -0.1180]],\n",
      "\n",
      "         [[ 0.3190, -0.3029],\n",
      "          [ 0.5126, -0.0193]],\n",
      "\n",
      "         [[ 0.8656, -0.3785],\n",
      "          [ 0.5885, -0.0883]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.2853,  0.6486]],\n",
      "\n",
      "         [[ 0.0537,  0.5483]],\n",
      "\n",
      "         [[-0.6890,  0.0548]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.2853,  0.6486],\n",
      "          [ 0.2853,  0.6486]],\n",
      "\n",
      "         [[ 0.0537,  0.5483],\n",
      "          [ 0.0537,  0.5483]],\n",
      "\n",
      "         [[-0.6890,  0.0548],\n",
      "          [-0.6890,  0.0548]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[0.4485, 0.9337],\n",
      "          [0.4485, 0.9337]],\n",
      "\n",
      "         [[0.6555, 0.6645],\n",
      "          [0.6555, 0.6645]],\n",
      "\n",
      "         [[0.3054, 0.7471],\n",
      "          [0.3054, 0.7471]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.5298,  0.2111],\n",
      "          [ 0.3190, -0.3029],\n",
      "          [ 0.8656, -0.3785]],\n",
      "\n",
      "         [[ 0.1592, -0.1180],\n",
      "          [ 0.5126, -0.0193],\n",
      "          [ 0.5885, -0.0883]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.2853,  0.6486],\n",
      "          [ 0.0537,  0.5483],\n",
      "          [-0.6890,  0.0548]],\n",
      "\n",
      "         [[ 0.2853,  0.6486],\n",
      "          [ 0.0537,  0.5483],\n",
      "          [-0.6890,  0.0548]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[0.4485, 0.9337],\n",
      "          [0.6555, 0.6645],\n",
      "          [0.3054, 0.7471]],\n",
      "\n",
      "         [[0.4485, 0.9337],\n",
      "          [0.6555, 0.6645],\n",
      "          [0.3054, 0.7471]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-0.0100,  0.0617,  0.2663],\n",
      "          [-0.0746, -0.1053, -0.1671],\n",
      "          [ 0.0010, -0.1139, -0.4364]],\n",
      "\n",
      "         [[-0.0220, -0.0397, -0.0821],\n",
      "          [ 0.0945,  0.0120, -0.2505],\n",
      "          [ 0.0782, -0.0119, -0.2901]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-1.0049e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [-7.4579e-02, -1.0534e-01, -2.3820e+38],\n",
      "          [ 9.9604e-04, -1.1392e-01, -4.3636e-01]],\n",
      "\n",
      "         [[-2.2016e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 9.4527e-02,  1.1956e-02, -2.3820e+38],\n",
      "          [ 7.8222e-02, -1.1893e-02, -2.9012e-01]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5077, 0.4923, 0.0000],\n",
      "          [0.3941, 0.3514, 0.2545]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5206, 0.4794, 0.0000],\n",
      "          [0.3838, 0.3507, 0.2655]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[0.4485, 0.9337],\n",
      "          [0.5504, 0.8012],\n",
      "          [0.4848, 0.7916]],\n",
      "\n",
      "         [[0.4485, 0.9337],\n",
      "          [0.5477, 0.8047],\n",
      "          [0.4831, 0.7897]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[0.4485, 0.9337, 0.4485, 0.9337],\n",
      "         [0.5504, 0.8012, 0.5477, 0.8047],\n",
      "         [0.4848, 0.7916, 0.4831, 0.7897]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.2359, -0.3445, -0.3074,  0.0164, -0.2715,  0.3457,  0.3244, -0.0531],\n",
      "        [ 0.0739, -0.0161,  0.0873, -0.1938,  0.0804,  0.0464, -0.1923,  0.2232],\n",
      "        [-0.2372, -0.1164,  0.1841, -0.1554,  0.0972,  0.1430,  0.1592,  0.1974],\n",
      "        [-0.1572,  0.2313, -0.3087,  0.3028, -0.1791,  0.2931, -0.3133,  0.2541],\n",
      "        [ 0.1753,  0.0905,  0.2442,  0.2022,  0.2775, -0.0964, -0.1781,  0.1421],\n",
      "        [-0.1795, -0.2951,  0.0751,  0.0858, -0.3370,  0.1048,  0.1379,  0.1900],\n",
      "        [-0.0702,  0.2856,  0.1283,  0.1785, -0.2395,  0.3145,  0.0259,  0.1171],\n",
      "        [-0.2090,  0.1637,  0.1075, -0.1175, -0.3137,  0.0028,  0.1549, -0.1207]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[ 0.0972,  0.1430,  0.1592,  0.1974],\n",
      "        [-0.1791,  0.2931, -0.3133,  0.2541],\n",
      "        [-0.2395,  0.3145,  0.0259,  0.1171],\n",
      "        [-0.3137,  0.0028,  0.1549, -0.1207]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.5239,  0.4815, -0.0648,  0.2656],\n",
      "         [-0.4736,  0.4881, -0.0245,  0.2792],\n",
      "         [-0.4581,  0.4555, -0.0360,  0.2581]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[-0.2310,  1.1697,  0.7765,  0.6926],\n",
      "         [-0.2957,  0.9167, -0.0040,  1.0690],\n",
      "         [ 0.2284,  0.8286, -0.0160,  0.5947]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.2310,  1.1697,  0.7765,  0.6926],\n",
      "         [-0.2957,  0.9167, -0.0040,  1.0690],\n",
      "         [ 0.2284,  0.8286, -0.0160,  0.5947]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.2919,  1.4784,  0.9814,  0.8753],\n",
      "         [-0.4111,  1.2742, -0.0056,  1.4858],\n",
      "         [ 0.4369,  1.5854, -0.0306,  1.1378]]], grad_fn=<MulBackward0>)\n",
      "dim: 4\n",
      "skip: 4\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.2919,  1.4784,  0.9814,  0.8753],\n",
      "         [-0.4111,  1.2742, -0.0056,  1.4858],\n",
      "         [ 0.4369,  1.5854, -0.0306,  1.1378]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.2919,  1.4784,  0.9814,  0.8753],\n",
      "         [-0.4111,  1.2742, -0.0056,  1.4858],\n",
      "         [ 0.4369,  1.5854, -0.0306,  1.1378]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 4\n",
      "d_skip: 4\n",
      "i_dim: 16\n",
      "i_skip: 16\n",
      "Wgate: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1760,  0.1617,  0.2807,  0.2303,  0.2907,  0.2507,  0.2461, -0.2108,\n",
      "         -0.1825, -0.1839,  0.0289, -0.3351,  0.0184, -0.3150,  0.2407, -0.2283,\n",
      "          0.3006,  0.1116, -0.1873, -0.0189,  0.2760,  0.1654, -0.3429, -0.2691,\n",
      "         -0.0934, -0.2141,  0.1404,  0.2679,  0.1097, -0.1541, -0.0320, -0.0230],\n",
      "        [-0.2891,  0.2322, -0.2442, -0.2503,  0.3135,  0.1324,  0.0568,  0.0847,\n",
      "         -0.2429,  0.3088, -0.0057,  0.0883, -0.3341,  0.1589,  0.3079, -0.1255,\n",
      "          0.2716,  0.1963, -0.2388,  0.3028, -0.2125, -0.1950,  0.0877, -0.3028,\n",
      "          0.2173, -0.2335, -0.0532, -0.0752,  0.2516,  0.2521,  0.0364, -0.1494],\n",
      "        [ 0.2527, -0.0988,  0.1256,  0.0559, -0.2523,  0.2330,  0.1019,  0.1221,\n",
      "         -0.2311, -0.3405,  0.0849, -0.2275, -0.2592, -0.3223,  0.1535, -0.2199,\n",
      "          0.2980,  0.2386,  0.2861, -0.1045,  0.3380,  0.1834,  0.3148, -0.2105,\n",
      "         -0.1854,  0.3025,  0.2121,  0.0624,  0.0137, -0.0299,  0.1318, -0.1496],\n",
      "        [-0.1045,  0.0137, -0.0626,  0.0391,  0.1527, -0.2717,  0.3227,  0.1922,\n",
      "         -0.0718,  0.0257,  0.2180, -0.0458,  0.1683,  0.1679,  0.3125,  0.0181,\n",
      "          0.2943,  0.2844,  0.0350,  0.0496,  0.2577, -0.2275,  0.0966, -0.2033,\n",
      "          0.2770,  0.1742, -0.2462,  0.2048,  0.1520, -0.1591,  0.0702, -0.0728],\n",
      "        [ 0.2875,  0.1488,  0.0089, -0.3011, -0.0944, -0.2060,  0.3534,  0.0776,\n",
      "          0.0262,  0.1434, -0.2318,  0.1620,  0.3342, -0.3227,  0.2201,  0.2323,\n",
      "         -0.0310, -0.2903,  0.1467, -0.3150,  0.2249, -0.0581, -0.0376,  0.3307,\n",
      "         -0.3353,  0.1747, -0.0357,  0.2419,  0.3352, -0.1686,  0.2382,  0.0151],\n",
      "        [-0.2751, -0.2114, -0.0754,  0.0091, -0.2831,  0.0468,  0.0837,  0.3076,\n",
      "         -0.2431,  0.3523,  0.0598,  0.1531, -0.2942,  0.1197,  0.0374,  0.2863,\n",
      "          0.1343, -0.1499,  0.1784, -0.2798, -0.3256,  0.2548, -0.2344,  0.2835,\n",
      "          0.1349,  0.0557,  0.0505,  0.2245, -0.3234,  0.1323,  0.1249,  0.1029],\n",
      "        [-0.2134,  0.0634,  0.0990, -0.3326,  0.2671, -0.0749,  0.1779, -0.0982,\n",
      "          0.1681, -0.2408, -0.1863,  0.1112,  0.0328, -0.3133,  0.2289,  0.2664,\n",
      "         -0.3323, -0.0777,  0.1480, -0.0807, -0.1593,  0.3455, -0.1876, -0.0862,\n",
      "         -0.2677,  0.0882,  0.2261, -0.3422, -0.0410,  0.0843,  0.1618, -0.1125],\n",
      "        [ 0.3267, -0.2617, -0.3281,  0.0869,  0.1342,  0.1968,  0.3423,  0.3504,\n",
      "         -0.1780, -0.0036, -0.1627,  0.0889,  0.0707,  0.0613, -0.2297, -0.2203,\n",
      "         -0.2489, -0.1318,  0.2520,  0.1960,  0.3480,  0.2359, -0.0771, -0.2166,\n",
      "          0.3351, -0.0614,  0.3102,  0.2966, -0.0357, -0.3250,  0.0561,  0.2476]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([4, 16])\n",
      "tensor([[-0.0310, -0.2903,  0.1467, -0.3150,  0.2249, -0.0581, -0.0376,  0.3307,\n",
      "         -0.3353,  0.1747, -0.0357,  0.2419,  0.3352, -0.1686,  0.2382,  0.0151],\n",
      "        [ 0.1343, -0.1499,  0.1784, -0.2798, -0.3256,  0.2548, -0.2344,  0.2835,\n",
      "          0.1349,  0.0557,  0.0505,  0.2245, -0.3234,  0.1323,  0.1249,  0.1029],\n",
      "        [-0.3323, -0.0777,  0.1480, -0.0807, -0.1593,  0.3455, -0.1876, -0.0862,\n",
      "         -0.2677,  0.0882,  0.2261, -0.3422, -0.0410,  0.0843,  0.1618, -0.1125],\n",
      "        [-0.2489, -0.1318,  0.2520,  0.1960,  0.3480,  0.2359, -0.0771, -0.2166,\n",
      "          0.3351, -0.0614,  0.3102,  0.2966, -0.0357, -0.3250,  0.0561,  0.2476]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([ 0.1882,  0.3071, -0.3309, -0.1394, -0.2129,  0.2383, -0.0447, -0.2578,\n",
      "         0.1179, -0.1039,  0.0776,  0.0136, -0.2235,  0.0889,  0.2874,  0.2905,\n",
      "         0.0105, -0.1938, -0.2764, -0.3181,  0.0679, -0.1205,  0.3002, -0.2597,\n",
      "        -0.2345,  0.3042, -0.0831,  0.1504, -0.2135, -0.0298, -0.1462,  0.2660],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([16])\n",
      "tensor([ 0.0105, -0.1938, -0.2764, -0.3181,  0.0679, -0.1205,  0.3002, -0.2597,\n",
      "        -0.2345,  0.3042, -0.0831,  0.1504, -0.2135, -0.0298, -0.1462,  0.2660],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.3258, -0.5223,  0.3104, -0.5474, -0.3308,  0.8187, -0.2870,\n",
      "          -0.2113,  0.0934,  0.3683,  0.4954,  0.3355, -0.8609,  0.0132,\n",
      "           0.1768,  0.5201],\n",
      "         [-0.1736, -0.4609,  0.2642, -0.2534,  0.0786,  0.5767, -0.0965,\n",
      "          -0.3557,  0.5746,  0.2116,  0.4555,  0.7796, -0.8161, -0.2753,\n",
      "          -0.0025,  0.7594],\n",
      "         [-0.0632, -0.7059,  0.3527, -0.6738,  0.0508,  0.5160, -0.1699,\n",
      "           0.0905,  0.2223,  0.3962,  0.3273,  0.9600, -0.6191, -0.2661,\n",
      "           0.2148,  0.7210]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.1213, -0.1571,  0.1930, -0.1599, -0.1225,  0.6497, -0.1111,\n",
      "          -0.0880,  0.0502,  0.2370,  0.3417,  0.2119, -0.1676,  0.0067,\n",
      "           0.1008,  0.3633],\n",
      "         [-0.0748, -0.1486,  0.1596, -0.1014,  0.0417,  0.4140, -0.0445,\n",
      "          -0.1284,  0.4121,  0.1235,  0.3077,  0.6098, -0.1691, -0.1078,\n",
      "          -0.0013,  0.5895],\n",
      "         [-0.0300, -0.1695,  0.2250, -0.1686,  0.0264,  0.3597, -0.0735,\n",
      "           0.0485,  0.1307,  0.2591,  0.2056,  0.7982, -0.1659, -0.1051,\n",
      "           0.1257,  0.5512]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[-0.0345, -0.1969, -0.1190,  0.1863, -0.2238,  0.2097, -0.2660,  0.2972,\n",
      "          0.0656,  0.1339, -0.2194,  0.2952, -0.2318,  0.3463,  0.1745,  0.3532,\n",
      "          0.3499,  0.3015, -0.1515, -0.3300, -0.2480,  0.1244,  0.2610, -0.0567,\n",
      "         -0.1432,  0.0933, -0.1775, -0.1142,  0.0685, -0.1558, -0.0243, -0.2271],\n",
      "        [-0.2902,  0.2369,  0.2011,  0.2889,  0.2330,  0.2568,  0.2464,  0.0580,\n",
      "          0.0351,  0.0331,  0.1303,  0.1047, -0.3347, -0.2566, -0.3468,  0.0391,\n",
      "          0.2075, -0.2814, -0.0450, -0.1566,  0.0538,  0.2321, -0.2026, -0.0330,\n",
      "          0.1764,  0.2381,  0.1418,  0.0668,  0.0517, -0.2228, -0.2373,  0.2438],\n",
      "        [ 0.2093, -0.0292, -0.2097, -0.1470,  0.1214, -0.0553, -0.1848, -0.1491,\n",
      "         -0.2325, -0.2515, -0.1211, -0.2482, -0.1080,  0.1149,  0.3401,  0.0577,\n",
      "          0.3318, -0.2809,  0.3446, -0.2718,  0.2066,  0.0388,  0.0363, -0.1684,\n",
      "         -0.2782, -0.0058, -0.2276,  0.0017,  0.2462, -0.2645, -0.3242, -0.2317],\n",
      "        [ 0.2072,  0.0229, -0.2189,  0.1211,  0.0018,  0.1960,  0.1039, -0.1614,\n",
      "         -0.2486, -0.1426,  0.0934,  0.2223,  0.1704,  0.1920, -0.2447, -0.1783,\n",
      "          0.1922,  0.2109,  0.2685,  0.0757,  0.1809, -0.1553, -0.0856, -0.1202,\n",
      "          0.2088,  0.3433,  0.1272,  0.0832, -0.0377, -0.0443,  0.1917, -0.1048],\n",
      "        [ 0.2383,  0.1409,  0.2620,  0.2927, -0.0319,  0.2527,  0.1483, -0.2413,\n",
      "         -0.1935, -0.0693, -0.3446,  0.2758, -0.0276, -0.0283,  0.0510, -0.0187,\n",
      "         -0.2073, -0.0565,  0.2295, -0.1010, -0.0683,  0.2095, -0.2510, -0.1570,\n",
      "         -0.1460, -0.2980, -0.2999, -0.1810,  0.3325, -0.0536,  0.1931, -0.2711],\n",
      "        [-0.0434, -0.0916,  0.1320,  0.1927, -0.0154,  0.1202,  0.3082, -0.0792,\n",
      "          0.0547, -0.3517,  0.1226, -0.0848,  0.2627,  0.0718,  0.0380,  0.1332,\n",
      "         -0.0371,  0.2526, -0.3197, -0.3464, -0.1235,  0.2974, -0.2889,  0.1585,\n",
      "          0.0261,  0.3018,  0.2250,  0.0010, -0.1260, -0.0969,  0.2368,  0.1239],\n",
      "        [-0.3124, -0.0888, -0.1800, -0.2414,  0.0562, -0.1828,  0.1735,  0.1378,\n",
      "         -0.2697, -0.2483,  0.3533, -0.0007,  0.1085,  0.3183,  0.2935,  0.3070,\n",
      "          0.2565,  0.1640,  0.1111, -0.3058, -0.2252,  0.3500,  0.0774,  0.2653,\n",
      "          0.0897,  0.0913, -0.2136, -0.2988, -0.2393, -0.2459, -0.2337, -0.1627],\n",
      "        [ 0.0855, -0.2387,  0.1997,  0.2633, -0.1513, -0.1762,  0.1027,  0.1329,\n",
      "          0.2105, -0.1041, -0.0207,  0.3455, -0.1155, -0.1458, -0.0798, -0.1300,\n",
      "          0.0774, -0.0783, -0.0123,  0.2760, -0.1969, -0.3498, -0.1978,  0.3129,\n",
      "          0.0143, -0.0914, -0.0456,  0.1058, -0.0915,  0.2500, -0.0875, -0.2095]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([4, 16])\n",
      "tensor([[-0.2073, -0.0565,  0.2295, -0.1010, -0.0683,  0.2095, -0.2510, -0.1570,\n",
      "         -0.1460, -0.2980, -0.2999, -0.1810,  0.3325, -0.0536,  0.1931, -0.2711],\n",
      "        [-0.0371,  0.2526, -0.3197, -0.3464, -0.1235,  0.2974, -0.2889,  0.1585,\n",
      "          0.0261,  0.3018,  0.2250,  0.0010, -0.1260, -0.0969,  0.2368,  0.1239],\n",
      "        [ 0.2565,  0.1640,  0.1111, -0.3058, -0.2252,  0.3500,  0.0774,  0.2653,\n",
      "          0.0897,  0.0913, -0.2136, -0.2988, -0.2393, -0.2459, -0.2337, -0.1627],\n",
      "        [ 0.0774, -0.0783, -0.0123,  0.2760, -0.1969, -0.3498, -0.1978,  0.3129,\n",
      "          0.0143, -0.0914, -0.0456,  0.1058, -0.0915,  0.2500, -0.0875, -0.2095]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([-0.3434,  0.0199, -0.1054,  0.1085, -0.3156,  0.0699,  0.0418, -0.0772,\n",
      "         0.3115, -0.1414, -0.2681,  0.1874, -0.3144,  0.3272, -0.3007,  0.0272,\n",
      "        -0.1218,  0.3070,  0.3219,  0.2014,  0.1873,  0.3412,  0.1516,  0.1287,\n",
      "        -0.2760, -0.0995,  0.3407, -0.1615, -0.1807,  0.0896,  0.0587,  0.0463],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([16])\n",
      "tensor([-0.1218,  0.3070,  0.3219,  0.2014,  0.1873,  0.3412,  0.1516,  0.1287,\n",
      "        -0.2760, -0.0995,  0.3407, -0.1615, -0.1807,  0.0896,  0.0587,  0.0463],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.2032,  0.7894, -0.1194, -0.3398, -0.3687,  0.7570, -0.2993,\n",
      "           0.9430, -0.0943,  0.4432,  0.5115, -0.3078, -0.7790, -0.0604,\n",
      "           0.0464, -0.0344],\n",
      "         [ 0.0296,  0.5348, -0.1986,  0.2133, -0.2332,  0.1123, -0.4076,\n",
      "           0.8586, -0.1620,  0.2712,  0.6842,  0.0731, -0.6125,  0.3610,\n",
      "           0.1523,  0.0053],\n",
      "         [-0.1911,  0.5887, -0.1020, -0.0686, -0.2554,  0.4955, -0.6434,\n",
      "           0.6592, -0.2849,  0.1420,  0.5211, -0.1094, -0.3320,  0.2046,\n",
      "           0.4260, -0.1091]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 16])\n",
      "tensor([[[-2.4652e-02, -1.2398e-01, -2.3038e-02,  5.4325e-02,  4.5172e-02,\n",
      "           4.9183e-01,  3.3242e-02, -8.2968e-02, -4.7283e-03,  1.0505e-01,\n",
      "           1.7477e-01, -6.5202e-02,  1.3054e-01, -4.0346e-04,  4.6826e-03,\n",
      "          -1.2489e-02],\n",
      "         [-2.2135e-03, -7.9471e-02, -3.1701e-02, -2.1617e-02, -9.7345e-03,\n",
      "           4.6504e-02,  1.8155e-02, -1.1026e-01, -6.6760e-02,  3.3496e-02,\n",
      "           2.1054e-01,  4.4583e-02,  1.0359e-01, -3.8917e-02, -1.9137e-04,\n",
      "           3.1139e-03],\n",
      "         [ 5.7318e-03, -9.9783e-02, -2.2951e-02,  1.1570e-02, -6.7541e-03,\n",
      "           1.7823e-01,  4.7281e-02,  3.1981e-02, -3.7238e-02,  3.6784e-02,\n",
      "           1.0716e-01, -8.7364e-02,  5.5066e-02, -2.1508e-02,  5.3531e-02,\n",
      "          -6.0113e-02]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([32, 8])\n",
      "Parameter containing:\n",
      "tensor([[-0.0850,  0.1153,  0.1581,  0.1439,  0.0503, -0.1568,  0.1261,  0.1764],\n",
      "        [ 0.0569, -0.0104,  0.1082, -0.1281, -0.1668,  0.1488,  0.1475, -0.0986],\n",
      "        [-0.1387, -0.1750, -0.1347, -0.0340, -0.1098,  0.1695, -0.0129, -0.1740],\n",
      "        [ 0.0198,  0.0893, -0.0751,  0.1264,  0.1262,  0.0622,  0.1746, -0.0427],\n",
      "        [ 0.1481, -0.1442, -0.0994,  0.1560, -0.0679, -0.0892,  0.0014, -0.0896],\n",
      "        [ 0.1427, -0.0932,  0.0327,  0.0317,  0.1637, -0.1696, -0.1525, -0.0409],\n",
      "        [ 0.0567, -0.0500, -0.1247, -0.0583,  0.0905, -0.0617,  0.1151, -0.0341],\n",
      "        [-0.0079, -0.1559, -0.1584, -0.0852,  0.1012,  0.0860, -0.1019, -0.0717],\n",
      "        [-0.0102,  0.0128, -0.0318, -0.0946, -0.1649, -0.0288,  0.0004,  0.1157],\n",
      "        [ 0.0497,  0.0184, -0.1080,  0.0066,  0.0440,  0.1178, -0.1676,  0.1211],\n",
      "        [ 0.0306,  0.1080,  0.1427,  0.0672, -0.0033, -0.0247,  0.0484, -0.1652],\n",
      "        [-0.0394,  0.1319,  0.0149, -0.0446,  0.1518,  0.0712,  0.0635, -0.0918],\n",
      "        [ 0.1189,  0.0833, -0.1566,  0.0450, -0.1586,  0.0322, -0.1621,  0.1114],\n",
      "        [ 0.1214,  0.0473, -0.1191, -0.0658, -0.1713, -0.0582,  0.1037, -0.1043],\n",
      "        [ 0.1116, -0.0727, -0.1730, -0.0014,  0.0986, -0.0410,  0.1223, -0.1329],\n",
      "        [ 0.0650,  0.1622, -0.1729,  0.0334,  0.0247, -0.1465, -0.0048,  0.1438],\n",
      "        [-0.0399,  0.1089,  0.0979, -0.0003, -0.0692, -0.1613,  0.0215,  0.1067],\n",
      "        [-0.0260, -0.0632,  0.1637, -0.0296, -0.0696,  0.0458, -0.1039,  0.0498],\n",
      "        [ 0.0445,  0.1640, -0.1516,  0.0887, -0.1539,  0.0745, -0.0450,  0.1303],\n",
      "        [ 0.1084,  0.1720,  0.0198,  0.1429, -0.0110, -0.0350, -0.1264, -0.1567],\n",
      "        [ 0.1279, -0.0088,  0.0901,  0.1618, -0.0099, -0.0392,  0.1676,  0.0993],\n",
      "        [-0.0247,  0.0870, -0.0077,  0.1028, -0.1554, -0.0746,  0.0758, -0.0241],\n",
      "        [-0.0216,  0.1248, -0.0355, -0.0416, -0.0078, -0.0313,  0.0194,  0.1227],\n",
      "        [ 0.0831, -0.0002, -0.0564,  0.0960, -0.0173, -0.0525,  0.0770,  0.0778],\n",
      "        [-0.0745, -0.0166,  0.1395, -0.0058,  0.1547,  0.0345, -0.1068,  0.1317],\n",
      "        [ 0.0487, -0.1538,  0.0905, -0.1369, -0.0311,  0.0144,  0.1008, -0.1313],\n",
      "        [-0.0726,  0.0640, -0.0446, -0.0528,  0.1134, -0.0013, -0.0278,  0.0920],\n",
      "        [ 0.0832, -0.0555,  0.0341, -0.0281,  0.0301,  0.0623, -0.1592,  0.1382],\n",
      "        [ 0.1725,  0.0517, -0.1166,  0.1355, -0.1035, -0.0208,  0.1060, -0.0896],\n",
      "        [ 0.0697, -0.0854,  0.0301, -0.0606,  0.0912,  0.1578,  0.0880, -0.0073],\n",
      "        [-0.1715,  0.0655, -0.0829, -0.1684,  0.0671,  0.1171, -0.1254, -0.1159],\n",
      "        [ 0.0771, -0.1681, -0.0308,  0.1270,  0.0781, -0.0324,  0.0923, -0.0970]],\n",
      "       requires_grad=True)\n",
      "Wdown spliced: torch.Size([16, 4])\n",
      "tensor([[-0.0692, -0.1613,  0.0215,  0.1067],\n",
      "        [-0.0696,  0.0458, -0.1039,  0.0498],\n",
      "        [-0.1539,  0.0745, -0.0450,  0.1303],\n",
      "        [-0.0110, -0.0350, -0.1264, -0.1567],\n",
      "        [-0.0099, -0.0392,  0.1676,  0.0993],\n",
      "        [-0.1554, -0.0746,  0.0758, -0.0241],\n",
      "        [-0.0078, -0.0313,  0.0194,  0.1227],\n",
      "        [-0.0173, -0.0525,  0.0770,  0.0778],\n",
      "        [ 0.1547,  0.0345, -0.1068,  0.1317],\n",
      "        [-0.0311,  0.0144,  0.1008, -0.1313],\n",
      "        [ 0.1134, -0.0013, -0.0278,  0.0920],\n",
      "        [ 0.0301,  0.0623, -0.1592,  0.1382],\n",
      "        [-0.1035, -0.0208,  0.1060, -0.0896],\n",
      "        [ 0.0912,  0.1578,  0.0880, -0.0073],\n",
      "        [ 0.0671,  0.1171, -0.1254, -0.1159],\n",
      "        [ 0.0781, -0.0324,  0.0923, -0.0970]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.0952,  0.0296,  0.1446, -0.0618, -0.1172,  0.1323,  0.0880,  0.1261],\n",
      "       requires_grad=True)\n",
      "Bdown spliced: torch.Size([4])\n",
      "tensor([-0.1172,  0.1323,  0.0880,  0.1261], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.1800,  0.0872,  0.1624,  0.0776],\n",
      "         [-0.1119,  0.1219,  0.0996,  0.1157],\n",
      "         [-0.1419,  0.1059,  0.1242,  0.1031]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[-0.4109,  1.2569,  0.9388,  0.7702],\n",
      "         [-0.4077,  1.0386,  0.0956,  1.1847],\n",
      "         [ 0.0864,  0.9345,  0.1082,  0.6978]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "y: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.4109,  1.2569,  0.9388,  0.7702],\n",
      "         [-0.4077,  1.0386,  0.0956,  1.1847],\n",
      "         [ 0.0864,  0.9345,  0.1082,  0.6978]]], grad_fn=<AddBackward0>)\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n",
      "head_dim_list:  [32, 16, 8]\n"
     ]
    }
   ],
   "source": [
    "# Testing our Layer's forwardTensor()\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4 = config.hidden_size, config.num_attention_heads, config.head_dim, config.max_position_embeddings\n",
    "config.hidden_size = 8\n",
    "config.num_attention_heads = 2\n",
    "config.head_dim = 4\n",
    "config.max_position_embeddings = 3\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "layer = Layer(config)\n",
    "y = layer(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size//config.split)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "y = layer(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size//config.split)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "y = layer(x, model=1)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.num_attention_heads = hold2\n",
    "config.head_dim = hold3\n",
    "config.max_position_embeddings = hold4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "# clear up memory\n",
    "del hold1, hold2, hold3, hold4, x, layer, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0531758f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [8, 4]\n",
      "head_dim_list:  [4, 2]\n",
      "x: ((tensor([[[-0.7193, -0.6913,  0.2116,  0.1197,  1.4033,  0.2525, -1.4282,\n",
      "          -1.9589],\n",
      "         [ 1.1528, -0.1403,  3.1788, -0.0754, -0.8046, -1.1919,  1.9537,\n",
      "          -0.4478],\n",
      "         [-1.7944,  0.7290, -1.7912,  0.9675, -0.5966,  0.3567,  1.2787,\n",
      "          -0.5683]]]),), (tensor([[[ 0.4044,  0.9881,  0.9838, -2.1977],\n",
      "         [-0.7348,  1.6922, -0.8843,  0.1139],\n",
      "         [-0.7336, -0.3367, -0.9742, -1.2100]]]), tensor([[[-0.7594, -0.8046, -0.5859, -1.3914],\n",
      "         [ 1.1770,  0.8434,  1.4686,  0.0103],\n",
      "         [ 1.6388,  0.5452,  0.6675,  0.4108]]])))\n",
      "---------- Layer Input: Tuple ------------\n",
      "------------- Layer.forwardTuple() ------------\n",
      "x:\n",
      "((tensor([[[-0.7193, -0.6913,  0.2116,  0.1197,  1.4033,  0.2525, -1.4282,\n",
      "          -1.9589],\n",
      "         [ 1.1528, -0.1403,  3.1788, -0.0754, -0.8046, -1.1919,  1.9537,\n",
      "          -0.4478],\n",
      "         [-1.7944,  0.7290, -1.7912,  0.9675, -0.5966,  0.3567,  1.2787,\n",
      "          -0.5683]]]),), (tensor([[[ 0.4044,  0.9881,  0.9838, -2.1977],\n",
      "         [-0.7348,  1.6922, -0.8843,  0.1139],\n",
      "         [-0.7336, -0.3367, -0.9742, -1.2100]]]), tensor([[[-0.7594, -0.8046, -0.5859, -1.3914],\n",
      "         [ 1.1770,  0.8434,  1.4686,  0.0103],\n",
      "         [ 1.6388,  0.5452,  0.6675,  0.4108]]])))\n",
      "input_len: 3\n",
      "num_levels: 2\n",
      "models_per_level: [1, 2]\n",
      "Level 0 from range(2)\n",
      "Model 0 from range(1)\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.7193, -0.6913,  0.2116,  0.1197,  1.4033,  0.2525, -1.4282,\n",
      "          -1.9589],\n",
      "         [ 1.1528, -0.1403,  3.1788, -0.0754, -0.8046, -1.1919,  1.9537,\n",
      "          -0.4478],\n",
      "         [-1.7944,  0.7290, -1.7912,  0.9675, -0.5966,  0.3567,  1.2787,\n",
      "          -0.5683]]])\n",
      "normed x: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.6795, -0.6530,  0.1998,  0.1130,  1.3257,  0.2385, -1.3492,\n",
      "          -1.8506],\n",
      "         [ 0.7784, -0.0948,  2.1465, -0.0509, -0.5433, -0.8048,  1.3193,\n",
      "          -0.3024],\n",
      "         [-1.5786,  0.6413, -1.5758,  0.8511, -0.5249,  0.3138,  1.1249,\n",
      "          -0.4999]]])\n",
      "dim: 8\n",
      "skip: 0\n",
      "spliced scale: torch.Size([8])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.6795, -0.6530,  0.1998,  0.1130,  1.3257,  0.2385, -1.3492,\n",
      "          -1.8506],\n",
      "         [ 0.7784, -0.0948,  2.1465, -0.0509, -0.5433, -0.8048,  1.3193,\n",
      "          -0.3024],\n",
      "         [-1.5786,  0.6413, -1.5758,  0.8511, -0.5249,  0.3138,  1.1249,\n",
      "          -0.4999]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 8])\n",
      "d_skip: 0\n",
      "models_in_this_level: 1\n",
      "h_dim: 4\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3082, -0.0375,  0.2938,  0.0952,  0.1068,  0.2869, -0.2548,  0.2737,\n",
      "          0.1577,  0.2032, -0.1882, -0.2408,  0.0541, -0.2956, -0.3144, -0.0215],\n",
      "        [ 0.0282,  0.0842, -0.3029, -0.2693, -0.2461,  0.0440,  0.1471, -0.0685,\n",
      "         -0.2607, -0.1407,  0.2482,  0.2583, -0.0811,  0.0538,  0.3521, -0.2299],\n",
      "        [-0.2697,  0.0807,  0.0751, -0.0655,  0.1450, -0.3520,  0.2306,  0.1296,\n",
      "         -0.1562, -0.0380, -0.0221,  0.2635,  0.0980,  0.3051, -0.2555,  0.0061],\n",
      "        [ 0.1652, -0.2571,  0.2203, -0.2631, -0.0092,  0.3367, -0.3005,  0.2128,\n",
      "         -0.1101, -0.3220,  0.2759, -0.1204,  0.3024, -0.0503, -0.2147, -0.1287],\n",
      "        [ 0.0111, -0.3078, -0.0968,  0.1583,  0.1930, -0.3113, -0.2252, -0.1391,\n",
      "          0.1673, -0.2444, -0.0069,  0.0760,  0.0661,  0.0243,  0.1211,  0.3435],\n",
      "        [-0.0053, -0.2899, -0.1091, -0.0459, -0.0735, -0.0206,  0.2626,  0.0280,\n",
      "         -0.2423,  0.1941, -0.2940,  0.1764, -0.1085,  0.0123,  0.0262,  0.3460],\n",
      "        [-0.2541, -0.1558, -0.1859, -0.1539,  0.1353, -0.1152, -0.0136, -0.2143,\n",
      "         -0.0768, -0.2811,  0.2954,  0.0775, -0.1482,  0.0178, -0.2664,  0.0608],\n",
      "        [-0.2611, -0.2725,  0.0680, -0.2697, -0.1009,  0.3087,  0.1102,  0.2571,\n",
      "         -0.1993, -0.0211, -0.0661,  0.3239,  0.1822,  0.0132, -0.0324,  0.2877]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[ 0.3082, -0.0375,  0.2938,  0.0952,  0.1068,  0.2869, -0.2548,  0.2737],\n",
      "        [ 0.0282,  0.0842, -0.3029, -0.2693, -0.2461,  0.0440,  0.1471, -0.0685],\n",
      "        [-0.2697,  0.0807,  0.0751, -0.0655,  0.1450, -0.3520,  0.2306,  0.1296],\n",
      "        [ 0.1652, -0.2571,  0.2203, -0.2631, -0.0092,  0.3367, -0.3005,  0.2128],\n",
      "        [ 0.0111, -0.3078, -0.0968,  0.1583,  0.1930, -0.3113, -0.2252, -0.1391],\n",
      "        [-0.0053, -0.2899, -0.1091, -0.0459, -0.0735, -0.0206,  0.2626,  0.0280],\n",
      "        [-0.2541, -0.1558, -0.1859, -0.1539,  0.1353, -0.1152, -0.0136, -0.2143],\n",
      "        [-0.2611, -0.2725,  0.0680, -0.2697, -0.1009,  0.3087,  0.1102,  0.2571]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[ 0.1577,  0.2032, -0.1882, -0.2408],\n",
      "        [-0.2607, -0.1407,  0.2482,  0.2583],\n",
      "        [-0.1562, -0.0380, -0.0221,  0.2635],\n",
      "        [-0.1101, -0.3220,  0.2759, -0.1204],\n",
      "        [ 0.1673, -0.2444, -0.0069,  0.0760],\n",
      "        [-0.2423,  0.1941, -0.2940,  0.1764],\n",
      "        [-0.0768, -0.2811,  0.2954,  0.0775],\n",
      "        [-0.1993, -0.0211, -0.0661,  0.3239]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.0541, -0.2956, -0.3144, -0.0215],\n",
      "        [-0.0811,  0.0538,  0.3521, -0.2299],\n",
      "        [ 0.0980,  0.3051, -0.2555,  0.0061],\n",
      "        [ 0.3024, -0.0503, -0.2147, -0.1287],\n",
      "        [ 0.0661,  0.0243,  0.1211,  0.3435],\n",
      "        [-0.1085,  0.0123,  0.0262,  0.3460],\n",
      "        [-0.1482,  0.0178, -0.2664,  0.0608],\n",
      "        [ 0.1822,  0.0132, -0.0324,  0.2877]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([8, 8])\n",
      "tensor([[ 0.3082, -0.0375,  0.2938,  0.0952,  0.1068,  0.2869, -0.2548,  0.2737],\n",
      "        [ 0.0282,  0.0842, -0.3029, -0.2693, -0.2461,  0.0440,  0.1471, -0.0685],\n",
      "        [-0.2697,  0.0807,  0.0751, -0.0655,  0.1450, -0.3520,  0.2306,  0.1296],\n",
      "        [ 0.1652, -0.2571,  0.2203, -0.2631, -0.0092,  0.3367, -0.3005,  0.2128],\n",
      "        [ 0.0111, -0.3078, -0.0968,  0.1583,  0.1930, -0.3113, -0.2252, -0.1391],\n",
      "        [-0.0053, -0.2899, -0.1091, -0.0459, -0.0735, -0.0206,  0.2626,  0.0280],\n",
      "        [-0.2541, -0.1558, -0.1859, -0.1539,  0.1353, -0.1152, -0.0136, -0.2143],\n",
      "        [-0.2611, -0.2725,  0.0680, -0.2697, -0.1009,  0.3087,  0.1102,  0.2571]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([8, 4])\n",
      "tensor([[ 0.1577,  0.2032, -0.1882, -0.2408],\n",
      "        [-0.2607, -0.1407,  0.2482,  0.2583],\n",
      "        [-0.1562, -0.0380, -0.0221,  0.2635],\n",
      "        [-0.1101, -0.3220,  0.2759, -0.1204],\n",
      "        [ 0.1673, -0.2444, -0.0069,  0.0760],\n",
      "        [-0.2423,  0.1941, -0.2940,  0.1764],\n",
      "        [-0.0768, -0.2811,  0.2954,  0.0775],\n",
      "        [-0.1993, -0.0211, -0.0661,  0.3239]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([8, 4])\n",
      "tensor([[ 0.0541, -0.2956, -0.3144, -0.0215],\n",
      "        [-0.0811,  0.0538,  0.3521, -0.2299],\n",
      "        [ 0.0980,  0.3051, -0.2555,  0.0061],\n",
      "        [ 0.3024, -0.0503, -0.2147, -0.1287],\n",
      "        [ 0.0661,  0.0243,  0.1211,  0.3435],\n",
      "        [-0.1085,  0.0123,  0.0262,  0.3460],\n",
      "        [-0.1482,  0.0178, -0.2664,  0.0608],\n",
      "        [ 0.1822,  0.0132, -0.0324,  0.2877]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([8, 16])\n",
      "tensor([[ 0.3082, -0.0375,  0.2938,  0.0952,  0.1068,  0.2869, -0.2548,  0.2737,\n",
      "          0.1577,  0.2032, -0.1882, -0.2408,  0.0541, -0.2956, -0.3144, -0.0215],\n",
      "        [ 0.0282,  0.0842, -0.3029, -0.2693, -0.2461,  0.0440,  0.1471, -0.0685,\n",
      "         -0.2607, -0.1407,  0.2482,  0.2583, -0.0811,  0.0538,  0.3521, -0.2299],\n",
      "        [-0.2697,  0.0807,  0.0751, -0.0655,  0.1450, -0.3520,  0.2306,  0.1296,\n",
      "         -0.1562, -0.0380, -0.0221,  0.2635,  0.0980,  0.3051, -0.2555,  0.0061],\n",
      "        [ 0.1652, -0.2571,  0.2203, -0.2631, -0.0092,  0.3367, -0.3005,  0.2128,\n",
      "         -0.1101, -0.3220,  0.2759, -0.1204,  0.3024, -0.0503, -0.2147, -0.1287],\n",
      "        [ 0.0111, -0.3078, -0.0968,  0.1583,  0.1930, -0.3113, -0.2252, -0.1391,\n",
      "          0.1673, -0.2444, -0.0069,  0.0760,  0.0661,  0.0243,  0.1211,  0.3435],\n",
      "        [-0.0053, -0.2899, -0.1091, -0.0459, -0.0735, -0.0206,  0.2626,  0.0280,\n",
      "         -0.2423,  0.1941, -0.2940,  0.1764, -0.1085,  0.0123,  0.0262,  0.3460],\n",
      "        [-0.2541, -0.1558, -0.1859, -0.1539,  0.1353, -0.1152, -0.0136, -0.2143,\n",
      "         -0.0768, -0.2811,  0.2954,  0.0775, -0.1482,  0.0178, -0.2664,  0.0608],\n",
      "        [-0.2611, -0.2725,  0.0680, -0.2697, -0.1009,  0.3087,  0.1102,  0.2571,\n",
      "         -0.1993, -0.0211, -0.0661,  0.3239,  0.1822,  0.0132, -0.0324,  0.2877]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.5764,  0.1949,  0.0087,  0.9741,  0.3585, -1.0894, -0.3324,\n",
      "          -0.4557,  0.6558,  0.0505, -0.3629, -0.5273, -0.0055,  0.2077,\n",
      "           0.4945,  0.0750],\n",
      "         [-0.6083,  0.4266,  0.2820, -0.1982,  0.5817, -0.6131,  0.1577,\n",
      "           0.1796, -0.1193, -0.2817,  0.4186,  0.1808,  0.0456,  0.4186,\n",
      "          -1.2441, -0.4472],\n",
      "         [-0.0656, -0.2013, -0.8154, -0.5794, -0.4845,  0.2896,  0.0078,\n",
      "          -0.7867, -0.4143, -0.7416,  1.0026, -0.0311, -0.3611, -0.0178,\n",
      "           0.6031, -0.3799]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.5764,  0.1949,  0.0087,  0.9741,  0.3585, -1.0894, -0.3324,\n",
      "          -0.4557],\n",
      "         [-0.6083,  0.4266,  0.2820, -0.1982,  0.5817, -0.6131,  0.1577,\n",
      "           0.1796],\n",
      "         [-0.0656, -0.2013, -0.8154, -0.5794, -0.4845,  0.2896,  0.0078,\n",
      "          -0.7867]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.6558,  0.0505, -0.3629, -0.5273],\n",
      "         [-0.1193, -0.2817,  0.4186,  0.1808],\n",
      "         [-0.4143, -0.7416,  1.0026, -0.0311]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0055,  0.2077,  0.4945,  0.0750],\n",
      "         [ 0.0456,  0.4186, -1.2441, -0.4472],\n",
      "         [-0.3611, -0.0178,  0.6031, -0.3799]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 0.5764,  0.1949,  0.0087,  0.9741],\n",
      "          [ 0.3585, -1.0894, -0.3324, -0.4557]],\n",
      "\n",
      "         [[-0.6083,  0.4266,  0.2820, -0.1982],\n",
      "          [ 0.5817, -0.6131,  0.1577,  0.1796]],\n",
      "\n",
      "         [[-0.0656, -0.2013, -0.8154, -0.5794],\n",
      "          [-0.4845,  0.2896,  0.0078, -0.7867]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[ 0.6558,  0.0505, -0.3629, -0.5273]],\n",
      "\n",
      "         [[-0.1193, -0.2817,  0.4186,  0.1808]],\n",
      "\n",
      "         [[-0.4143, -0.7416,  1.0026, -0.0311]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[-0.0055,  0.2077,  0.4945,  0.0750]],\n",
      "\n",
      "         [[ 0.0456,  0.4186, -1.2441, -0.4472]],\n",
      "\n",
      "         [[-0.3611, -0.0178,  0.6031, -0.3799]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 0.5764,  0.1949,  0.0087,  0.9741],\n",
      "          [ 0.3585, -1.0894, -0.3324, -0.4557]],\n",
      "\n",
      "         [[-0.5660,  0.4442, -0.3595, -0.1546],\n",
      "          [ 0.1816, -0.6279,  0.5746,  0.1175]],\n",
      "\n",
      "         [[ 0.7688, -0.0821,  0.2797, -0.6079],\n",
      "          [ 0.1945,  0.4401, -0.4438, -0.7135]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 4])\n",
      "tensor([[[[ 0.6558,  0.0505, -0.3629, -0.5273]],\n",
      "\n",
      "         [[-0.4166, -0.2984,  0.1258,  0.1518]],\n",
      "\n",
      "         [[-0.7393, -0.7207, -0.7939, -0.1778]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[ 0.6558,  0.0505, -0.3629, -0.5273],\n",
      "          [ 0.6558,  0.0505, -0.3629, -0.5273]],\n",
      "\n",
      "         [[-0.4166, -0.2984,  0.1258,  0.1518],\n",
      "          [-0.4166, -0.2984,  0.1258,  0.1518]],\n",
      "\n",
      "         [[-0.7393, -0.7207, -0.7939, -0.1778],\n",
      "          [-0.7393, -0.7207, -0.7939, -0.1778]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 4])\n",
      "tensor([[[[-0.0055,  0.2077,  0.4945,  0.0750],\n",
      "          [-0.0055,  0.2077,  0.4945,  0.0750]],\n",
      "\n",
      "         [[ 0.0456,  0.4186, -1.2441, -0.4472],\n",
      "          [ 0.0456,  0.4186, -1.2441, -0.4472]],\n",
      "\n",
      "         [[-0.3611, -0.0178,  0.6031, -0.3799],\n",
      "          [-0.3611, -0.0178,  0.6031, -0.3799]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[ 0.5764,  0.1949,  0.0087,  0.9741],\n",
      "          [-0.5660,  0.4442, -0.3595, -0.1546],\n",
      "          [ 0.7688, -0.0821,  0.2797, -0.6079]],\n",
      "\n",
      "         [[ 0.3585, -1.0894, -0.3324, -0.4557],\n",
      "          [ 0.1816, -0.6279,  0.5746,  0.1175],\n",
      "          [ 0.1945,  0.4401, -0.4438, -0.7135]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[ 0.6558,  0.0505, -0.3629, -0.5273],\n",
      "          [-0.4166, -0.2984,  0.1258,  0.1518],\n",
      "          [-0.7393, -0.7207, -0.7939, -0.1778]],\n",
      "\n",
      "         [[ 0.6558,  0.0505, -0.3629, -0.5273],\n",
      "          [-0.4166, -0.2984,  0.1258,  0.1518],\n",
      "          [-0.7393, -0.7207, -0.7939, -0.1778]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.0055,  0.2077,  0.4945,  0.0750],\n",
      "          [ 0.0456,  0.4186, -1.2441, -0.4472],\n",
      "          [-0.3611, -0.0178,  0.6031, -0.3799]],\n",
      "\n",
      "         [[-0.0055,  0.2077,  0.4945,  0.0750],\n",
      "          [ 0.0456,  0.4186, -1.2441, -0.4472],\n",
      "          [-0.3611, -0.0178,  0.6031, -0.3799]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-0.0644, -0.0747, -0.3734],\n",
      "          [-0.0684,  0.0173,  0.2056],\n",
      "          [ 0.3595, -0.1765, -0.3115]],\n",
      "\n",
      "         [[ 0.2705,  0.0323,  0.4325],\n",
      "          [-0.0915,  0.1009, -0.0794],\n",
      "          [ 0.3435, -0.1883,  0.0091]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-6.4434e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [-6.8385e-02,  1.7282e-02, -2.3820e+38],\n",
      "          [ 3.5953e-01, -1.7645e-01, -3.1155e-01]],\n",
      "\n",
      "         [[ 2.7050e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [-9.1541e-02,  1.0091e-01, -2.3820e+38],\n",
      "          [ 3.4353e-01, -1.8826e-01,  9.1414e-03]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4786, 0.5214, 0.0000],\n",
      "          [0.4770, 0.2791, 0.2438]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4520, 0.5480, 0.0000],\n",
      "          [0.4342, 0.2551, 0.3108]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-0.0055,  0.2077,  0.4945,  0.0750],\n",
      "          [ 0.0211,  0.3177, -0.4120, -0.1973],\n",
      "          [-0.0780,  0.2116,  0.0357, -0.1817]],\n",
      "\n",
      "         [[-0.0055,  0.2077,  0.4945,  0.0750],\n",
      "          [ 0.0225,  0.3233, -0.4582, -0.2112],\n",
      "          [-0.1030,  0.1914,  0.0848, -0.1996]]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.0055,  0.2077,  0.4945,  0.0750, -0.0055,  0.2077,  0.4945,\n",
      "           0.0750],\n",
      "         [ 0.0211,  0.3177, -0.4120, -0.1973,  0.0225,  0.3233, -0.4582,\n",
      "          -0.2112],\n",
      "         [-0.0780,  0.2116,  0.0357, -0.1817, -0.1030,  0.1914,  0.0848,\n",
      "          -0.1996]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1812, -0.1638, -0.1178,  0.1815, -0.0778, -0.0560,  0.3211,  0.3358],\n",
      "        [-0.1059,  0.2363,  0.1847, -0.2948,  0.2672,  0.3174,  0.3080,  0.2044],\n",
      "        [ 0.2060,  0.0377, -0.2959, -0.2276, -0.2968,  0.1777, -0.0755, -0.1395],\n",
      "        [ 0.3207,  0.0063,  0.1376, -0.2537,  0.3473, -0.3356,  0.0987, -0.2497],\n",
      "        [-0.3443,  0.2761, -0.2423, -0.0421,  0.0527, -0.2862,  0.0705,  0.1438],\n",
      "        [ 0.1588, -0.2870,  0.0738, -0.2562,  0.0793,  0.2388,  0.3065,  0.1916],\n",
      "        [ 0.0715, -0.2669,  0.0129,  0.3258, -0.1468, -0.0368, -0.2941,  0.0135],\n",
      "        [-0.0419, -0.0367, -0.1927, -0.0744,  0.0314, -0.2928,  0.1988, -0.1777]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([8, 8])\n",
      "tensor([[ 0.1812, -0.1638, -0.1178,  0.1815, -0.0778, -0.0560,  0.3211,  0.3358],\n",
      "        [-0.1059,  0.2363,  0.1847, -0.2948,  0.2672,  0.3174,  0.3080,  0.2044],\n",
      "        [ 0.2060,  0.0377, -0.2959, -0.2276, -0.2968,  0.1777, -0.0755, -0.1395],\n",
      "        [ 0.3207,  0.0063,  0.1376, -0.2537,  0.3473, -0.3356,  0.0987, -0.2497],\n",
      "        [-0.3443,  0.2761, -0.2423, -0.0421,  0.0527, -0.2862,  0.0705,  0.1438],\n",
      "        [ 0.1588, -0.2870,  0.0738, -0.2562,  0.0793,  0.2388,  0.3065,  0.1916],\n",
      "        [ 0.0715, -0.2669,  0.0129,  0.3258, -0.1468, -0.0368, -0.2941,  0.0135],\n",
      "        [-0.0419, -0.0367, -0.1927, -0.0744,  0.0314, -0.2928,  0.1988, -0.1777]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.1700, -0.1268, -0.0884, -0.0912, -0.1189,  0.1400, -0.0350,\n",
      "          -0.0147],\n",
      "         [-0.1583,  0.0983,  0.2042, -0.1633,  0.2244,  0.2421,  0.3097,\n",
      "           0.2752],\n",
      "         [-0.0072, -0.0357,  0.0913, -0.0408, -0.0201,  0.2694,  0.0063,\n",
      "           0.1159]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[-0.5492, -0.8181,  0.1231,  0.0284,  1.2845,  0.3924, -1.4632,\n",
      "          -1.9737],\n",
      "         [ 0.9945, -0.0421,  3.3830, -0.2387, -0.5802, -0.9498,  2.2634,\n",
      "          -0.1725],\n",
      "         [-1.8016,  0.6933, -1.6999,  0.9267, -0.6167,  0.6260,  1.2850,\n",
      "          -0.4524]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.5492, -0.8181,  0.1231,  0.0284,  1.2845,  0.3924, -1.4632,\n",
      "          -1.9737],\n",
      "         [ 0.9945, -0.0421,  3.3830, -0.2387, -0.5802, -0.9498,  2.2634,\n",
      "          -0.1725],\n",
      "         [-1.8016,  0.6933, -1.6999,  0.9267, -0.6167,  0.6260,  1.2850,\n",
      "          -0.4524]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.5229, -0.7788,  0.1172,  0.0270,  1.2228,  0.3736, -1.3930,\n",
      "          -1.8789],\n",
      "         [ 0.6473, -0.0274,  2.2019, -0.1553, -0.3776, -0.6182,  1.4732,\n",
      "          -0.1123],\n",
      "         [-1.6032,  0.6169, -1.5127,  0.8246, -0.5488,  0.5571,  1.1434,\n",
      "          -0.4025]]], grad_fn=<MulBackward0>)\n",
      "dim: 8\n",
      "skip: 0\n",
      "spliced scale: torch.Size([8])\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.5229, -0.7788,  0.1172,  0.0270,  1.2228,  0.3736, -1.3930,\n",
      "          -1.8789],\n",
      "         [ 0.6473, -0.0274,  2.2019, -0.1553, -0.3776, -0.6182,  1.4732,\n",
      "          -0.1123],\n",
      "         [-1.6032,  0.6169, -1.5127,  0.8246, -0.5488,  0.5571,  1.1434,\n",
      "          -0.4025]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.5229, -0.7788,  0.1172,  0.0270,  1.2228,  0.3736, -1.3930,\n",
      "          -1.8789],\n",
      "         [ 0.6473, -0.0274,  2.2019, -0.1553, -0.3776, -0.6182,  1.4732,\n",
      "          -0.1123],\n",
      "         [-1.6032,  0.6169, -1.5127,  0.8246, -0.5488,  0.5571,  1.1434,\n",
      "          -0.4025]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 8\n",
      "d_skip: 0\n",
      "i_dim: 32\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[-0.2811, -0.0270,  0.0777, -0.0039,  0.2796, -0.2670,  0.1663,  0.0266,\n",
      "          0.2387,  0.1946, -0.1248,  0.0487,  0.1504, -0.0958,  0.2620,  0.0300,\n",
      "         -0.0412,  0.1858, -0.2139, -0.2279,  0.1933, -0.0472,  0.2015, -0.1428,\n",
      "         -0.1714,  0.3420, -0.1261, -0.0445, -0.2970, -0.2236,  0.1774,  0.3273],\n",
      "        [-0.0559, -0.1062,  0.2060, -0.3148, -0.0610,  0.0727, -0.2189, -0.2999,\n",
      "         -0.0502,  0.1810,  0.1343,  0.0851, -0.0477,  0.0827,  0.0995,  0.0167,\n",
      "         -0.1973,  0.0140,  0.1064, -0.0729,  0.1845, -0.3449, -0.2266, -0.0014,\n",
      "         -0.3306, -0.1396, -0.3074, -0.2133,  0.3525, -0.0846, -0.3425,  0.1338],\n",
      "        [ 0.3045, -0.1883, -0.2371,  0.3285, -0.0387,  0.3390,  0.3480, -0.2650,\n",
      "         -0.0216,  0.2641,  0.2369, -0.1652, -0.3093,  0.0436, -0.1806,  0.2700,\n",
      "          0.2034, -0.2746,  0.3244, -0.0298,  0.1362,  0.2964, -0.2646, -0.0550,\n",
      "         -0.1278,  0.2398, -0.0377,  0.0583, -0.2364,  0.3412, -0.2262,  0.1816],\n",
      "        [ 0.2170,  0.0379, -0.0893, -0.0081,  0.2499, -0.3397,  0.2077,  0.3244,\n",
      "         -0.2439, -0.2049,  0.1377,  0.3533, -0.2209,  0.0883, -0.0685,  0.2863,\n",
      "         -0.0748,  0.2120, -0.0254, -0.1597,  0.0182, -0.3100, -0.2707,  0.0629,\n",
      "          0.1587,  0.1991, -0.1898,  0.2316,  0.1283, -0.1907,  0.0939,  0.1159],\n",
      "        [ 0.0412, -0.1712,  0.0801, -0.1478, -0.2345, -0.0489,  0.1414, -0.0028,\n",
      "          0.0027,  0.0525, -0.1322, -0.0448,  0.2901, -0.2900,  0.0118, -0.1953,\n",
      "          0.0151, -0.1399,  0.1249,  0.3276, -0.0825,  0.1432,  0.2753,  0.2337,\n",
      "          0.0480,  0.3424,  0.0860, -0.2939,  0.0054, -0.1741, -0.3390, -0.2313],\n",
      "        [ 0.0542,  0.0592,  0.3068,  0.1669, -0.2551,  0.3092,  0.3315,  0.1246,\n",
      "         -0.0527,  0.1310, -0.3207,  0.2923,  0.2547, -0.2331, -0.3039, -0.3103,\n",
      "         -0.0332, -0.2420,  0.1207, -0.0299,  0.0884, -0.1770, -0.3310,  0.1731,\n",
      "          0.0462, -0.0148, -0.1853, -0.3507, -0.0339, -0.1803, -0.0213,  0.2568],\n",
      "        [-0.1879,  0.0668,  0.1165,  0.2397, -0.2110, -0.0725, -0.3138,  0.2530,\n",
      "          0.2190, -0.1220, -0.3086,  0.2517, -0.1614,  0.0849, -0.3084,  0.0757,\n",
      "          0.0819,  0.2163,  0.2924,  0.0604, -0.1102, -0.3133, -0.3265,  0.1126,\n",
      "          0.3116, -0.2196, -0.3490, -0.1459,  0.2048, -0.3398,  0.3354,  0.3366],\n",
      "        [ 0.2473,  0.2045,  0.0686, -0.1509,  0.2188, -0.2802, -0.0783,  0.1187,\n",
      "         -0.2610,  0.0220, -0.1511,  0.2359,  0.0023, -0.0620, -0.2135, -0.2747,\n",
      "          0.2094,  0.0222,  0.1783,  0.3206, -0.2119,  0.2887,  0.1856,  0.3166,\n",
      "         -0.0897, -0.1708, -0.3043, -0.1225, -0.1501,  0.2560,  0.0135, -0.3519]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([8, 32])\n",
      "tensor([[-0.2811, -0.0270,  0.0777, -0.0039,  0.2796, -0.2670,  0.1663,  0.0266,\n",
      "          0.2387,  0.1946, -0.1248,  0.0487,  0.1504, -0.0958,  0.2620,  0.0300,\n",
      "         -0.0412,  0.1858, -0.2139, -0.2279,  0.1933, -0.0472,  0.2015, -0.1428,\n",
      "         -0.1714,  0.3420, -0.1261, -0.0445, -0.2970, -0.2236,  0.1774,  0.3273],\n",
      "        [-0.0559, -0.1062,  0.2060, -0.3148, -0.0610,  0.0727, -0.2189, -0.2999,\n",
      "         -0.0502,  0.1810,  0.1343,  0.0851, -0.0477,  0.0827,  0.0995,  0.0167,\n",
      "         -0.1973,  0.0140,  0.1064, -0.0729,  0.1845, -0.3449, -0.2266, -0.0014,\n",
      "         -0.3306, -0.1396, -0.3074, -0.2133,  0.3525, -0.0846, -0.3425,  0.1338],\n",
      "        [ 0.3045, -0.1883, -0.2371,  0.3285, -0.0387,  0.3390,  0.3480, -0.2650,\n",
      "         -0.0216,  0.2641,  0.2369, -0.1652, -0.3093,  0.0436, -0.1806,  0.2700,\n",
      "          0.2034, -0.2746,  0.3244, -0.0298,  0.1362,  0.2964, -0.2646, -0.0550,\n",
      "         -0.1278,  0.2398, -0.0377,  0.0583, -0.2364,  0.3412, -0.2262,  0.1816],\n",
      "        [ 0.2170,  0.0379, -0.0893, -0.0081,  0.2499, -0.3397,  0.2077,  0.3244,\n",
      "         -0.2439, -0.2049,  0.1377,  0.3533, -0.2209,  0.0883, -0.0685,  0.2863,\n",
      "         -0.0748,  0.2120, -0.0254, -0.1597,  0.0182, -0.3100, -0.2707,  0.0629,\n",
      "          0.1587,  0.1991, -0.1898,  0.2316,  0.1283, -0.1907,  0.0939,  0.1159],\n",
      "        [ 0.0412, -0.1712,  0.0801, -0.1478, -0.2345, -0.0489,  0.1414, -0.0028,\n",
      "          0.0027,  0.0525, -0.1322, -0.0448,  0.2901, -0.2900,  0.0118, -0.1953,\n",
      "          0.0151, -0.1399,  0.1249,  0.3276, -0.0825,  0.1432,  0.2753,  0.2337,\n",
      "          0.0480,  0.3424,  0.0860, -0.2939,  0.0054, -0.1741, -0.3390, -0.2313],\n",
      "        [ 0.0542,  0.0592,  0.3068,  0.1669, -0.2551,  0.3092,  0.3315,  0.1246,\n",
      "         -0.0527,  0.1310, -0.3207,  0.2923,  0.2547, -0.2331, -0.3039, -0.3103,\n",
      "         -0.0332, -0.2420,  0.1207, -0.0299,  0.0884, -0.1770, -0.3310,  0.1731,\n",
      "          0.0462, -0.0148, -0.1853, -0.3507, -0.0339, -0.1803, -0.0213,  0.2568],\n",
      "        [-0.1879,  0.0668,  0.1165,  0.2397, -0.2110, -0.0725, -0.3138,  0.2530,\n",
      "          0.2190, -0.1220, -0.3086,  0.2517, -0.1614,  0.0849, -0.3084,  0.0757,\n",
      "          0.0819,  0.2163,  0.2924,  0.0604, -0.1102, -0.3133, -0.3265,  0.1126,\n",
      "          0.3116, -0.2196, -0.3490, -0.1459,  0.2048, -0.3398,  0.3354,  0.3366],\n",
      "        [ 0.2473,  0.2045,  0.0686, -0.1509,  0.2188, -0.2802, -0.0783,  0.1187,\n",
      "         -0.2610,  0.0220, -0.1511,  0.2359,  0.0023, -0.0620, -0.2135, -0.2747,\n",
      "          0.2094,  0.0222,  0.1783,  0.3206, -0.2119,  0.2887,  0.1856,  0.3166,\n",
      "         -0.0897, -0.1708, -0.3043, -0.1225, -0.1501,  0.2560,  0.0135, -0.3519]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bgate: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([ 0.2344, -0.0453, -0.2316, -0.0515, -0.3145,  0.1585, -0.3512,  0.0260,\n",
      "        -0.0817,  0.1658,  0.0793, -0.3017, -0.3280, -0.1102,  0.3004, -0.3055,\n",
      "        -0.0573,  0.3171, -0.1961, -0.0773, -0.1722,  0.2801, -0.0123, -0.3422,\n",
      "        -0.2469, -0.2034, -0.1560,  0.2893,  0.3363, -0.0824,  0.0287, -0.3314],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([32])\n",
      "tensor([ 0.2344, -0.0453, -0.2316, -0.0515, -0.3145,  0.1585, -0.3512,  0.0260,\n",
      "        -0.0817,  0.1658,  0.0793, -0.3017, -0.3280, -0.1102,  0.3004, -0.3055,\n",
      "        -0.0573,  0.3171, -0.1961, -0.0773, -0.1722,  0.2801, -0.0123, -0.3422,\n",
      "        -0.2469, -0.2034, -0.1560,  0.2893,  0.3363, -0.0824,  0.0287, -0.3314],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 32])\n",
      "tensor([[[ 0.3342, -0.6341, -0.5414,  0.0654, -0.9103,  0.9553,  0.6597,\n",
      "          -0.3089, -0.0077,  0.1901,  0.5038, -1.1426,  0.2587, -0.5604,\n",
      "           0.7946, -0.2388, -0.3616, -0.4219, -0.6743, -0.2063,  0.0833,\n",
      "           0.6027,  0.3395, -0.6723, -0.1000,  0.8000,  1.2336,  0.4349,\n",
      "           0.1835, -0.1527, -0.7364, -0.5768],\n",
      "         [ 0.3372, -0.3768, -0.7512,  1.0019, -0.3450,  0.5349, -0.2153,\n",
      "          -0.2989,  0.4482,  0.6172,  0.3055, -0.5105, -1.3811,  0.2935,\n",
      "          -0.1668,  0.6715,  0.4928,  0.3181,  0.6698, -0.3159,  0.0830,\n",
      "           0.5212, -0.8174, -0.6305, -0.2325,  0.0945, -0.6807,  0.4852,\n",
      "          -0.0685,  0.2041,  0.2741,  0.7225],\n",
      "         [ 0.0620,  0.3694,  0.2884, -0.2342, -0.8785,  0.0673, -1.3282,\n",
      "           0.7792, -0.3392, -0.7073, -0.2807,  0.5942, -0.5157,  0.2525,\n",
      "          -0.2840, -0.3840, -0.4996,  0.7985, -0.0380, -0.0999, -0.5054,\n",
      "          -1.2125, -1.0815, -0.0096,  0.5401, -1.4151, -0.6700,  0.1803,\n",
      "           1.7661, -0.9459,  0.5049, -0.1561]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 32])\n",
      "tensor([[[ 0.2109, -0.1668, -0.1592,  0.0344, -0.1651,  0.7932,  0.4917,\n",
      "          -0.1170, -0.0038,  0.1094,  0.3490, -0.1447,  0.1558, -0.1612,\n",
      "           0.6250, -0.0969, -0.1298, -0.1420, -0.1686, -0.0863,  0.0444,\n",
      "           0.4380,  0.2149, -0.1685, -0.0460,  0.6306,  1.0996,  0.2906,\n",
      "           0.1051, -0.0671, -0.1699, -0.1627],\n",
      "         [ 0.2131, -0.1331, -0.1700,  0.8434, -0.1259,  0.3764, -0.0893,\n",
      "          -0.1143,  0.3016,  0.4515,  0.1894, -0.1556, -0.1155,  0.1806,\n",
      "          -0.0723,  0.5030,  0.3395,  0.1988,  0.5013, -0.1188,  0.0442,\n",
      "           0.3643, -0.1691, -0.1666, -0.0949,  0.0508, -0.1688,  0.3330,\n",
      "          -0.0324,  0.1185,  0.1666,  0.5527],\n",
      "         [ 0.0325,  0.2380,  0.1770, -0.0954, -0.1668,  0.0355, -0.1223,\n",
      "           0.6094, -0.1246, -0.1695, -0.1093,  0.4301, -0.1563,  0.1514,\n",
      "          -0.1102, -0.1346, -0.1542,  0.6290, -0.0184, -0.0460, -0.1550,\n",
      "          -0.1366, -0.1511, -0.0047,  0.3810, -0.1111, -0.1685,  0.1031,\n",
      "           1.6978, -0.1628,  0.3500, -0.0684]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2822, -0.1828, -0.2777,  0.1082,  0.2238, -0.1134, -0.3103,  0.1056,\n",
      "         -0.2703,  0.3090, -0.0819, -0.3297, -0.3104,  0.3282,  0.1361,  0.2350,\n",
      "          0.0163, -0.0788, -0.0889, -0.1623,  0.0625,  0.2353,  0.1543, -0.2189,\n",
      "         -0.2807, -0.1850, -0.1731, -0.0408, -0.0843,  0.1762,  0.1462, -0.2785],\n",
      "        [ 0.2543, -0.0184, -0.1636,  0.0951, -0.0598, -0.1587,  0.1128, -0.3349,\n",
      "          0.2041, -0.2667,  0.2606, -0.2418, -0.1451, -0.0544,  0.0391,  0.2140,\n",
      "          0.1148,  0.2194,  0.3487, -0.2808,  0.2093,  0.1741, -0.3231, -0.3367,\n",
      "          0.1741,  0.0723,  0.3435,  0.3155, -0.2377,  0.0187,  0.2241, -0.0306],\n",
      "        [ 0.0182,  0.2018, -0.3436, -0.1494, -0.1850,  0.3460, -0.1126, -0.1586,\n",
      "          0.2040,  0.2067, -0.0948,  0.0301, -0.2446, -0.0837,  0.3216, -0.3421,\n",
      "         -0.0927, -0.1508, -0.0646, -0.1180, -0.2482,  0.1263, -0.2934, -0.0028,\n",
      "          0.2603, -0.0577, -0.2085,  0.1523, -0.0853, -0.2301,  0.3381,  0.3102],\n",
      "        [-0.1570, -0.2126, -0.0957,  0.1882,  0.2634, -0.1622, -0.0596,  0.1292,\n",
      "         -0.2789, -0.1275, -0.1206, -0.2811,  0.2709, -0.2125,  0.2594,  0.0267,\n",
      "          0.2853, -0.2005,  0.3447,  0.2926, -0.2513, -0.2647, -0.2399,  0.0386,\n",
      "         -0.1857, -0.2409, -0.0541, -0.3132,  0.0247, -0.2355,  0.2472, -0.0075],\n",
      "        [ 0.1237, -0.2308,  0.1627, -0.1708, -0.1267,  0.3394,  0.0920,  0.3374,\n",
      "         -0.0833,  0.3465,  0.0505,  0.2654, -0.1465,  0.1833,  0.2793,  0.0661,\n",
      "         -0.2190,  0.0816,  0.2301, -0.2003,  0.3257,  0.2920, -0.3441,  0.0195,\n",
      "         -0.2930,  0.1064,  0.3039, -0.0885, -0.0492,  0.2339,  0.1432, -0.2975],\n",
      "        [-0.3399, -0.1084,  0.0510,  0.1905,  0.0075, -0.0773,  0.0372,  0.1646,\n",
      "         -0.0846,  0.1196,  0.2669, -0.2610, -0.3391,  0.3039,  0.3301, -0.0300,\n",
      "          0.0947,  0.0918, -0.1067,  0.1715, -0.0934,  0.2799,  0.1185, -0.0652,\n",
      "         -0.2257,  0.3120, -0.0038, -0.0091, -0.2985,  0.2226, -0.2071,  0.0148],\n",
      "        [ 0.0602,  0.1339,  0.0864, -0.2389, -0.0267,  0.3286, -0.0006, -0.1042,\n",
      "         -0.2978, -0.1909,  0.2360, -0.1668,  0.3218, -0.1158, -0.2307,  0.3201,\n",
      "          0.1812, -0.3420,  0.2597,  0.0130,  0.0328,  0.1331, -0.3222, -0.1871,\n",
      "          0.3457,  0.1235, -0.1492,  0.2934,  0.0940,  0.0699, -0.2982,  0.1518],\n",
      "        [ 0.1666,  0.0884,  0.2785,  0.0009, -0.2720,  0.0974, -0.0325,  0.2428,\n",
      "          0.1092,  0.2763, -0.3083, -0.3298, -0.2186,  0.2330,  0.2474, -0.0754,\n",
      "         -0.0026,  0.2785,  0.1558, -0.2691,  0.3140, -0.1810,  0.0921,  0.2948,\n",
      "          0.2219,  0.0859,  0.1021, -0.1126, -0.1979, -0.2360, -0.1078, -0.1703]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([8, 32])\n",
      "tensor([[ 0.2822, -0.1828, -0.2777,  0.1082,  0.2238, -0.1134, -0.3103,  0.1056,\n",
      "         -0.2703,  0.3090, -0.0819, -0.3297, -0.3104,  0.3282,  0.1361,  0.2350,\n",
      "          0.0163, -0.0788, -0.0889, -0.1623,  0.0625,  0.2353,  0.1543, -0.2189,\n",
      "         -0.2807, -0.1850, -0.1731, -0.0408, -0.0843,  0.1762,  0.1462, -0.2785],\n",
      "        [ 0.2543, -0.0184, -0.1636,  0.0951, -0.0598, -0.1587,  0.1128, -0.3349,\n",
      "          0.2041, -0.2667,  0.2606, -0.2418, -0.1451, -0.0544,  0.0391,  0.2140,\n",
      "          0.1148,  0.2194,  0.3487, -0.2808,  0.2093,  0.1741, -0.3231, -0.3367,\n",
      "          0.1741,  0.0723,  0.3435,  0.3155, -0.2377,  0.0187,  0.2241, -0.0306],\n",
      "        [ 0.0182,  0.2018, -0.3436, -0.1494, -0.1850,  0.3460, -0.1126, -0.1586,\n",
      "          0.2040,  0.2067, -0.0948,  0.0301, -0.2446, -0.0837,  0.3216, -0.3421,\n",
      "         -0.0927, -0.1508, -0.0646, -0.1180, -0.2482,  0.1263, -0.2934, -0.0028,\n",
      "          0.2603, -0.0577, -0.2085,  0.1523, -0.0853, -0.2301,  0.3381,  0.3102],\n",
      "        [-0.1570, -0.2126, -0.0957,  0.1882,  0.2634, -0.1622, -0.0596,  0.1292,\n",
      "         -0.2789, -0.1275, -0.1206, -0.2811,  0.2709, -0.2125,  0.2594,  0.0267,\n",
      "          0.2853, -0.2005,  0.3447,  0.2926, -0.2513, -0.2647, -0.2399,  0.0386,\n",
      "         -0.1857, -0.2409, -0.0541, -0.3132,  0.0247, -0.2355,  0.2472, -0.0075],\n",
      "        [ 0.1237, -0.2308,  0.1627, -0.1708, -0.1267,  0.3394,  0.0920,  0.3374,\n",
      "         -0.0833,  0.3465,  0.0505,  0.2654, -0.1465,  0.1833,  0.2793,  0.0661,\n",
      "         -0.2190,  0.0816,  0.2301, -0.2003,  0.3257,  0.2920, -0.3441,  0.0195,\n",
      "         -0.2930,  0.1064,  0.3039, -0.0885, -0.0492,  0.2339,  0.1432, -0.2975],\n",
      "        [-0.3399, -0.1084,  0.0510,  0.1905,  0.0075, -0.0773,  0.0372,  0.1646,\n",
      "         -0.0846,  0.1196,  0.2669, -0.2610, -0.3391,  0.3039,  0.3301, -0.0300,\n",
      "          0.0947,  0.0918, -0.1067,  0.1715, -0.0934,  0.2799,  0.1185, -0.0652,\n",
      "         -0.2257,  0.3120, -0.0038, -0.0091, -0.2985,  0.2226, -0.2071,  0.0148],\n",
      "        [ 0.0602,  0.1339,  0.0864, -0.2389, -0.0267,  0.3286, -0.0006, -0.1042,\n",
      "         -0.2978, -0.1909,  0.2360, -0.1668,  0.3218, -0.1158, -0.2307,  0.3201,\n",
      "          0.1812, -0.3420,  0.2597,  0.0130,  0.0328,  0.1331, -0.3222, -0.1871,\n",
      "          0.3457,  0.1235, -0.1492,  0.2934,  0.0940,  0.0699, -0.2982,  0.1518],\n",
      "        [ 0.1666,  0.0884,  0.2785,  0.0009, -0.2720,  0.0974, -0.0325,  0.2428,\n",
      "          0.1092,  0.2763, -0.3083, -0.3298, -0.2186,  0.2330,  0.2474, -0.0754,\n",
      "         -0.0026,  0.2785,  0.1558, -0.2691,  0.3140, -0.1810,  0.0921,  0.2948,\n",
      "          0.2219,  0.0859,  0.1021, -0.1126, -0.1979, -0.2360, -0.1078, -0.1703]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bup: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([ 0.1502, -0.2245, -0.3040, -0.0625,  0.0431,  0.1823, -0.0395,  0.3423,\n",
      "         0.1442,  0.0085,  0.0481, -0.1862, -0.1928, -0.2941,  0.1352,  0.0659,\n",
      "        -0.2695, -0.2388,  0.1631,  0.3170, -0.2047, -0.2018, -0.0727,  0.0341,\n",
      "        -0.1847,  0.2189, -0.3191, -0.0122,  0.0883, -0.2231,  0.3436,  0.0368],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([32])\n",
      "tensor([ 0.1502, -0.2245, -0.3040, -0.0625,  0.0431,  0.1823, -0.0395,  0.3423,\n",
      "         0.1442,  0.0085,  0.0481, -0.1862, -0.1928, -0.2941,  0.1352,  0.0659,\n",
      "        -0.2695, -0.2388,  0.1631,  0.3170, -0.2047, -0.2018, -0.0727,  0.0341,\n",
      "        -0.1847,  0.2189, -0.3191, -0.0122,  0.0883, -0.2231,  0.3436,  0.0368],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 32])\n",
      "tensor([[[-0.5701, -0.7720, -0.5000, -0.0121,  0.3543,  0.1466,  0.2084,\n",
      "           0.6958,  0.2192,  0.2905,  0.2856,  1.2496, -0.2824, -0.3776,\n",
      "           0.3996, -0.4975, -0.8504, -0.3044, -0.4732,  0.9211, -0.7085,\n",
      "           0.1638, -0.0434,  0.1177, -1.4890,  0.1594, -0.1359, -0.5359,\n",
      "           0.3775,  0.3521,  0.8547, -0.0073],\n",
      "         [ 0.6237,  0.4765, -1.2179, -0.7586, -0.2242,  1.2931, -0.5371,\n",
      "          -0.3594,  0.0890,  0.1737, -0.0038, -0.4307, -0.2068, -0.6853,\n",
      "           0.2128, -0.0717, -0.2191, -1.2194,  0.2443, -0.0667, -0.7295,\n",
      "           0.1981, -1.0012, -0.3865,  0.9654, -0.0533, -1.2349,  0.8208,\n",
      "           0.2125, -0.6761,  0.7848,  0.8876],\n",
      "         [-0.5578, -0.2397,  0.4068,  0.1304,  0.2970, -0.2837,  0.6314,\n",
      "           0.0025, -0.2210, -1.5223,  0.8993, -0.4331,  1.1561, -1.0598,\n",
      "          -0.6643,  0.7041,  0.5318, -0.4112,  0.9513,  1.1525, -0.3272,\n",
      "          -0.6602, -0.4242, -0.1663,  0.1668,  0.6708,  0.0605,  0.1835,\n",
      "           0.2738, -0.1695, -0.5518,  0.4028]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 32])\n",
      "tensor([[[-1.2022e-01,  1.2876e-01,  7.9608e-02, -4.1760e-04, -5.8484e-02,\n",
      "           1.1627e-01,  1.0246e-01, -8.1399e-02, -8.3592e-04,  3.1778e-02,\n",
      "           9.9670e-02, -1.8075e-01, -4.3981e-02,  6.0861e-02,  2.4976e-01,\n",
      "           4.8190e-02,  1.1035e-01,  4.3225e-02,  7.9782e-02, -7.9476e-02,\n",
      "          -3.1479e-02,  7.1723e-02, -9.3330e-03, -1.9835e-02,  6.8502e-02,\n",
      "           1.0049e-01, -1.4945e-01, -1.5571e-01,  3.9675e-02, -2.3615e-02,\n",
      "          -1.4523e-01,  1.1871e-03],\n",
      "         [ 1.3292e-01, -6.3416e-02,  2.0701e-01, -6.3986e-01,  2.8238e-02,\n",
      "           4.8668e-01,  4.7974e-02,  4.1091e-02,  2.6855e-02,  7.8418e-02,\n",
      "          -7.2721e-04,  6.7024e-02,  2.3883e-02, -1.2379e-01, -1.5395e-02,\n",
      "          -3.6073e-02, -7.4388e-02, -2.4237e-01,  1.2250e-01,  7.9185e-03,\n",
      "          -3.2272e-02,  7.2144e-02,  1.6928e-01,  6.4381e-02, -9.1586e-02,\n",
      "          -2.7072e-03,  2.0849e-01,  2.7330e-01, -6.8816e-03, -8.0141e-02,\n",
      "           1.3077e-01,  4.9062e-01],\n",
      "         [-1.8145e-02, -5.7046e-02,  7.1981e-02, -1.2438e-02, -4.9538e-02,\n",
      "          -1.0061e-02, -7.7198e-02,  1.5089e-03,  2.7522e-02,  2.5808e-01,\n",
      "          -9.8302e-02, -1.8627e-01, -1.8067e-01, -1.6046e-01,  7.3233e-02,\n",
      "          -9.4765e-02, -8.2013e-02, -2.5866e-01, -1.7513e-02, -5.2996e-02,\n",
      "           5.0704e-02,  9.0188e-02,  6.4099e-02,  7.8839e-04,  6.3543e-02,\n",
      "          -7.4543e-02, -1.0183e-02,  1.8916e-02,  4.6490e-01,  2.7590e-02,\n",
      "          -1.9312e-01, -2.7533e-02]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([32, 8])\n",
      "Parameter containing:\n",
      "tensor([[-6.2705e-03,  1.7087e-01, -6.2752e-02, -6.2246e-02,  6.8582e-02,\n",
      "         -1.3300e-01, -2.4336e-02, -8.4735e-02],\n",
      "        [ 5.1137e-02,  3.8288e-02, -1.0181e-01,  1.6525e-01,  1.1211e-01,\n",
      "         -1.5877e-01, -1.6302e-01,  1.3192e-01],\n",
      "        [-1.6308e-01,  8.1501e-04,  1.6355e-01, -9.2252e-02, -5.5730e-02,\n",
      "          6.0194e-02,  8.2182e-02, -4.9122e-02],\n",
      "        [ 1.6523e-01,  1.2935e-01,  6.9711e-02, -1.1357e-01,  4.4770e-02,\n",
      "          4.7724e-02,  3.9811e-02, -1.9432e-02],\n",
      "        [ 1.7026e-01, -1.0126e-01, -8.7763e-02,  2.3960e-02, -9.2687e-02,\n",
      "          3.1562e-02, -4.2784e-02,  5.0818e-02],\n",
      "        [-5.0923e-02, -1.4666e-01, -8.2458e-02,  4.8273e-02,  7.0162e-02,\n",
      "         -5.5578e-02, -4.7904e-02,  1.6376e-01],\n",
      "        [ 5.9954e-02,  6.3359e-02,  1.6240e-01, -1.2038e-01, -1.4918e-01,\n",
      "          8.5838e-02,  1.2609e-01, -9.7529e-02],\n",
      "        [ 1.0761e-03,  1.7226e-01, -9.8626e-02,  6.3615e-02, -6.2643e-02,\n",
      "         -1.6054e-02,  1.5028e-01,  8.0325e-02],\n",
      "        [ 1.2182e-01, -2.9880e-02,  3.4341e-03, -1.7226e-01, -1.6388e-01,\n",
      "         -1.4763e-01, -6.8185e-02, -6.8890e-02],\n",
      "        [-4.9057e-02,  1.6263e-01,  3.7982e-02, -1.1945e-01,  1.4013e-01,\n",
      "         -7.0778e-03,  2.2981e-02, -1.3161e-03],\n",
      "        [ 1.0184e-01, -1.0099e-01, -1.1654e-01,  6.7053e-02, -1.2938e-01,\n",
      "          1.6578e-01,  7.8409e-02,  1.1408e-01],\n",
      "        [-1.6861e-02, -1.1192e-01,  7.3365e-02,  7.6899e-02,  7.7671e-02,\n",
      "         -1.5597e-01, -1.4244e-01,  4.3034e-02],\n",
      "        [ 1.4185e-01, -2.1652e-02,  4.7518e-02,  3.3936e-03,  9.0820e-02,\n",
      "          3.6127e-03, -1.4041e-01, -5.4591e-02],\n",
      "        [ 1.4952e-01,  1.2989e-01, -1.3148e-01,  1.4601e-02, -1.3439e-01,\n",
      "          1.2325e-01, -1.3866e-01, -4.2996e-02],\n",
      "        [ 1.4415e-01, -5.4852e-02,  1.3252e-01, -1.6311e-01,  4.0809e-02,\n",
      "          1.6915e-01, -9.0345e-03, -1.3468e-01],\n",
      "        [ 1.0943e-01,  8.3819e-02,  5.1325e-02,  4.0454e-02, -9.2413e-02,\n",
      "         -2.4211e-02, -3.0873e-02, -1.0869e-01],\n",
      "        [ 1.5980e-01, -1.2678e-01,  5.3513e-03, -1.3808e-01, -1.5095e-01,\n",
      "         -1.0633e-02, -1.2122e-01,  8.8511e-02],\n",
      "        [-5.6132e-02, -1.4101e-01,  9.2021e-02, -5.3810e-02,  2.5294e-02,\n",
      "         -1.6808e-01,  5.3713e-02,  1.3115e-02],\n",
      "        [ 1.7022e-01, -7.4945e-02, -1.0871e-01, -1.4303e-01, -6.4070e-02,\n",
      "         -3.8516e-02,  1.6345e-01, -1.8564e-02],\n",
      "        [ 1.8691e-02,  1.5467e-01, -6.3141e-02, -1.2526e-01, -1.3534e-01,\n",
      "         -1.6311e-01, -1.5884e-01, -2.2790e-02],\n",
      "        [ 1.5216e-01, -5.5006e-03, -8.8667e-02,  1.7551e-01, -2.6523e-02,\n",
      "         -4.6100e-03,  6.4389e-02, -4.1098e-02],\n",
      "        [-1.2356e-01,  7.0946e-02, -4.6502e-02, -1.2491e-01,  1.3155e-01,\n",
      "          1.6781e-01,  8.3644e-02,  8.5635e-02],\n",
      "        [ 1.0658e-01,  8.7222e-02, -9.4226e-02,  1.8287e-02, -1.5606e-01,\n",
      "          7.5877e-02,  1.1103e-01,  1.3385e-01],\n",
      "        [ 6.6123e-03, -1.1708e-01, -5.6805e-02, -4.0316e-02,  1.8463e-03,\n",
      "          6.6550e-03, -1.6498e-01, -2.5916e-02],\n",
      "        [-8.2317e-02, -1.5264e-01,  6.5833e-02, -1.4837e-01,  1.1552e-01,\n",
      "          1.0273e-01,  1.3319e-01, -1.3248e-01],\n",
      "        [ 5.7529e-02, -1.6429e-01, -1.4998e-02,  7.5544e-02,  2.3620e-02,\n",
      "         -1.8969e-02, -9.7055e-02,  4.2203e-02],\n",
      "        [ 8.1459e-03, -8.3137e-02,  1.4563e-01, -2.7819e-02,  1.3285e-01,\n",
      "          1.0402e-02, -7.8985e-03, -1.0396e-01],\n",
      "        [ 1.0753e-01,  9.4934e-02, -1.0795e-01, -2.9810e-02, -8.4199e-02,\n",
      "         -1.1534e-01, -1.5309e-01, -1.4743e-01],\n",
      "        [ 1.3129e-02, -2.3096e-02,  2.9924e-05,  6.5598e-03, -6.9706e-02,\n",
      "         -5.7110e-02,  1.0288e-01, -1.6200e-01],\n",
      "        [ 8.2480e-02, -1.4823e-01, -2.3083e-02, -3.6991e-03, -1.0295e-01,\n",
      "         -1.3000e-01, -3.3539e-02,  6.8534e-02],\n",
      "        [ 1.6094e-02, -1.3947e-01, -1.4683e-01, -1.3126e-01,  1.2650e-01,\n",
      "          1.1030e-01,  8.9424e-02,  1.3378e-01],\n",
      "        [-1.4624e-02, -1.0183e-02, -1.0338e-01,  8.8699e-02,  1.1455e-01,\n",
      "         -1.1869e-01, -6.4768e-02, -1.7289e-01]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([32, 8])\n",
      "tensor([[-6.2705e-03,  1.7087e-01, -6.2752e-02, -6.2246e-02,  6.8582e-02,\n",
      "         -1.3300e-01, -2.4336e-02, -8.4735e-02],\n",
      "        [ 5.1137e-02,  3.8288e-02, -1.0181e-01,  1.6525e-01,  1.1211e-01,\n",
      "         -1.5877e-01, -1.6302e-01,  1.3192e-01],\n",
      "        [-1.6308e-01,  8.1501e-04,  1.6355e-01, -9.2252e-02, -5.5730e-02,\n",
      "          6.0194e-02,  8.2182e-02, -4.9122e-02],\n",
      "        [ 1.6523e-01,  1.2935e-01,  6.9711e-02, -1.1357e-01,  4.4770e-02,\n",
      "          4.7724e-02,  3.9811e-02, -1.9432e-02],\n",
      "        [ 1.7026e-01, -1.0126e-01, -8.7763e-02,  2.3960e-02, -9.2687e-02,\n",
      "          3.1562e-02, -4.2784e-02,  5.0818e-02],\n",
      "        [-5.0923e-02, -1.4666e-01, -8.2458e-02,  4.8273e-02,  7.0162e-02,\n",
      "         -5.5578e-02, -4.7904e-02,  1.6376e-01],\n",
      "        [ 5.9954e-02,  6.3359e-02,  1.6240e-01, -1.2038e-01, -1.4918e-01,\n",
      "          8.5838e-02,  1.2609e-01, -9.7529e-02],\n",
      "        [ 1.0761e-03,  1.7226e-01, -9.8626e-02,  6.3615e-02, -6.2643e-02,\n",
      "         -1.6054e-02,  1.5028e-01,  8.0325e-02],\n",
      "        [ 1.2182e-01, -2.9880e-02,  3.4341e-03, -1.7226e-01, -1.6388e-01,\n",
      "         -1.4763e-01, -6.8185e-02, -6.8890e-02],\n",
      "        [-4.9057e-02,  1.6263e-01,  3.7982e-02, -1.1945e-01,  1.4013e-01,\n",
      "         -7.0778e-03,  2.2981e-02, -1.3161e-03],\n",
      "        [ 1.0184e-01, -1.0099e-01, -1.1654e-01,  6.7053e-02, -1.2938e-01,\n",
      "          1.6578e-01,  7.8409e-02,  1.1408e-01],\n",
      "        [-1.6861e-02, -1.1192e-01,  7.3365e-02,  7.6899e-02,  7.7671e-02,\n",
      "         -1.5597e-01, -1.4244e-01,  4.3034e-02],\n",
      "        [ 1.4185e-01, -2.1652e-02,  4.7518e-02,  3.3936e-03,  9.0820e-02,\n",
      "          3.6127e-03, -1.4041e-01, -5.4591e-02],\n",
      "        [ 1.4952e-01,  1.2989e-01, -1.3148e-01,  1.4601e-02, -1.3439e-01,\n",
      "          1.2325e-01, -1.3866e-01, -4.2996e-02],\n",
      "        [ 1.4415e-01, -5.4852e-02,  1.3252e-01, -1.6311e-01,  4.0809e-02,\n",
      "          1.6915e-01, -9.0345e-03, -1.3468e-01],\n",
      "        [ 1.0943e-01,  8.3819e-02,  5.1325e-02,  4.0454e-02, -9.2413e-02,\n",
      "         -2.4211e-02, -3.0873e-02, -1.0869e-01],\n",
      "        [ 1.5980e-01, -1.2678e-01,  5.3513e-03, -1.3808e-01, -1.5095e-01,\n",
      "         -1.0633e-02, -1.2122e-01,  8.8511e-02],\n",
      "        [-5.6132e-02, -1.4101e-01,  9.2021e-02, -5.3810e-02,  2.5294e-02,\n",
      "         -1.6808e-01,  5.3713e-02,  1.3115e-02],\n",
      "        [ 1.7022e-01, -7.4945e-02, -1.0871e-01, -1.4303e-01, -6.4070e-02,\n",
      "         -3.8516e-02,  1.6345e-01, -1.8564e-02],\n",
      "        [ 1.8691e-02,  1.5467e-01, -6.3141e-02, -1.2526e-01, -1.3534e-01,\n",
      "         -1.6311e-01, -1.5884e-01, -2.2790e-02],\n",
      "        [ 1.5216e-01, -5.5006e-03, -8.8667e-02,  1.7551e-01, -2.6523e-02,\n",
      "         -4.6100e-03,  6.4389e-02, -4.1098e-02],\n",
      "        [-1.2356e-01,  7.0946e-02, -4.6502e-02, -1.2491e-01,  1.3155e-01,\n",
      "          1.6781e-01,  8.3644e-02,  8.5635e-02],\n",
      "        [ 1.0658e-01,  8.7222e-02, -9.4226e-02,  1.8287e-02, -1.5606e-01,\n",
      "          7.5877e-02,  1.1103e-01,  1.3385e-01],\n",
      "        [ 6.6123e-03, -1.1708e-01, -5.6805e-02, -4.0316e-02,  1.8463e-03,\n",
      "          6.6550e-03, -1.6498e-01, -2.5916e-02],\n",
      "        [-8.2317e-02, -1.5264e-01,  6.5833e-02, -1.4837e-01,  1.1552e-01,\n",
      "          1.0273e-01,  1.3319e-01, -1.3248e-01],\n",
      "        [ 5.7529e-02, -1.6429e-01, -1.4998e-02,  7.5544e-02,  2.3620e-02,\n",
      "         -1.8969e-02, -9.7055e-02,  4.2203e-02],\n",
      "        [ 8.1459e-03, -8.3137e-02,  1.4563e-01, -2.7819e-02,  1.3285e-01,\n",
      "          1.0402e-02, -7.8985e-03, -1.0396e-01],\n",
      "        [ 1.0753e-01,  9.4934e-02, -1.0795e-01, -2.9810e-02, -8.4199e-02,\n",
      "         -1.1534e-01, -1.5309e-01, -1.4743e-01],\n",
      "        [ 1.3129e-02, -2.3096e-02,  2.9924e-05,  6.5598e-03, -6.9706e-02,\n",
      "         -5.7110e-02,  1.0288e-01, -1.6200e-01],\n",
      "        [ 8.2480e-02, -1.4823e-01, -2.3083e-02, -3.6991e-03, -1.0295e-01,\n",
      "         -1.3000e-01, -3.3539e-02,  6.8534e-02],\n",
      "        [ 1.6094e-02, -1.3947e-01, -1.4683e-01, -1.3126e-01,  1.2650e-01,\n",
      "          1.1030e-01,  8.9424e-02,  1.3378e-01],\n",
      "        [-1.4624e-02, -1.0183e-02, -1.0338e-01,  8.8699e-02,  1.1455e-01,\n",
      "         -1.1869e-01, -6.4768e-02, -1.7289e-01]], grad_fn=<AliasBackward0>)\n",
      "Bdown: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.0726,  0.1130, -0.0860, -0.0169, -0.1460,  0.0320,  0.0488,  0.0124],\n",
      "       requires_grad=True)\n",
      "Bdown spliced: torch.Size([8])\n",
      "tensor([-0.0726,  0.1130, -0.0860, -0.0169, -0.1460,  0.0320,  0.0488,  0.0124],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.0415,  0.0552, -0.0343, -0.0648, -0.1829,  0.1461,  0.1003,\n",
      "           0.0231],\n",
      "         [-0.2022,  0.0323, -0.2457,  0.0460, -0.0733, -0.0590, -0.0060,\n",
      "          -0.0197],\n",
      "         [-0.1684,  0.2202, -0.0520, -0.0543, -0.1417,  0.0716,  0.1935,\n",
      "          -0.1004]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[-0.5908, -0.7629,  0.0888, -0.0364,  1.1015,  0.5386, -1.3629,\n",
      "          -1.9506],\n",
      "         [ 0.7923, -0.0098,  3.1373, -0.1927, -0.6534, -1.0088,  2.2574,\n",
      "          -0.1922],\n",
      "         [-1.9700,  0.9135, -1.7519,  0.8724, -0.7584,  0.6977,  1.4785,\n",
      "          -0.5527]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.5908, -0.7629,  0.0888, -0.0364,  1.1015,  0.5386, -1.3629,\n",
      "          -1.9506],\n",
      "         [ 0.7923, -0.0098,  3.1373, -0.1927, -0.6534, -1.0088,  2.2574,\n",
      "          -0.1922],\n",
      "         [-1.9700,  0.9135, -1.7519,  0.8724, -0.7584,  0.6977,  1.4785,\n",
      "          -0.5527]]], grad_fn=<AddBackward0>)\n",
      "Level 1 from range(2)\n",
      "Model 0 from range(2)\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.4044,  0.9881,  0.9838, -2.1977],\n",
      "         [-0.7348,  1.6922, -0.8843,  0.1139],\n",
      "         [-0.7336, -0.3367, -0.9742, -1.2100]]])\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.3070,  0.7503,  0.7471, -1.6688],\n",
      "         [-0.7172,  1.6517, -0.8632,  0.1111],\n",
      "         [-0.8381, -0.3847, -1.1130, -1.3824]]])\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.3070,  0.7503,  0.7471, -1.6688],\n",
      "         [-0.7172,  1.6517, -0.8632,  0.1111],\n",
      "         [-0.8381, -0.3847, -1.1130, -1.3824]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 0\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3082, -0.0375,  0.2938,  0.0952,  0.1068,  0.2869, -0.2548,  0.2737,\n",
      "          0.1577,  0.2032, -0.1882, -0.2408,  0.0541, -0.2956, -0.3144, -0.0215],\n",
      "        [ 0.0282,  0.0842, -0.3029, -0.2693, -0.2461,  0.0440,  0.1471, -0.0685,\n",
      "         -0.2607, -0.1407,  0.2482,  0.2583, -0.0811,  0.0538,  0.3521, -0.2299],\n",
      "        [-0.2697,  0.0807,  0.0751, -0.0655,  0.1450, -0.3520,  0.2306,  0.1296,\n",
      "         -0.1562, -0.0380, -0.0221,  0.2635,  0.0980,  0.3051, -0.2555,  0.0061],\n",
      "        [ 0.1652, -0.2571,  0.2203, -0.2631, -0.0092,  0.3367, -0.3005,  0.2128,\n",
      "         -0.1101, -0.3220,  0.2759, -0.1204,  0.3024, -0.0503, -0.2147, -0.1287],\n",
      "        [ 0.0111, -0.3078, -0.0968,  0.1583,  0.1930, -0.3113, -0.2252, -0.1391,\n",
      "          0.1673, -0.2444, -0.0069,  0.0760,  0.0661,  0.0243,  0.1211,  0.3435],\n",
      "        [-0.0053, -0.2899, -0.1091, -0.0459, -0.0735, -0.0206,  0.2626,  0.0280,\n",
      "         -0.2423,  0.1941, -0.2940,  0.1764, -0.1085,  0.0123,  0.0262,  0.3460],\n",
      "        [-0.2541, -0.1558, -0.1859, -0.1539,  0.1353, -0.1152, -0.0136, -0.2143,\n",
      "         -0.0768, -0.2811,  0.2954,  0.0775, -0.1482,  0.0178, -0.2664,  0.0608],\n",
      "        [-0.2611, -0.2725,  0.0680, -0.2697, -0.1009,  0.3087,  0.1102,  0.2571,\n",
      "         -0.1993, -0.0211, -0.0661,  0.3239,  0.1822,  0.0132, -0.0324,  0.2877]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[ 0.3082, -0.0375,  0.2938,  0.0952,  0.1068,  0.2869, -0.2548,  0.2737],\n",
      "        [ 0.0282,  0.0842, -0.3029, -0.2693, -0.2461,  0.0440,  0.1471, -0.0685],\n",
      "        [-0.2697,  0.0807,  0.0751, -0.0655,  0.1450, -0.3520,  0.2306,  0.1296],\n",
      "        [ 0.1652, -0.2571,  0.2203, -0.2631, -0.0092,  0.3367, -0.3005,  0.2128],\n",
      "        [ 0.0111, -0.3078, -0.0968,  0.1583,  0.1930, -0.3113, -0.2252, -0.1391],\n",
      "        [-0.0053, -0.2899, -0.1091, -0.0459, -0.0735, -0.0206,  0.2626,  0.0280],\n",
      "        [-0.2541, -0.1558, -0.1859, -0.1539,  0.1353, -0.1152, -0.0136, -0.2143],\n",
      "        [-0.2611, -0.2725,  0.0680, -0.2697, -0.1009,  0.3087,  0.1102,  0.2571]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[ 0.1577,  0.2032, -0.1882, -0.2408],\n",
      "        [-0.2607, -0.1407,  0.2482,  0.2583],\n",
      "        [-0.1562, -0.0380, -0.0221,  0.2635],\n",
      "        [-0.1101, -0.3220,  0.2759, -0.1204],\n",
      "        [ 0.1673, -0.2444, -0.0069,  0.0760],\n",
      "        [-0.2423,  0.1941, -0.2940,  0.1764],\n",
      "        [-0.0768, -0.2811,  0.2954,  0.0775],\n",
      "        [-0.1993, -0.0211, -0.0661,  0.3239]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.0541, -0.2956, -0.3144, -0.0215],\n",
      "        [-0.0811,  0.0538,  0.3521, -0.2299],\n",
      "        [ 0.0980,  0.3051, -0.2555,  0.0061],\n",
      "        [ 0.3024, -0.0503, -0.2147, -0.1287],\n",
      "        [ 0.0661,  0.0243,  0.1211,  0.3435],\n",
      "        [-0.1085,  0.0123,  0.0262,  0.3460],\n",
      "        [-0.1482,  0.0178, -0.2664,  0.0608],\n",
      "        [ 0.1822,  0.0132, -0.0324,  0.2877]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[ 0.3082, -0.0375,  0.1068,  0.2869],\n",
      "        [ 0.0282,  0.0842, -0.2461,  0.0440],\n",
      "        [-0.2697,  0.0807,  0.1450, -0.3520],\n",
      "        [ 0.1652, -0.2571, -0.0092,  0.3367]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.1577,  0.2032],\n",
      "        [-0.2607, -0.1407],\n",
      "        [-0.1562, -0.0380],\n",
      "        [-0.1101, -0.3220]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.0541, -0.2956],\n",
      "        [-0.0811,  0.0538],\n",
      "        [ 0.0980,  0.3051],\n",
      "        [ 0.3024, -0.0503]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[ 0.3082, -0.0375,  0.1068,  0.2869,  0.1577,  0.2032,  0.0541, -0.2956],\n",
      "        [ 0.0282,  0.0842, -0.2461,  0.0440, -0.2607, -0.1407, -0.0811,  0.0538],\n",
      "        [-0.2697,  0.0807,  0.1450, -0.3520, -0.1562, -0.0380,  0.0980,  0.3051],\n",
      "        [ 0.1652, -0.2571, -0.0092,  0.3367, -0.1101, -0.3220,  0.3024, -0.0503]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.3614,  0.5410, -0.0281, -0.7037, -0.0802,  0.4657, -0.4757,\n",
      "           0.2614],\n",
      "         [ 0.0768,  0.0677, -0.6093,  0.2081, -0.4210, -0.3811, -0.2238,\n",
      "           0.0320],\n",
      "         [-0.1973,  0.2646, -0.1436, -0.3311,  0.2941,  0.3713, -0.5413,\n",
      "          -0.0429]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.3614,  0.5410, -0.0281, -0.7037],\n",
      "         [ 0.0768,  0.0677, -0.6093,  0.2081],\n",
      "         [-0.1973,  0.2646, -0.1436, -0.3311]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.0802,  0.4657],\n",
      "         [-0.4210, -0.3811],\n",
      "         [ 0.2941,  0.3713]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.4757,  0.2614],\n",
      "         [-0.2238,  0.0320],\n",
      "         [-0.5413, -0.0429]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.3614,  0.5410],\n",
      "          [-0.0281, -0.7037]],\n",
      "\n",
      "         [[ 0.0768,  0.0677],\n",
      "          [-0.6093,  0.2081]],\n",
      "\n",
      "         [[-0.1973,  0.2646],\n",
      "          [-0.1436, -0.3311]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.0802,  0.4657]],\n",
      "\n",
      "         [[-0.4210, -0.3811]],\n",
      "\n",
      "         [[ 0.2941,  0.3713]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.4757,  0.2614]],\n",
      "\n",
      "         [[-0.2238,  0.0320]],\n",
      "\n",
      "         [[-0.5413, -0.0429]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.3614,  0.5410],\n",
      "          [-0.0281, -0.7037]],\n",
      "\n",
      "         [[-0.0155,  0.1012],\n",
      "          [-0.5043, -0.4003]],\n",
      "\n",
      "         [[-0.1585, -0.2895],\n",
      "          [ 0.3608,  0.0072]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[-0.0802,  0.4657]],\n",
      "\n",
      "         [[ 0.0932, -0.5602]],\n",
      "\n",
      "         [[-0.4600,  0.1129]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.0802,  0.4657],\n",
      "          [-0.0802,  0.4657]],\n",
      "\n",
      "         [[ 0.0932, -0.5602],\n",
      "          [ 0.0932, -0.5602]],\n",
      "\n",
      "         [[-0.4600,  0.1129],\n",
      "          [-0.4600,  0.1129]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[-0.4757,  0.2614],\n",
      "          [-0.4757,  0.2614]],\n",
      "\n",
      "         [[-0.2238,  0.0320],\n",
      "          [-0.2238,  0.0320]],\n",
      "\n",
      "         [[-0.5413, -0.0429],\n",
      "          [-0.5413, -0.0429]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.3614,  0.5410],\n",
      "          [-0.0155,  0.1012],\n",
      "          [-0.1585, -0.2895]],\n",
      "\n",
      "         [[-0.0281, -0.7037],\n",
      "          [-0.5043, -0.4003],\n",
      "          [ 0.3608,  0.0072]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.0802,  0.4657],\n",
      "          [ 0.0932, -0.5602],\n",
      "          [-0.4600,  0.1129]],\n",
      "\n",
      "         [[-0.0802,  0.4657],\n",
      "          [ 0.0932, -0.5602],\n",
      "          [-0.4600,  0.1129]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.4757,  0.2614],\n",
      "          [-0.2238,  0.0320],\n",
      "          [-0.5413, -0.0429]],\n",
      "\n",
      "         [[-0.4757,  0.2614],\n",
      "          [-0.2238,  0.0320],\n",
      "          [-0.5413, -0.0429]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 0.1986, -0.2381,  0.1607],\n",
      "          [ 0.0342, -0.0411,  0.0131],\n",
      "          [-0.0863,  0.1042,  0.0284]],\n",
      "\n",
      "         [[-0.2301,  0.2769, -0.0470],\n",
      "          [-0.1032,  0.1253,  0.1321],\n",
      "          [-0.0181,  0.0209, -0.1168]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[ 1.9861e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 3.4188e-02, -4.1096e-02, -2.3820e+38],\n",
      "          [-8.6341e-02,  1.0424e-01,  2.8427e-02]],\n",
      "\n",
      "         [[-2.3013e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [-1.0320e-01,  1.2531e-01, -2.3820e+38],\n",
      "          [-1.8080e-02,  2.0928e-02, -1.1677e-01]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5188, 0.4812, 0.0000],\n",
      "          [0.3002, 0.3632, 0.3367]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4431, 0.5569, 0.0000],\n",
      "          [0.3395, 0.3530, 0.3076]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[-0.4757,  0.2614],\n",
      "          [-0.3545,  0.1510],\n",
      "          [-0.4063,  0.0756]],\n",
      "\n",
      "         [[-0.4757,  0.2614],\n",
      "          [-0.3354,  0.1336],\n",
      "          [-0.4070,  0.0868]]]], grad_fn=<UnsafeViewBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.4757,  0.2614, -0.4757,  0.2614],\n",
      "         [-0.3545,  0.1510, -0.3354,  0.1336],\n",
      "         [-0.4063,  0.0756, -0.4070,  0.0868]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1812, -0.1638, -0.1178,  0.1815, -0.0778, -0.0560,  0.3211,  0.3358],\n",
      "        [-0.1059,  0.2363,  0.1847, -0.2948,  0.2672,  0.3174,  0.3080,  0.2044],\n",
      "        [ 0.2060,  0.0377, -0.2959, -0.2276, -0.2968,  0.1777, -0.0755, -0.1395],\n",
      "        [ 0.3207,  0.0063,  0.1376, -0.2537,  0.3473, -0.3356,  0.0987, -0.2497],\n",
      "        [-0.3443,  0.2761, -0.2423, -0.0421,  0.0527, -0.2862,  0.0705,  0.1438],\n",
      "        [ 0.1588, -0.2870,  0.0738, -0.2562,  0.0793,  0.2388,  0.3065,  0.1916],\n",
      "        [ 0.0715, -0.2669,  0.0129,  0.3258, -0.1468, -0.0368, -0.2941,  0.0135],\n",
      "        [-0.0419, -0.0367, -0.1927, -0.0744,  0.0314, -0.2928,  0.1988, -0.1777]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[ 0.1812, -0.1638, -0.1178,  0.1815],\n",
      "        [-0.1059,  0.2363,  0.1847, -0.2948],\n",
      "        [-0.3443,  0.2761, -0.2423, -0.0421],\n",
      "        [ 0.1588, -0.2870,  0.0738, -0.2562]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0914, -0.0667,  0.2389, -0.2103],\n",
      "         [ 0.0565, -0.0372,  0.1608, -0.1290],\n",
      "         [ 0.0722, -0.0528,  0.1668, -0.1011]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[ 0.4957,  0.9214,  1.2227, -2.4080],\n",
      "         [-0.6784,  1.6550, -0.7235, -0.0151],\n",
      "         [-0.6614, -0.3896, -0.8074, -1.3112]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.4957,  0.9214,  1.2227, -2.4080],\n",
      "         [-0.6784,  1.6550, -0.7235, -0.0151],\n",
      "         [-0.6614, -0.3896, -0.8074, -1.3112]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.3423,  0.6363,  0.8443, -1.6629],\n",
      "         [-0.7031,  1.7155, -0.7500, -0.0157],\n",
      "         [-0.7688, -0.4528, -0.9385, -1.5242]]], grad_fn=<MulBackward0>)\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.3423,  0.6363,  0.8443, -1.6629],\n",
      "         [-0.7031,  1.7155, -0.7500, -0.0157],\n",
      "         [-0.7688, -0.4528, -0.9385, -1.5242]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.3423,  0.6363,  0.8443, -1.6629],\n",
      "         [-0.7031,  1.7155, -0.7500, -0.0157],\n",
      "         [-0.7688, -0.4528, -0.9385, -1.5242]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 4\n",
      "d_skip: 0\n",
      "i_dim: 16\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[-0.2811, -0.0270,  0.0777, -0.0039,  0.2796, -0.2670,  0.1663,  0.0266,\n",
      "          0.2387,  0.1946, -0.1248,  0.0487,  0.1504, -0.0958,  0.2620,  0.0300,\n",
      "         -0.0412,  0.1858, -0.2139, -0.2279,  0.1933, -0.0472,  0.2015, -0.1428,\n",
      "         -0.1714,  0.3420, -0.1261, -0.0445, -0.2970, -0.2236,  0.1774,  0.3273],\n",
      "        [-0.0559, -0.1062,  0.2060, -0.3148, -0.0610,  0.0727, -0.2189, -0.2999,\n",
      "         -0.0502,  0.1810,  0.1343,  0.0851, -0.0477,  0.0827,  0.0995,  0.0167,\n",
      "         -0.1973,  0.0140,  0.1064, -0.0729,  0.1845, -0.3449, -0.2266, -0.0014,\n",
      "         -0.3306, -0.1396, -0.3074, -0.2133,  0.3525, -0.0846, -0.3425,  0.1338],\n",
      "        [ 0.3045, -0.1883, -0.2371,  0.3285, -0.0387,  0.3390,  0.3480, -0.2650,\n",
      "         -0.0216,  0.2641,  0.2369, -0.1652, -0.3093,  0.0436, -0.1806,  0.2700,\n",
      "          0.2034, -0.2746,  0.3244, -0.0298,  0.1362,  0.2964, -0.2646, -0.0550,\n",
      "         -0.1278,  0.2398, -0.0377,  0.0583, -0.2364,  0.3412, -0.2262,  0.1816],\n",
      "        [ 0.2170,  0.0379, -0.0893, -0.0081,  0.2499, -0.3397,  0.2077,  0.3244,\n",
      "         -0.2439, -0.2049,  0.1377,  0.3533, -0.2209,  0.0883, -0.0685,  0.2863,\n",
      "         -0.0748,  0.2120, -0.0254, -0.1597,  0.0182, -0.3100, -0.2707,  0.0629,\n",
      "          0.1587,  0.1991, -0.1898,  0.2316,  0.1283, -0.1907,  0.0939,  0.1159],\n",
      "        [ 0.0412, -0.1712,  0.0801, -0.1478, -0.2345, -0.0489,  0.1414, -0.0028,\n",
      "          0.0027,  0.0525, -0.1322, -0.0448,  0.2901, -0.2900,  0.0118, -0.1953,\n",
      "          0.0151, -0.1399,  0.1249,  0.3276, -0.0825,  0.1432,  0.2753,  0.2337,\n",
      "          0.0480,  0.3424,  0.0860, -0.2939,  0.0054, -0.1741, -0.3390, -0.2313],\n",
      "        [ 0.0542,  0.0592,  0.3068,  0.1669, -0.2551,  0.3092,  0.3315,  0.1246,\n",
      "         -0.0527,  0.1310, -0.3207,  0.2923,  0.2547, -0.2331, -0.3039, -0.3103,\n",
      "         -0.0332, -0.2420,  0.1207, -0.0299,  0.0884, -0.1770, -0.3310,  0.1731,\n",
      "          0.0462, -0.0148, -0.1853, -0.3507, -0.0339, -0.1803, -0.0213,  0.2568],\n",
      "        [-0.1879,  0.0668,  0.1165,  0.2397, -0.2110, -0.0725, -0.3138,  0.2530,\n",
      "          0.2190, -0.1220, -0.3086,  0.2517, -0.1614,  0.0849, -0.3084,  0.0757,\n",
      "          0.0819,  0.2163,  0.2924,  0.0604, -0.1102, -0.3133, -0.3265,  0.1126,\n",
      "          0.3116, -0.2196, -0.3490, -0.1459,  0.2048, -0.3398,  0.3354,  0.3366],\n",
      "        [ 0.2473,  0.2045,  0.0686, -0.1509,  0.2188, -0.2802, -0.0783,  0.1187,\n",
      "         -0.2610,  0.0220, -0.1511,  0.2359,  0.0023, -0.0620, -0.2135, -0.2747,\n",
      "          0.2094,  0.0222,  0.1783,  0.3206, -0.2119,  0.2887,  0.1856,  0.3166,\n",
      "         -0.0897, -0.1708, -0.3043, -0.1225, -0.1501,  0.2560,  0.0135, -0.3519]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([4, 16])\n",
      "tensor([[-0.2811, -0.0270,  0.0777, -0.0039,  0.2796, -0.2670,  0.1663,  0.0266,\n",
      "          0.2387,  0.1946, -0.1248,  0.0487,  0.1504, -0.0958,  0.2620,  0.0300],\n",
      "        [-0.0559, -0.1062,  0.2060, -0.3148, -0.0610,  0.0727, -0.2189, -0.2999,\n",
      "         -0.0502,  0.1810,  0.1343,  0.0851, -0.0477,  0.0827,  0.0995,  0.0167],\n",
      "        [ 0.3045, -0.1883, -0.2371,  0.3285, -0.0387,  0.3390,  0.3480, -0.2650,\n",
      "         -0.0216,  0.2641,  0.2369, -0.1652, -0.3093,  0.0436, -0.1806,  0.2700],\n",
      "        [ 0.2170,  0.0379, -0.0893, -0.0081,  0.2499, -0.3397,  0.2077,  0.3244,\n",
      "         -0.2439, -0.2049,  0.1377,  0.3533, -0.2209,  0.0883, -0.0685,  0.2863]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([ 0.2344, -0.0453, -0.2316, -0.0515, -0.3145,  0.1585, -0.3512,  0.0260,\n",
      "        -0.0817,  0.1658,  0.0793, -0.3017, -0.3280, -0.1102,  0.3004, -0.3055,\n",
      "        -0.0573,  0.3171, -0.1961, -0.0773, -0.1722,  0.2801, -0.0123, -0.3422,\n",
      "        -0.2469, -0.2034, -0.1560,  0.2893,  0.3363, -0.0824,  0.0287, -0.3314],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([16])\n",
      "tensor([ 0.2344, -0.0453, -0.2316, -0.0515, -0.3145,  0.1585, -0.3512,  0.0260,\n",
      "        -0.0817,  0.1658,  0.0793, -0.3017, -0.3280, -0.1102,  0.3004, -0.3055],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[-1.0474e-03, -3.4411e-01, -1.2561e-01,  3.7693e-02, -7.0583e-01,\n",
      "           9.6452e-01, -4.8503e-01, -9.1885e-01,  3.5557e-01,  9.1134e-01,\n",
      "           9.3034e-02, -9.5775e-01, -2.0074e-01, -2.0033e-01,  4.1488e-01,\n",
      "          -5.3270e-01],\n",
      "         [ 1.0435e-01, -6.7917e-02,  2.4643e-01, -8.3504e-01, -5.9063e-01,\n",
      "           2.2196e-01, -1.1079e+00, -3.1344e-01, -3.1554e-01,  1.4453e-01,\n",
      "           2.1765e-01, -7.1555e-02, -2.8018e-01,  6.4932e-02,  4.2342e-01,\n",
      "          -5.0479e-01],\n",
      "         [-1.4065e-01,  1.4248e-01, -2.6033e-02, -2.0193e-01, -8.4642e-01,\n",
      "           5.3042e-01, -1.0230e+00, -1.0440e-01,  1.4956e-01, -1.3565e-03,\n",
      "          -3.1781e-01, -7.6104e-01,  2.0481e-01, -2.4957e-01,  3.2794e-01,\n",
      "          -1.0259e+00]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[-5.2327e-04, -1.2573e-01, -5.6529e-02,  1.9413e-02, -1.6950e-01,\n",
      "           8.0307e-01, -1.5222e-01, -1.6455e-01,  2.2718e-01,  7.4634e-01,\n",
      "           4.9965e-02, -1.6195e-01, -8.4401e-02, -8.4262e-02,  2.7419e-01,\n",
      "          -1.5828e-01],\n",
      "         [ 5.6510e-02, -3.2120e-02,  1.4720e-01, -1.6855e-01, -1.6383e-01,\n",
      "           1.3048e-01, -1.4841e-01, -1.1816e-01, -1.1870e-01,  8.0567e-02,\n",
      "           1.2757e-01, -3.3737e-02, -1.0918e-01,  3.4147e-02,  2.8115e-01,\n",
      "          -1.5490e-01],\n",
      "         [-6.2457e-02,  7.9313e-02, -1.2746e-02, -8.4809e-02, -1.6815e-01,\n",
      "           3.7240e-01, -1.5668e-01, -4.7862e-02,  8.3671e-02, -6.7750e-04,\n",
      "          -1.1928e-01, -1.6995e-01,  1.1902e-01, -1.0019e-01,  2.0612e-01,\n",
      "          -1.5642e-01]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2822, -0.1828, -0.2777,  0.1082,  0.2238, -0.1134, -0.3103,  0.1056,\n",
      "         -0.2703,  0.3090, -0.0819, -0.3297, -0.3104,  0.3282,  0.1361,  0.2350,\n",
      "          0.0163, -0.0788, -0.0889, -0.1623,  0.0625,  0.2353,  0.1543, -0.2189,\n",
      "         -0.2807, -0.1850, -0.1731, -0.0408, -0.0843,  0.1762,  0.1462, -0.2785],\n",
      "        [ 0.2543, -0.0184, -0.1636,  0.0951, -0.0598, -0.1587,  0.1128, -0.3349,\n",
      "          0.2041, -0.2667,  0.2606, -0.2418, -0.1451, -0.0544,  0.0391,  0.2140,\n",
      "          0.1148,  0.2194,  0.3487, -0.2808,  0.2093,  0.1741, -0.3231, -0.3367,\n",
      "          0.1741,  0.0723,  0.3435,  0.3155, -0.2377,  0.0187,  0.2241, -0.0306],\n",
      "        [ 0.0182,  0.2018, -0.3436, -0.1494, -0.1850,  0.3460, -0.1126, -0.1586,\n",
      "          0.2040,  0.2067, -0.0948,  0.0301, -0.2446, -0.0837,  0.3216, -0.3421,\n",
      "         -0.0927, -0.1508, -0.0646, -0.1180, -0.2482,  0.1263, -0.2934, -0.0028,\n",
      "          0.2603, -0.0577, -0.2085,  0.1523, -0.0853, -0.2301,  0.3381,  0.3102],\n",
      "        [-0.1570, -0.2126, -0.0957,  0.1882,  0.2634, -0.1622, -0.0596,  0.1292,\n",
      "         -0.2789, -0.1275, -0.1206, -0.2811,  0.2709, -0.2125,  0.2594,  0.0267,\n",
      "          0.2853, -0.2005,  0.3447,  0.2926, -0.2513, -0.2647, -0.2399,  0.0386,\n",
      "         -0.1857, -0.2409, -0.0541, -0.3132,  0.0247, -0.2355,  0.2472, -0.0075],\n",
      "        [ 0.1237, -0.2308,  0.1627, -0.1708, -0.1267,  0.3394,  0.0920,  0.3374,\n",
      "         -0.0833,  0.3465,  0.0505,  0.2654, -0.1465,  0.1833,  0.2793,  0.0661,\n",
      "         -0.2190,  0.0816,  0.2301, -0.2003,  0.3257,  0.2920, -0.3441,  0.0195,\n",
      "         -0.2930,  0.1064,  0.3039, -0.0885, -0.0492,  0.2339,  0.1432, -0.2975],\n",
      "        [-0.3399, -0.1084,  0.0510,  0.1905,  0.0075, -0.0773,  0.0372,  0.1646,\n",
      "         -0.0846,  0.1196,  0.2669, -0.2610, -0.3391,  0.3039,  0.3301, -0.0300,\n",
      "          0.0947,  0.0918, -0.1067,  0.1715, -0.0934,  0.2799,  0.1185, -0.0652,\n",
      "         -0.2257,  0.3120, -0.0038, -0.0091, -0.2985,  0.2226, -0.2071,  0.0148],\n",
      "        [ 0.0602,  0.1339,  0.0864, -0.2389, -0.0267,  0.3286, -0.0006, -0.1042,\n",
      "         -0.2978, -0.1909,  0.2360, -0.1668,  0.3218, -0.1158, -0.2307,  0.3201,\n",
      "          0.1812, -0.3420,  0.2597,  0.0130,  0.0328,  0.1331, -0.3222, -0.1871,\n",
      "          0.3457,  0.1235, -0.1492,  0.2934,  0.0940,  0.0699, -0.2982,  0.1518],\n",
      "        [ 0.1666,  0.0884,  0.2785,  0.0009, -0.2720,  0.0974, -0.0325,  0.2428,\n",
      "          0.1092,  0.2763, -0.3083, -0.3298, -0.2186,  0.2330,  0.2474, -0.0754,\n",
      "         -0.0026,  0.2785,  0.1558, -0.2691,  0.3140, -0.1810,  0.0921,  0.2948,\n",
      "          0.2219,  0.0859,  0.1021, -0.1126, -0.1979, -0.2360, -0.1078, -0.1703]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([4, 16])\n",
      "tensor([[ 0.2822, -0.1828, -0.2777,  0.1082,  0.2238, -0.1134, -0.3103,  0.1056,\n",
      "         -0.2703,  0.3090, -0.0819, -0.3297, -0.3104,  0.3282,  0.1361,  0.2350],\n",
      "        [ 0.2543, -0.0184, -0.1636,  0.0951, -0.0598, -0.1587,  0.1128, -0.3349,\n",
      "          0.2041, -0.2667,  0.2606, -0.2418, -0.1451, -0.0544,  0.0391,  0.2140],\n",
      "        [ 0.0182,  0.2018, -0.3436, -0.1494, -0.1850,  0.3460, -0.1126, -0.1586,\n",
      "          0.2040,  0.2067, -0.0948,  0.0301, -0.2446, -0.0837,  0.3216, -0.3421],\n",
      "        [-0.1570, -0.2126, -0.0957,  0.1882,  0.2634, -0.1622, -0.0596,  0.1292,\n",
      "         -0.2789, -0.1275, -0.1206, -0.2811,  0.2709, -0.2125,  0.2594,  0.0267]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([ 0.1502, -0.2245, -0.3040, -0.0625,  0.0431,  0.1823, -0.0395,  0.3423,\n",
      "         0.1442,  0.0085,  0.0481, -0.1862, -0.1928, -0.2941,  0.1352,  0.0659,\n",
      "        -0.2695, -0.2388,  0.1631,  0.3170, -0.2047, -0.2018, -0.0727,  0.0341,\n",
      "        -0.1847,  0.2189, -0.3191, -0.0122,  0.0883, -0.2231,  0.3436,  0.0368],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([16])\n",
      "tensor([ 0.1502, -0.2245, -0.3040, -0.0625,  0.0431,  0.1823, -0.0395,  0.3423,\n",
      "         0.1442,  0.0085,  0.0481, -0.1862, -0.1928, -0.2941,  0.1352,  0.0659],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.6850,  0.2250, -0.6342, -0.4041, -0.5125,  0.6044, -0.0699,\n",
      "          -0.1833,  0.8177,  0.3311,  0.3064,  0.0399, -1.0484,  0.0663,\n",
      "           0.0468, -0.0508],\n",
      "         [ 0.3768, -0.2757, -0.1303,  0.1337, -0.0822, -0.2671,  0.4576,\n",
      "          -0.1896,  0.5358, -0.8193,  0.6258, -0.3873, -0.0443, -0.5520,\n",
      "          -0.1387,  0.5239],\n",
      "         [ 0.0402,  0.0589,  0.4518, -0.3354, -0.3297,  0.2638,  0.3446,\n",
      "           0.3648,  0.4933, -0.1080,  0.2659,  0.5769, -0.0718, -0.1194,\n",
      "          -0.6843,  0.0687]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 16])\n",
      "tensor([[[-3.5842e-04, -2.8294e-02,  3.5849e-02, -7.8454e-03,  8.6865e-02,\n",
      "           4.8534e-01,  1.0639e-02,  3.0165e-02,  1.8575e-01,  2.4710e-01,\n",
      "           1.5307e-02, -6.4577e-03,  8.8486e-02, -5.5864e-03,  1.2832e-02,\n",
      "           8.0481e-03],\n",
      "         [ 2.1293e-02,  8.8552e-03, -1.9173e-02, -2.2536e-02,  1.3469e-02,\n",
      "          -3.4854e-02, -6.7910e-02,  2.2398e-02, -6.3599e-02, -6.6009e-02,\n",
      "           7.9843e-02,  1.3067e-02,  4.8410e-03, -1.8850e-02, -3.9006e-02,\n",
      "          -8.1148e-02],\n",
      "         [-2.5131e-03,  4.6693e-03, -5.7584e-03,  2.8441e-02,  5.5436e-02,\n",
      "           9.8236e-02, -5.3986e-02, -1.7461e-02,  4.1276e-02,  7.3199e-05,\n",
      "          -3.1714e-02, -9.8039e-02, -8.5494e-03,  1.1962e-02, -1.4105e-01,\n",
      "          -1.0741e-02]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([32, 8])\n",
      "Parameter containing:\n",
      "tensor([[-6.2705e-03,  1.7087e-01, -6.2752e-02, -6.2246e-02,  6.8582e-02,\n",
      "         -1.3300e-01, -2.4336e-02, -8.4735e-02],\n",
      "        [ 5.1137e-02,  3.8288e-02, -1.0181e-01,  1.6525e-01,  1.1211e-01,\n",
      "         -1.5877e-01, -1.6302e-01,  1.3192e-01],\n",
      "        [-1.6308e-01,  8.1501e-04,  1.6355e-01, -9.2252e-02, -5.5730e-02,\n",
      "          6.0194e-02,  8.2182e-02, -4.9122e-02],\n",
      "        [ 1.6523e-01,  1.2935e-01,  6.9711e-02, -1.1357e-01,  4.4770e-02,\n",
      "          4.7724e-02,  3.9811e-02, -1.9432e-02],\n",
      "        [ 1.7026e-01, -1.0126e-01, -8.7763e-02,  2.3960e-02, -9.2687e-02,\n",
      "          3.1562e-02, -4.2784e-02,  5.0818e-02],\n",
      "        [-5.0923e-02, -1.4666e-01, -8.2458e-02,  4.8273e-02,  7.0162e-02,\n",
      "         -5.5578e-02, -4.7904e-02,  1.6376e-01],\n",
      "        [ 5.9954e-02,  6.3359e-02,  1.6240e-01, -1.2038e-01, -1.4918e-01,\n",
      "          8.5838e-02,  1.2609e-01, -9.7529e-02],\n",
      "        [ 1.0761e-03,  1.7226e-01, -9.8626e-02,  6.3615e-02, -6.2643e-02,\n",
      "         -1.6054e-02,  1.5028e-01,  8.0325e-02],\n",
      "        [ 1.2182e-01, -2.9880e-02,  3.4341e-03, -1.7226e-01, -1.6388e-01,\n",
      "         -1.4763e-01, -6.8185e-02, -6.8890e-02],\n",
      "        [-4.9057e-02,  1.6263e-01,  3.7982e-02, -1.1945e-01,  1.4013e-01,\n",
      "         -7.0778e-03,  2.2981e-02, -1.3161e-03],\n",
      "        [ 1.0184e-01, -1.0099e-01, -1.1654e-01,  6.7053e-02, -1.2938e-01,\n",
      "          1.6578e-01,  7.8409e-02,  1.1408e-01],\n",
      "        [-1.6861e-02, -1.1192e-01,  7.3365e-02,  7.6899e-02,  7.7671e-02,\n",
      "         -1.5597e-01, -1.4244e-01,  4.3034e-02],\n",
      "        [ 1.4185e-01, -2.1652e-02,  4.7518e-02,  3.3936e-03,  9.0820e-02,\n",
      "          3.6127e-03, -1.4041e-01, -5.4591e-02],\n",
      "        [ 1.4952e-01,  1.2989e-01, -1.3148e-01,  1.4601e-02, -1.3439e-01,\n",
      "          1.2325e-01, -1.3866e-01, -4.2996e-02],\n",
      "        [ 1.4415e-01, -5.4852e-02,  1.3252e-01, -1.6311e-01,  4.0809e-02,\n",
      "          1.6915e-01, -9.0345e-03, -1.3468e-01],\n",
      "        [ 1.0943e-01,  8.3819e-02,  5.1325e-02,  4.0454e-02, -9.2413e-02,\n",
      "         -2.4211e-02, -3.0873e-02, -1.0869e-01],\n",
      "        [ 1.5980e-01, -1.2678e-01,  5.3513e-03, -1.3808e-01, -1.5095e-01,\n",
      "         -1.0633e-02, -1.2122e-01,  8.8511e-02],\n",
      "        [-5.6132e-02, -1.4101e-01,  9.2021e-02, -5.3810e-02,  2.5294e-02,\n",
      "         -1.6808e-01,  5.3713e-02,  1.3115e-02],\n",
      "        [ 1.7022e-01, -7.4945e-02, -1.0871e-01, -1.4303e-01, -6.4070e-02,\n",
      "         -3.8516e-02,  1.6345e-01, -1.8564e-02],\n",
      "        [ 1.8691e-02,  1.5467e-01, -6.3141e-02, -1.2526e-01, -1.3534e-01,\n",
      "         -1.6311e-01, -1.5884e-01, -2.2790e-02],\n",
      "        [ 1.5216e-01, -5.5006e-03, -8.8667e-02,  1.7551e-01, -2.6523e-02,\n",
      "         -4.6100e-03,  6.4389e-02, -4.1098e-02],\n",
      "        [-1.2356e-01,  7.0946e-02, -4.6502e-02, -1.2491e-01,  1.3155e-01,\n",
      "          1.6781e-01,  8.3644e-02,  8.5635e-02],\n",
      "        [ 1.0658e-01,  8.7222e-02, -9.4226e-02,  1.8287e-02, -1.5606e-01,\n",
      "          7.5877e-02,  1.1103e-01,  1.3385e-01],\n",
      "        [ 6.6123e-03, -1.1708e-01, -5.6805e-02, -4.0316e-02,  1.8463e-03,\n",
      "          6.6550e-03, -1.6498e-01, -2.5916e-02],\n",
      "        [-8.2317e-02, -1.5264e-01,  6.5833e-02, -1.4837e-01,  1.1552e-01,\n",
      "          1.0273e-01,  1.3319e-01, -1.3248e-01],\n",
      "        [ 5.7529e-02, -1.6429e-01, -1.4998e-02,  7.5544e-02,  2.3620e-02,\n",
      "         -1.8969e-02, -9.7055e-02,  4.2203e-02],\n",
      "        [ 8.1459e-03, -8.3137e-02,  1.4563e-01, -2.7819e-02,  1.3285e-01,\n",
      "          1.0402e-02, -7.8985e-03, -1.0396e-01],\n",
      "        [ 1.0753e-01,  9.4934e-02, -1.0795e-01, -2.9810e-02, -8.4199e-02,\n",
      "         -1.1534e-01, -1.5309e-01, -1.4743e-01],\n",
      "        [ 1.3129e-02, -2.3096e-02,  2.9924e-05,  6.5598e-03, -6.9706e-02,\n",
      "         -5.7110e-02,  1.0288e-01, -1.6200e-01],\n",
      "        [ 8.2480e-02, -1.4823e-01, -2.3083e-02, -3.6991e-03, -1.0295e-01,\n",
      "         -1.3000e-01, -3.3539e-02,  6.8534e-02],\n",
      "        [ 1.6094e-02, -1.3947e-01, -1.4683e-01, -1.3126e-01,  1.2650e-01,\n",
      "          1.1030e-01,  8.9424e-02,  1.3378e-01],\n",
      "        [-1.4624e-02, -1.0183e-02, -1.0338e-01,  8.8699e-02,  1.1455e-01,\n",
      "         -1.1869e-01, -6.4768e-02, -1.7289e-01]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([16, 4])\n",
      "tensor([[-0.0063,  0.1709, -0.0628, -0.0622],\n",
      "        [ 0.0511,  0.0383, -0.1018,  0.1653],\n",
      "        [-0.1631,  0.0008,  0.1636, -0.0923],\n",
      "        [ 0.1652,  0.1294,  0.0697, -0.1136],\n",
      "        [ 0.1703, -0.1013, -0.0878,  0.0240],\n",
      "        [-0.0509, -0.1467, -0.0825,  0.0483],\n",
      "        [ 0.0600,  0.0634,  0.1624, -0.1204],\n",
      "        [ 0.0011,  0.1723, -0.0986,  0.0636],\n",
      "        [ 0.1218, -0.0299,  0.0034, -0.1723],\n",
      "        [-0.0491,  0.1626,  0.0380, -0.1194],\n",
      "        [ 0.1018, -0.1010, -0.1165,  0.0671],\n",
      "        [-0.0169, -0.1119,  0.0734,  0.0769],\n",
      "        [ 0.1418, -0.0217,  0.0475,  0.0034],\n",
      "        [ 0.1495,  0.1299, -0.1315,  0.0146],\n",
      "        [ 0.1442, -0.0549,  0.1325, -0.1631],\n",
      "        [ 0.1094,  0.0838,  0.0513,  0.0405]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.0726,  0.1130, -0.0860, -0.0169, -0.1460,  0.0320,  0.0488,  0.0124],\n",
      "       requires_grad=True)\n",
      "Bdown spliced: torch.Size([4])\n",
      "tensor([-0.0726,  0.1130, -0.0860, -0.0169], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.0638,  0.0679, -0.1118, -0.0604],\n",
      "         [-0.0861,  0.0917, -0.1221,  0.0238],\n",
      "         [-0.0830,  0.1115, -0.1298, -0.0013]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[ 0.4319,  0.9893,  1.1109, -2.4684],\n",
      "         [-0.7645,  1.7468, -0.8457,  0.0087],\n",
      "         [-0.7444, -0.2781, -0.9372, -1.3125]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.4319,  0.9893,  1.1109, -2.4684],\n",
      "         [-0.7645,  1.7468, -0.8457,  0.0087],\n",
      "         [-0.7444, -0.2781, -0.9372, -1.3125]]], grad_fn=<AddBackward0>)\n",
      "Model 1 from range(2)\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.7594, -0.8046, -0.5859, -1.3914],\n",
      "         [ 1.1770,  0.8434,  1.4686,  0.0103],\n",
      "         [ 1.6388,  0.5452,  0.6675,  0.4108]]])\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.8114, -0.8598, -0.6260, -1.4868],\n",
      "         [ 1.1413,  0.8179,  1.4242,  0.0100],\n",
      "         [ 1.7281,  0.5749,  0.7039,  0.4332]]])\n",
      "dim: 4\n",
      "skip: 4\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.8114, -0.8598, -0.6260, -1.4868],\n",
      "         [ 1.1413,  0.8179,  1.4242,  0.0100],\n",
      "         [ 1.7281,  0.5749,  0.7039,  0.4332]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 4\n",
      "models_in_this_level: 2\n",
      "h_dim: 2\n",
      "h_skip: 2\n",
      "self.Wqkv: torch.Size([8, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3082, -0.0375,  0.2938,  0.0952,  0.1068,  0.2869, -0.2548,  0.2737,\n",
      "          0.1577,  0.2032, -0.1882, -0.2408,  0.0541, -0.2956, -0.3144, -0.0215],\n",
      "        [ 0.0282,  0.0842, -0.3029, -0.2693, -0.2461,  0.0440,  0.1471, -0.0685,\n",
      "         -0.2607, -0.1407,  0.2482,  0.2583, -0.0811,  0.0538,  0.3521, -0.2299],\n",
      "        [-0.2697,  0.0807,  0.0751, -0.0655,  0.1450, -0.3520,  0.2306,  0.1296,\n",
      "         -0.1562, -0.0380, -0.0221,  0.2635,  0.0980,  0.3051, -0.2555,  0.0061],\n",
      "        [ 0.1652, -0.2571,  0.2203, -0.2631, -0.0092,  0.3367, -0.3005,  0.2128,\n",
      "         -0.1101, -0.3220,  0.2759, -0.1204,  0.3024, -0.0503, -0.2147, -0.1287],\n",
      "        [ 0.0111, -0.3078, -0.0968,  0.1583,  0.1930, -0.3113, -0.2252, -0.1391,\n",
      "          0.1673, -0.2444, -0.0069,  0.0760,  0.0661,  0.0243,  0.1211,  0.3435],\n",
      "        [-0.0053, -0.2899, -0.1091, -0.0459, -0.0735, -0.0206,  0.2626,  0.0280,\n",
      "         -0.2423,  0.1941, -0.2940,  0.1764, -0.1085,  0.0123,  0.0262,  0.3460],\n",
      "        [-0.2541, -0.1558, -0.1859, -0.1539,  0.1353, -0.1152, -0.0136, -0.2143,\n",
      "         -0.0768, -0.2811,  0.2954,  0.0775, -0.1482,  0.0178, -0.2664,  0.0608],\n",
      "        [-0.2611, -0.2725,  0.0680, -0.2697, -0.1009,  0.3087,  0.1102,  0.2571,\n",
      "         -0.1993, -0.0211, -0.0661,  0.3239,  0.1822,  0.0132, -0.0324,  0.2877]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([8, 8])\n",
      "tensor([[ 0.3082, -0.0375,  0.2938,  0.0952,  0.1068,  0.2869, -0.2548,  0.2737],\n",
      "        [ 0.0282,  0.0842, -0.3029, -0.2693, -0.2461,  0.0440,  0.1471, -0.0685],\n",
      "        [-0.2697,  0.0807,  0.0751, -0.0655,  0.1450, -0.3520,  0.2306,  0.1296],\n",
      "        [ 0.1652, -0.2571,  0.2203, -0.2631, -0.0092,  0.3367, -0.3005,  0.2128],\n",
      "        [ 0.0111, -0.3078, -0.0968,  0.1583,  0.1930, -0.3113, -0.2252, -0.1391],\n",
      "        [-0.0053, -0.2899, -0.1091, -0.0459, -0.0735, -0.0206,  0.2626,  0.0280],\n",
      "        [-0.2541, -0.1558, -0.1859, -0.1539,  0.1353, -0.1152, -0.0136, -0.2143],\n",
      "        [-0.2611, -0.2725,  0.0680, -0.2697, -0.1009,  0.3087,  0.1102,  0.2571]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([8, 4])\n",
      "tensor([[ 0.1577,  0.2032, -0.1882, -0.2408],\n",
      "        [-0.2607, -0.1407,  0.2482,  0.2583],\n",
      "        [-0.1562, -0.0380, -0.0221,  0.2635],\n",
      "        [-0.1101, -0.3220,  0.2759, -0.1204],\n",
      "        [ 0.1673, -0.2444, -0.0069,  0.0760],\n",
      "        [-0.2423,  0.1941, -0.2940,  0.1764],\n",
      "        [-0.0768, -0.2811,  0.2954,  0.0775],\n",
      "        [-0.1993, -0.0211, -0.0661,  0.3239]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([8, 4])\n",
      "tensor([[ 0.0541, -0.2956, -0.3144, -0.0215],\n",
      "        [-0.0811,  0.0538,  0.3521, -0.2299],\n",
      "        [ 0.0980,  0.3051, -0.2555,  0.0061],\n",
      "        [ 0.3024, -0.0503, -0.2147, -0.1287],\n",
      "        [ 0.0661,  0.0243,  0.1211,  0.3435],\n",
      "        [-0.1085,  0.0123,  0.0262,  0.3460],\n",
      "        [-0.1482,  0.0178, -0.2664,  0.0608],\n",
      "        [ 0.1822,  0.0132, -0.0324,  0.2877]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 4])\n",
      "tensor([[-0.0968,  0.1583, -0.2252, -0.1391],\n",
      "        [-0.1091, -0.0459,  0.2626,  0.0280],\n",
      "        [-0.1859, -0.1539, -0.0136, -0.2143],\n",
      "        [ 0.0680, -0.2697,  0.1102,  0.2571]], grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([4, 2])\n",
      "tensor([[-0.0069,  0.0760],\n",
      "        [-0.2940,  0.1764],\n",
      "        [ 0.2954,  0.0775],\n",
      "        [-0.0661,  0.3239]], grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 2])\n",
      "tensor([[ 0.1211,  0.3435],\n",
      "        [ 0.0262,  0.3460],\n",
      "        [-0.2664,  0.0608],\n",
      "        [-0.0324,  0.2877]], grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 8])\n",
      "tensor([[-0.0968,  0.1583, -0.2252, -0.1391, -0.0069,  0.0760,  0.1211,  0.3435],\n",
      "        [-0.1091, -0.0459,  0.2626,  0.0280, -0.2940,  0.1764,  0.0262,  0.3460],\n",
      "        [-0.1859, -0.1539, -0.0136, -0.2143,  0.2954,  0.0775, -0.2664,  0.0608],\n",
      "        [ 0.0680, -0.2697,  0.1102,  0.2571, -0.0661,  0.3239, -0.0324,  0.2877]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.1876,  0.4085, -0.1984, -0.1592,  0.1717, -0.7435,  0.0942,\n",
      "          -1.0420],\n",
      "         [-0.4637, -0.0788, -0.0604, -0.4385,  0.1718,  0.3447, -0.2200,\n",
      "           0.7645],\n",
      "         [-0.3313,  0.0219, -0.2000, -0.2638, -0.0015,  0.4276,  0.0228,\n",
      "           0.9600]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.1876,  0.4085, -0.1984, -0.1592],\n",
      "         [-0.4637, -0.0788, -0.0604, -0.4385],\n",
      "         [-0.3313,  0.0219, -0.2000, -0.2638]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.1717, -0.7435],\n",
      "         [ 0.1718,  0.3447],\n",
      "         [-0.0015,  0.4276]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.0942, -1.0420],\n",
      "         [-0.2200,  0.7645],\n",
      "         [ 0.0228,  0.9600]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.1876,  0.4085],\n",
      "          [-0.1984, -0.1592]],\n",
      "\n",
      "         [[-0.4637, -0.0788],\n",
      "          [-0.0604, -0.4385]],\n",
      "\n",
      "         [[-0.3313,  0.0219],\n",
      "          [-0.2000, -0.2638]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.1717, -0.7435]],\n",
      "\n",
      "         [[ 0.1718,  0.3447]],\n",
      "\n",
      "         [[-0.0015,  0.4276]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.0942, -1.0420]],\n",
      "\n",
      "         [[-0.2200,  0.7645]],\n",
      "\n",
      "         [[ 0.0228,  0.9600]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.1876,  0.4085],\n",
      "          [-0.1984, -0.1592]],\n",
      "\n",
      "         [[-0.1842, -0.4328],\n",
      "          [ 0.3363, -0.2878]],\n",
      "\n",
      "         [[ 0.1179, -0.3104],\n",
      "          [ 0.3231, -0.0721]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 2])\n",
      "tensor([[[[ 0.1717, -0.7435]],\n",
      "\n",
      "         [[-0.1972,  0.3308]],\n",
      "\n",
      "         [[-0.3882, -0.1794]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.1717, -0.7435],\n",
      "          [ 0.1717, -0.7435]],\n",
      "\n",
      "         [[-0.1972,  0.3308],\n",
      "          [-0.1972,  0.3308]],\n",
      "\n",
      "         [[-0.3882, -0.1794],\n",
      "          [-0.3882, -0.1794]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 2, 2])\n",
      "tensor([[[[ 0.0942, -1.0420],\n",
      "          [ 0.0942, -1.0420]],\n",
      "\n",
      "         [[-0.2200,  0.7645],\n",
      "          [-0.2200,  0.7645]],\n",
      "\n",
      "         [[ 0.0228,  0.9600],\n",
      "          [ 0.0228,  0.9600]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.1876,  0.4085],\n",
      "          [-0.1842, -0.4328],\n",
      "          [ 0.1179, -0.3104]],\n",
      "\n",
      "         [[-0.1984, -0.1592],\n",
      "          [ 0.3363, -0.2878],\n",
      "          [ 0.3231, -0.0721]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.1717, -0.7435],\n",
      "          [-0.1972,  0.3308],\n",
      "          [-0.3882, -0.1794]],\n",
      "\n",
      "         [[ 0.1717, -0.7435],\n",
      "          [-0.1972,  0.3308],\n",
      "          [-0.3882, -0.1794]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.0942, -1.0420],\n",
      "          [-0.2200,  0.7645],\n",
      "          [ 0.0228,  0.9600]],\n",
      "\n",
      "         [[ 0.0942, -1.0420],\n",
      "          [-0.2200,  0.7645],\n",
      "          [ 0.0228,  0.9600]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-0.1920,  0.0694, -0.1033],\n",
      "          [ 0.2052, -0.0756,  0.1055],\n",
      "          [ 0.1775, -0.0891,  0.0070]],\n",
      "\n",
      "         [[ 0.0596, -0.0096,  0.0747],\n",
      "          [ 0.1921, -0.1142, -0.0558],\n",
      "          [ 0.0771, -0.0619, -0.0796]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[-1.9196e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 2.0515e-01, -7.5552e-02, -2.3820e+38],\n",
      "          [ 1.7750e-01, -8.9056e-02,  6.9929e-03]],\n",
      "\n",
      "         [[ 5.9619e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 1.9212e-01, -1.1422e-01, -2.3820e+38],\n",
      "          [ 7.7105e-02, -6.1914e-02, -7.9560e-02]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 2, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5697, 0.4303, 0.0000],\n",
      "          [0.3833, 0.2936, 0.3232]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5760, 0.4240, 0.0000],\n",
      "          [0.3669, 0.3193, 0.3137]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 2, 3, 2])\n",
      "tensor([[[[ 0.0942, -1.0420],\n",
      "          [-0.0410, -0.2647],\n",
      "          [-0.0211,  0.1354]],\n",
      "\n",
      "         [[ 0.0942, -1.0420],\n",
      "          [-0.0391, -0.2760],\n",
      "          [-0.0286,  0.1630]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0942, -1.0420,  0.0942, -1.0420],\n",
      "         [-0.0410, -0.2647, -0.0391, -0.2760],\n",
      "         [-0.0211,  0.1354, -0.0286,  0.1630]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([8, 8])\n",
      "Parameter containing:\n",
      "tensor([[ 0.1812, -0.1638, -0.1178,  0.1815, -0.0778, -0.0560,  0.3211,  0.3358],\n",
      "        [-0.1059,  0.2363,  0.1847, -0.2948,  0.2672,  0.3174,  0.3080,  0.2044],\n",
      "        [ 0.2060,  0.0377, -0.2959, -0.2276, -0.2968,  0.1777, -0.0755, -0.1395],\n",
      "        [ 0.3207,  0.0063,  0.1376, -0.2537,  0.3473, -0.3356,  0.0987, -0.2497],\n",
      "        [-0.3443,  0.2761, -0.2423, -0.0421,  0.0527, -0.2862,  0.0705,  0.1438],\n",
      "        [ 0.1588, -0.2870,  0.0738, -0.2562,  0.0793,  0.2388,  0.3065,  0.1916],\n",
      "        [ 0.0715, -0.2669,  0.0129,  0.3258, -0.1468, -0.0368, -0.2941,  0.0135],\n",
      "        [-0.0419, -0.0367, -0.1927, -0.0744,  0.0314, -0.2928,  0.1988, -0.1777]],\n",
      "       requires_grad=True)\n",
      "spliced Wo: torch.Size([4, 4])\n",
      "tensor([[-0.2968,  0.1777, -0.0755, -0.1395],\n",
      "        [ 0.3473, -0.3356,  0.0987, -0.2497],\n",
      "        [-0.1468, -0.0368, -0.2941,  0.0135],\n",
      "        [ 0.0314, -0.2928,  0.1988, -0.1777]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.4364,  0.6681, -0.3448,  0.4335],\n",
      "         [-0.0827,  0.1638, -0.0664,  0.1203],\n",
      "         [ 0.0626, -0.0959,  0.0557, -0.0602]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[-1.1958, -0.1366, -0.9307, -0.9580],\n",
      "         [ 1.0943,  1.0072,  1.4022,  0.1306],\n",
      "         [ 1.7014,  0.4493,  0.7233,  0.3506]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[-1.1958, -0.1366, -0.9307, -0.9580],\n",
      "         [ 1.0943,  1.0072,  1.4022,  0.1306],\n",
      "         [ 1.7014,  0.4493,  0.7233,  0.3506]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[-1.3302, -0.1519, -1.0353, -1.0656],\n",
      "         [ 1.0685,  0.9835,  1.3692,  0.1275],\n",
      "         [ 1.7589,  0.4645,  0.7477,  0.3624]]], grad_fn=<MulBackward0>)\n",
      "dim: 4\n",
      "skip: 4\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[-1.3302, -0.1519, -1.0353, -1.0656],\n",
      "         [ 1.0685,  0.9835,  1.3692,  0.1275],\n",
      "         [ 1.7589,  0.4645,  0.7477,  0.3624]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[-1.3302, -0.1519, -1.0353, -1.0656],\n",
      "         [ 1.0685,  0.9835,  1.3692,  0.1275],\n",
      "         [ 1.7589,  0.4645,  0.7477,  0.3624]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 4\n",
      "d_skip: 4\n",
      "i_dim: 16\n",
      "i_skip: 16\n",
      "Wgate: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[-0.2811, -0.0270,  0.0777, -0.0039,  0.2796, -0.2670,  0.1663,  0.0266,\n",
      "          0.2387,  0.1946, -0.1248,  0.0487,  0.1504, -0.0958,  0.2620,  0.0300,\n",
      "         -0.0412,  0.1858, -0.2139, -0.2279,  0.1933, -0.0472,  0.2015, -0.1428,\n",
      "         -0.1714,  0.3420, -0.1261, -0.0445, -0.2970, -0.2236,  0.1774,  0.3273],\n",
      "        [-0.0559, -0.1062,  0.2060, -0.3148, -0.0610,  0.0727, -0.2189, -0.2999,\n",
      "         -0.0502,  0.1810,  0.1343,  0.0851, -0.0477,  0.0827,  0.0995,  0.0167,\n",
      "         -0.1973,  0.0140,  0.1064, -0.0729,  0.1845, -0.3449, -0.2266, -0.0014,\n",
      "         -0.3306, -0.1396, -0.3074, -0.2133,  0.3525, -0.0846, -0.3425,  0.1338],\n",
      "        [ 0.3045, -0.1883, -0.2371,  0.3285, -0.0387,  0.3390,  0.3480, -0.2650,\n",
      "         -0.0216,  0.2641,  0.2369, -0.1652, -0.3093,  0.0436, -0.1806,  0.2700,\n",
      "          0.2034, -0.2746,  0.3244, -0.0298,  0.1362,  0.2964, -0.2646, -0.0550,\n",
      "         -0.1278,  0.2398, -0.0377,  0.0583, -0.2364,  0.3412, -0.2262,  0.1816],\n",
      "        [ 0.2170,  0.0379, -0.0893, -0.0081,  0.2499, -0.3397,  0.2077,  0.3244,\n",
      "         -0.2439, -0.2049,  0.1377,  0.3533, -0.2209,  0.0883, -0.0685,  0.2863,\n",
      "         -0.0748,  0.2120, -0.0254, -0.1597,  0.0182, -0.3100, -0.2707,  0.0629,\n",
      "          0.1587,  0.1991, -0.1898,  0.2316,  0.1283, -0.1907,  0.0939,  0.1159],\n",
      "        [ 0.0412, -0.1712,  0.0801, -0.1478, -0.2345, -0.0489,  0.1414, -0.0028,\n",
      "          0.0027,  0.0525, -0.1322, -0.0448,  0.2901, -0.2900,  0.0118, -0.1953,\n",
      "          0.0151, -0.1399,  0.1249,  0.3276, -0.0825,  0.1432,  0.2753,  0.2337,\n",
      "          0.0480,  0.3424,  0.0860, -0.2939,  0.0054, -0.1741, -0.3390, -0.2313],\n",
      "        [ 0.0542,  0.0592,  0.3068,  0.1669, -0.2551,  0.3092,  0.3315,  0.1246,\n",
      "         -0.0527,  0.1310, -0.3207,  0.2923,  0.2547, -0.2331, -0.3039, -0.3103,\n",
      "         -0.0332, -0.2420,  0.1207, -0.0299,  0.0884, -0.1770, -0.3310,  0.1731,\n",
      "          0.0462, -0.0148, -0.1853, -0.3507, -0.0339, -0.1803, -0.0213,  0.2568],\n",
      "        [-0.1879,  0.0668,  0.1165,  0.2397, -0.2110, -0.0725, -0.3138,  0.2530,\n",
      "          0.2190, -0.1220, -0.3086,  0.2517, -0.1614,  0.0849, -0.3084,  0.0757,\n",
      "          0.0819,  0.2163,  0.2924,  0.0604, -0.1102, -0.3133, -0.3265,  0.1126,\n",
      "          0.3116, -0.2196, -0.3490, -0.1459,  0.2048, -0.3398,  0.3354,  0.3366],\n",
      "        [ 0.2473,  0.2045,  0.0686, -0.1509,  0.2188, -0.2802, -0.0783,  0.1187,\n",
      "         -0.2610,  0.0220, -0.1511,  0.2359,  0.0023, -0.0620, -0.2135, -0.2747,\n",
      "          0.2094,  0.0222,  0.1783,  0.3206, -0.2119,  0.2887,  0.1856,  0.3166,\n",
      "         -0.0897, -0.1708, -0.3043, -0.1225, -0.1501,  0.2560,  0.0135, -0.3519]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([4, 16])\n",
      "tensor([[ 0.0151, -0.1399,  0.1249,  0.3276, -0.0825,  0.1432,  0.2753,  0.2337,\n",
      "          0.0480,  0.3424,  0.0860, -0.2939,  0.0054, -0.1741, -0.3390, -0.2313],\n",
      "        [-0.0332, -0.2420,  0.1207, -0.0299,  0.0884, -0.1770, -0.3310,  0.1731,\n",
      "          0.0462, -0.0148, -0.1853, -0.3507, -0.0339, -0.1803, -0.0213,  0.2568],\n",
      "        [ 0.0819,  0.2163,  0.2924,  0.0604, -0.1102, -0.3133, -0.3265,  0.1126,\n",
      "          0.3116, -0.2196, -0.3490, -0.1459,  0.2048, -0.3398,  0.3354,  0.3366],\n",
      "        [ 0.2094,  0.0222,  0.1783,  0.3206, -0.2119,  0.2887,  0.1856,  0.3166,\n",
      "         -0.0897, -0.1708, -0.3043, -0.1225, -0.1501,  0.2560,  0.0135, -0.3519]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([ 0.2344, -0.0453, -0.2316, -0.0515, -0.3145,  0.1585, -0.3512,  0.0260,\n",
      "        -0.0817,  0.1658,  0.0793, -0.3017, -0.3280, -0.1102,  0.3004, -0.3055,\n",
      "        -0.0573,  0.3171, -0.1961, -0.0773, -0.1722,  0.2801, -0.0123, -0.3422,\n",
      "        -0.2469, -0.2034, -0.1560,  0.2893,  0.3363, -0.0824,  0.0287, -0.3314],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([16])\n",
      "tensor([-0.0573,  0.3171, -0.1961, -0.0773, -0.1722,  0.2801, -0.0123, -0.3422,\n",
      "        -0.2469, -0.2034, -0.1560,  0.2893,  0.3363, -0.0824,  0.0287, -0.3314],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.3801,  0.2924, -0.8733, -0.9126,  0.2640,  0.1332, -0.1879,\n",
      "          -1.1333, -0.5448, -0.2473,  0.4434,  1.0152,  0.2822,  0.2556,\n",
      "           0.1213, -0.0361],\n",
      "         [ 0.0651,  0.2286,  0.4792,  0.3669, -0.3512, -0.1330, -0.4671,\n",
      "           0.2723,  0.2651, -0.1745, -0.7630, -0.5850,  0.5700, -0.8783,\n",
      "           0.1066,  0.0899],\n",
      "         [ 0.0910,  0.1284,  0.3629,  0.6464, -0.4353,  0.3203,  0.1413,\n",
      "           0.3482,  0.0595,  0.1659, -0.4621, -0.5440,  0.4288, -0.6336,\n",
      "          -0.3218, -0.4949]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.1338,  0.1798, -0.1670, -0.1649,  0.1595,  0.0736, -0.0799,\n",
      "          -0.1457, -0.1596, -0.0995,  0.2977,  0.8578,  0.1725,  0.1536,\n",
      "           0.0665, -0.0175],\n",
      "         [ 0.0342,  0.1350,  0.3278,  0.2360, -0.1274, -0.0595, -0.1496,\n",
      "           0.1654,  0.1602, -0.0752, -0.1699, -0.1634,  0.4080, -0.1668,\n",
      "           0.0578,  0.0482],\n",
      "         [ 0.0488,  0.0707,  0.2329,  0.4789, -0.1444,  0.2004,  0.0786,\n",
      "           0.2215,  0.0312,  0.0939, -0.1488, -0.1595,  0.2856, -0.1667,\n",
      "          -0.1203, -0.1536]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([8, 32])\n",
      "Parameter containing:\n",
      "tensor([[ 0.2822, -0.1828, -0.2777,  0.1082,  0.2238, -0.1134, -0.3103,  0.1056,\n",
      "         -0.2703,  0.3090, -0.0819, -0.3297, -0.3104,  0.3282,  0.1361,  0.2350,\n",
      "          0.0163, -0.0788, -0.0889, -0.1623,  0.0625,  0.2353,  0.1543, -0.2189,\n",
      "         -0.2807, -0.1850, -0.1731, -0.0408, -0.0843,  0.1762,  0.1462, -0.2785],\n",
      "        [ 0.2543, -0.0184, -0.1636,  0.0951, -0.0598, -0.1587,  0.1128, -0.3349,\n",
      "          0.2041, -0.2667,  0.2606, -0.2418, -0.1451, -0.0544,  0.0391,  0.2140,\n",
      "          0.1148,  0.2194,  0.3487, -0.2808,  0.2093,  0.1741, -0.3231, -0.3367,\n",
      "          0.1741,  0.0723,  0.3435,  0.3155, -0.2377,  0.0187,  0.2241, -0.0306],\n",
      "        [ 0.0182,  0.2018, -0.3436, -0.1494, -0.1850,  0.3460, -0.1126, -0.1586,\n",
      "          0.2040,  0.2067, -0.0948,  0.0301, -0.2446, -0.0837,  0.3216, -0.3421,\n",
      "         -0.0927, -0.1508, -0.0646, -0.1180, -0.2482,  0.1263, -0.2934, -0.0028,\n",
      "          0.2603, -0.0577, -0.2085,  0.1523, -0.0853, -0.2301,  0.3381,  0.3102],\n",
      "        [-0.1570, -0.2126, -0.0957,  0.1882,  0.2634, -0.1622, -0.0596,  0.1292,\n",
      "         -0.2789, -0.1275, -0.1206, -0.2811,  0.2709, -0.2125,  0.2594,  0.0267,\n",
      "          0.2853, -0.2005,  0.3447,  0.2926, -0.2513, -0.2647, -0.2399,  0.0386,\n",
      "         -0.1857, -0.2409, -0.0541, -0.3132,  0.0247, -0.2355,  0.2472, -0.0075],\n",
      "        [ 0.1237, -0.2308,  0.1627, -0.1708, -0.1267,  0.3394,  0.0920,  0.3374,\n",
      "         -0.0833,  0.3465,  0.0505,  0.2654, -0.1465,  0.1833,  0.2793,  0.0661,\n",
      "         -0.2190,  0.0816,  0.2301, -0.2003,  0.3257,  0.2920, -0.3441,  0.0195,\n",
      "         -0.2930,  0.1064,  0.3039, -0.0885, -0.0492,  0.2339,  0.1432, -0.2975],\n",
      "        [-0.3399, -0.1084,  0.0510,  0.1905,  0.0075, -0.0773,  0.0372,  0.1646,\n",
      "         -0.0846,  0.1196,  0.2669, -0.2610, -0.3391,  0.3039,  0.3301, -0.0300,\n",
      "          0.0947,  0.0918, -0.1067,  0.1715, -0.0934,  0.2799,  0.1185, -0.0652,\n",
      "         -0.2257,  0.3120, -0.0038, -0.0091, -0.2985,  0.2226, -0.2071,  0.0148],\n",
      "        [ 0.0602,  0.1339,  0.0864, -0.2389, -0.0267,  0.3286, -0.0006, -0.1042,\n",
      "         -0.2978, -0.1909,  0.2360, -0.1668,  0.3218, -0.1158, -0.2307,  0.3201,\n",
      "          0.1812, -0.3420,  0.2597,  0.0130,  0.0328,  0.1331, -0.3222, -0.1871,\n",
      "          0.3457,  0.1235, -0.1492,  0.2934,  0.0940,  0.0699, -0.2982,  0.1518],\n",
      "        [ 0.1666,  0.0884,  0.2785,  0.0009, -0.2720,  0.0974, -0.0325,  0.2428,\n",
      "          0.1092,  0.2763, -0.3083, -0.3298, -0.2186,  0.2330,  0.2474, -0.0754,\n",
      "         -0.0026,  0.2785,  0.1558, -0.2691,  0.3140, -0.1810,  0.0921,  0.2948,\n",
      "          0.2219,  0.0859,  0.1021, -0.1126, -0.1979, -0.2360, -0.1078, -0.1703]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([4, 16])\n",
      "tensor([[-0.2190,  0.0816,  0.2301, -0.2003,  0.3257,  0.2920, -0.3441,  0.0195,\n",
      "         -0.2930,  0.1064,  0.3039, -0.0885, -0.0492,  0.2339,  0.1432, -0.2975],\n",
      "        [ 0.0947,  0.0918, -0.1067,  0.1715, -0.0934,  0.2799,  0.1185, -0.0652,\n",
      "         -0.2257,  0.3120, -0.0038, -0.0091, -0.2985,  0.2226, -0.2071,  0.0148],\n",
      "        [ 0.1812, -0.3420,  0.2597,  0.0130,  0.0328,  0.1331, -0.3222, -0.1871,\n",
      "          0.3457,  0.1235, -0.1492,  0.2934,  0.0940,  0.0699, -0.2982,  0.1518],\n",
      "        [-0.0026,  0.2785,  0.1558, -0.2691,  0.3140, -0.1810,  0.0921,  0.2948,\n",
      "          0.2219,  0.0859,  0.1021, -0.1126, -0.1979, -0.2360, -0.1078, -0.1703]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([32])\n",
      "Parameter containing:\n",
      "tensor([ 0.1502, -0.2245, -0.3040, -0.0625,  0.0431,  0.1823, -0.0395,  0.3423,\n",
      "         0.1442,  0.0085,  0.0481, -0.1862, -0.1928, -0.2941,  0.1352,  0.0659,\n",
      "        -0.2695, -0.2388,  0.1631,  0.3170, -0.2047, -0.2018, -0.0727,  0.0341,\n",
      "        -0.1847,  0.2189, -0.3191, -0.0122,  0.0883, -0.2231,  0.3436,  0.0368],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([16])\n",
      "tensor([-0.2695, -0.2388,  0.1631,  0.3170, -0.2047, -0.2018, -0.0727,  0.0341,\n",
      "        -0.1847,  0.2189, -0.3191, -0.0122,  0.0883, -0.2231,  0.3436,  0.0368],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.1774, -0.3041, -0.5617,  0.8307, -0.9922, -0.5776,  0.6024,\n",
      "          -0.1024, -0.3550, -0.1894, -0.6772, -0.0769,  0.3127, -0.3890,\n",
      "           0.6081,  0.4547],\n",
      "         [-0.1626, -0.4941,  0.6795,  0.2551,  0.1364,  0.5447, -0.7533,\n",
      "          -0.2278, -0.2181,  0.8195, -0.1894,  0.2718, -0.1544,  0.3115,\n",
      "          -0.1292, -0.0804],\n",
      "         [-0.4761, -0.2074,  0.7689, -0.0435,  0.4631,  0.4759, -0.8304,\n",
      "           0.0051, -0.4660,  0.6744,  0.1391,  0.0066, -0.1383,  0.2585,\n",
      "           0.2372, -0.4279]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.0237, -0.0547,  0.0938, -0.1370, -0.1582, -0.0425, -0.0482,\n",
      "           0.0149,  0.0567,  0.0188, -0.2016, -0.0660,  0.0539, -0.0597,\n",
      "           0.0404, -0.0080],\n",
      "         [-0.0056, -0.0667,  0.2228,  0.0602, -0.0174, -0.0324,  0.1127,\n",
      "          -0.0377, -0.0349, -0.0616,  0.0322, -0.0444, -0.0630, -0.0519,\n",
      "          -0.0075, -0.0039],\n",
      "         [-0.0232, -0.0147,  0.1791, -0.0208, -0.0669,  0.0953, -0.0653,\n",
      "           0.0011, -0.0145,  0.0633, -0.0207, -0.0011, -0.0395, -0.0431,\n",
      "          -0.0285,  0.0657]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([32, 8])\n",
      "Parameter containing:\n",
      "tensor([[-6.2705e-03,  1.7087e-01, -6.2752e-02, -6.2246e-02,  6.8582e-02,\n",
      "         -1.3300e-01, -2.4336e-02, -8.4735e-02],\n",
      "        [ 5.1137e-02,  3.8288e-02, -1.0181e-01,  1.6525e-01,  1.1211e-01,\n",
      "         -1.5877e-01, -1.6302e-01,  1.3192e-01],\n",
      "        [-1.6308e-01,  8.1501e-04,  1.6355e-01, -9.2252e-02, -5.5730e-02,\n",
      "          6.0194e-02,  8.2182e-02, -4.9122e-02],\n",
      "        [ 1.6523e-01,  1.2935e-01,  6.9711e-02, -1.1357e-01,  4.4770e-02,\n",
      "          4.7724e-02,  3.9811e-02, -1.9432e-02],\n",
      "        [ 1.7026e-01, -1.0126e-01, -8.7763e-02,  2.3960e-02, -9.2687e-02,\n",
      "          3.1562e-02, -4.2784e-02,  5.0818e-02],\n",
      "        [-5.0923e-02, -1.4666e-01, -8.2458e-02,  4.8273e-02,  7.0162e-02,\n",
      "         -5.5578e-02, -4.7904e-02,  1.6376e-01],\n",
      "        [ 5.9954e-02,  6.3359e-02,  1.6240e-01, -1.2038e-01, -1.4918e-01,\n",
      "          8.5838e-02,  1.2609e-01, -9.7529e-02],\n",
      "        [ 1.0761e-03,  1.7226e-01, -9.8626e-02,  6.3615e-02, -6.2643e-02,\n",
      "         -1.6054e-02,  1.5028e-01,  8.0325e-02],\n",
      "        [ 1.2182e-01, -2.9880e-02,  3.4341e-03, -1.7226e-01, -1.6388e-01,\n",
      "         -1.4763e-01, -6.8185e-02, -6.8890e-02],\n",
      "        [-4.9057e-02,  1.6263e-01,  3.7982e-02, -1.1945e-01,  1.4013e-01,\n",
      "         -7.0778e-03,  2.2981e-02, -1.3161e-03],\n",
      "        [ 1.0184e-01, -1.0099e-01, -1.1654e-01,  6.7053e-02, -1.2938e-01,\n",
      "          1.6578e-01,  7.8409e-02,  1.1408e-01],\n",
      "        [-1.6861e-02, -1.1192e-01,  7.3365e-02,  7.6899e-02,  7.7671e-02,\n",
      "         -1.5597e-01, -1.4244e-01,  4.3034e-02],\n",
      "        [ 1.4185e-01, -2.1652e-02,  4.7518e-02,  3.3936e-03,  9.0820e-02,\n",
      "          3.6127e-03, -1.4041e-01, -5.4591e-02],\n",
      "        [ 1.4952e-01,  1.2989e-01, -1.3148e-01,  1.4601e-02, -1.3439e-01,\n",
      "          1.2325e-01, -1.3866e-01, -4.2996e-02],\n",
      "        [ 1.4415e-01, -5.4852e-02,  1.3252e-01, -1.6311e-01,  4.0809e-02,\n",
      "          1.6915e-01, -9.0345e-03, -1.3468e-01],\n",
      "        [ 1.0943e-01,  8.3819e-02,  5.1325e-02,  4.0454e-02, -9.2413e-02,\n",
      "         -2.4211e-02, -3.0873e-02, -1.0869e-01],\n",
      "        [ 1.5980e-01, -1.2678e-01,  5.3513e-03, -1.3808e-01, -1.5095e-01,\n",
      "         -1.0633e-02, -1.2122e-01,  8.8511e-02],\n",
      "        [-5.6132e-02, -1.4101e-01,  9.2021e-02, -5.3810e-02,  2.5294e-02,\n",
      "         -1.6808e-01,  5.3713e-02,  1.3115e-02],\n",
      "        [ 1.7022e-01, -7.4945e-02, -1.0871e-01, -1.4303e-01, -6.4070e-02,\n",
      "         -3.8516e-02,  1.6345e-01, -1.8564e-02],\n",
      "        [ 1.8691e-02,  1.5467e-01, -6.3141e-02, -1.2526e-01, -1.3534e-01,\n",
      "         -1.6311e-01, -1.5884e-01, -2.2790e-02],\n",
      "        [ 1.5216e-01, -5.5006e-03, -8.8667e-02,  1.7551e-01, -2.6523e-02,\n",
      "         -4.6100e-03,  6.4389e-02, -4.1098e-02],\n",
      "        [-1.2356e-01,  7.0946e-02, -4.6502e-02, -1.2491e-01,  1.3155e-01,\n",
      "          1.6781e-01,  8.3644e-02,  8.5635e-02],\n",
      "        [ 1.0658e-01,  8.7222e-02, -9.4226e-02,  1.8287e-02, -1.5606e-01,\n",
      "          7.5877e-02,  1.1103e-01,  1.3385e-01],\n",
      "        [ 6.6123e-03, -1.1708e-01, -5.6805e-02, -4.0316e-02,  1.8463e-03,\n",
      "          6.6550e-03, -1.6498e-01, -2.5916e-02],\n",
      "        [-8.2317e-02, -1.5264e-01,  6.5833e-02, -1.4837e-01,  1.1552e-01,\n",
      "          1.0273e-01,  1.3319e-01, -1.3248e-01],\n",
      "        [ 5.7529e-02, -1.6429e-01, -1.4998e-02,  7.5544e-02,  2.3620e-02,\n",
      "         -1.8969e-02, -9.7055e-02,  4.2203e-02],\n",
      "        [ 8.1459e-03, -8.3137e-02,  1.4563e-01, -2.7819e-02,  1.3285e-01,\n",
      "          1.0402e-02, -7.8985e-03, -1.0396e-01],\n",
      "        [ 1.0753e-01,  9.4934e-02, -1.0795e-01, -2.9810e-02, -8.4199e-02,\n",
      "         -1.1534e-01, -1.5309e-01, -1.4743e-01],\n",
      "        [ 1.3129e-02, -2.3096e-02,  2.9924e-05,  6.5598e-03, -6.9706e-02,\n",
      "         -5.7110e-02,  1.0288e-01, -1.6200e-01],\n",
      "        [ 8.2480e-02, -1.4823e-01, -2.3083e-02, -3.6991e-03, -1.0295e-01,\n",
      "         -1.3000e-01, -3.3539e-02,  6.8534e-02],\n",
      "        [ 1.6094e-02, -1.3947e-01, -1.4683e-01, -1.3126e-01,  1.2650e-01,\n",
      "          1.1030e-01,  8.9424e-02,  1.3378e-01],\n",
      "        [-1.4624e-02, -1.0183e-02, -1.0338e-01,  8.8699e-02,  1.1455e-01,\n",
      "         -1.1869e-01, -6.4768e-02, -1.7289e-01]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([16, 4])\n",
      "tensor([[-0.1510, -0.0106, -0.1212,  0.0885],\n",
      "        [ 0.0253, -0.1681,  0.0537,  0.0131],\n",
      "        [-0.0641, -0.0385,  0.1634, -0.0186],\n",
      "        [-0.1353, -0.1631, -0.1588, -0.0228],\n",
      "        [-0.0265, -0.0046,  0.0644, -0.0411],\n",
      "        [ 0.1315,  0.1678,  0.0836,  0.0856],\n",
      "        [-0.1561,  0.0759,  0.1110,  0.1338],\n",
      "        [ 0.0018,  0.0067, -0.1650, -0.0259],\n",
      "        [ 0.1155,  0.1027,  0.1332, -0.1325],\n",
      "        [ 0.0236, -0.0190, -0.0971,  0.0422],\n",
      "        [ 0.1328,  0.0104, -0.0079, -0.1040],\n",
      "        [-0.0842, -0.1153, -0.1531, -0.1474],\n",
      "        [-0.0697, -0.0571,  0.1029, -0.1620],\n",
      "        [-0.1030, -0.1300, -0.0335,  0.0685],\n",
      "        [ 0.1265,  0.1103,  0.0894,  0.1338],\n",
      "        [ 0.1146, -0.1187, -0.0648, -0.1729]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([8])\n",
      "Parameter containing:\n",
      "tensor([-0.0726,  0.1130, -0.0860, -0.0169, -0.1460,  0.0320,  0.0488,  0.0124],\n",
      "       requires_grad=True)\n",
      "Bdown spliced: torch.Size([4])\n",
      "tensor([-0.1460,  0.0320,  0.0488,  0.0124], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 4])\n",
      "tensor([[[-0.1399,  0.0708,  0.0876,  0.0291],\n",
      "         [-0.1799,  0.0409,  0.0904,  0.0311],\n",
      "         [-0.1188,  0.0367,  0.0625,  0.0046]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[-1.3357, -0.0658, -0.8430, -0.9288],\n",
      "         [ 0.9144,  1.0481,  1.4926,  0.1617],\n",
      "         [ 1.5826,  0.4860,  0.7858,  0.3552]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 4])\n",
      "tensor([[[-1.3357, -0.0658, -0.8430, -0.9288],\n",
      "         [ 0.9144,  1.0481,  1.4926,  0.1617],\n",
      "         [ 1.5826,  0.4860,  0.7858,  0.3552]]], grad_fn=<AddBackward0>)\n",
      "final output: ((tensor([[[-0.5908, -0.7629,  0.0888, -0.0364,  1.1015,  0.5386, -1.3629,\n",
      "          -1.9506],\n",
      "         [ 0.7923, -0.0098,  3.1373, -0.1927, -0.6534, -1.0088,  2.2574,\n",
      "          -0.1922],\n",
      "         [-1.9700,  0.9135, -1.7519,  0.8724, -0.7584,  0.6977,  1.4785,\n",
      "          -0.5527]]], grad_fn=<AddBackward0>),), (tensor([[[ 0.4319,  0.9893,  1.1109, -2.4684],\n",
      "         [-0.7645,  1.7468, -0.8457,  0.0087],\n",
      "         [-0.7444, -0.2781, -0.9372, -1.3125]]], grad_fn=<AddBackward0>), tensor([[[-1.3357, -0.0658, -0.8430, -0.9288],\n",
      "         [ 0.9144,  1.0481,  1.4926,  0.1617],\n",
      "         [ 1.5826,  0.4860,  0.7858,  0.3552]]], grad_fn=<AddBackward0>)))\n",
      "------------- END Layer.forwardTuple() ------------\n",
      "out: ((tensor([[[-0.5908, -0.7629,  0.0888, -0.0364,  1.1015,  0.5386, -1.3629,\n",
      "          -1.9506],\n",
      "         [ 0.7923, -0.0098,  3.1373, -0.1927, -0.6534, -1.0088,  2.2574,\n",
      "          -0.1922],\n",
      "         [-1.9700,  0.9135, -1.7519,  0.8724, -0.7584,  0.6977,  1.4785,\n",
      "          -0.5527]]], grad_fn=<AddBackward0>),), (tensor([[[ 0.4319,  0.9893,  1.1109, -2.4684],\n",
      "         [-0.7645,  1.7468, -0.8457,  0.0087],\n",
      "         [-0.7444, -0.2781, -0.9372, -1.3125]]], grad_fn=<AddBackward0>), tensor([[[-1.3357, -0.0658, -0.8430, -0.9288],\n",
      "         [ 0.9144,  1.0481,  1.4926,  0.1617],\n",
      "         [ 1.5826,  0.4860,  0.7858,  0.3552]]], grad_fn=<AddBackward0>)))\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n",
      "head_dim_list:  [32, 16, 8]\n"
     ]
    }
   ],
   "source": [
    "# Testing our Layer's forwardTuple()\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4, hold5 = config.hidden_size, config.num_attention_heads, config.head_dim, config.levels, config.max_position_embeddings\n",
    "config.hidden_size = 8\n",
    "config.num_attention_heads = 2\n",
    "config.head_dim = 4\n",
    "config.levels = 2\n",
    "config.max_position_embeddings = 3\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "layer = Layer(config)\n",
    "# we need to make sure to send in a tuple of the expected size. above we set hidden_size=4 and levels=2\n",
    "x = ((torch.randn((1,3,8)),),(torch.randn((1,3,4)),torch.randn((1,3,4))))\n",
    "print(f\"x: {x}\")\n",
    "out = layer(x)\n",
    "print(f\"out: {out}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.num_attention_heads = hold2\n",
    "config.head_dim = hold3\n",
    "config.levels = hold4\n",
    "config.max_position_embeddings = hold5\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "print(\"head_dim_list: \", config.head_dim_list)\n",
    "\n",
    "# clear up memory\n",
    "del hold1, hold2, hold3, hold4, hold5, x, layer, out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b302f731",
   "metadata": {},
   "source": [
    "# Output Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "59b19f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutputLayer(nn.Module):\n",
    "    def __init__(self, embedding: torch.Tensor, config: Config):\n",
    "        super().__init__()\n",
    "        self.embedding = embedding\n",
    "        self.v = config.vocab_size\n",
    "        self.model_dim_list = config.model_dim_list\n",
    "\n",
    "        # applies RMSNorm to the embedding matrix\n",
    "        self.embedding_norm = RMSNorm(config.hidden_size,\n",
    "                                      eps = config.rms_norm_eps)\n",
    "        \n",
    "        # Applies RMSNorm to the model's final residual state before we use the embedding matrix to get logits\n",
    "        self.final_norm = RMSNorm(config.hidden_size,\n",
    "                                  eps = config.rms_norm_eps)\n",
    "\n",
    "    def forwardTensor(self, x, model=0):\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- OutputLayer.forwardTensor() ------------\")\n",
    "            print(f\"x: {x.shape}\\n{x}\")\n",
    "\n",
    "        # setting up our splicing logic\n",
    "        d_i = x.shape[-1]\n",
    "        skip = model * d_i\n",
    "        if verbose:\n",
    "            print(f\"d_i: {d_i}\")\n",
    "            print(f\"skip: {skip}\")\n",
    "            print(f\"embedding: {self.embedding.shape}\\n{self.embedding}\")\n",
    "\n",
    "        # splice out our embedding matrix according to what model we're using\n",
    "        sliced_embed = self.embedding[:,skip:skip + d_i]\n",
    "        if verbose: print(f\"sliced_embed: {sliced_embed.shape}\\n{sliced_embed}\")\n",
    "\n",
    "        # normalize our sliced embedding matrix\n",
    "        normed_sliced_embed = self.embedding_norm(sliced_embed)\n",
    "        if verbose: print(f\"normed & sliced embedding: {normed_sliced_embed.shape}\\n{normed_sliced_embed}\")\n",
    "\n",
    "        # normalize the residual state before the final linear layer\n",
    "        x = self.final_norm(x, model)\n",
    "        if verbose: print(f\"normed x: {x.shape}\\n{x}\")\n",
    "\n",
    "        # calculating the final output logits of the model\n",
    "        logits = x @ normed_sliced_embed.t()\n",
    "        if verbose: \n",
    "            print(f\"final logits: {logits.shape}\\n{logits}\")\n",
    "            print(\"------------- END OutputLayer.forwardTensor() ------------\")\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def forwardTuple(self, x):\n",
    "        \"\"\"\n",
    "        Defines the forward pass of the final embedding classification layer during training.\n",
    "\n",
    "        Parameters:\n",
    "            x (Tuple[Tuple[Tensor]]): \n",
    "                The input tuple of tuples of tensors \n",
    "                first tuple is of length config.levels and second layer of tuples have lengths of config.model_count\n",
    "                tensors are shape (batch size, sequence length, hidden dimension) where hidden dimension changes by which model was used\n",
    "\n",
    "        Returns:\n",
    "            output (Tuple[Tuple[Tensor]]): \n",
    "                The output tuple of tuples of tensors after applying the final embedding classification\n",
    "        \"\"\"\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- OutputLayer.forwardTuple() ------------\")\n",
    "            print(f\"x:\\n{x}\")\n",
    "            \n",
    "        # forwardTuple() should only be used during training, so we assert input_len == max_position_embeddings\n",
    "        assert type(x) == tuple\n",
    "        input_len = x[0][0].shape[1]\n",
    "        if verbose: print(f\"input_len: {input_len}\")\n",
    "        assert input_len == config.max_position_embeddings\n",
    "\n",
    "        # we could define these from the config but this way the method is more flexible to testing\n",
    "        num_levels = len(x)\n",
    "        models_per_level = [len(x[i]) for i in range(num_levels)]\n",
    "        if verbose: \n",
    "            print(f\"num_levels: {num_levels}\")\n",
    "            print(f\"models_per_level: {models_per_level}\")\n",
    "\n",
    "        # the loop that iterates over levels, aka the different potential sizes of models\n",
    "        out = ()\n",
    "        for i in range(num_levels):\n",
    "            if verbose: print(f\"Level {i} from range({num_levels})\")\n",
    "\n",
    "            # now for the loop that iterates over models in this level\n",
    "            out_lvl = ()\n",
    "            for j in range(models_per_level[i]):\n",
    "                if verbose: print(f\"Model {j} from range({models_per_level[i]})\")\n",
    "\n",
    "                output = self.forwardTensor(x[i][j], model = j)\n",
    "                if verbose: print(f\"forwardTensor() output: {output.shape}\\n{output}\")\n",
    "                \n",
    "                out_lvl += (output,)\n",
    "            \n",
    "            out += (out_lvl,)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"final output: {out}\")\n",
    "            print(\"------------- END Layer.forwardTuple() ------------\")\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def forward(self, x, model=0):\n",
    "        train = True if type(x) == tuple else False\n",
    "        if verbose: print(f\"---------- Layer Input: {'Tuple' if train else 'torch.Tensor'} ------------\")\n",
    "        return self.forwardTuple(x) if train else self.forwardTensor(x, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "feeb1bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [4, 2, 1]\n",
      "embedding: torch.Size([5, 4])\n",
      "tensor([[ 1.1402,  0.2886, -0.5796, -0.3033],\n",
      "        [-0.2475, -0.0909,  0.8898, -1.5566],\n",
      "        [ 0.4822,  1.4014, -0.2881, -0.0487],\n",
      "        [ 0.0914, -0.2257,  0.1666, -0.1041],\n",
      "        [-0.0122, -0.8733,  1.2139, -0.5787]])\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.5298, 0.9770, 0.7230, 0.6953],\n",
      "         [0.3665, 0.9225, 0.7337, 0.1682],\n",
      "         [0.1566, 0.0722, 0.5491, 0.9139]]])\n",
      "---------- Layer Input: torch.Tensor ------------\n",
      "------------- OutputLayer.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.5298, 0.9770, 0.7230, 0.6953],\n",
      "         [0.3665, 0.9225, 0.7337, 0.1682],\n",
      "         [0.1566, 0.0722, 0.5491, 0.9139]]])\n",
      "d_i: 4\n",
      "skip: 0\n",
      "embedding: torch.Size([5, 4])\n",
      "tensor([[ 1.1402,  0.2886, -0.5796, -0.3033],\n",
      "        [-0.2475, -0.0909,  0.8898, -1.5566],\n",
      "        [ 0.4822,  1.4014, -0.2881, -0.0487],\n",
      "        [ 0.0914, -0.2257,  0.1666, -0.1041],\n",
      "        [-0.0122, -0.8733,  1.2139, -0.5787]])\n",
      "sliced_embed: torch.Size([5, 4])\n",
      "tensor([[ 1.1402,  0.2886, -0.5796, -0.3033],\n",
      "        [-0.2475, -0.0909,  0.8898, -1.5566],\n",
      "        [ 0.4822,  1.4014, -0.2881, -0.0487],\n",
      "        [ 0.0914, -0.2257,  0.1666, -0.1041],\n",
      "        [-0.0122, -0.8733,  1.2139, -0.5787]])\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([5, 4])\n",
      "tensor([[ 1.1402,  0.2886, -0.5796, -0.3033],\n",
      "        [-0.2475, -0.0909,  0.8898, -1.5566],\n",
      "        [ 0.4822,  1.4014, -0.2881, -0.0487],\n",
      "        [ 0.0914, -0.2257,  0.1666, -0.1041],\n",
      "        [-0.0122, -0.8733,  1.2139, -0.5787]])\n",
      "normed x: torch.Size([5, 4])\n",
      "tensor([[ 1.6944,  0.4290, -0.8613, -0.4507],\n",
      "        [-0.2732, -0.1003,  0.9820, -1.7178],\n",
      "        [ 0.6384,  1.8555, -0.3814, -0.0644],\n",
      "        [ 0.5845, -1.4426,  1.0650, -0.6655],\n",
      "        [-0.0152, -1.0892,  1.5141, -0.7218]])\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([5, 4])\n",
      "tensor([[ 1.6944,  0.4290, -0.8613, -0.4507],\n",
      "        [-0.2732, -0.1003,  0.9820, -1.7178],\n",
      "        [ 0.6384,  1.8555, -0.3814, -0.0644],\n",
      "        [ 0.5845, -1.4426,  1.0650, -0.6655],\n",
      "        [-0.0152, -1.0892,  1.5141, -0.7218]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "normed & sliced embedding: torch.Size([5, 4])\n",
      "tensor([[ 1.6944,  0.4290, -0.8613, -0.4507],\n",
      "        [-0.2732, -0.1003,  0.9820, -1.7178],\n",
      "        [ 0.6384,  1.8555, -0.3814, -0.0644],\n",
      "        [ 0.5845, -1.4426,  1.0650, -0.6655],\n",
      "        [-0.0152, -1.0892,  1.5141, -0.7218]], grad_fn=<MulBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.5298, 0.9770, 0.7230, 0.6953],\n",
      "         [0.3665, 0.9225, 0.7337, 0.1682],\n",
      "         [0.1566, 0.0722, 0.5491, 0.9139]]])\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.7077, 1.3052, 0.9658, 0.9289],\n",
      "         [0.5885, 1.4810, 1.1780, 0.2700],\n",
      "         [0.2900, 0.1337, 1.0168, 1.6924]]])\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.7077, 1.3052, 0.9658, 0.9289],\n",
      "         [0.5885, 1.4810, 1.1780, 0.2700],\n",
      "         [0.2900, 0.1337, 1.0168, 1.6924]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[0.7077, 1.3052, 0.9658, 0.9289],\n",
      "         [0.5885, 1.4810, 1.1780, 0.2700],\n",
      "         [0.2900, 0.1337, 1.0168, 1.6924]]], grad_fn=<MulBackward0>)\n",
      "final logits: torch.Size([1, 3, 5])\n",
      "tensor([[[ 0.5085, -0.9715,  2.4453, -1.0588, -0.6405],\n",
      "         [ 0.4960,  0.3836,  2.6568, -0.7177, -0.0334],\n",
      "         [-1.0898, -2.0014, -0.0637, -0.0668,  0.1679]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "------------- END OutputLayer.forwardTensor() ------------\n",
      "y: torch.Size([1, 3, 5])\n",
      "tensor([[[ 0.5085, -0.9715,  2.4453, -1.0588, -0.6405],\n",
      "         [ 0.4960,  0.3836,  2.6568, -0.7177, -0.0334],\n",
      "         [-1.0898, -2.0014, -0.0637, -0.0668,  0.1679]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.8341, 0.1438],\n",
      "         [0.2851, 0.7197],\n",
      "         [0.9800, 0.7917]]])\n",
      "---------- Layer Input: torch.Tensor ------------\n",
      "------------- OutputLayer.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.8341, 0.1438],\n",
      "         [0.2851, 0.7197],\n",
      "         [0.9800, 0.7917]]])\n",
      "d_i: 2\n",
      "skip: 0\n",
      "embedding: torch.Size([5, 4])\n",
      "tensor([[ 1.1402,  0.2886, -0.5796, -0.3033],\n",
      "        [-0.2475, -0.0909,  0.8898, -1.5566],\n",
      "        [ 0.4822,  1.4014, -0.2881, -0.0487],\n",
      "        [ 0.0914, -0.2257,  0.1666, -0.1041],\n",
      "        [-0.0122, -0.8733,  1.2139, -0.5787]])\n",
      "sliced_embed: torch.Size([5, 2])\n",
      "tensor([[ 1.1402,  0.2886],\n",
      "        [-0.2475, -0.0909],\n",
      "        [ 0.4822,  1.4014],\n",
      "        [ 0.0914, -0.2257],\n",
      "        [-0.0122, -0.8733]])\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([5, 2])\n",
      "tensor([[ 1.1402,  0.2886],\n",
      "        [-0.2475, -0.0909],\n",
      "        [ 0.4822,  1.4014],\n",
      "        [ 0.0914, -0.2257],\n",
      "        [-0.0122, -0.8733]])\n",
      "normed x: torch.Size([5, 2])\n",
      "tensor([[ 1.3710,  0.3471],\n",
      "        [-1.3275, -0.4876],\n",
      "        [ 0.4601,  1.3373],\n",
      "        [ 0.5310, -1.3107],\n",
      "        [-0.0197, -1.4141]])\n",
      "dim: 2\n",
      "skip: 0\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([5, 2])\n",
      "tensor([[ 1.3710,  0.3471],\n",
      "        [-1.3275, -0.4876],\n",
      "        [ 0.4601,  1.3373],\n",
      "        [ 0.5310, -1.3107],\n",
      "        [-0.0197, -1.4141]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "normed & sliced embedding: torch.Size([5, 2])\n",
      "tensor([[ 1.3710,  0.3471],\n",
      "        [-1.3275, -0.4876],\n",
      "        [ 0.4601,  1.3373],\n",
      "        [ 0.5310, -1.3107],\n",
      "        [-0.0197, -1.4141]], grad_fn=<MulBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.8341, 0.1438],\n",
      "         [0.2851, 0.7197],\n",
      "         [0.9800, 0.7917]]])\n",
      "normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[1.3937, 0.2402],\n",
      "         [0.5208, 1.3148],\n",
      "         [1.1001, 0.8887]]])\n",
      "dim: 2\n",
      "skip: 0\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[1.3937, 0.2402],\n",
      "         [0.5208, 1.3148],\n",
      "         [1.1001, 0.8887]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[1.3937, 0.2402],\n",
      "         [0.5208, 1.3148],\n",
      "         [1.1001, 0.8887]]], grad_fn=<MulBackward0>)\n",
      "final logits: torch.Size([1, 3, 5])\n",
      "tensor([[[ 1.9940, -1.9672,  0.9625,  0.4252, -0.3671],\n",
      "         [ 1.1704, -1.3325,  1.9979, -1.4467, -1.8695],\n",
      "         [ 1.8166, -1.8937,  1.6946, -0.5806, -1.2783]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "------------- END OutputLayer.forwardTensor() ------------\n",
      "y: torch.Size([1, 3, 5])\n",
      "tensor([[[ 1.9940, -1.9672,  0.9625,  0.4252, -0.3671],\n",
      "         [ 1.1704, -1.3325,  1.9979, -1.4467, -1.8695],\n",
      "         [ 1.8166, -1.8937,  1.6946, -0.5806, -1.2783]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.8370, 0.1579],\n",
      "         [0.6240, 0.4167],\n",
      "         [0.5467, 0.6049]]])\n",
      "---------- Layer Input: torch.Tensor ------------\n",
      "------------- OutputLayer.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.8370, 0.1579],\n",
      "         [0.6240, 0.4167],\n",
      "         [0.5467, 0.6049]]])\n",
      "d_i: 2\n",
      "skip: 2\n",
      "embedding: torch.Size([5, 4])\n",
      "tensor([[ 1.1402,  0.2886, -0.5796, -0.3033],\n",
      "        [-0.2475, -0.0909,  0.8898, -1.5566],\n",
      "        [ 0.4822,  1.4014, -0.2881, -0.0487],\n",
      "        [ 0.0914, -0.2257,  0.1666, -0.1041],\n",
      "        [-0.0122, -0.8733,  1.2139, -0.5787]])\n",
      "sliced_embed: torch.Size([5, 2])\n",
      "tensor([[-0.5796, -0.3033],\n",
      "        [ 0.8898, -1.5566],\n",
      "        [-0.2881, -0.0487],\n",
      "        [ 0.1666, -0.1041],\n",
      "        [ 1.2139, -0.5787]])\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([5, 2])\n",
      "tensor([[-0.5796, -0.3033],\n",
      "        [ 0.8898, -1.5566],\n",
      "        [-0.2881, -0.0487],\n",
      "        [ 0.1666, -0.1041],\n",
      "        [ 1.2139, -0.5787]])\n",
      "normed x: torch.Size([5, 2])\n",
      "tensor([[-1.2530, -0.6557],\n",
      "        [ 0.7019, -1.2278],\n",
      "        [-1.3944, -0.2356],\n",
      "        [ 1.1993, -0.7494],\n",
      "        [ 1.2766, -0.6086]])\n",
      "dim: 2\n",
      "skip: 0\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([5, 2])\n",
      "tensor([[-1.2530, -0.6557],\n",
      "        [ 0.7019, -1.2278],\n",
      "        [-1.3944, -0.2356],\n",
      "        [ 1.1993, -0.7494],\n",
      "        [ 1.2766, -0.6086]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "normed & sliced embedding: torch.Size([5, 2])\n",
      "tensor([[-1.2530, -0.6557],\n",
      "        [ 0.7019, -1.2278],\n",
      "        [-1.3944, -0.2356],\n",
      "        [ 1.1993, -0.7494],\n",
      "        [ 1.2766, -0.6086]], grad_fn=<MulBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[0.8370, 0.1579],\n",
      "         [0.6240, 0.4167],\n",
      "         [0.5467, 0.6049]]])\n",
      "normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[1.3897, 0.2622],\n",
      "         [1.1761, 0.7854],\n",
      "         [0.9482, 1.0492]]])\n",
      "dim: 2\n",
      "skip: 2\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[1.3897, 0.2622],\n",
      "         [1.1761, 0.7854],\n",
      "         [0.9482, 1.0492]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[1.3897, 0.2622],\n",
      "         [1.1761, 0.7854],\n",
      "         [0.9482, 1.0492]]], grad_fn=<MulBackward0>)\n",
      "final logits: torch.Size([1, 3, 5])\n",
      "tensor([[[-1.9133,  0.6534, -1.9996,  1.4701,  1.6145],\n",
      "         [-1.9886, -0.1388, -1.8250,  0.8219,  1.0234],\n",
      "         [-1.8761, -0.6226, -1.5694,  0.3509,  0.5720]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "------------- END OutputLayer.forwardTensor() ------------\n",
      "y: torch.Size([1, 3, 5])\n",
      "tensor([[[-1.9133,  0.6534, -1.9996,  1.4701,  1.6145],\n",
      "         [-1.9886, -0.1388, -1.8250,  0.8219,  1.0234],\n",
      "         [-1.8761, -0.6226, -1.5694,  0.3509,  0.5720]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n"
     ]
    }
   ],
   "source": [
    "# Testing our OutputLayer's forwardTensor()\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2 = config.hidden_size, config.vocab_size\n",
    "config.hidden_size = 4\n",
    "config.vocab_size = 5\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "embedding = torch.randn(config.vocab_size, config.hidden_size)\n",
    "print(f\"embedding: {embedding.shape}\\n{embedding}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the big model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "layer = OutputLayer(embedding, config)\n",
    "y = layer(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the first sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size//config.split)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "y = layer(x)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "print(f\"|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|- the second sub-model |-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-|-\")\n",
    "x = torch.rand(1,3,config.hidden_size//config.split)\n",
    "print(f\"x: {x.shape}\\n{x}\")\n",
    "y = layer(x, model=1)\n",
    "print(f\"y: {y.shape}\\n{y}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.vocab_size = hold2\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "# clear up memory\n",
    "del hold1, hold2, x, layer, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "87ca01f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [4, 2]\n",
      "embedding: torch.Size([5, 4])\n",
      "tensor([[ 1.0172, -0.1558, -0.1834,  0.0112],\n",
      "        [ 0.4118,  0.4647, -0.0051,  0.9454],\n",
      "        [-0.1046, -1.1461,  0.3192,  0.1052],\n",
      "        [ 0.3178, -0.1745,  1.4441, -0.8474],\n",
      "        [ 0.7193,  1.2875,  0.2692,  1.9234]])\n",
      "x: ((tensor([[[ 0.9494, -0.2405,  0.5879,  1.4847],\n",
      "         [ 1.0608, -1.3170,  1.0158,  1.9927],\n",
      "         [ 1.5784,  1.0044, -1.6931, -0.0397]]]),), (tensor([[[ 0.6289,  0.4586],\n",
      "         [-0.6905,  1.2571],\n",
      "         [ 0.7723,  3.0443]]]), tensor([[[-0.4059, -0.8723],\n",
      "         [ 0.0212,  1.2475],\n",
      "         [ 0.7738,  0.3308]]])))\n",
      "---------- Layer Input: Tuple ------------\n",
      "------------- Layer.forwardTuple() ------------\n",
      "x:\n",
      "((tensor([[[ 0.9494, -0.2405,  0.5879,  1.4847],\n",
      "         [ 1.0608, -1.3170,  1.0158,  1.9927],\n",
      "         [ 1.5784,  1.0044, -1.6931, -0.0397]]]),), (tensor([[[ 0.6289,  0.4586],\n",
      "         [-0.6905,  1.2571],\n",
      "         [ 0.7723,  3.0443]]]), tensor([[[-0.4059, -0.8723],\n",
      "         [ 0.0212,  1.2475],\n",
      "         [ 0.7738,  0.3308]]])))\n",
      "input_len: 3\n",
      "num_levels: 2\n",
      "models_per_level: [1, 2]\n",
      "Level 0 from range(2)\n",
      "Model 0 from range(1)\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.9494, -0.2405,  0.5879,  1.4847],\n",
      "         [ 1.0608, -1.3170,  1.0158,  1.9927],\n",
      "         [ 1.5784,  1.0044, -1.6931, -0.0397]]])\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 1.0136, -0.2568,  0.6277,  1.5851],\n",
      "         [ 0.7566, -0.9394,  0.7245,  1.4213],\n",
      "         [ 1.2509,  0.7960, -1.3418, -0.0314]]])\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 1.0136, -0.2568,  0.6277,  1.5851],\n",
      "         [ 0.7566, -0.9394,  0.7245,  1.4213],\n",
      "         [ 1.2509,  0.7960, -1.3418, -0.0314]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 4])\n",
      "d_skip: 0\n",
      "models_in_this_level: 1\n",
      "h_dim: 32\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([4, 192])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3301,  0.3833,  0.4804,  0.3799, -0.3784,  0.0511,  0.3683,  0.1008,\n",
      "         -0.2285, -0.0197,  0.0348,  0.1120, -0.3621, -0.2547,  0.4441, -0.4699,\n",
      "         -0.3608, -0.4284,  0.1076,  0.4837, -0.0746,  0.1340, -0.4201, -0.0809,\n",
      "          0.1141, -0.1077,  0.0644,  0.3932,  0.0016,  0.0923, -0.3204, -0.1327,\n",
      "         -0.3313, -0.1889, -0.1339,  0.0845,  0.1315, -0.0367, -0.3632, -0.3186,\n",
      "         -0.0542, -0.2293, -0.4923, -0.4534, -0.3555, -0.4473, -0.2461,  0.3808,\n",
      "         -0.2491, -0.0109, -0.3062,  0.1362, -0.3017,  0.0435,  0.1770,  0.4863,\n",
      "         -0.2845, -0.0166,  0.4380,  0.1111,  0.3862,  0.1065,  0.2000, -0.2471,\n",
      "         -0.0877, -0.4372,  0.4612,  0.0399,  0.2884,  0.0085, -0.3506,  0.0517,\n",
      "          0.2164, -0.1687,  0.3631,  0.2543, -0.0103,  0.4510,  0.3025, -0.0815,\n",
      "         -0.0690,  0.4011, -0.0612, -0.3017, -0.4315, -0.0644, -0.0030,  0.1530,\n",
      "         -0.3891, -0.2778, -0.2134, -0.0432,  0.4629, -0.0109, -0.0543,  0.3298,\n",
      "          0.1822, -0.2275, -0.2900, -0.0932, -0.0859,  0.1376,  0.4553,  0.2669,\n",
      "         -0.1041, -0.4566,  0.4032,  0.4596,  0.0240,  0.2582,  0.4123,  0.0547,\n",
      "          0.2229,  0.0054, -0.2271,  0.2998,  0.3078,  0.1199,  0.0656, -0.4977,\n",
      "         -0.1922, -0.1638, -0.0580, -0.1722, -0.0207, -0.2017, -0.4377, -0.2301,\n",
      "          0.1247,  0.4458,  0.0637, -0.4035, -0.1888, -0.3908,  0.4974,  0.0636,\n",
      "         -0.3287,  0.3867, -0.3518, -0.0131,  0.0452, -0.4103, -0.0953,  0.3811,\n",
      "          0.1975,  0.4332,  0.1069,  0.0880, -0.3776,  0.1469, -0.3684,  0.2586,\n",
      "          0.0258,  0.2367, -0.4173, -0.4539,  0.0401,  0.3268,  0.4671,  0.3535,\n",
      "          0.3936,  0.4408,  0.4123,  0.1102, -0.4779,  0.2480,  0.2368, -0.1251,\n",
      "         -0.3720,  0.2231, -0.0660,  0.0564, -0.3418, -0.2669,  0.4652, -0.4518,\n",
      "         -0.3219,  0.2803, -0.4718, -0.3934, -0.3866, -0.2158,  0.4596,  0.4616,\n",
      "          0.3334,  0.4542, -0.2137, -0.0657, -0.2930,  0.2428,  0.0470, -0.3103],\n",
      "        [-0.3085,  0.2079, -0.0558,  0.4727, -0.0572,  0.4325, -0.4721, -0.3933,\n",
      "         -0.4037,  0.2223, -0.3074, -0.4287,  0.4854, -0.1563,  0.2378,  0.3082,\n",
      "          0.2669, -0.4955, -0.4475,  0.1592, -0.2937,  0.1021, -0.2588,  0.0658,\n",
      "          0.1464,  0.0969, -0.4747, -0.1736, -0.2305,  0.3631,  0.2602,  0.4455,\n",
      "         -0.4695, -0.4149,  0.2928, -0.2088, -0.2163,  0.2472,  0.2846, -0.3211,\n",
      "         -0.2504,  0.3590, -0.4542, -0.4578,  0.0834, -0.2651, -0.3716,  0.0151,\n",
      "         -0.0984,  0.0979,  0.0320,  0.0527, -0.4798, -0.0049,  0.3435, -0.2685,\n",
      "          0.0051, -0.3738, -0.0213,  0.3898,  0.4789,  0.2627, -0.2252,  0.1004,\n",
      "         -0.4895, -0.0907, -0.1520, -0.2988,  0.0696,  0.3762, -0.0568,  0.3029,\n",
      "          0.2704,  0.1733,  0.0607, -0.3022,  0.0958,  0.2246,  0.0864,  0.1957,\n",
      "         -0.1262,  0.0189, -0.3558,  0.3480, -0.4218,  0.3846,  0.2977, -0.2668,\n",
      "         -0.4719, -0.3136,  0.0346, -0.2221, -0.0843, -0.1978,  0.3260, -0.1030,\n",
      "          0.2973,  0.2319, -0.3665,  0.3388, -0.4193, -0.3237,  0.3003,  0.1727,\n",
      "          0.1312,  0.3687,  0.3775, -0.1518, -0.0341, -0.4558,  0.4252,  0.1452,\n",
      "          0.4464, -0.0587,  0.2010,  0.3056,  0.1037, -0.2781,  0.1119,  0.4075,\n",
      "          0.3024,  0.4253,  0.3235, -0.2391, -0.1235, -0.2415,  0.3376,  0.0750,\n",
      "          0.4636, -0.2067, -0.2724,  0.2813,  0.3083, -0.1645,  0.4072, -0.3367,\n",
      "         -0.4844,  0.4903, -0.2458,  0.3983,  0.4745,  0.1093,  0.3478, -0.3051,\n",
      "          0.2002, -0.0206,  0.2579, -0.2674, -0.3509,  0.4186, -0.1168,  0.3344,\n",
      "          0.4014, -0.1908,  0.1122, -0.2635, -0.2765, -0.4911,  0.3065,  0.2265,\n",
      "         -0.4627, -0.0526, -0.4488,  0.0989, -0.0009,  0.3884,  0.4852, -0.1694,\n",
      "         -0.1860, -0.0708, -0.4281,  0.1182, -0.2934,  0.0983, -0.2847,  0.1333,\n",
      "          0.3494,  0.2996, -0.4393,  0.2396,  0.0914, -0.1308,  0.4602,  0.1519,\n",
      "          0.0964,  0.2173, -0.3352,  0.2081, -0.3686,  0.4634,  0.1625,  0.1153],\n",
      "        [-0.4529, -0.1572, -0.0539,  0.3450,  0.4261,  0.2598,  0.2026, -0.0352,\n",
      "          0.2761, -0.3891,  0.3350, -0.2972, -0.0346, -0.0281, -0.1072,  0.4468,\n",
      "          0.0352,  0.2435, -0.1058,  0.3529,  0.0723,  0.2023,  0.3687, -0.4046,\n",
      "         -0.4893, -0.1057,  0.0078, -0.4062,  0.2501,  0.4469,  0.2588,  0.3636,\n",
      "         -0.4504, -0.4279, -0.2497, -0.4591, -0.4598, -0.1506,  0.0301,  0.1856,\n",
      "         -0.3996,  0.0477, -0.2218,  0.4408,  0.3175, -0.2819, -0.3891, -0.2715,\n",
      "         -0.0119, -0.1215,  0.1585,  0.4206, -0.1848,  0.1778,  0.4221, -0.1031,\n",
      "          0.1428,  0.2237, -0.3037, -0.0875,  0.3658,  0.3300,  0.1022, -0.1516,\n",
      "         -0.4768,  0.0470,  0.2139,  0.3245, -0.2838, -0.2326,  0.3793, -0.4275,\n",
      "          0.0624, -0.1127,  0.3912, -0.2025, -0.1955,  0.0486, -0.3604,  0.1436,\n",
      "         -0.2875, -0.3124, -0.1595, -0.0667, -0.0748, -0.3298, -0.3057,  0.2011,\n",
      "          0.1131, -0.0505, -0.2193,  0.2211,  0.3423, -0.1574, -0.4652,  0.2528,\n",
      "          0.1820,  0.1900, -0.2276, -0.0111, -0.4589, -0.4369,  0.2751,  0.2070,\n",
      "          0.2440, -0.3492,  0.1704, -0.0839, -0.4068,  0.1689, -0.2187, -0.2404,\n",
      "         -0.4594,  0.1789, -0.4587, -0.1004,  0.4327,  0.1572,  0.1019,  0.1318,\n",
      "          0.3062,  0.3306,  0.1911,  0.4272,  0.0892,  0.2553,  0.4253,  0.1659,\n",
      "         -0.0153, -0.3781,  0.4079,  0.0030,  0.4418, -0.4477,  0.2635, -0.3938,\n",
      "          0.0555, -0.3003, -0.2474, -0.3613,  0.0365,  0.0919,  0.0444, -0.0326,\n",
      "          0.1988, -0.0043,  0.4960, -0.2726,  0.2577, -0.1399,  0.0490, -0.2605,\n",
      "         -0.2334,  0.1966, -0.3514, -0.0269, -0.2524, -0.0522,  0.3142, -0.0619,\n",
      "         -0.2309,  0.3951, -0.2231,  0.4976, -0.0028, -0.4960, -0.3880,  0.3877,\n",
      "         -0.3430,  0.3044,  0.0261,  0.3276, -0.1741, -0.0334,  0.0502,  0.0558,\n",
      "         -0.2254,  0.2467,  0.3906, -0.3988, -0.2671,  0.3635, -0.2941, -0.4413,\n",
      "         -0.2717, -0.1500,  0.1175, -0.0211, -0.1650,  0.1372, -0.1467, -0.4682],\n",
      "        [ 0.1006,  0.1690, -0.2365, -0.2573, -0.3941, -0.1949,  0.1145,  0.4879,\n",
      "          0.3939, -0.4653,  0.3927, -0.3928, -0.1648, -0.3126, -0.2240, -0.2955,\n",
      "          0.4953,  0.0201, -0.4571, -0.3099,  0.1722, -0.3074, -0.2650, -0.0343,\n",
      "          0.2155,  0.1219,  0.4715,  0.1549,  0.0180, -0.3136, -0.1017,  0.0442,\n",
      "         -0.0807,  0.3326, -0.2793,  0.4693,  0.2938,  0.2537,  0.4737, -0.4222,\n",
      "         -0.1041,  0.0470, -0.1856,  0.1775, -0.1044,  0.3396,  0.3710,  0.1461,\n",
      "          0.3862,  0.0915,  0.0245, -0.4862,  0.2971, -0.4279, -0.0600, -0.3252,\n",
      "          0.0575,  0.0737, -0.1541,  0.0132, -0.3831,  0.1708,  0.1641,  0.3259,\n",
      "          0.2080, -0.2994,  0.3289, -0.1726,  0.1728, -0.4535,  0.2559,  0.0939,\n",
      "          0.1923, -0.4447,  0.4706,  0.1378,  0.3362,  0.3585, -0.4693,  0.0737,\n",
      "         -0.0180, -0.1533,  0.1602,  0.0328,  0.3808,  0.0183, -0.4225,  0.2609,\n",
      "         -0.2920, -0.3166,  0.0291,  0.4230,  0.3254, -0.1631,  0.0777,  0.0084,\n",
      "          0.3930,  0.0684,  0.3393, -0.4065,  0.0184,  0.1434, -0.4191, -0.2668,\n",
      "         -0.2583, -0.3938,  0.2692,  0.3379, -0.1610, -0.4425, -0.3757, -0.3587,\n",
      "         -0.4249,  0.2199,  0.3004,  0.0443,  0.3836,  0.0263,  0.2958, -0.1507,\n",
      "          0.0537, -0.0687,  0.1514, -0.4911,  0.0052,  0.0761, -0.1913,  0.1853,\n",
      "         -0.0124, -0.3541,  0.0277, -0.3630, -0.3104, -0.1769, -0.1923,  0.1557,\n",
      "          0.4131,  0.2811,  0.3978,  0.4198,  0.4174,  0.1364, -0.3672,  0.0243,\n",
      "         -0.0475, -0.2074, -0.3462,  0.3039,  0.0092, -0.2067,  0.4334, -0.2698,\n",
      "         -0.4175,  0.4882,  0.1428,  0.1868, -0.2379, -0.2610,  0.2044,  0.1492,\n",
      "         -0.4313, -0.2646,  0.0871, -0.2397,  0.1344,  0.3381, -0.0484,  0.2644,\n",
      "         -0.2736, -0.4856, -0.1567,  0.0605,  0.1655,  0.1458,  0.1922, -0.2275,\n",
      "          0.3972, -0.1485, -0.0114,  0.1299,  0.2137,  0.2654,  0.2137, -0.3936,\n",
      "         -0.4087,  0.1286,  0.3506, -0.1719, -0.0443, -0.4216, -0.0135,  0.2737]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([4, 128])\n",
      "tensor([[ 0.3301,  0.3833,  0.4804,  0.3799, -0.3784,  0.0511,  0.3683,  0.1008,\n",
      "         -0.2285, -0.0197,  0.0348,  0.1120, -0.3621, -0.2547,  0.4441, -0.4699,\n",
      "         -0.3608, -0.4284,  0.1076,  0.4837, -0.0746,  0.1340, -0.4201, -0.0809,\n",
      "          0.1141, -0.1077,  0.0644,  0.3932,  0.0016,  0.0923, -0.3204, -0.1327,\n",
      "         -0.3313, -0.1889, -0.1339,  0.0845,  0.1315, -0.0367, -0.3632, -0.3186,\n",
      "         -0.0542, -0.2293, -0.4923, -0.4534, -0.3555, -0.4473, -0.2461,  0.3808,\n",
      "         -0.2491, -0.0109, -0.3062,  0.1362, -0.3017,  0.0435,  0.1770,  0.4863,\n",
      "         -0.2845, -0.0166,  0.4380,  0.1111,  0.3862,  0.1065,  0.2000, -0.2471,\n",
      "         -0.0877, -0.4372,  0.4612,  0.0399,  0.2884,  0.0085, -0.3506,  0.0517,\n",
      "          0.2164, -0.1687,  0.3631,  0.2543, -0.0103,  0.4510,  0.3025, -0.0815,\n",
      "         -0.0690,  0.4011, -0.0612, -0.3017, -0.4315, -0.0644, -0.0030,  0.1530,\n",
      "         -0.3891, -0.2778, -0.2134, -0.0432,  0.4629, -0.0109, -0.0543,  0.3298,\n",
      "          0.1822, -0.2275, -0.2900, -0.0932, -0.0859,  0.1376,  0.4553,  0.2669,\n",
      "         -0.1041, -0.4566,  0.4032,  0.4596,  0.0240,  0.2582,  0.4123,  0.0547,\n",
      "          0.2229,  0.0054, -0.2271,  0.2998,  0.3078,  0.1199,  0.0656, -0.4977,\n",
      "         -0.1922, -0.1638, -0.0580, -0.1722, -0.0207, -0.2017, -0.4377, -0.2301],\n",
      "        [-0.3085,  0.2079, -0.0558,  0.4727, -0.0572,  0.4325, -0.4721, -0.3933,\n",
      "         -0.4037,  0.2223, -0.3074, -0.4287,  0.4854, -0.1563,  0.2378,  0.3082,\n",
      "          0.2669, -0.4955, -0.4475,  0.1592, -0.2937,  0.1021, -0.2588,  0.0658,\n",
      "          0.1464,  0.0969, -0.4747, -0.1736, -0.2305,  0.3631,  0.2602,  0.4455,\n",
      "         -0.4695, -0.4149,  0.2928, -0.2088, -0.2163,  0.2472,  0.2846, -0.3211,\n",
      "         -0.2504,  0.3590, -0.4542, -0.4578,  0.0834, -0.2651, -0.3716,  0.0151,\n",
      "         -0.0984,  0.0979,  0.0320,  0.0527, -0.4798, -0.0049,  0.3435, -0.2685,\n",
      "          0.0051, -0.3738, -0.0213,  0.3898,  0.4789,  0.2627, -0.2252,  0.1004,\n",
      "         -0.4895, -0.0907, -0.1520, -0.2988,  0.0696,  0.3762, -0.0568,  0.3029,\n",
      "          0.2704,  0.1733,  0.0607, -0.3022,  0.0958,  0.2246,  0.0864,  0.1957,\n",
      "         -0.1262,  0.0189, -0.3558,  0.3480, -0.4218,  0.3846,  0.2977, -0.2668,\n",
      "         -0.4719, -0.3136,  0.0346, -0.2221, -0.0843, -0.1978,  0.3260, -0.1030,\n",
      "          0.2973,  0.2319, -0.3665,  0.3388, -0.4193, -0.3237,  0.3003,  0.1727,\n",
      "          0.1312,  0.3687,  0.3775, -0.1518, -0.0341, -0.4558,  0.4252,  0.1452,\n",
      "          0.4464, -0.0587,  0.2010,  0.3056,  0.1037, -0.2781,  0.1119,  0.4075,\n",
      "          0.3024,  0.4253,  0.3235, -0.2391, -0.1235, -0.2415,  0.3376,  0.0750],\n",
      "        [-0.4529, -0.1572, -0.0539,  0.3450,  0.4261,  0.2598,  0.2026, -0.0352,\n",
      "          0.2761, -0.3891,  0.3350, -0.2972, -0.0346, -0.0281, -0.1072,  0.4468,\n",
      "          0.0352,  0.2435, -0.1058,  0.3529,  0.0723,  0.2023,  0.3687, -0.4046,\n",
      "         -0.4893, -0.1057,  0.0078, -0.4062,  0.2501,  0.4469,  0.2588,  0.3636,\n",
      "         -0.4504, -0.4279, -0.2497, -0.4591, -0.4598, -0.1506,  0.0301,  0.1856,\n",
      "         -0.3996,  0.0477, -0.2218,  0.4408,  0.3175, -0.2819, -0.3891, -0.2715,\n",
      "         -0.0119, -0.1215,  0.1585,  0.4206, -0.1848,  0.1778,  0.4221, -0.1031,\n",
      "          0.1428,  0.2237, -0.3037, -0.0875,  0.3658,  0.3300,  0.1022, -0.1516,\n",
      "         -0.4768,  0.0470,  0.2139,  0.3245, -0.2838, -0.2326,  0.3793, -0.4275,\n",
      "          0.0624, -0.1127,  0.3912, -0.2025, -0.1955,  0.0486, -0.3604,  0.1436,\n",
      "         -0.2875, -0.3124, -0.1595, -0.0667, -0.0748, -0.3298, -0.3057,  0.2011,\n",
      "          0.1131, -0.0505, -0.2193,  0.2211,  0.3423, -0.1574, -0.4652,  0.2528,\n",
      "          0.1820,  0.1900, -0.2276, -0.0111, -0.4589, -0.4369,  0.2751,  0.2070,\n",
      "          0.2440, -0.3492,  0.1704, -0.0839, -0.4068,  0.1689, -0.2187, -0.2404,\n",
      "         -0.4594,  0.1789, -0.4587, -0.1004,  0.4327,  0.1572,  0.1019,  0.1318,\n",
      "          0.3062,  0.3306,  0.1911,  0.4272,  0.0892,  0.2553,  0.4253,  0.1659],\n",
      "        [ 0.1006,  0.1690, -0.2365, -0.2573, -0.3941, -0.1949,  0.1145,  0.4879,\n",
      "          0.3939, -0.4653,  0.3927, -0.3928, -0.1648, -0.3126, -0.2240, -0.2955,\n",
      "          0.4953,  0.0201, -0.4571, -0.3099,  0.1722, -0.3074, -0.2650, -0.0343,\n",
      "          0.2155,  0.1219,  0.4715,  0.1549,  0.0180, -0.3136, -0.1017,  0.0442,\n",
      "         -0.0807,  0.3326, -0.2793,  0.4693,  0.2938,  0.2537,  0.4737, -0.4222,\n",
      "         -0.1041,  0.0470, -0.1856,  0.1775, -0.1044,  0.3396,  0.3710,  0.1461,\n",
      "          0.3862,  0.0915,  0.0245, -0.4862,  0.2971, -0.4279, -0.0600, -0.3252,\n",
      "          0.0575,  0.0737, -0.1541,  0.0132, -0.3831,  0.1708,  0.1641,  0.3259,\n",
      "          0.2080, -0.2994,  0.3289, -0.1726,  0.1728, -0.4535,  0.2559,  0.0939,\n",
      "          0.1923, -0.4447,  0.4706,  0.1378,  0.3362,  0.3585, -0.4693,  0.0737,\n",
      "         -0.0180, -0.1533,  0.1602,  0.0328,  0.3808,  0.0183, -0.4225,  0.2609,\n",
      "         -0.2920, -0.3166,  0.0291,  0.4230,  0.3254, -0.1631,  0.0777,  0.0084,\n",
      "          0.3930,  0.0684,  0.3393, -0.4065,  0.0184,  0.1434, -0.4191, -0.2668,\n",
      "         -0.2583, -0.3938,  0.2692,  0.3379, -0.1610, -0.4425, -0.3757, -0.3587,\n",
      "         -0.4249,  0.2199,  0.3004,  0.0443,  0.3836,  0.0263,  0.2958, -0.1507,\n",
      "          0.0537, -0.0687,  0.1514, -0.4911,  0.0052,  0.0761, -0.1913,  0.1853]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([4, 32])\n",
      "tensor([[ 0.1247,  0.4458,  0.0637, -0.4035, -0.1888, -0.3908,  0.4974,  0.0636,\n",
      "         -0.3287,  0.3867, -0.3518, -0.0131,  0.0452, -0.4103, -0.0953,  0.3811,\n",
      "          0.1975,  0.4332,  0.1069,  0.0880, -0.3776,  0.1469, -0.3684,  0.2586,\n",
      "          0.0258,  0.2367, -0.4173, -0.4539,  0.0401,  0.3268,  0.4671,  0.3535],\n",
      "        [ 0.4636, -0.2067, -0.2724,  0.2813,  0.3083, -0.1645,  0.4072, -0.3367,\n",
      "         -0.4844,  0.4903, -0.2458,  0.3983,  0.4745,  0.1093,  0.3478, -0.3051,\n",
      "          0.2002, -0.0206,  0.2579, -0.2674, -0.3509,  0.4186, -0.1168,  0.3344,\n",
      "          0.4014, -0.1908,  0.1122, -0.2635, -0.2765, -0.4911,  0.3065,  0.2265],\n",
      "        [-0.0153, -0.3781,  0.4079,  0.0030,  0.4418, -0.4477,  0.2635, -0.3938,\n",
      "          0.0555, -0.3003, -0.2474, -0.3613,  0.0365,  0.0919,  0.0444, -0.0326,\n",
      "          0.1988, -0.0043,  0.4960, -0.2726,  0.2577, -0.1399,  0.0490, -0.2605,\n",
      "         -0.2334,  0.1966, -0.3514, -0.0269, -0.2524, -0.0522,  0.3142, -0.0619],\n",
      "        [-0.0124, -0.3541,  0.0277, -0.3630, -0.3104, -0.1769, -0.1923,  0.1557,\n",
      "          0.4131,  0.2811,  0.3978,  0.4198,  0.4174,  0.1364, -0.3672,  0.0243,\n",
      "         -0.0475, -0.2074, -0.3462,  0.3039,  0.0092, -0.2067,  0.4334, -0.2698,\n",
      "         -0.4175,  0.4882,  0.1428,  0.1868, -0.2379, -0.2610,  0.2044,  0.1492]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([4, 32])\n",
      "tensor([[ 0.3936,  0.4408,  0.4123,  0.1102, -0.4779,  0.2480,  0.2368, -0.1251,\n",
      "         -0.3720,  0.2231, -0.0660,  0.0564, -0.3418, -0.2669,  0.4652, -0.4518,\n",
      "         -0.3219,  0.2803, -0.4718, -0.3934, -0.3866, -0.2158,  0.4596,  0.4616,\n",
      "          0.3334,  0.4542, -0.2137, -0.0657, -0.2930,  0.2428,  0.0470, -0.3103],\n",
      "        [-0.4627, -0.0526, -0.4488,  0.0989, -0.0009,  0.3884,  0.4852, -0.1694,\n",
      "         -0.1860, -0.0708, -0.4281,  0.1182, -0.2934,  0.0983, -0.2847,  0.1333,\n",
      "          0.3494,  0.2996, -0.4393,  0.2396,  0.0914, -0.1308,  0.4602,  0.1519,\n",
      "          0.0964,  0.2173, -0.3352,  0.2081, -0.3686,  0.4634,  0.1625,  0.1153],\n",
      "        [-0.2309,  0.3951, -0.2231,  0.4976, -0.0028, -0.4960, -0.3880,  0.3877,\n",
      "         -0.3430,  0.3044,  0.0261,  0.3276, -0.1741, -0.0334,  0.0502,  0.0558,\n",
      "         -0.2254,  0.2467,  0.3906, -0.3988, -0.2671,  0.3635, -0.2941, -0.4413,\n",
      "         -0.2717, -0.1500,  0.1175, -0.0211, -0.1650,  0.1372, -0.1467, -0.4682],\n",
      "        [-0.4313, -0.2646,  0.0871, -0.2397,  0.1344,  0.3381, -0.0484,  0.2644,\n",
      "         -0.2736, -0.4856, -0.1567,  0.0605,  0.1655,  0.1458,  0.1922, -0.2275,\n",
      "          0.3972, -0.1485, -0.0114,  0.1299,  0.2137,  0.2654,  0.2137, -0.3936,\n",
      "         -0.4087,  0.1286,  0.3506, -0.1719, -0.0443, -0.4216, -0.0135,  0.2737]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([4, 128])\n",
      "tensor([[ 0.3301,  0.3833,  0.4804,  0.3799, -0.3784,  0.0511,  0.3683,  0.1008,\n",
      "         -0.2285, -0.0197,  0.0348,  0.1120, -0.3621, -0.2547,  0.4441, -0.4699,\n",
      "         -0.3608, -0.4284,  0.1076,  0.4837, -0.0746,  0.1340, -0.4201, -0.0809,\n",
      "          0.1141, -0.1077,  0.0644,  0.3932,  0.0016,  0.0923, -0.3204, -0.1327,\n",
      "         -0.3313, -0.1889, -0.1339,  0.0845,  0.1315, -0.0367, -0.3632, -0.3186,\n",
      "         -0.0542, -0.2293, -0.4923, -0.4534, -0.3555, -0.4473, -0.2461,  0.3808,\n",
      "         -0.2491, -0.0109, -0.3062,  0.1362, -0.3017,  0.0435,  0.1770,  0.4863,\n",
      "         -0.2845, -0.0166,  0.4380,  0.1111,  0.3862,  0.1065,  0.2000, -0.2471,\n",
      "         -0.0877, -0.4372,  0.4612,  0.0399,  0.2884,  0.0085, -0.3506,  0.0517,\n",
      "          0.2164, -0.1687,  0.3631,  0.2543, -0.0103,  0.4510,  0.3025, -0.0815,\n",
      "         -0.0690,  0.4011, -0.0612, -0.3017, -0.4315, -0.0644, -0.0030,  0.1530,\n",
      "         -0.3891, -0.2778, -0.2134, -0.0432,  0.4629, -0.0109, -0.0543,  0.3298,\n",
      "          0.1822, -0.2275, -0.2900, -0.0932, -0.0859,  0.1376,  0.4553,  0.2669,\n",
      "         -0.1041, -0.4566,  0.4032,  0.4596,  0.0240,  0.2582,  0.4123,  0.0547,\n",
      "          0.2229,  0.0054, -0.2271,  0.2998,  0.3078,  0.1199,  0.0656, -0.4977,\n",
      "         -0.1922, -0.1638, -0.0580, -0.1722, -0.0207, -0.2017, -0.4377, -0.2301],\n",
      "        [-0.3085,  0.2079, -0.0558,  0.4727, -0.0572,  0.4325, -0.4721, -0.3933,\n",
      "         -0.4037,  0.2223, -0.3074, -0.4287,  0.4854, -0.1563,  0.2378,  0.3082,\n",
      "          0.2669, -0.4955, -0.4475,  0.1592, -0.2937,  0.1021, -0.2588,  0.0658,\n",
      "          0.1464,  0.0969, -0.4747, -0.1736, -0.2305,  0.3631,  0.2602,  0.4455,\n",
      "         -0.4695, -0.4149,  0.2928, -0.2088, -0.2163,  0.2472,  0.2846, -0.3211,\n",
      "         -0.2504,  0.3590, -0.4542, -0.4578,  0.0834, -0.2651, -0.3716,  0.0151,\n",
      "         -0.0984,  0.0979,  0.0320,  0.0527, -0.4798, -0.0049,  0.3435, -0.2685,\n",
      "          0.0051, -0.3738, -0.0213,  0.3898,  0.4789,  0.2627, -0.2252,  0.1004,\n",
      "         -0.4895, -0.0907, -0.1520, -0.2988,  0.0696,  0.3762, -0.0568,  0.3029,\n",
      "          0.2704,  0.1733,  0.0607, -0.3022,  0.0958,  0.2246,  0.0864,  0.1957,\n",
      "         -0.1262,  0.0189, -0.3558,  0.3480, -0.4218,  0.3846,  0.2977, -0.2668,\n",
      "         -0.4719, -0.3136,  0.0346, -0.2221, -0.0843, -0.1978,  0.3260, -0.1030,\n",
      "          0.2973,  0.2319, -0.3665,  0.3388, -0.4193, -0.3237,  0.3003,  0.1727,\n",
      "          0.1312,  0.3687,  0.3775, -0.1518, -0.0341, -0.4558,  0.4252,  0.1452,\n",
      "          0.4464, -0.0587,  0.2010,  0.3056,  0.1037, -0.2781,  0.1119,  0.4075,\n",
      "          0.3024,  0.4253,  0.3235, -0.2391, -0.1235, -0.2415,  0.3376,  0.0750],\n",
      "        [-0.4529, -0.1572, -0.0539,  0.3450,  0.4261,  0.2598,  0.2026, -0.0352,\n",
      "          0.2761, -0.3891,  0.3350, -0.2972, -0.0346, -0.0281, -0.1072,  0.4468,\n",
      "          0.0352,  0.2435, -0.1058,  0.3529,  0.0723,  0.2023,  0.3687, -0.4046,\n",
      "         -0.4893, -0.1057,  0.0078, -0.4062,  0.2501,  0.4469,  0.2588,  0.3636,\n",
      "         -0.4504, -0.4279, -0.2497, -0.4591, -0.4598, -0.1506,  0.0301,  0.1856,\n",
      "         -0.3996,  0.0477, -0.2218,  0.4408,  0.3175, -0.2819, -0.3891, -0.2715,\n",
      "         -0.0119, -0.1215,  0.1585,  0.4206, -0.1848,  0.1778,  0.4221, -0.1031,\n",
      "          0.1428,  0.2237, -0.3037, -0.0875,  0.3658,  0.3300,  0.1022, -0.1516,\n",
      "         -0.4768,  0.0470,  0.2139,  0.3245, -0.2838, -0.2326,  0.3793, -0.4275,\n",
      "          0.0624, -0.1127,  0.3912, -0.2025, -0.1955,  0.0486, -0.3604,  0.1436,\n",
      "         -0.2875, -0.3124, -0.1595, -0.0667, -0.0748, -0.3298, -0.3057,  0.2011,\n",
      "          0.1131, -0.0505, -0.2193,  0.2211,  0.3423, -0.1574, -0.4652,  0.2528,\n",
      "          0.1820,  0.1900, -0.2276, -0.0111, -0.4589, -0.4369,  0.2751,  0.2070,\n",
      "          0.2440, -0.3492,  0.1704, -0.0839, -0.4068,  0.1689, -0.2187, -0.2404,\n",
      "         -0.4594,  0.1789, -0.4587, -0.1004,  0.4327,  0.1572,  0.1019,  0.1318,\n",
      "          0.3062,  0.3306,  0.1911,  0.4272,  0.0892,  0.2553,  0.4253,  0.1659],\n",
      "        [ 0.1006,  0.1690, -0.2365, -0.2573, -0.3941, -0.1949,  0.1145,  0.4879,\n",
      "          0.3939, -0.4653,  0.3927, -0.3928, -0.1648, -0.3126, -0.2240, -0.2955,\n",
      "          0.4953,  0.0201, -0.4571, -0.3099,  0.1722, -0.3074, -0.2650, -0.0343,\n",
      "          0.2155,  0.1219,  0.4715,  0.1549,  0.0180, -0.3136, -0.1017,  0.0442,\n",
      "         -0.0807,  0.3326, -0.2793,  0.4693,  0.2938,  0.2537,  0.4737, -0.4222,\n",
      "         -0.1041,  0.0470, -0.1856,  0.1775, -0.1044,  0.3396,  0.3710,  0.1461,\n",
      "          0.3862,  0.0915,  0.0245, -0.4862,  0.2971, -0.4279, -0.0600, -0.3252,\n",
      "          0.0575,  0.0737, -0.1541,  0.0132, -0.3831,  0.1708,  0.1641,  0.3259,\n",
      "          0.2080, -0.2994,  0.3289, -0.1726,  0.1728, -0.4535,  0.2559,  0.0939,\n",
      "          0.1923, -0.4447,  0.4706,  0.1378,  0.3362,  0.3585, -0.4693,  0.0737,\n",
      "         -0.0180, -0.1533,  0.1602,  0.0328,  0.3808,  0.0183, -0.4225,  0.2609,\n",
      "         -0.2920, -0.3166,  0.0291,  0.4230,  0.3254, -0.1631,  0.0777,  0.0084,\n",
      "          0.3930,  0.0684,  0.3393, -0.4065,  0.0184,  0.1434, -0.4191, -0.2668,\n",
      "         -0.2583, -0.3938,  0.2692,  0.3379, -0.1610, -0.4425, -0.3757, -0.3587,\n",
      "         -0.4249,  0.2199,  0.3004,  0.0443,  0.3836,  0.0263,  0.2958, -0.1507,\n",
      "          0.0537, -0.0687,  0.1514, -0.4911,  0.0052,  0.0761, -0.1913,  0.1853]],\n",
      "       grad_fn=<CatBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wk spliced: torch.Size([4, 32])\n",
      "tensor([[ 0.1247,  0.4458,  0.0637, -0.4035, -0.1888, -0.3908,  0.4974,  0.0636,\n",
      "         -0.3287,  0.3867, -0.3518, -0.0131,  0.0452, -0.4103, -0.0953,  0.3811,\n",
      "          0.1975,  0.4332,  0.1069,  0.0880, -0.3776,  0.1469, -0.3684,  0.2586,\n",
      "          0.0258,  0.2367, -0.4173, -0.4539,  0.0401,  0.3268,  0.4671,  0.3535],\n",
      "        [ 0.4636, -0.2067, -0.2724,  0.2813,  0.3083, -0.1645,  0.4072, -0.3367,\n",
      "         -0.4844,  0.4903, -0.2458,  0.3983,  0.4745,  0.1093,  0.3478, -0.3051,\n",
      "          0.2002, -0.0206,  0.2579, -0.2674, -0.3509,  0.4186, -0.1168,  0.3344,\n",
      "          0.4014, -0.1908,  0.1122, -0.2635, -0.2765, -0.4911,  0.3065,  0.2265],\n",
      "        [-0.0153, -0.3781,  0.4079,  0.0030,  0.4418, -0.4477,  0.2635, -0.3938,\n",
      "          0.0555, -0.3003, -0.2474, -0.3613,  0.0365,  0.0919,  0.0444, -0.0326,\n",
      "          0.1988, -0.0043,  0.4960, -0.2726,  0.2577, -0.1399,  0.0490, -0.2605,\n",
      "         -0.2334,  0.1966, -0.3514, -0.0269, -0.2524, -0.0522,  0.3142, -0.0619],\n",
      "        [-0.0124, -0.3541,  0.0277, -0.3630, -0.3104, -0.1769, -0.1923,  0.1557,\n",
      "          0.4131,  0.2811,  0.3978,  0.4198,  0.4174,  0.1364, -0.3672,  0.0243,\n",
      "         -0.0475, -0.2074, -0.3462,  0.3039,  0.0092, -0.2067,  0.4334, -0.2698,\n",
      "         -0.4175,  0.4882,  0.1428,  0.1868, -0.2379, -0.2610,  0.2044,  0.1492]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([4, 32])\n",
      "tensor([[ 0.3936,  0.4408,  0.4123,  0.1102, -0.4779,  0.2480,  0.2368, -0.1251,\n",
      "         -0.3720,  0.2231, -0.0660,  0.0564, -0.3418, -0.2669,  0.4652, -0.4518,\n",
      "         -0.3219,  0.2803, -0.4718, -0.3934, -0.3866, -0.2158,  0.4596,  0.4616,\n",
      "          0.3334,  0.4542, -0.2137, -0.0657, -0.2930,  0.2428,  0.0470, -0.3103],\n",
      "        [-0.4627, -0.0526, -0.4488,  0.0989, -0.0009,  0.3884,  0.4852, -0.1694,\n",
      "         -0.1860, -0.0708, -0.4281,  0.1182, -0.2934,  0.0983, -0.2847,  0.1333,\n",
      "          0.3494,  0.2996, -0.4393,  0.2396,  0.0914, -0.1308,  0.4602,  0.1519,\n",
      "          0.0964,  0.2173, -0.3352,  0.2081, -0.3686,  0.4634,  0.1625,  0.1153],\n",
      "        [-0.2309,  0.3951, -0.2231,  0.4976, -0.0028, -0.4960, -0.3880,  0.3877,\n",
      "         -0.3430,  0.3044,  0.0261,  0.3276, -0.1741, -0.0334,  0.0502,  0.0558,\n",
      "         -0.2254,  0.2467,  0.3906, -0.3988, -0.2671,  0.3635, -0.2941, -0.4413,\n",
      "         -0.2717, -0.1500,  0.1175, -0.0211, -0.1650,  0.1372, -0.1467, -0.4682],\n",
      "        [-0.4313, -0.2646,  0.0871, -0.2397,  0.1344,  0.3381, -0.0484,  0.2644,\n",
      "         -0.2736, -0.4856, -0.1567,  0.0605,  0.1655,  0.1458,  0.1922, -0.2275,\n",
      "          0.3972, -0.1485, -0.0114,  0.1299,  0.2137,  0.2654,  0.2137, -0.3936,\n",
      "         -0.4087,  0.1286,  0.3506, -0.1719, -0.0443, -0.4216, -0.0135,  0.2737]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([4, 192])\n",
      "tensor([[ 0.3301,  0.3833,  0.4804,  0.3799, -0.3784,  0.0511,  0.3683,  0.1008,\n",
      "         -0.2285, -0.0197,  0.0348,  0.1120, -0.3621, -0.2547,  0.4441, -0.4699,\n",
      "         -0.3608, -0.4284,  0.1076,  0.4837, -0.0746,  0.1340, -0.4201, -0.0809,\n",
      "          0.1141, -0.1077,  0.0644,  0.3932,  0.0016,  0.0923, -0.3204, -0.1327,\n",
      "         -0.3313, -0.1889, -0.1339,  0.0845,  0.1315, -0.0367, -0.3632, -0.3186,\n",
      "         -0.0542, -0.2293, -0.4923, -0.4534, -0.3555, -0.4473, -0.2461,  0.3808,\n",
      "         -0.2491, -0.0109, -0.3062,  0.1362, -0.3017,  0.0435,  0.1770,  0.4863,\n",
      "         -0.2845, -0.0166,  0.4380,  0.1111,  0.3862,  0.1065,  0.2000, -0.2471,\n",
      "         -0.0877, -0.4372,  0.4612,  0.0399,  0.2884,  0.0085, -0.3506,  0.0517,\n",
      "          0.2164, -0.1687,  0.3631,  0.2543, -0.0103,  0.4510,  0.3025, -0.0815,\n",
      "         -0.0690,  0.4011, -0.0612, -0.3017, -0.4315, -0.0644, -0.0030,  0.1530,\n",
      "         -0.3891, -0.2778, -0.2134, -0.0432,  0.4629, -0.0109, -0.0543,  0.3298,\n",
      "          0.1822, -0.2275, -0.2900, -0.0932, -0.0859,  0.1376,  0.4553,  0.2669,\n",
      "         -0.1041, -0.4566,  0.4032,  0.4596,  0.0240,  0.2582,  0.4123,  0.0547,\n",
      "          0.2229,  0.0054, -0.2271,  0.2998,  0.3078,  0.1199,  0.0656, -0.4977,\n",
      "         -0.1922, -0.1638, -0.0580, -0.1722, -0.0207, -0.2017, -0.4377, -0.2301,\n",
      "          0.1247,  0.4458,  0.0637, -0.4035, -0.1888, -0.3908,  0.4974,  0.0636,\n",
      "         -0.3287,  0.3867, -0.3518, -0.0131,  0.0452, -0.4103, -0.0953,  0.3811,\n",
      "          0.1975,  0.4332,  0.1069,  0.0880, -0.3776,  0.1469, -0.3684,  0.2586,\n",
      "          0.0258,  0.2367, -0.4173, -0.4539,  0.0401,  0.3268,  0.4671,  0.3535,\n",
      "          0.3936,  0.4408,  0.4123,  0.1102, -0.4779,  0.2480,  0.2368, -0.1251,\n",
      "         -0.3720,  0.2231, -0.0660,  0.0564, -0.3418, -0.2669,  0.4652, -0.4518,\n",
      "         -0.3219,  0.2803, -0.4718, -0.3934, -0.3866, -0.2158,  0.4596,  0.4616,\n",
      "          0.3334,  0.4542, -0.2137, -0.0657, -0.2930,  0.2428,  0.0470, -0.3103],\n",
      "        [-0.3085,  0.2079, -0.0558,  0.4727, -0.0572,  0.4325, -0.4721, -0.3933,\n",
      "         -0.4037,  0.2223, -0.3074, -0.4287,  0.4854, -0.1563,  0.2378,  0.3082,\n",
      "          0.2669, -0.4955, -0.4475,  0.1592, -0.2937,  0.1021, -0.2588,  0.0658,\n",
      "          0.1464,  0.0969, -0.4747, -0.1736, -0.2305,  0.3631,  0.2602,  0.4455,\n",
      "         -0.4695, -0.4149,  0.2928, -0.2088, -0.2163,  0.2472,  0.2846, -0.3211,\n",
      "         -0.2504,  0.3590, -0.4542, -0.4578,  0.0834, -0.2651, -0.3716,  0.0151,\n",
      "         -0.0984,  0.0979,  0.0320,  0.0527, -0.4798, -0.0049,  0.3435, -0.2685,\n",
      "          0.0051, -0.3738, -0.0213,  0.3898,  0.4789,  0.2627, -0.2252,  0.1004,\n",
      "         -0.4895, -0.0907, -0.1520, -0.2988,  0.0696,  0.3762, -0.0568,  0.3029,\n",
      "          0.2704,  0.1733,  0.0607, -0.3022,  0.0958,  0.2246,  0.0864,  0.1957,\n",
      "         -0.1262,  0.0189, -0.3558,  0.3480, -0.4218,  0.3846,  0.2977, -0.2668,\n",
      "         -0.4719, -0.3136,  0.0346, -0.2221, -0.0843, -0.1978,  0.3260, -0.1030,\n",
      "          0.2973,  0.2319, -0.3665,  0.3388, -0.4193, -0.3237,  0.3003,  0.1727,\n",
      "          0.1312,  0.3687,  0.3775, -0.1518, -0.0341, -0.4558,  0.4252,  0.1452,\n",
      "          0.4464, -0.0587,  0.2010,  0.3056,  0.1037, -0.2781,  0.1119,  0.4075,\n",
      "          0.3024,  0.4253,  0.3235, -0.2391, -0.1235, -0.2415,  0.3376,  0.0750,\n",
      "          0.4636, -0.2067, -0.2724,  0.2813,  0.3083, -0.1645,  0.4072, -0.3367,\n",
      "         -0.4844,  0.4903, -0.2458,  0.3983,  0.4745,  0.1093,  0.3478, -0.3051,\n",
      "          0.2002, -0.0206,  0.2579, -0.2674, -0.3509,  0.4186, -0.1168,  0.3344,\n",
      "          0.4014, -0.1908,  0.1122, -0.2635, -0.2765, -0.4911,  0.3065,  0.2265,\n",
      "         -0.4627, -0.0526, -0.4488,  0.0989, -0.0009,  0.3884,  0.4852, -0.1694,\n",
      "         -0.1860, -0.0708, -0.4281,  0.1182, -0.2934,  0.0983, -0.2847,  0.1333,\n",
      "          0.3494,  0.2996, -0.4393,  0.2396,  0.0914, -0.1308,  0.4602,  0.1519,\n",
      "          0.0964,  0.2173, -0.3352,  0.2081, -0.3686,  0.4634,  0.1625,  0.1153],\n",
      "        [-0.4529, -0.1572, -0.0539,  0.3450,  0.4261,  0.2598,  0.2026, -0.0352,\n",
      "          0.2761, -0.3891,  0.3350, -0.2972, -0.0346, -0.0281, -0.1072,  0.4468,\n",
      "          0.0352,  0.2435, -0.1058,  0.3529,  0.0723,  0.2023,  0.3687, -0.4046,\n",
      "         -0.4893, -0.1057,  0.0078, -0.4062,  0.2501,  0.4469,  0.2588,  0.3636,\n",
      "         -0.4504, -0.4279, -0.2497, -0.4591, -0.4598, -0.1506,  0.0301,  0.1856,\n",
      "         -0.3996,  0.0477, -0.2218,  0.4408,  0.3175, -0.2819, -0.3891, -0.2715,\n",
      "         -0.0119, -0.1215,  0.1585,  0.4206, -0.1848,  0.1778,  0.4221, -0.1031,\n",
      "          0.1428,  0.2237, -0.3037, -0.0875,  0.3658,  0.3300,  0.1022, -0.1516,\n",
      "         -0.4768,  0.0470,  0.2139,  0.3245, -0.2838, -0.2326,  0.3793, -0.4275,\n",
      "          0.0624, -0.1127,  0.3912, -0.2025, -0.1955,  0.0486, -0.3604,  0.1436,\n",
      "         -0.2875, -0.3124, -0.1595, -0.0667, -0.0748, -0.3298, -0.3057,  0.2011,\n",
      "          0.1131, -0.0505, -0.2193,  0.2211,  0.3423, -0.1574, -0.4652,  0.2528,\n",
      "          0.1820,  0.1900, -0.2276, -0.0111, -0.4589, -0.4369,  0.2751,  0.2070,\n",
      "          0.2440, -0.3492,  0.1704, -0.0839, -0.4068,  0.1689, -0.2187, -0.2404,\n",
      "         -0.4594,  0.1789, -0.4587, -0.1004,  0.4327,  0.1572,  0.1019,  0.1318,\n",
      "          0.3062,  0.3306,  0.1911,  0.4272,  0.0892,  0.2553,  0.4253,  0.1659,\n",
      "         -0.0153, -0.3781,  0.4079,  0.0030,  0.4418, -0.4477,  0.2635, -0.3938,\n",
      "          0.0555, -0.3003, -0.2474, -0.3613,  0.0365,  0.0919,  0.0444, -0.0326,\n",
      "          0.1988, -0.0043,  0.4960, -0.2726,  0.2577, -0.1399,  0.0490, -0.2605,\n",
      "         -0.2334,  0.1966, -0.3514, -0.0269, -0.2524, -0.0522,  0.3142, -0.0619,\n",
      "         -0.2309,  0.3951, -0.2231,  0.4976, -0.0028, -0.4960, -0.3880,  0.3877,\n",
      "         -0.3430,  0.3044,  0.0261,  0.3276, -0.1741, -0.0334,  0.0502,  0.0558,\n",
      "         -0.2254,  0.2467,  0.3906, -0.3988, -0.2671,  0.3635, -0.2941, -0.4413,\n",
      "         -0.2717, -0.1500,  0.1175, -0.0211, -0.1650,  0.1372, -0.1467, -0.4682],\n",
      "        [ 0.1006,  0.1690, -0.2365, -0.2573, -0.3941, -0.1949,  0.1145,  0.4879,\n",
      "          0.3939, -0.4653,  0.3927, -0.3928, -0.1648, -0.3126, -0.2240, -0.2955,\n",
      "          0.4953,  0.0201, -0.4571, -0.3099,  0.1722, -0.3074, -0.2650, -0.0343,\n",
      "          0.2155,  0.1219,  0.4715,  0.1549,  0.0180, -0.3136, -0.1017,  0.0442,\n",
      "         -0.0807,  0.3326, -0.2793,  0.4693,  0.2938,  0.2537,  0.4737, -0.4222,\n",
      "         -0.1041,  0.0470, -0.1856,  0.1775, -0.1044,  0.3396,  0.3710,  0.1461,\n",
      "          0.3862,  0.0915,  0.0245, -0.4862,  0.2971, -0.4279, -0.0600, -0.3252,\n",
      "          0.0575,  0.0737, -0.1541,  0.0132, -0.3831,  0.1708,  0.1641,  0.3259,\n",
      "          0.2080, -0.2994,  0.3289, -0.1726,  0.1728, -0.4535,  0.2559,  0.0939,\n",
      "          0.1923, -0.4447,  0.4706,  0.1378,  0.3362,  0.3585, -0.4693,  0.0737,\n",
      "         -0.0180, -0.1533,  0.1602,  0.0328,  0.3808,  0.0183, -0.4225,  0.2609,\n",
      "         -0.2920, -0.3166,  0.0291,  0.4230,  0.3254, -0.1631,  0.0777,  0.0084,\n",
      "          0.3930,  0.0684,  0.3393, -0.4065,  0.0184,  0.1434, -0.4191, -0.2668,\n",
      "         -0.2583, -0.3938,  0.2692,  0.3379, -0.1610, -0.4425, -0.3757, -0.3587,\n",
      "         -0.4249,  0.2199,  0.3004,  0.0443,  0.3836,  0.0263,  0.2958, -0.1507,\n",
      "          0.0537, -0.0687,  0.1514, -0.4911,  0.0052,  0.0761, -0.1913,  0.1853,\n",
      "         -0.0124, -0.3541,  0.0277, -0.3630, -0.3104, -0.1769, -0.1923,  0.1557,\n",
      "          0.4131,  0.2811,  0.3978,  0.4198,  0.4174,  0.1364, -0.3672,  0.0243,\n",
      "         -0.0475, -0.2074, -0.3462,  0.3039,  0.0092, -0.2067,  0.4334, -0.2698,\n",
      "         -0.4175,  0.4882,  0.1428,  0.1868, -0.2379, -0.2610,  0.2044,  0.1492,\n",
      "         -0.4313, -0.2646,  0.0871, -0.2397,  0.1344,  0.3381, -0.0484,  0.2644,\n",
      "         -0.2736, -0.4856, -0.1567,  0.0605,  0.1655,  0.1458,  0.1922, -0.2275,\n",
      "          0.3972, -0.1485, -0.0114,  0.1299,  0.2137,  0.2654,  0.2137, -0.3936,\n",
      "         -0.4087,  0.1286,  0.3506, -0.1719, -0.0443, -0.4216, -0.0135,  0.2737]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 192])\n",
      "tensor([[[ 0.2890,  0.5045,  0.0926,  0.0723, -0.7261, -0.2052,  0.8032,\n",
      "           0.9544,  0.6698, -1.0589,  0.9470, -0.5856, -0.7746, -0.7312,\n",
      "          -0.0333, -0.7434,  0.3730, -0.1223, -0.5671,  0.1796,  0.3181,\n",
      "          -0.2507, -0.5480, -0.4073,  0.1126, -0.0072,  0.9394,  0.4337,\n",
      "           0.2464, -0.2163, -0.3904,  0.0494, -0.6258,  0.1737, -0.8103,\n",
      "           0.5950,  0.3659,  0.2069,  0.3287, -0.7932, -0.4064, -0.2201,\n",
      "          -0.8158,  0.2160, -0.3480, -0.0240,  0.1897,  0.4433,  0.3775,\n",
      "           0.0327, -0.1802, -0.3820,  0.1724, -0.5213,  0.2611, -0.0183,\n",
      "          -0.1089,  0.3364,  0.0147, -0.0214, -0.1091,  0.5183,  0.5848,\n",
      "           0.1451,  0.0672, -0.8650,  1.1622,  0.0472,  0.3702, -0.9528,\n",
      "           0.3030, -0.1449,  0.4939, -0.9911,  1.3440,  0.4267,  0.3752,\n",
      "           0.9982, -0.6857,  0.0740, -0.2465, -0.0374,  0.1830, -0.3851,\n",
      "           0.2276, -0.3420, -0.9411,  0.7634, -0.6652, -0.7347, -0.3168,\n",
      "           0.8225,  1.2216, -0.3176, -0.3076,  0.5327,  0.8455, -0.0625,\n",
      "           0.1951, -0.8327, -0.2382,  0.1757, -0.1072, -0.0668, -0.3955,\n",
      "          -1.4008,  0.8453,  0.9878, -0.4775, -0.2166, -0.4241, -0.7014,\n",
      "          -0.8506,  0.4814, -0.0935,  0.2326,  1.1650,  0.3334,  0.5707,\n",
      "          -0.7653,  0.0047, -0.1767,  0.2181, -0.6234,  0.0749,  0.1384,\n",
      "          -0.5667,  0.1454, -0.0220, -0.2937,  0.4345, -1.0548, -0.4853,\n",
      "          -0.9152,  0.2601,  0.1506,  0.4809,  0.5231,  0.1818,  0.3231,\n",
      "           0.6085, -0.1701, -0.7401,  0.4828,  0.1982,  0.1130, -0.1954,\n",
      "           0.4686, -0.1164, -0.3740,  0.3744, -0.4149, -0.8852,  1.1861,\n",
      "          -0.4459, -0.1132, -0.4239,  0.0109,  0.9160,  0.4978, -0.3109,\n",
      "           0.2888,  0.5313,  0.0186, -0.2729,  0.3762, -0.2048,  0.5792,\n",
      "          -0.9783, -0.3343, -0.1889,  0.3284, -0.1180, -0.0856,  0.8809,\n",
      "          -0.8177,  0.0720,  0.1267, -0.1383, -0.5048, -0.2442,  0.4637,\n",
      "           0.5020, -0.4720, -0.5051,  0.5143,  0.4989, -0.4059, -0.3761,\n",
      "          -0.4550, -0.1075, -0.2041],\n",
      "         [ 0.3544,  0.2212,  0.0408, -0.2725, -0.4840, -0.4565,  1.0317,\n",
      "           1.1137,  0.9663, -1.1671,  1.1160, -0.2861, -0.9892, -0.5106,\n",
      "          -0.2834, -0.7413,  0.2058,  0.3464, -0.2246,  0.0316,  0.5165,\n",
      "          -0.2849, -0.1843, -0.4649, -0.0993, -0.0758,  1.1704,  0.3864,\n",
      "           0.4246, -0.3932, -0.4440, -0.1926, -0.2506,  0.4095, -0.9542,\n",
      "           0.5945,  0.3871, -0.0085,  0.1531, -0.4050, -0.2432, -0.4093,\n",
      "          -0.3703,  0.6586, -0.2657,  0.1890,  0.4082,  0.2849,  0.4443,\n",
      "          -0.0581, -0.1120, -0.3326,  0.5109, -0.4418,  0.0318,  0.0833,\n",
      "          -0.0349,  0.6054, -0.0875, -0.3267, -0.4371,  0.3156,  0.6701,\n",
      "           0.0720,  0.3436, -0.6371,  1.1143,  0.3006,  0.1928, -1.1600,\n",
      "           0.4267, -0.4218,  0.2282, -1.0041,  1.1700,  0.5254,  0.2384,\n",
      "           0.6750, -0.7804, -0.0368, -0.1675, -0.1586,  0.4000, -0.5569,\n",
      "           0.5568, -0.6229, -1.1039,  0.8829, -0.1843, -0.4023, -0.3115,\n",
      "           0.9373,  1.1400, -0.1683, -0.5739,  0.5413,  0.5490, -0.1551,\n",
      "           0.4421, -0.9745,  0.0226,  0.2955, -0.3340, -0.1895, -0.3924,\n",
      "          -1.5045,  0.4564,  0.9098, -0.4734,  0.1171, -0.7799, -0.7790,\n",
      "          -1.1875,  0.5014, -0.2659, -0.0700,  0.9942,  0.5033,  0.4389,\n",
      "          -0.8781, -0.1314, -0.3817,  0.0059, -0.2942,  0.1723,  0.3673,\n",
      "          -0.6121,  0.1391, -0.3699, -0.2457,  0.6390, -1.0833, -0.5536,\n",
      "          -0.7169, -0.0886,  0.3004,  0.8337,  0.0139,  0.3509, -0.0491,\n",
      "           0.2082, -0.1527, -0.8886,  0.5859,  0.0379,  0.0492, -0.2941,\n",
      "           0.5523,  0.2436, -0.6772,  0.4825, -0.6907, -1.1200,  1.1945,\n",
      "          -0.4727,  0.1501, -0.2310,  0.2998,  0.5837,  0.2219, -0.0479,\n",
      "           0.2930,  0.6957,  0.0102, -0.1717, -0.0560, -0.6265,  0.7212,\n",
      "          -0.7441, -0.2342,  0.1485,  0.2551,  0.1262, -0.1112,  0.9290,\n",
      "          -0.7499, -0.1707, -0.1016,  0.3225, -0.6271, -0.2681,  0.6002,\n",
      "           0.0062, -0.6725, -0.6160,  0.2137,  0.7366, -0.5049, -0.0580,\n",
      "          -0.7514, -0.2424, -0.2933],\n",
      "         [ 0.7720,  0.8506,  0.6363,  0.3967, -1.0783,  0.0657, -0.1906,\n",
      "          -0.1552, -0.9901,  0.6891, -0.6630,  0.2099, -0.0149, -0.3955,\n",
      "           0.8958, -0.9327, -0.3016, -1.2577, -0.0653,  0.2680, -0.4295,\n",
      "          -0.0129, -1.2179,  0.4951,  0.9091,  0.0803, -0.3225,  0.8939,\n",
      "          -0.5177, -0.1853, -0.5376, -0.3006, -0.1812, -0.0029,  0.4094,\n",
      "           0.5407,  0.5999,  0.3450, -0.2831, -0.8899,  0.2723, -0.0665,\n",
      "          -0.6740, -1.5286, -0.8011, -0.4030, -0.0933,  0.8481, -0.3861,\n",
      "           0.2245, -0.5711, -0.3368, -0.5207, -0.1746, -0.0696,  0.5432,\n",
      "          -0.5453, -0.6208,  0.9433,  0.5663,  0.3855, -0.1058, -0.0715,\n",
      "          -0.0360,  0.1337, -0.6727,  0.1586, -0.6179,  0.7915,  0.6364,\n",
      "          -1.0008,  0.8765,  0.3962,  0.0921, -0.0372,  0.3450,  0.3150,\n",
      "           0.6665,  0.9456, -0.1411,  0.1995,  0.9409, -0.1508, -0.0119,\n",
      "          -0.7872,  0.6676,  0.6567, -0.2991, -1.0049, -0.5193,  0.0538,\n",
      "          -0.5408,  0.0424,  0.0451,  0.8132, -0.0089,  0.2080, -0.3570,\n",
      "          -0.3597,  0.1808,  0.1740,  0.4962,  0.4526,  0.2019, -0.3451,\n",
      "           0.2033,  0.5677,  0.5560,  0.5537, -0.2527,  1.1595,  0.5179,\n",
      "           1.2641, -0.2870,  0.4820,  0.7516, -0.1251, -0.2832,  0.0251,\n",
      "          -0.4703, -0.4122, -0.3078, -0.0762, -0.9634, -0.2441, -0.7895,\n",
      "          -0.8435, -0.4566,  0.5459,  0.9115, -0.6854, -0.2735, -0.5737,\n",
      "          -0.0135,  0.5988,  0.3351, -0.8842,  1.2682, -0.3163,  0.7722,\n",
      "           0.3721, -0.5538,  0.1096,  0.2769,  0.1412,  0.5378, -0.3156,\n",
      "           0.2535, -1.0977,  0.7111, -0.6331,  0.9477,  0.6781, -0.1350,\n",
      "           0.0343, -0.7473,  0.1763,  0.0961,  0.4002,  0.7008,  0.4473,\n",
      "          -0.0123,  0.4552, -0.4437, -0.5990,  1.2742,  1.2046, -0.8199,\n",
      "          -0.1446, -0.1705, -0.4536, -0.2769, -0.4327, -0.2153,  0.2819,\n",
      "          -0.5267,  0.1655,  0.2627, -1.4637,  0.2295, -0.0591, -0.8702,\n",
      "           1.3292,  1.3028,  0.8712,  0.9384, -0.7028,  0.1171, -0.4370,\n",
      "           0.5018,  0.3854,  0.3234]]], grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 128])\n",
      "tensor([[[ 0.2890,  0.5045,  0.0926,  0.0723, -0.7261, -0.2052,  0.8032,\n",
      "           0.9544,  0.6698, -1.0589,  0.9470, -0.5856, -0.7746, -0.7312,\n",
      "          -0.0333, -0.7434,  0.3730, -0.1223, -0.5671,  0.1796,  0.3181,\n",
      "          -0.2507, -0.5480, -0.4073,  0.1126, -0.0072,  0.9394,  0.4337,\n",
      "           0.2464, -0.2163, -0.3904,  0.0494, -0.6258,  0.1737, -0.8103,\n",
      "           0.5950,  0.3659,  0.2069,  0.3287, -0.7932, -0.4064, -0.2201,\n",
      "          -0.8158,  0.2160, -0.3480, -0.0240,  0.1897,  0.4433,  0.3775,\n",
      "           0.0327, -0.1802, -0.3820,  0.1724, -0.5213,  0.2611, -0.0183,\n",
      "          -0.1089,  0.3364,  0.0147, -0.0214, -0.1091,  0.5183,  0.5848,\n",
      "           0.1451,  0.0672, -0.8650,  1.1622,  0.0472,  0.3702, -0.9528,\n",
      "           0.3030, -0.1449,  0.4939, -0.9911,  1.3440,  0.4267,  0.3752,\n",
      "           0.9982, -0.6857,  0.0740, -0.2465, -0.0374,  0.1830, -0.3851,\n",
      "           0.2276, -0.3420, -0.9411,  0.7634, -0.6652, -0.7347, -0.3168,\n",
      "           0.8225,  1.2216, -0.3176, -0.3076,  0.5327,  0.8455, -0.0625,\n",
      "           0.1951, -0.8327, -0.2382,  0.1757, -0.1072, -0.0668, -0.3955,\n",
      "          -1.4008,  0.8453,  0.9878, -0.4775, -0.2166, -0.4241, -0.7014,\n",
      "          -0.8506,  0.4814, -0.0935,  0.2326,  1.1650,  0.3334,  0.5707,\n",
      "          -0.7653,  0.0047, -0.1767,  0.2181, -0.6234,  0.0749,  0.1384,\n",
      "          -0.5667,  0.1454],\n",
      "         [ 0.3544,  0.2212,  0.0408, -0.2725, -0.4840, -0.4565,  1.0317,\n",
      "           1.1137,  0.9663, -1.1671,  1.1160, -0.2861, -0.9892, -0.5106,\n",
      "          -0.2834, -0.7413,  0.2058,  0.3464, -0.2246,  0.0316,  0.5165,\n",
      "          -0.2849, -0.1843, -0.4649, -0.0993, -0.0758,  1.1704,  0.3864,\n",
      "           0.4246, -0.3932, -0.4440, -0.1926, -0.2506,  0.4095, -0.9542,\n",
      "           0.5945,  0.3871, -0.0085,  0.1531, -0.4050, -0.2432, -0.4093,\n",
      "          -0.3703,  0.6586, -0.2657,  0.1890,  0.4082,  0.2849,  0.4443,\n",
      "          -0.0581, -0.1120, -0.3326,  0.5109, -0.4418,  0.0318,  0.0833,\n",
      "          -0.0349,  0.6054, -0.0875, -0.3267, -0.4371,  0.3156,  0.6701,\n",
      "           0.0720,  0.3436, -0.6371,  1.1143,  0.3006,  0.1928, -1.1600,\n",
      "           0.4267, -0.4218,  0.2282, -1.0041,  1.1700,  0.5254,  0.2384,\n",
      "           0.6750, -0.7804, -0.0368, -0.1675, -0.1586,  0.4000, -0.5569,\n",
      "           0.5568, -0.6229, -1.1039,  0.8829, -0.1843, -0.4023, -0.3115,\n",
      "           0.9373,  1.1400, -0.1683, -0.5739,  0.5413,  0.5490, -0.1551,\n",
      "           0.4421, -0.9745,  0.0226,  0.2955, -0.3340, -0.1895, -0.3924,\n",
      "          -1.5045,  0.4564,  0.9098, -0.4734,  0.1171, -0.7799, -0.7790,\n",
      "          -1.1875,  0.5014, -0.2659, -0.0700,  0.9942,  0.5033,  0.4389,\n",
      "          -0.8781, -0.1314, -0.3817,  0.0059, -0.2942,  0.1723,  0.3673,\n",
      "          -0.6121,  0.1391],\n",
      "         [ 0.7720,  0.8506,  0.6363,  0.3967, -1.0783,  0.0657, -0.1906,\n",
      "          -0.1552, -0.9901,  0.6891, -0.6630,  0.2099, -0.0149, -0.3955,\n",
      "           0.8958, -0.9327, -0.3016, -1.2577, -0.0653,  0.2680, -0.4295,\n",
      "          -0.0129, -1.2179,  0.4951,  0.9091,  0.0803, -0.3225,  0.8939,\n",
      "          -0.5177, -0.1853, -0.5376, -0.3006, -0.1812, -0.0029,  0.4094,\n",
      "           0.5407,  0.5999,  0.3450, -0.2831, -0.8899,  0.2723, -0.0665,\n",
      "          -0.6740, -1.5286, -0.8011, -0.4030, -0.0933,  0.8481, -0.3861,\n",
      "           0.2245, -0.5711, -0.3368, -0.5207, -0.1746, -0.0696,  0.5432,\n",
      "          -0.5453, -0.6208,  0.9433,  0.5663,  0.3855, -0.1058, -0.0715,\n",
      "          -0.0360,  0.1337, -0.6727,  0.1586, -0.6179,  0.7915,  0.6364,\n",
      "          -1.0008,  0.8765,  0.3962,  0.0921, -0.0372,  0.3450,  0.3150,\n",
      "           0.6665,  0.9456, -0.1411,  0.1995,  0.9409, -0.1508, -0.0119,\n",
      "          -0.7872,  0.6676,  0.6567, -0.2991, -1.0049, -0.5193,  0.0538,\n",
      "          -0.5408,  0.0424,  0.0451,  0.8132, -0.0089,  0.2080, -0.3570,\n",
      "          -0.3597,  0.1808,  0.1740,  0.4962,  0.4526,  0.2019, -0.3451,\n",
      "           0.2033,  0.5677,  0.5560,  0.5537, -0.2527,  1.1595,  0.5179,\n",
      "           1.2641, -0.2870,  0.4820,  0.7516, -0.1251, -0.2832,  0.0251,\n",
      "          -0.4703, -0.4122, -0.3078, -0.0762, -0.9634, -0.2441, -0.7895,\n",
      "          -0.8435, -0.4566]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 32])\n",
      "tensor([[[-0.0220, -0.2937,  0.4345, -1.0548, -0.4853, -0.9152,  0.2601,\n",
      "           0.1506,  0.4809,  0.5231,  0.1818,  0.3231,  0.6085, -0.1701,\n",
      "          -0.7401,  0.4828,  0.1982,  0.1130, -0.1954,  0.4686, -0.1164,\n",
      "          -0.3740,  0.3744, -0.4149, -0.8852,  1.1861, -0.4459, -0.1132,\n",
      "          -0.4239,  0.0109,  0.9160,  0.4978],\n",
      "         [-0.3699, -0.2457,  0.6390, -1.0833, -0.5536, -0.7169, -0.0886,\n",
      "           0.3004,  0.8337,  0.0139,  0.3509, -0.0491,  0.2082, -0.1527,\n",
      "          -0.8886,  0.5859,  0.0379,  0.0492, -0.2941,  0.5523,  0.2436,\n",
      "          -0.6772,  0.4825, -0.6907, -1.1200,  1.1945, -0.4727,  0.1501,\n",
      "          -0.2310,  0.2998,  0.5837,  0.2219],\n",
      "         [ 0.5459,  0.9115, -0.6854, -0.2735, -0.5737, -0.0135,  0.5988,\n",
      "           0.3351, -0.8842,  1.2682, -0.3163,  0.7722,  0.3721, -0.5538,\n",
      "           0.1096,  0.2769,  0.1412,  0.5378, -0.3156,  0.2535, -1.0977,\n",
      "           0.7111, -0.6331,  0.9477,  0.6781, -0.1350,  0.0343, -0.7473,\n",
      "           0.1763,  0.0961,  0.4002,  0.7008]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 32])\n",
      "tensor([[[-0.3109,  0.2888,  0.5313,  0.0186, -0.2729,  0.3762, -0.2048,\n",
      "           0.5792, -0.9783, -0.3343, -0.1889,  0.3284, -0.1180, -0.0856,\n",
      "           0.8809, -0.8177,  0.0720,  0.1267, -0.1383, -0.5048, -0.2442,\n",
      "           0.4637,  0.5020, -0.4720, -0.5051,  0.5143,  0.4989, -0.4059,\n",
      "          -0.3761, -0.4550, -0.1075, -0.2041],\n",
      "         [-0.0479,  0.2930,  0.6957,  0.0102, -0.1717, -0.0560, -0.6265,\n",
      "           0.7212, -0.7441, -0.2342,  0.1485,  0.2551,  0.1262, -0.1112,\n",
      "           0.9290, -0.7499, -0.1707, -0.1016,  0.3225, -0.6271, -0.2681,\n",
      "           0.6002,  0.0062, -0.6725, -0.6160,  0.2137,  0.7366, -0.5049,\n",
      "          -0.0580, -0.7514, -0.2424, -0.2933],\n",
      "         [ 0.4473, -0.0123,  0.4552, -0.4437, -0.5990,  1.2742,  1.2046,\n",
      "          -0.8199, -0.1446, -0.1705, -0.4536, -0.2769, -0.4327, -0.2153,\n",
      "           0.2819, -0.5267,  0.1655,  0.2627, -1.4637,  0.2295, -0.0591,\n",
      "          -0.8702,  1.3292,  1.3028,  0.8712,  0.9384, -0.7028,  0.1171,\n",
      "          -0.4370,  0.5018,  0.3854,  0.3234]]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 4, 32])\n",
      "tensor([[[[ 0.2890,  0.5045,  0.0926,  0.0723, -0.7261, -0.2052,  0.8032,\n",
      "            0.9544,  0.6698, -1.0589,  0.9470, -0.5856, -0.7746, -0.7312,\n",
      "           -0.0333, -0.7434,  0.3730, -0.1223, -0.5671,  0.1796,  0.3181,\n",
      "           -0.2507, -0.5480, -0.4073,  0.1126, -0.0072,  0.9394,  0.4337,\n",
      "            0.2464, -0.2163, -0.3904,  0.0494],\n",
      "          [-0.6258,  0.1737, -0.8103,  0.5950,  0.3659,  0.2069,  0.3287,\n",
      "           -0.7932, -0.4064, -0.2201, -0.8158,  0.2160, -0.3480, -0.0240,\n",
      "            0.1897,  0.4433,  0.3775,  0.0327, -0.1802, -0.3820,  0.1724,\n",
      "           -0.5213,  0.2611, -0.0183, -0.1089,  0.3364,  0.0147, -0.0214,\n",
      "           -0.1091,  0.5183,  0.5848,  0.1451],\n",
      "          [ 0.0672, -0.8650,  1.1622,  0.0472,  0.3702, -0.9528,  0.3030,\n",
      "           -0.1449,  0.4939, -0.9911,  1.3440,  0.4267,  0.3752,  0.9982,\n",
      "           -0.6857,  0.0740, -0.2465, -0.0374,  0.1830, -0.3851,  0.2276,\n",
      "           -0.3420, -0.9411,  0.7634, -0.6652, -0.7347, -0.3168,  0.8225,\n",
      "            1.2216, -0.3176, -0.3076,  0.5327],\n",
      "          [ 0.8455, -0.0625,  0.1951, -0.8327, -0.2382,  0.1757, -0.1072,\n",
      "           -0.0668, -0.3955, -1.4008,  0.8453,  0.9878, -0.4775, -0.2166,\n",
      "           -0.4241, -0.7014, -0.8506,  0.4814, -0.0935,  0.2326,  1.1650,\n",
      "            0.3334,  0.5707, -0.7653,  0.0047, -0.1767,  0.2181, -0.6234,\n",
      "            0.0749,  0.1384, -0.5667,  0.1454]],\n",
      "\n",
      "         [[ 0.3544,  0.2212,  0.0408, -0.2725, -0.4840, -0.4565,  1.0317,\n",
      "            1.1137,  0.9663, -1.1671,  1.1160, -0.2861, -0.9892, -0.5106,\n",
      "           -0.2834, -0.7413,  0.2058,  0.3464, -0.2246,  0.0316,  0.5165,\n",
      "           -0.2849, -0.1843, -0.4649, -0.0993, -0.0758,  1.1704,  0.3864,\n",
      "            0.4246, -0.3932, -0.4440, -0.1926],\n",
      "          [-0.2506,  0.4095, -0.9542,  0.5945,  0.3871, -0.0085,  0.1531,\n",
      "           -0.4050, -0.2432, -0.4093, -0.3703,  0.6586, -0.2657,  0.1890,\n",
      "            0.4082,  0.2849,  0.4443, -0.0581, -0.1120, -0.3326,  0.5109,\n",
      "           -0.4418,  0.0318,  0.0833, -0.0349,  0.6054, -0.0875, -0.3267,\n",
      "           -0.4371,  0.3156,  0.6701,  0.0720],\n",
      "          [ 0.3436, -0.6371,  1.1143,  0.3006,  0.1928, -1.1600,  0.4267,\n",
      "           -0.4218,  0.2282, -1.0041,  1.1700,  0.5254,  0.2384,  0.6750,\n",
      "           -0.7804, -0.0368, -0.1675, -0.1586,  0.4000, -0.5569,  0.5568,\n",
      "           -0.6229, -1.1039,  0.8829, -0.1843, -0.4023, -0.3115,  0.9373,\n",
      "            1.1400, -0.1683, -0.5739,  0.5413],\n",
      "          [ 0.5490, -0.1551,  0.4421, -0.9745,  0.0226,  0.2955, -0.3340,\n",
      "           -0.1895, -0.3924, -1.5045,  0.4564,  0.9098, -0.4734,  0.1171,\n",
      "           -0.7799, -0.7790, -1.1875,  0.5014, -0.2659, -0.0700,  0.9942,\n",
      "            0.5033,  0.4389, -0.8781, -0.1314, -0.3817,  0.0059, -0.2942,\n",
      "            0.1723,  0.3673, -0.6121,  0.1391]],\n",
      "\n",
      "         [[ 0.7720,  0.8506,  0.6363,  0.3967, -1.0783,  0.0657, -0.1906,\n",
      "           -0.1552, -0.9901,  0.6891, -0.6630,  0.2099, -0.0149, -0.3955,\n",
      "            0.8958, -0.9327, -0.3016, -1.2577, -0.0653,  0.2680, -0.4295,\n",
      "           -0.0129, -1.2179,  0.4951,  0.9091,  0.0803, -0.3225,  0.8939,\n",
      "           -0.5177, -0.1853, -0.5376, -0.3006],\n",
      "          [-0.1812, -0.0029,  0.4094,  0.5407,  0.5999,  0.3450, -0.2831,\n",
      "           -0.8899,  0.2723, -0.0665, -0.6740, -1.5286, -0.8011, -0.4030,\n",
      "           -0.0933,  0.8481, -0.3861,  0.2245, -0.5711, -0.3368, -0.5207,\n",
      "           -0.1746, -0.0696,  0.5432, -0.5453, -0.6208,  0.9433,  0.5663,\n",
      "            0.3855, -0.1058, -0.0715, -0.0360],\n",
      "          [ 0.1337, -0.6727,  0.1586, -0.6179,  0.7915,  0.6364, -1.0008,\n",
      "            0.8765,  0.3962,  0.0921, -0.0372,  0.3450,  0.3150,  0.6665,\n",
      "            0.9456, -0.1411,  0.1995,  0.9409, -0.1508, -0.0119, -0.7872,\n",
      "            0.6676,  0.6567, -0.2991, -1.0049, -0.5193,  0.0538, -0.5408,\n",
      "            0.0424,  0.0451,  0.8132, -0.0089],\n",
      "          [ 0.2080, -0.3570, -0.3597,  0.1808,  0.1740,  0.4962,  0.4526,\n",
      "            0.2019, -0.3451,  0.2033,  0.5677,  0.5560,  0.5537, -0.2527,\n",
      "            1.1595,  0.5179,  1.2641, -0.2870,  0.4820,  0.7516, -0.1251,\n",
      "           -0.2832,  0.0251, -0.4703, -0.4122, -0.3078, -0.0762, -0.9634,\n",
      "           -0.2441, -0.7895, -0.8435, -0.4566]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 32])\n",
      "tensor([[[[-0.0220, -0.2937,  0.4345, -1.0548, -0.4853, -0.9152,  0.2601,\n",
      "            0.1506,  0.4809,  0.5231,  0.1818,  0.3231,  0.6085, -0.1701,\n",
      "           -0.7401,  0.4828,  0.1982,  0.1130, -0.1954,  0.4686, -0.1164,\n",
      "           -0.3740,  0.3744, -0.4149, -0.8852,  1.1861, -0.4459, -0.1132,\n",
      "           -0.4239,  0.0109,  0.9160,  0.4978]],\n",
      "\n",
      "         [[-0.3699, -0.2457,  0.6390, -1.0833, -0.5536, -0.7169, -0.0886,\n",
      "            0.3004,  0.8337,  0.0139,  0.3509, -0.0491,  0.2082, -0.1527,\n",
      "           -0.8886,  0.5859,  0.0379,  0.0492, -0.2941,  0.5523,  0.2436,\n",
      "           -0.6772,  0.4825, -0.6907, -1.1200,  1.1945, -0.4727,  0.1501,\n",
      "           -0.2310,  0.2998,  0.5837,  0.2219]],\n",
      "\n",
      "         [[ 0.5459,  0.9115, -0.6854, -0.2735, -0.5737, -0.0135,  0.5988,\n",
      "            0.3351, -0.8842,  1.2682, -0.3163,  0.7722,  0.3721, -0.5538,\n",
      "            0.1096,  0.2769,  0.1412,  0.5378, -0.3156,  0.2535, -1.0977,\n",
      "            0.7111, -0.6331,  0.9477,  0.6781, -0.1350,  0.0343, -0.7473,\n",
      "            0.1763,  0.0961,  0.4002,  0.7008]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 32])\n",
      "tensor([[[[-0.3109,  0.2888,  0.5313,  0.0186, -0.2729,  0.3762, -0.2048,\n",
      "            0.5792, -0.9783, -0.3343, -0.1889,  0.3284, -0.1180, -0.0856,\n",
      "            0.8809, -0.8177,  0.0720,  0.1267, -0.1383, -0.5048, -0.2442,\n",
      "            0.4637,  0.5020, -0.4720, -0.5051,  0.5143,  0.4989, -0.4059,\n",
      "           -0.3761, -0.4550, -0.1075, -0.2041]],\n",
      "\n",
      "         [[-0.0479,  0.2930,  0.6957,  0.0102, -0.1717, -0.0560, -0.6265,\n",
      "            0.7212, -0.7441, -0.2342,  0.1485,  0.2551,  0.1262, -0.1112,\n",
      "            0.9290, -0.7499, -0.1707, -0.1016,  0.3225, -0.6271, -0.2681,\n",
      "            0.6002,  0.0062, -0.6725, -0.6160,  0.2137,  0.7366, -0.5049,\n",
      "           -0.0580, -0.7514, -0.2424, -0.2933]],\n",
      "\n",
      "         [[ 0.4473, -0.0123,  0.4552, -0.4437, -0.5990,  1.2742,  1.2046,\n",
      "           -0.8199, -0.1446, -0.1705, -0.4536, -0.2769, -0.4327, -0.2153,\n",
      "            0.2819, -0.5267,  0.1655,  0.2627, -1.4637,  0.2295, -0.0591,\n",
      "           -0.8702,  1.3292,  1.3028,  0.8712,  0.9384, -0.7028,  0.1171,\n",
      "           -0.4370,  0.5018,  0.3854,  0.3234]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 4, 32])\n",
      "tensor([[[[ 2.8900e-01,  5.0447e-01,  9.2648e-02,  7.2303e-02, -7.2613e-01,\n",
      "           -2.0525e-01,  8.0321e-01,  9.5439e-01,  6.6975e-01, -1.0589e+00,\n",
      "            9.4697e-01, -5.8559e-01, -7.7459e-01, -7.3121e-01, -3.3291e-02,\n",
      "           -7.4341e-01,  3.7299e-01, -1.2226e-01, -5.6705e-01,  1.7958e-01,\n",
      "            3.1806e-01, -2.5074e-01, -5.4801e-01, -4.0729e-01,  1.1260e-01,\n",
      "           -7.1991e-03,  9.3943e-01,  4.3370e-01,  2.4639e-01, -2.1626e-01,\n",
      "           -3.9039e-01,  4.9422e-02],\n",
      "          [-6.2584e-01,  1.7372e-01, -8.1033e-01,  5.9500e-01,  3.6590e-01,\n",
      "            2.0694e-01,  3.2865e-01, -7.9320e-01, -4.0644e-01, -2.2009e-01,\n",
      "           -8.1577e-01,  2.1604e-01, -3.4800e-01, -2.4002e-02,  1.8975e-01,\n",
      "            4.4331e-01,  3.7747e-01,  3.2696e-02, -1.8023e-01, -3.8204e-01,\n",
      "            1.7236e-01, -5.2127e-01,  2.6109e-01, -1.8281e-02, -1.0890e-01,\n",
      "            3.3642e-01,  1.4671e-02, -2.1429e-02, -1.0915e-01,  5.1833e-01,\n",
      "            5.8477e-01,  1.4514e-01],\n",
      "          [ 6.7160e-02, -8.6499e-01,  1.1622e+00,  4.7219e-02,  3.7021e-01,\n",
      "           -9.5277e-01,  3.0298e-01, -1.4492e-01,  4.9388e-01, -9.9109e-01,\n",
      "            1.3440e+00,  4.2665e-01,  3.7517e-01,  9.9825e-01, -6.8566e-01,\n",
      "            7.4045e-02, -2.4653e-01, -3.7417e-02,  1.8305e-01, -3.8507e-01,\n",
      "            2.2763e-01, -3.4197e-01, -9.4106e-01,  7.6336e-01, -6.6521e-01,\n",
      "           -7.3468e-01, -3.1676e-01,  8.2248e-01,  1.2216e+00, -3.1759e-01,\n",
      "           -3.0761e-01,  5.3267e-01],\n",
      "          [ 8.4547e-01, -6.2468e-02,  1.9506e-01, -8.3273e-01, -2.3821e-01,\n",
      "            1.7565e-01, -1.0725e-01, -6.6793e-02, -3.9549e-01, -1.4008e+00,\n",
      "            8.4535e-01,  9.8775e-01, -4.7752e-01, -2.1658e-01, -4.2408e-01,\n",
      "           -7.0136e-01, -8.5064e-01,  4.8143e-01, -9.3463e-02,  2.3264e-01,\n",
      "            1.1650e+00,  3.3341e-01,  5.7066e-01, -7.6531e-01,  4.7241e-03,\n",
      "           -1.7669e-01,  2.1809e-01, -6.2340e-01,  7.4918e-02,  1.3841e-01,\n",
      "           -5.6668e-01,  1.4541e-01]],\n",
      "\n",
      "         [[ 1.8331e-02, -7.4232e-02,  1.5429e-01, -2.6150e-01, -6.2064e-01,\n",
      "           -3.7679e-01,  1.0480e+00,  1.1656e+00,  9.7136e-01, -1.1581e+00,\n",
      "            1.0484e+00, -3.0218e-01, -1.0021e+00, -5.0112e-01, -2.7547e-01,\n",
      "           -7.3870e-01,  4.0941e-01,  4.0419e-01, -1.6829e-01, -8.2731e-02,\n",
      "            3.4037e-01, -3.8417e-01,  1.1310e-03, -3.1275e-01, -2.3534e-03,\n",
      "           -1.6305e-01,  1.2313e+00,  3.7400e-01,  3.9310e-01, -4.0517e-01,\n",
      "           -4.4894e-01, -2.0249e-01],\n",
      "          [-5.0925e-01,  3.3927e-01, -7.4752e-01,  6.7857e-01,  2.0907e-01,\n",
      "            9.5532e-02,  1.4503e-01, -4.1252e-01, -2.3853e-01, -4.5350e-01,\n",
      "           -3.6477e-01,  6.7181e-01, -2.5179e-01,  1.8143e-01,  3.9626e-01,\n",
      "            2.8394e-01,  2.9133e-02,  2.3661e-01, -6.0350e-01, -6.0170e-02,\n",
      "            6.0593e-01, -4.3145e-01,  5.8382e-02,  2.8701e-02, -5.8958e-02,\n",
      "            5.7304e-01, -1.0819e-01, -2.9862e-01, -4.4533e-01,  3.2001e-01,\n",
      "            6.7730e-01,  7.5836e-02],\n",
      "          [ 3.2663e-01, -3.5813e-01,  7.2942e-01,  5.0220e-01,  1.0070e-02,\n",
      "           -9.8119e-01,  6.1522e-01, -5.3540e-01,  2.4549e-01, -9.7113e-01,\n",
      "            1.1857e+00,  4.8543e-01,  2.0228e-01,  6.7879e-01, -7.7006e-01,\n",
      "           -4.4000e-02,  1.9865e-01, -5.5025e-01,  9.3245e-01, -3.8509e-01,\n",
      "            5.8919e-01, -8.7798e-01, -1.0110e+00,  8.1898e-01, -1.6063e-01,\n",
      "           -4.7637e-01, -2.4524e-01,  9.5861e-01,  1.1470e+00, -1.5220e-01,\n",
      "           -5.8770e-01,  5.4079e-01],\n",
      "          [ 1.2959e+00, -4.5524e-01,  5.1582e-01, -8.6049e-01, -2.8770e-01,\n",
      "            1.6897e-01, -4.0633e-01, -7.1064e-02, -3.7729e-01, -1.4717e+00,\n",
      "            4.5539e-01,  9.2136e-01, -4.7861e-01,  1.0831e-01, -7.6890e-01,\n",
      "           -7.8083e-01, -1.7967e-01,  2.6123e-01,  1.0755e-02, -4.6274e-01,\n",
      "            9.5194e-01,  5.5865e-01,  3.7286e-01, -8.9553e-01, -1.6996e-01,\n",
      "           -4.9330e-01,  3.1546e-02, -2.5556e-01,  1.5726e-01,  3.7001e-01,\n",
      "           -6.2585e-01,  1.2867e-01]],\n",
      "\n",
      "         [[-4.7024e-02,  1.3149e+00,  3.3345e-01,  6.3620e-02, -6.1582e-01,\n",
      "            6.4333e-02,  2.4540e-01, -2.8018e-01, -1.1510e+00,  6.6936e-01,\n",
      "           -6.2264e-01,  1.3383e-01,  1.7801e-02, -3.8630e-01,  9.1433e-01,\n",
      "           -9.2437e-01,  8.2744e-01,  7.5922e-01,  5.4584e-01,  4.7450e-01,\n",
      "           -9.8385e-01,  1.8533e-02, -1.2081e+00,  4.3671e-01,  6.9423e-01,\n",
      "            1.8240e-01, -3.9487e-01,  9.0841e-01, -5.1758e-01, -2.0388e-01,\n",
      "           -5.0545e-01, -3.2532e-01],\n",
      "          [ 4.2651e-01, -2.2411e-01,  6.9186e-01,  6.1109e-01,  7.9170e-01,\n",
      "            3.8663e-01, -2.4116e-01, -1.0016e+00,  3.7516e-01,  2.7011e-02,\n",
      "           -7.7558e-01, -1.5709e+00, -8.2383e-01, -3.9751e-01, -9.0694e-02,\n",
      "            8.4879e-01, -4.1287e-03,  1.3049e-02,  1.2288e-01,  1.7986e-01,\n",
      "           -6.5362e-02,  2.2158e-03, -1.6386e-01,  2.8946e-01, -4.8034e-01,\n",
      "           -6.2374e-01,  8.6167e-01,  4.3548e-01,  3.3409e-01, -1.2481e-01,\n",
      "           -7.4726e-02, -1.3390e-02],\n",
      "          [-2.3710e-01, -9.8623e-01,  2.0445e-01, -4.0195e-01,  1.1038e+00,\n",
      "            2.6130e-01, -1.1668e+00,  9.2429e-01,  5.8797e-01,  1.6865e-01,\n",
      "           -4.2994e-02,  3.8929e-01,  3.1173e-01,  6.6358e-01,  9.1607e-01,\n",
      "           -1.4079e-01,  3.8565e-02, -6.0428e-01,  7.7989e-02, -4.6941e-01,\n",
      "           -1.6703e-01,  8.8453e-01,  2.6713e-01, -5.7537e-02, -9.0615e-01,\n",
      "           -4.9974e-01,  4.9329e-02, -5.0977e-01,  6.2253e-02,  7.6668e-02,\n",
      "            8.4633e-01, -1.2639e-02],\n",
      "          [-1.2360e+00,  2.6092e-01, -5.9004e-01, -4.4116e-01,  2.1432e-01,\n",
      "            5.7074e-01,  4.1554e-01,  3.1871e-01, -2.5628e-01,  2.4697e-01,\n",
      "            5.7269e-01,  6.3522e-01,  5.6799e-01, -2.1494e-01,  1.1887e+00,\n",
      "            5.2986e-01, -3.3688e-01, -3.7648e-01, -1.1658e-01,  6.3480e-01,\n",
      "            1.9786e-03, -2.5311e-02,  1.8114e-01, -4.0049e-01, -4.7255e-01,\n",
      "           -2.7395e-01, -1.2051e-02, -9.1317e-01, -2.0863e-01, -8.0061e-01,\n",
      "           -8.0173e-01, -4.4261e-01]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 32])\n",
      "tensor([[[[-0.0220, -0.2937,  0.4345, -1.0548, -0.4853, -0.9152,  0.2601,\n",
      "            0.1506,  0.4809,  0.5231,  0.1818,  0.3231,  0.6085, -0.1701,\n",
      "           -0.7401,  0.4828,  0.1982,  0.1130, -0.1954,  0.4686, -0.1164,\n",
      "           -0.3740,  0.3744, -0.4149, -0.8852,  1.1861, -0.4459, -0.1132,\n",
      "           -0.4239,  0.0109,  0.9160,  0.4978]],\n",
      "\n",
      "         [[-0.2317, -0.2134,  0.6975, -1.2145, -0.6019, -0.5377, -0.1726,\n",
      "            0.3895,  0.9414, -0.0757,  0.3769, -0.0554,  0.2154, -0.1598,\n",
      "           -0.8988,  0.5829, -0.2908, -0.1315,  0.0919,  0.0605,  0.0593,\n",
      "           -0.8266,  0.4592, -0.6446, -1.0312,  1.1922, -0.4523,  0.1479,\n",
      "           -0.2243,  0.2961,  0.5678,  0.2297]],\n",
      "\n",
      "         [[-0.3555, -0.4718, -0.0110, -0.3712,  0.1861, -0.3368,  0.7818,\n",
      "            0.0734, -1.0013,  1.2742, -0.3181,  0.8324,  0.3602, -0.5577,\n",
      "            0.0953,  0.2581,  0.4376,  0.9474, -0.7545, -0.0358, -1.2245,\n",
      "            0.6265, -0.3850,  1.0025,  0.4889,  0.0560, -0.0014, -0.6796,\n",
      "            0.1994,  0.0698,  0.4038,  0.7080]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 4, 32])\n",
      "tensor([[[[-0.0220, -0.2937,  0.4345, -1.0548, -0.4853, -0.9152,  0.2601,\n",
      "            0.1506,  0.4809,  0.5231,  0.1818,  0.3231,  0.6085, -0.1701,\n",
      "           -0.7401,  0.4828,  0.1982,  0.1130, -0.1954,  0.4686, -0.1164,\n",
      "           -0.3740,  0.3744, -0.4149, -0.8852,  1.1861, -0.4459, -0.1132,\n",
      "           -0.4239,  0.0109,  0.9160,  0.4978],\n",
      "          [-0.0220, -0.2937,  0.4345, -1.0548, -0.4853, -0.9152,  0.2601,\n",
      "            0.1506,  0.4809,  0.5231,  0.1818,  0.3231,  0.6085, -0.1701,\n",
      "           -0.7401,  0.4828,  0.1982,  0.1130, -0.1954,  0.4686, -0.1164,\n",
      "           -0.3740,  0.3744, -0.4149, -0.8852,  1.1861, -0.4459, -0.1132,\n",
      "           -0.4239,  0.0109,  0.9160,  0.4978],\n",
      "          [-0.0220, -0.2937,  0.4345, -1.0548, -0.4853, -0.9152,  0.2601,\n",
      "            0.1506,  0.4809,  0.5231,  0.1818,  0.3231,  0.6085, -0.1701,\n",
      "           -0.7401,  0.4828,  0.1982,  0.1130, -0.1954,  0.4686, -0.1164,\n",
      "           -0.3740,  0.3744, -0.4149, -0.8852,  1.1861, -0.4459, -0.1132,\n",
      "           -0.4239,  0.0109,  0.9160,  0.4978],\n",
      "          [-0.0220, -0.2937,  0.4345, -1.0548, -0.4853, -0.9152,  0.2601,\n",
      "            0.1506,  0.4809,  0.5231,  0.1818,  0.3231,  0.6085, -0.1701,\n",
      "           -0.7401,  0.4828,  0.1982,  0.1130, -0.1954,  0.4686, -0.1164,\n",
      "           -0.3740,  0.3744, -0.4149, -0.8852,  1.1861, -0.4459, -0.1132,\n",
      "           -0.4239,  0.0109,  0.9160,  0.4978]],\n",
      "\n",
      "         [[-0.2317, -0.2134,  0.6975, -1.2145, -0.6019, -0.5377, -0.1726,\n",
      "            0.3895,  0.9414, -0.0757,  0.3769, -0.0554,  0.2154, -0.1598,\n",
      "           -0.8988,  0.5829, -0.2908, -0.1315,  0.0919,  0.0605,  0.0593,\n",
      "           -0.8266,  0.4592, -0.6446, -1.0312,  1.1922, -0.4523,  0.1479,\n",
      "           -0.2243,  0.2961,  0.5678,  0.2297],\n",
      "          [-0.2317, -0.2134,  0.6975, -1.2145, -0.6019, -0.5377, -0.1726,\n",
      "            0.3895,  0.9414, -0.0757,  0.3769, -0.0554,  0.2154, -0.1598,\n",
      "           -0.8988,  0.5829, -0.2908, -0.1315,  0.0919,  0.0605,  0.0593,\n",
      "           -0.8266,  0.4592, -0.6446, -1.0312,  1.1922, -0.4523,  0.1479,\n",
      "           -0.2243,  0.2961,  0.5678,  0.2297],\n",
      "          [-0.2317, -0.2134,  0.6975, -1.2145, -0.6019, -0.5377, -0.1726,\n",
      "            0.3895,  0.9414, -0.0757,  0.3769, -0.0554,  0.2154, -0.1598,\n",
      "           -0.8988,  0.5829, -0.2908, -0.1315,  0.0919,  0.0605,  0.0593,\n",
      "           -0.8266,  0.4592, -0.6446, -1.0312,  1.1922, -0.4523,  0.1479,\n",
      "           -0.2243,  0.2961,  0.5678,  0.2297],\n",
      "          [-0.2317, -0.2134,  0.6975, -1.2145, -0.6019, -0.5377, -0.1726,\n",
      "            0.3895,  0.9414, -0.0757,  0.3769, -0.0554,  0.2154, -0.1598,\n",
      "           -0.8988,  0.5829, -0.2908, -0.1315,  0.0919,  0.0605,  0.0593,\n",
      "           -0.8266,  0.4592, -0.6446, -1.0312,  1.1922, -0.4523,  0.1479,\n",
      "           -0.2243,  0.2961,  0.5678,  0.2297]],\n",
      "\n",
      "         [[-0.3555, -0.4718, -0.0110, -0.3712,  0.1861, -0.3368,  0.7818,\n",
      "            0.0734, -1.0013,  1.2742, -0.3181,  0.8324,  0.3602, -0.5577,\n",
      "            0.0953,  0.2581,  0.4376,  0.9474, -0.7545, -0.0358, -1.2245,\n",
      "            0.6265, -0.3850,  1.0025,  0.4889,  0.0560, -0.0014, -0.6796,\n",
      "            0.1994,  0.0698,  0.4038,  0.7080],\n",
      "          [-0.3555, -0.4718, -0.0110, -0.3712,  0.1861, -0.3368,  0.7818,\n",
      "            0.0734, -1.0013,  1.2742, -0.3181,  0.8324,  0.3602, -0.5577,\n",
      "            0.0953,  0.2581,  0.4376,  0.9474, -0.7545, -0.0358, -1.2245,\n",
      "            0.6265, -0.3850,  1.0025,  0.4889,  0.0560, -0.0014, -0.6796,\n",
      "            0.1994,  0.0698,  0.4038,  0.7080],\n",
      "          [-0.3555, -0.4718, -0.0110, -0.3712,  0.1861, -0.3368,  0.7818,\n",
      "            0.0734, -1.0013,  1.2742, -0.3181,  0.8324,  0.3602, -0.5577,\n",
      "            0.0953,  0.2581,  0.4376,  0.9474, -0.7545, -0.0358, -1.2245,\n",
      "            0.6265, -0.3850,  1.0025,  0.4889,  0.0560, -0.0014, -0.6796,\n",
      "            0.1994,  0.0698,  0.4038,  0.7080],\n",
      "          [-0.3555, -0.4718, -0.0110, -0.3712,  0.1861, -0.3368,  0.7818,\n",
      "            0.0734, -1.0013,  1.2742, -0.3181,  0.8324,  0.3602, -0.5577,\n",
      "            0.0953,  0.2581,  0.4376,  0.9474, -0.7545, -0.0358, -1.2245,\n",
      "            0.6265, -0.3850,  1.0025,  0.4889,  0.0560, -0.0014, -0.6796,\n",
      "            0.1994,  0.0698,  0.4038,  0.7080]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 4, 32])\n",
      "tensor([[[[-0.3109,  0.2888,  0.5313,  0.0186, -0.2729,  0.3762, -0.2048,\n",
      "            0.5792, -0.9783, -0.3343, -0.1889,  0.3284, -0.1180, -0.0856,\n",
      "            0.8809, -0.8177,  0.0720,  0.1267, -0.1383, -0.5048, -0.2442,\n",
      "            0.4637,  0.5020, -0.4720, -0.5051,  0.5143,  0.4989, -0.4059,\n",
      "           -0.3761, -0.4550, -0.1075, -0.2041],\n",
      "          [-0.3109,  0.2888,  0.5313,  0.0186, -0.2729,  0.3762, -0.2048,\n",
      "            0.5792, -0.9783, -0.3343, -0.1889,  0.3284, -0.1180, -0.0856,\n",
      "            0.8809, -0.8177,  0.0720,  0.1267, -0.1383, -0.5048, -0.2442,\n",
      "            0.4637,  0.5020, -0.4720, -0.5051,  0.5143,  0.4989, -0.4059,\n",
      "           -0.3761, -0.4550, -0.1075, -0.2041],\n",
      "          [-0.3109,  0.2888,  0.5313,  0.0186, -0.2729,  0.3762, -0.2048,\n",
      "            0.5792, -0.9783, -0.3343, -0.1889,  0.3284, -0.1180, -0.0856,\n",
      "            0.8809, -0.8177,  0.0720,  0.1267, -0.1383, -0.5048, -0.2442,\n",
      "            0.4637,  0.5020, -0.4720, -0.5051,  0.5143,  0.4989, -0.4059,\n",
      "           -0.3761, -0.4550, -0.1075, -0.2041],\n",
      "          [-0.3109,  0.2888,  0.5313,  0.0186, -0.2729,  0.3762, -0.2048,\n",
      "            0.5792, -0.9783, -0.3343, -0.1889,  0.3284, -0.1180, -0.0856,\n",
      "            0.8809, -0.8177,  0.0720,  0.1267, -0.1383, -0.5048, -0.2442,\n",
      "            0.4637,  0.5020, -0.4720, -0.5051,  0.5143,  0.4989, -0.4059,\n",
      "           -0.3761, -0.4550, -0.1075, -0.2041]],\n",
      "\n",
      "         [[-0.0479,  0.2930,  0.6957,  0.0102, -0.1717, -0.0560, -0.6265,\n",
      "            0.7212, -0.7441, -0.2342,  0.1485,  0.2551,  0.1262, -0.1112,\n",
      "            0.9290, -0.7499, -0.1707, -0.1016,  0.3225, -0.6271, -0.2681,\n",
      "            0.6002,  0.0062, -0.6725, -0.6160,  0.2137,  0.7366, -0.5049,\n",
      "           -0.0580, -0.7514, -0.2424, -0.2933],\n",
      "          [-0.0479,  0.2930,  0.6957,  0.0102, -0.1717, -0.0560, -0.6265,\n",
      "            0.7212, -0.7441, -0.2342,  0.1485,  0.2551,  0.1262, -0.1112,\n",
      "            0.9290, -0.7499, -0.1707, -0.1016,  0.3225, -0.6271, -0.2681,\n",
      "            0.6002,  0.0062, -0.6725, -0.6160,  0.2137,  0.7366, -0.5049,\n",
      "           -0.0580, -0.7514, -0.2424, -0.2933],\n",
      "          [-0.0479,  0.2930,  0.6957,  0.0102, -0.1717, -0.0560, -0.6265,\n",
      "            0.7212, -0.7441, -0.2342,  0.1485,  0.2551,  0.1262, -0.1112,\n",
      "            0.9290, -0.7499, -0.1707, -0.1016,  0.3225, -0.6271, -0.2681,\n",
      "            0.6002,  0.0062, -0.6725, -0.6160,  0.2137,  0.7366, -0.5049,\n",
      "           -0.0580, -0.7514, -0.2424, -0.2933],\n",
      "          [-0.0479,  0.2930,  0.6957,  0.0102, -0.1717, -0.0560, -0.6265,\n",
      "            0.7212, -0.7441, -0.2342,  0.1485,  0.2551,  0.1262, -0.1112,\n",
      "            0.9290, -0.7499, -0.1707, -0.1016,  0.3225, -0.6271, -0.2681,\n",
      "            0.6002,  0.0062, -0.6725, -0.6160,  0.2137,  0.7366, -0.5049,\n",
      "           -0.0580, -0.7514, -0.2424, -0.2933]],\n",
      "\n",
      "         [[ 0.4473, -0.0123,  0.4552, -0.4437, -0.5990,  1.2742,  1.2046,\n",
      "           -0.8199, -0.1446, -0.1705, -0.4536, -0.2769, -0.4327, -0.2153,\n",
      "            0.2819, -0.5267,  0.1655,  0.2627, -1.4637,  0.2295, -0.0591,\n",
      "           -0.8702,  1.3292,  1.3028,  0.8712,  0.9384, -0.7028,  0.1171,\n",
      "           -0.4370,  0.5018,  0.3854,  0.3234],\n",
      "          [ 0.4473, -0.0123,  0.4552, -0.4437, -0.5990,  1.2742,  1.2046,\n",
      "           -0.8199, -0.1446, -0.1705, -0.4536, -0.2769, -0.4327, -0.2153,\n",
      "            0.2819, -0.5267,  0.1655,  0.2627, -1.4637,  0.2295, -0.0591,\n",
      "           -0.8702,  1.3292,  1.3028,  0.8712,  0.9384, -0.7028,  0.1171,\n",
      "           -0.4370,  0.5018,  0.3854,  0.3234],\n",
      "          [ 0.4473, -0.0123,  0.4552, -0.4437, -0.5990,  1.2742,  1.2046,\n",
      "           -0.8199, -0.1446, -0.1705, -0.4536, -0.2769, -0.4327, -0.2153,\n",
      "            0.2819, -0.5267,  0.1655,  0.2627, -1.4637,  0.2295, -0.0591,\n",
      "           -0.8702,  1.3292,  1.3028,  0.8712,  0.9384, -0.7028,  0.1171,\n",
      "           -0.4370,  0.5018,  0.3854,  0.3234],\n",
      "          [ 0.4473, -0.0123,  0.4552, -0.4437, -0.5990,  1.2742,  1.2046,\n",
      "           -0.8199, -0.1446, -0.1705, -0.4536, -0.2769, -0.4327, -0.2153,\n",
      "            0.2819, -0.5267,  0.1655,  0.2627, -1.4637,  0.2295, -0.0591,\n",
      "           -0.8702,  1.3292,  1.3028,  0.8712,  0.9384, -0.7028,  0.1171,\n",
      "           -0.4370,  0.5018,  0.3854,  0.3234]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 4, 3, 32])\n",
      "tensor([[[[ 2.8900e-01,  5.0447e-01,  9.2648e-02,  7.2303e-02, -7.2613e-01,\n",
      "           -2.0525e-01,  8.0321e-01,  9.5439e-01,  6.6975e-01, -1.0589e+00,\n",
      "            9.4697e-01, -5.8559e-01, -7.7459e-01, -7.3121e-01, -3.3291e-02,\n",
      "           -7.4341e-01,  3.7299e-01, -1.2226e-01, -5.6705e-01,  1.7958e-01,\n",
      "            3.1806e-01, -2.5074e-01, -5.4801e-01, -4.0729e-01,  1.1260e-01,\n",
      "           -7.1991e-03,  9.3943e-01,  4.3370e-01,  2.4639e-01, -2.1626e-01,\n",
      "           -3.9039e-01,  4.9422e-02],\n",
      "          [ 1.8331e-02, -7.4232e-02,  1.5429e-01, -2.6150e-01, -6.2064e-01,\n",
      "           -3.7679e-01,  1.0480e+00,  1.1656e+00,  9.7136e-01, -1.1581e+00,\n",
      "            1.0484e+00, -3.0218e-01, -1.0021e+00, -5.0112e-01, -2.7547e-01,\n",
      "           -7.3870e-01,  4.0941e-01,  4.0419e-01, -1.6829e-01, -8.2731e-02,\n",
      "            3.4037e-01, -3.8417e-01,  1.1310e-03, -3.1275e-01, -2.3534e-03,\n",
      "           -1.6305e-01,  1.2313e+00,  3.7400e-01,  3.9310e-01, -4.0517e-01,\n",
      "           -4.4894e-01, -2.0249e-01],\n",
      "          [-4.7024e-02,  1.3149e+00,  3.3345e-01,  6.3620e-02, -6.1582e-01,\n",
      "            6.4333e-02,  2.4540e-01, -2.8018e-01, -1.1510e+00,  6.6936e-01,\n",
      "           -6.2264e-01,  1.3383e-01,  1.7801e-02, -3.8630e-01,  9.1433e-01,\n",
      "           -9.2437e-01,  8.2744e-01,  7.5922e-01,  5.4584e-01,  4.7450e-01,\n",
      "           -9.8385e-01,  1.8533e-02, -1.2081e+00,  4.3671e-01,  6.9423e-01,\n",
      "            1.8240e-01, -3.9487e-01,  9.0841e-01, -5.1758e-01, -2.0388e-01,\n",
      "           -5.0545e-01, -3.2532e-01]],\n",
      "\n",
      "         [[-6.2584e-01,  1.7372e-01, -8.1033e-01,  5.9500e-01,  3.6590e-01,\n",
      "            2.0694e-01,  3.2865e-01, -7.9320e-01, -4.0644e-01, -2.2009e-01,\n",
      "           -8.1577e-01,  2.1604e-01, -3.4800e-01, -2.4002e-02,  1.8975e-01,\n",
      "            4.4331e-01,  3.7747e-01,  3.2696e-02, -1.8023e-01, -3.8204e-01,\n",
      "            1.7236e-01, -5.2127e-01,  2.6109e-01, -1.8281e-02, -1.0890e-01,\n",
      "            3.3642e-01,  1.4671e-02, -2.1429e-02, -1.0915e-01,  5.1833e-01,\n",
      "            5.8477e-01,  1.4514e-01],\n",
      "          [-5.0925e-01,  3.3927e-01, -7.4752e-01,  6.7857e-01,  2.0907e-01,\n",
      "            9.5532e-02,  1.4503e-01, -4.1252e-01, -2.3853e-01, -4.5350e-01,\n",
      "           -3.6477e-01,  6.7181e-01, -2.5179e-01,  1.8143e-01,  3.9626e-01,\n",
      "            2.8394e-01,  2.9133e-02,  2.3661e-01, -6.0350e-01, -6.0170e-02,\n",
      "            6.0593e-01, -4.3145e-01,  5.8382e-02,  2.8701e-02, -5.8958e-02,\n",
      "            5.7304e-01, -1.0819e-01, -2.9862e-01, -4.4533e-01,  3.2001e-01,\n",
      "            6.7730e-01,  7.5836e-02],\n",
      "          [ 4.2651e-01, -2.2411e-01,  6.9186e-01,  6.1109e-01,  7.9170e-01,\n",
      "            3.8663e-01, -2.4116e-01, -1.0016e+00,  3.7516e-01,  2.7011e-02,\n",
      "           -7.7558e-01, -1.5709e+00, -8.2383e-01, -3.9751e-01, -9.0694e-02,\n",
      "            8.4879e-01, -4.1287e-03,  1.3049e-02,  1.2288e-01,  1.7986e-01,\n",
      "           -6.5362e-02,  2.2158e-03, -1.6386e-01,  2.8946e-01, -4.8034e-01,\n",
      "           -6.2374e-01,  8.6167e-01,  4.3548e-01,  3.3409e-01, -1.2481e-01,\n",
      "           -7.4726e-02, -1.3390e-02]],\n",
      "\n",
      "         [[ 6.7160e-02, -8.6499e-01,  1.1622e+00,  4.7219e-02,  3.7021e-01,\n",
      "           -9.5277e-01,  3.0298e-01, -1.4492e-01,  4.9388e-01, -9.9109e-01,\n",
      "            1.3440e+00,  4.2665e-01,  3.7517e-01,  9.9825e-01, -6.8566e-01,\n",
      "            7.4045e-02, -2.4653e-01, -3.7417e-02,  1.8305e-01, -3.8507e-01,\n",
      "            2.2763e-01, -3.4197e-01, -9.4106e-01,  7.6336e-01, -6.6521e-01,\n",
      "           -7.3468e-01, -3.1676e-01,  8.2248e-01,  1.2216e+00, -3.1759e-01,\n",
      "           -3.0761e-01,  5.3267e-01],\n",
      "          [ 3.2663e-01, -3.5813e-01,  7.2942e-01,  5.0220e-01,  1.0070e-02,\n",
      "           -9.8119e-01,  6.1522e-01, -5.3540e-01,  2.4549e-01, -9.7113e-01,\n",
      "            1.1857e+00,  4.8543e-01,  2.0228e-01,  6.7879e-01, -7.7006e-01,\n",
      "           -4.4000e-02,  1.9865e-01, -5.5025e-01,  9.3245e-01, -3.8509e-01,\n",
      "            5.8919e-01, -8.7798e-01, -1.0110e+00,  8.1898e-01, -1.6063e-01,\n",
      "           -4.7637e-01, -2.4524e-01,  9.5861e-01,  1.1470e+00, -1.5220e-01,\n",
      "           -5.8770e-01,  5.4079e-01],\n",
      "          [-2.3710e-01, -9.8623e-01,  2.0445e-01, -4.0195e-01,  1.1038e+00,\n",
      "            2.6130e-01, -1.1668e+00,  9.2429e-01,  5.8797e-01,  1.6865e-01,\n",
      "           -4.2994e-02,  3.8929e-01,  3.1173e-01,  6.6358e-01,  9.1607e-01,\n",
      "           -1.4079e-01,  3.8565e-02, -6.0428e-01,  7.7989e-02, -4.6941e-01,\n",
      "           -1.6703e-01,  8.8453e-01,  2.6713e-01, -5.7537e-02, -9.0615e-01,\n",
      "           -4.9974e-01,  4.9329e-02, -5.0977e-01,  6.2253e-02,  7.6668e-02,\n",
      "            8.4633e-01, -1.2639e-02]],\n",
      "\n",
      "         [[ 8.4547e-01, -6.2468e-02,  1.9506e-01, -8.3273e-01, -2.3821e-01,\n",
      "            1.7565e-01, -1.0725e-01, -6.6793e-02, -3.9549e-01, -1.4008e+00,\n",
      "            8.4535e-01,  9.8775e-01, -4.7752e-01, -2.1658e-01, -4.2408e-01,\n",
      "           -7.0136e-01, -8.5064e-01,  4.8143e-01, -9.3463e-02,  2.3264e-01,\n",
      "            1.1650e+00,  3.3341e-01,  5.7066e-01, -7.6531e-01,  4.7241e-03,\n",
      "           -1.7669e-01,  2.1809e-01, -6.2340e-01,  7.4918e-02,  1.3841e-01,\n",
      "           -5.6668e-01,  1.4541e-01],\n",
      "          [ 1.2959e+00, -4.5524e-01,  5.1582e-01, -8.6049e-01, -2.8770e-01,\n",
      "            1.6897e-01, -4.0633e-01, -7.1064e-02, -3.7729e-01, -1.4717e+00,\n",
      "            4.5539e-01,  9.2136e-01, -4.7861e-01,  1.0831e-01, -7.6890e-01,\n",
      "           -7.8083e-01, -1.7967e-01,  2.6123e-01,  1.0755e-02, -4.6274e-01,\n",
      "            9.5194e-01,  5.5865e-01,  3.7286e-01, -8.9553e-01, -1.6996e-01,\n",
      "           -4.9330e-01,  3.1546e-02, -2.5556e-01,  1.5726e-01,  3.7001e-01,\n",
      "           -6.2585e-01,  1.2867e-01],\n",
      "          [-1.2360e+00,  2.6092e-01, -5.9004e-01, -4.4116e-01,  2.1432e-01,\n",
      "            5.7074e-01,  4.1554e-01,  3.1871e-01, -2.5628e-01,  2.4697e-01,\n",
      "            5.7269e-01,  6.3522e-01,  5.6799e-01, -2.1494e-01,  1.1887e+00,\n",
      "            5.2986e-01, -3.3688e-01, -3.7648e-01, -1.1658e-01,  6.3480e-01,\n",
      "            1.9786e-03, -2.5311e-02,  1.8114e-01, -4.0049e-01, -4.7255e-01,\n",
      "           -2.7395e-01, -1.2051e-02, -9.1317e-01, -2.0863e-01, -8.0061e-01,\n",
      "           -8.0173e-01, -4.4261e-01]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 4, 3, 32])\n",
      "tensor([[[[-0.0220, -0.2937,  0.4345, -1.0548, -0.4853, -0.9152,  0.2601,\n",
      "            0.1506,  0.4809,  0.5231,  0.1818,  0.3231,  0.6085, -0.1701,\n",
      "           -0.7401,  0.4828,  0.1982,  0.1130, -0.1954,  0.4686, -0.1164,\n",
      "           -0.3740,  0.3744, -0.4149, -0.8852,  1.1861, -0.4459, -0.1132,\n",
      "           -0.4239,  0.0109,  0.9160,  0.4978],\n",
      "          [-0.2317, -0.2134,  0.6975, -1.2145, -0.6019, -0.5377, -0.1726,\n",
      "            0.3895,  0.9414, -0.0757,  0.3769, -0.0554,  0.2154, -0.1598,\n",
      "           -0.8988,  0.5829, -0.2908, -0.1315,  0.0919,  0.0605,  0.0593,\n",
      "           -0.8266,  0.4592, -0.6446, -1.0312,  1.1922, -0.4523,  0.1479,\n",
      "           -0.2243,  0.2961,  0.5678,  0.2297],\n",
      "          [-0.3555, -0.4718, -0.0110, -0.3712,  0.1861, -0.3368,  0.7818,\n",
      "            0.0734, -1.0013,  1.2742, -0.3181,  0.8324,  0.3602, -0.5577,\n",
      "            0.0953,  0.2581,  0.4376,  0.9474, -0.7545, -0.0358, -1.2245,\n",
      "            0.6265, -0.3850,  1.0025,  0.4889,  0.0560, -0.0014, -0.6796,\n",
      "            0.1994,  0.0698,  0.4038,  0.7080]],\n",
      "\n",
      "         [[-0.0220, -0.2937,  0.4345, -1.0548, -0.4853, -0.9152,  0.2601,\n",
      "            0.1506,  0.4809,  0.5231,  0.1818,  0.3231,  0.6085, -0.1701,\n",
      "           -0.7401,  0.4828,  0.1982,  0.1130, -0.1954,  0.4686, -0.1164,\n",
      "           -0.3740,  0.3744, -0.4149, -0.8852,  1.1861, -0.4459, -0.1132,\n",
      "           -0.4239,  0.0109,  0.9160,  0.4978],\n",
      "          [-0.2317, -0.2134,  0.6975, -1.2145, -0.6019, -0.5377, -0.1726,\n",
      "            0.3895,  0.9414, -0.0757,  0.3769, -0.0554,  0.2154, -0.1598,\n",
      "           -0.8988,  0.5829, -0.2908, -0.1315,  0.0919,  0.0605,  0.0593,\n",
      "           -0.8266,  0.4592, -0.6446, -1.0312,  1.1922, -0.4523,  0.1479,\n",
      "           -0.2243,  0.2961,  0.5678,  0.2297],\n",
      "          [-0.3555, -0.4718, -0.0110, -0.3712,  0.1861, -0.3368,  0.7818,\n",
      "            0.0734, -1.0013,  1.2742, -0.3181,  0.8324,  0.3602, -0.5577,\n",
      "            0.0953,  0.2581,  0.4376,  0.9474, -0.7545, -0.0358, -1.2245,\n",
      "            0.6265, -0.3850,  1.0025,  0.4889,  0.0560, -0.0014, -0.6796,\n",
      "            0.1994,  0.0698,  0.4038,  0.7080]],\n",
      "\n",
      "         [[-0.0220, -0.2937,  0.4345, -1.0548, -0.4853, -0.9152,  0.2601,\n",
      "            0.1506,  0.4809,  0.5231,  0.1818,  0.3231,  0.6085, -0.1701,\n",
      "           -0.7401,  0.4828,  0.1982,  0.1130, -0.1954,  0.4686, -0.1164,\n",
      "           -0.3740,  0.3744, -0.4149, -0.8852,  1.1861, -0.4459, -0.1132,\n",
      "           -0.4239,  0.0109,  0.9160,  0.4978],\n",
      "          [-0.2317, -0.2134,  0.6975, -1.2145, -0.6019, -0.5377, -0.1726,\n",
      "            0.3895,  0.9414, -0.0757,  0.3769, -0.0554,  0.2154, -0.1598,\n",
      "           -0.8988,  0.5829, -0.2908, -0.1315,  0.0919,  0.0605,  0.0593,\n",
      "           -0.8266,  0.4592, -0.6446, -1.0312,  1.1922, -0.4523,  0.1479,\n",
      "           -0.2243,  0.2961,  0.5678,  0.2297],\n",
      "          [-0.3555, -0.4718, -0.0110, -0.3712,  0.1861, -0.3368,  0.7818,\n",
      "            0.0734, -1.0013,  1.2742, -0.3181,  0.8324,  0.3602, -0.5577,\n",
      "            0.0953,  0.2581,  0.4376,  0.9474, -0.7545, -0.0358, -1.2245,\n",
      "            0.6265, -0.3850,  1.0025,  0.4889,  0.0560, -0.0014, -0.6796,\n",
      "            0.1994,  0.0698,  0.4038,  0.7080]],\n",
      "\n",
      "         [[-0.0220, -0.2937,  0.4345, -1.0548, -0.4853, -0.9152,  0.2601,\n",
      "            0.1506,  0.4809,  0.5231,  0.1818,  0.3231,  0.6085, -0.1701,\n",
      "           -0.7401,  0.4828,  0.1982,  0.1130, -0.1954,  0.4686, -0.1164,\n",
      "           -0.3740,  0.3744, -0.4149, -0.8852,  1.1861, -0.4459, -0.1132,\n",
      "           -0.4239,  0.0109,  0.9160,  0.4978],\n",
      "          [-0.2317, -0.2134,  0.6975, -1.2145, -0.6019, -0.5377, -0.1726,\n",
      "            0.3895,  0.9414, -0.0757,  0.3769, -0.0554,  0.2154, -0.1598,\n",
      "           -0.8988,  0.5829, -0.2908, -0.1315,  0.0919,  0.0605,  0.0593,\n",
      "           -0.8266,  0.4592, -0.6446, -1.0312,  1.1922, -0.4523,  0.1479,\n",
      "           -0.2243,  0.2961,  0.5678,  0.2297],\n",
      "          [-0.3555, -0.4718, -0.0110, -0.3712,  0.1861, -0.3368,  0.7818,\n",
      "            0.0734, -1.0013,  1.2742, -0.3181,  0.8324,  0.3602, -0.5577,\n",
      "            0.0953,  0.2581,  0.4376,  0.9474, -0.7545, -0.0358, -1.2245,\n",
      "            0.6265, -0.3850,  1.0025,  0.4889,  0.0560, -0.0014, -0.6796,\n",
      "            0.1994,  0.0698,  0.4038,  0.7080]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 4, 3, 32])\n",
      "tensor([[[[-0.3109,  0.2888,  0.5313,  0.0186, -0.2729,  0.3762, -0.2048,\n",
      "            0.5792, -0.9783, -0.3343, -0.1889,  0.3284, -0.1180, -0.0856,\n",
      "            0.8809, -0.8177,  0.0720,  0.1267, -0.1383, -0.5048, -0.2442,\n",
      "            0.4637,  0.5020, -0.4720, -0.5051,  0.5143,  0.4989, -0.4059,\n",
      "           -0.3761, -0.4550, -0.1075, -0.2041],\n",
      "          [-0.0479,  0.2930,  0.6957,  0.0102, -0.1717, -0.0560, -0.6265,\n",
      "            0.7212, -0.7441, -0.2342,  0.1485,  0.2551,  0.1262, -0.1112,\n",
      "            0.9290, -0.7499, -0.1707, -0.1016,  0.3225, -0.6271, -0.2681,\n",
      "            0.6002,  0.0062, -0.6725, -0.6160,  0.2137,  0.7366, -0.5049,\n",
      "           -0.0580, -0.7514, -0.2424, -0.2933],\n",
      "          [ 0.4473, -0.0123,  0.4552, -0.4437, -0.5990,  1.2742,  1.2046,\n",
      "           -0.8199, -0.1446, -0.1705, -0.4536, -0.2769, -0.4327, -0.2153,\n",
      "            0.2819, -0.5267,  0.1655,  0.2627, -1.4637,  0.2295, -0.0591,\n",
      "           -0.8702,  1.3292,  1.3028,  0.8712,  0.9384, -0.7028,  0.1171,\n",
      "           -0.4370,  0.5018,  0.3854,  0.3234]],\n",
      "\n",
      "         [[-0.3109,  0.2888,  0.5313,  0.0186, -0.2729,  0.3762, -0.2048,\n",
      "            0.5792, -0.9783, -0.3343, -0.1889,  0.3284, -0.1180, -0.0856,\n",
      "            0.8809, -0.8177,  0.0720,  0.1267, -0.1383, -0.5048, -0.2442,\n",
      "            0.4637,  0.5020, -0.4720, -0.5051,  0.5143,  0.4989, -0.4059,\n",
      "           -0.3761, -0.4550, -0.1075, -0.2041],\n",
      "          [-0.0479,  0.2930,  0.6957,  0.0102, -0.1717, -0.0560, -0.6265,\n",
      "            0.7212, -0.7441, -0.2342,  0.1485,  0.2551,  0.1262, -0.1112,\n",
      "            0.9290, -0.7499, -0.1707, -0.1016,  0.3225, -0.6271, -0.2681,\n",
      "            0.6002,  0.0062, -0.6725, -0.6160,  0.2137,  0.7366, -0.5049,\n",
      "           -0.0580, -0.7514, -0.2424, -0.2933],\n",
      "          [ 0.4473, -0.0123,  0.4552, -0.4437, -0.5990,  1.2742,  1.2046,\n",
      "           -0.8199, -0.1446, -0.1705, -0.4536, -0.2769, -0.4327, -0.2153,\n",
      "            0.2819, -0.5267,  0.1655,  0.2627, -1.4637,  0.2295, -0.0591,\n",
      "           -0.8702,  1.3292,  1.3028,  0.8712,  0.9384, -0.7028,  0.1171,\n",
      "           -0.4370,  0.5018,  0.3854,  0.3234]],\n",
      "\n",
      "         [[-0.3109,  0.2888,  0.5313,  0.0186, -0.2729,  0.3762, -0.2048,\n",
      "            0.5792, -0.9783, -0.3343, -0.1889,  0.3284, -0.1180, -0.0856,\n",
      "            0.8809, -0.8177,  0.0720,  0.1267, -0.1383, -0.5048, -0.2442,\n",
      "            0.4637,  0.5020, -0.4720, -0.5051,  0.5143,  0.4989, -0.4059,\n",
      "           -0.3761, -0.4550, -0.1075, -0.2041],\n",
      "          [-0.0479,  0.2930,  0.6957,  0.0102, -0.1717, -0.0560, -0.6265,\n",
      "            0.7212, -0.7441, -0.2342,  0.1485,  0.2551,  0.1262, -0.1112,\n",
      "            0.9290, -0.7499, -0.1707, -0.1016,  0.3225, -0.6271, -0.2681,\n",
      "            0.6002,  0.0062, -0.6725, -0.6160,  0.2137,  0.7366, -0.5049,\n",
      "           -0.0580, -0.7514, -0.2424, -0.2933],\n",
      "          [ 0.4473, -0.0123,  0.4552, -0.4437, -0.5990,  1.2742,  1.2046,\n",
      "           -0.8199, -0.1446, -0.1705, -0.4536, -0.2769, -0.4327, -0.2153,\n",
      "            0.2819, -0.5267,  0.1655,  0.2627, -1.4637,  0.2295, -0.0591,\n",
      "           -0.8702,  1.3292,  1.3028,  0.8712,  0.9384, -0.7028,  0.1171,\n",
      "           -0.4370,  0.5018,  0.3854,  0.3234]],\n",
      "\n",
      "         [[-0.3109,  0.2888,  0.5313,  0.0186, -0.2729,  0.3762, -0.2048,\n",
      "            0.5792, -0.9783, -0.3343, -0.1889,  0.3284, -0.1180, -0.0856,\n",
      "            0.8809, -0.8177,  0.0720,  0.1267, -0.1383, -0.5048, -0.2442,\n",
      "            0.4637,  0.5020, -0.4720, -0.5051,  0.5143,  0.4989, -0.4059,\n",
      "           -0.3761, -0.4550, -0.1075, -0.2041],\n",
      "          [-0.0479,  0.2930,  0.6957,  0.0102, -0.1717, -0.0560, -0.6265,\n",
      "            0.7212, -0.7441, -0.2342,  0.1485,  0.2551,  0.1262, -0.1112,\n",
      "            0.9290, -0.7499, -0.1707, -0.1016,  0.3225, -0.6271, -0.2681,\n",
      "            0.6002,  0.0062, -0.6725, -0.6160,  0.2137,  0.7366, -0.5049,\n",
      "           -0.0580, -0.7514, -0.2424, -0.2933],\n",
      "          [ 0.4473, -0.0123,  0.4552, -0.4437, -0.5990,  1.2742,  1.2046,\n",
      "           -0.8199, -0.1446, -0.1705, -0.4536, -0.2769, -0.4327, -0.2153,\n",
      "            0.2819, -0.5267,  0.1655,  0.2627, -1.4637,  0.2295, -0.0591,\n",
      "           -0.8702,  1.3292,  1.3028,  0.8712,  0.9384, -0.7028,  0.1171,\n",
      "           -0.4370,  0.5018,  0.3854,  0.3234]]]],\n",
      "       grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[-1.7115e-01,  9.1521e-02, -5.6861e-01],\n",
      "          [-6.6338e-02,  2.6967e-01, -5.5007e-01],\n",
      "          [-3.9926e-01, -7.5381e-01,  6.6151e-01]],\n",
      "\n",
      "         [[-1.0163e-01, -1.9086e-01,  1.2991e-01],\n",
      "          [ 6.3812e-04, -1.1561e-01,  4.9154e-02],\n",
      "          [-4.8257e-01, -3.0269e-01, -3.5259e-01]],\n",
      "\n",
      "         [[ 9.7371e-02,  3.1879e-01, -2.4947e-01],\n",
      "          [-1.1071e-01,  1.0901e-01, -4.9362e-01],\n",
      "          [ 3.4193e-02,  7.1468e-02, -8.2639e-03]],\n",
      "\n",
      "         [[-4.9918e-02,  1.7249e-01, -5.3190e-01],\n",
      "          [-1.0749e-01,  1.6552e-01, -6.1337e-01],\n",
      "          [-8.0643e-02, -1.5722e-01,  1.5665e-01]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[-1.7115e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [-6.6338e-02,  2.6967e-01, -2.3820e+38],\n",
      "          [-3.9926e-01, -7.5381e-01,  6.6151e-01]],\n",
      "\n",
      "         [[-1.0163e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 6.3812e-04, -1.1561e-01, -2.3820e+38],\n",
      "          [-4.8257e-01, -3.0269e-01, -3.5259e-01]],\n",
      "\n",
      "         [[ 9.7371e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [-1.1071e-01,  1.0901e-01, -2.3820e+38],\n",
      "          [ 3.4193e-02,  7.1468e-02, -8.2639e-03]],\n",
      "\n",
      "         [[-4.9918e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [-1.0749e-01,  1.6552e-01, -2.3820e+38],\n",
      "          [-8.0643e-02, -1.5722e-01,  1.5665e-01]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4168, 0.5832, 0.0000],\n",
      "          [0.2179, 0.1528, 0.6293]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5290, 0.4710, 0.0000],\n",
      "          [0.2998, 0.3588, 0.3414]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4453, 0.5547, 0.0000],\n",
      "          [0.3337, 0.3464, 0.3199]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4322, 0.5678, 0.0000],\n",
      "          [0.3131, 0.2900, 0.3969]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 4, 3, 32])\n",
      "tensor([[[[-3.1089e-01,  2.8882e-01,  5.3125e-01,  1.8608e-02, -2.7287e-01,\n",
      "            3.7620e-01, -2.0476e-01,  5.7916e-01, -9.7827e-01, -3.3428e-01,\n",
      "           -1.8894e-01,  3.2843e-01, -1.1799e-01, -8.5572e-02,  8.8086e-01,\n",
      "           -8.1772e-01,  7.1988e-02,  1.2672e-01, -1.3833e-01, -5.0478e-01,\n",
      "           -2.4418e-01,  4.6371e-01,  5.0196e-01, -4.7195e-01, -5.0514e-01,\n",
      "            5.1434e-01,  4.9891e-01, -4.0585e-01, -3.7613e-01, -4.5500e-01,\n",
      "           -1.0745e-01, -2.0410e-01],\n",
      "          [-1.5750e-01,  2.9127e-01,  6.2718e-01,  1.3722e-02, -2.1389e-01,\n",
      "            1.2412e-01, -4.5072e-01,  6.6198e-01, -8.4168e-01, -2.7594e-01,\n",
      "            7.8393e-03,  2.8565e-01,  2.4403e-02, -1.0053e-01,  9.0896e-01,\n",
      "           -7.7819e-01, -6.9537e-02, -6.4623e-03,  1.3044e-01, -5.7611e-01,\n",
      "           -2.5813e-01,  5.4332e-01,  2.1284e-01, -5.8893e-01, -5.6979e-01,\n",
      "            3.3899e-01,  6.3751e-01, -4.6359e-01, -1.9058e-01, -6.2785e-01,\n",
      "           -1.8618e-01, -2.5610e-01],\n",
      "          [ 2.0647e-01,  9.9972e-02,  5.0850e-01, -2.7358e-01, -4.6265e-01,\n",
      "            8.7528e-01,  6.1773e-01, -2.7959e-01, -4.1783e-01, -2.1595e-01,\n",
      "           -3.0391e-01, -6.3741e-02, -2.7870e-01, -1.7111e-01,  5.1126e-01,\n",
      "           -6.2423e-01,  9.3723e-02,  1.7742e-01, -9.0198e-01, -6.1351e-02,\n",
      "           -1.3137e-01, -3.5488e-01,  9.4676e-01,  6.1430e-01,  3.4406e-01,\n",
      "            7.3526e-01, -2.2102e-01, -9.1905e-02, -3.6584e-01,  1.0180e-01,\n",
      "            1.8208e-01,  1.1422e-01]],\n",
      "\n",
      "         [[-3.1089e-01,  2.8882e-01,  5.3125e-01,  1.8608e-02, -2.7287e-01,\n",
      "            3.7620e-01, -2.0476e-01,  5.7916e-01, -9.7827e-01, -3.3428e-01,\n",
      "           -1.8894e-01,  3.2843e-01, -1.1799e-01, -8.5572e-02,  8.8086e-01,\n",
      "           -8.1772e-01,  7.1988e-02,  1.2672e-01, -1.3833e-01, -5.0478e-01,\n",
      "           -2.4418e-01,  4.6371e-01,  5.0196e-01, -4.7195e-01, -5.0514e-01,\n",
      "            5.1434e-01,  4.9891e-01, -4.0585e-01, -3.7613e-01, -4.5500e-01,\n",
      "           -1.0745e-01, -2.0410e-01],\n",
      "          [-1.8702e-01,  2.9080e-01,  6.0872e-01,  1.4662e-02, -2.2524e-01,\n",
      "            1.7264e-01, -4.0338e-01,  6.4604e-01, -8.6797e-01, -2.8717e-01,\n",
      "           -3.0034e-02,  2.9388e-01, -3.0015e-03, -9.7648e-02,  9.0355e-01,\n",
      "           -7.8580e-01, -4.2299e-02,  1.9170e-02,  7.8709e-02, -5.6238e-01,\n",
      "           -2.5544e-01,  5.2800e-01,  2.6849e-01, -5.6642e-01, -5.5735e-01,\n",
      "            3.7274e-01,  6.1083e-01, -4.5248e-01, -2.2629e-01, -5.9458e-01,\n",
      "           -1.7103e-01, -2.4609e-01],\n",
      "          [ 4.2333e-02,  1.8754e-01,  5.6430e-01, -1.4221e-01, -3.4792e-01,\n",
      "            5.2767e-01,  1.2504e-01,  1.5250e-01, -6.0962e-01, -2.4248e-01,\n",
      "           -1.5820e-01,  9.5449e-02, -1.3780e-01, -1.3905e-01,  6.9366e-01,\n",
      "           -6.9406e-01,  1.6817e-02,  9.1210e-02, -4.2542e-01, -2.9798e-01,\n",
      "           -1.8958e-01,  5.7319e-02,  6.0646e-01,  6.1956e-02, -7.5062e-02,\n",
      "            5.5122e-01,  1.7395e-01, -2.6287e-01, -2.8276e-01, -2.3474e-01,\n",
      "            1.2362e-02, -5.6025e-02]],\n",
      "\n",
      "         [[-3.1089e-01,  2.8882e-01,  5.3125e-01,  1.8608e-02, -2.7287e-01,\n",
      "            3.7620e-01, -2.0476e-01,  5.7916e-01, -9.7827e-01, -3.3428e-01,\n",
      "           -1.8894e-01,  3.2843e-01, -1.1799e-01, -8.5572e-02,  8.8086e-01,\n",
      "           -8.1772e-01,  7.1988e-02,  1.2672e-01, -1.3833e-01, -5.0478e-01,\n",
      "           -2.4418e-01,  4.6371e-01,  5.0196e-01, -4.7195e-01, -5.0514e-01,\n",
      "            5.1434e-01,  4.9891e-01, -4.0585e-01, -3.7613e-01, -4.5500e-01,\n",
      "           -1.0745e-01, -2.0410e-01],\n",
      "          [-1.6500e-01,  2.9115e-01,  6.2249e-01,  1.3960e-02, -2.1677e-01,\n",
      "            1.3644e-01, -4.3869e-01,  6.5793e-01, -8.4836e-01, -2.7879e-01,\n",
      "           -1.7802e-03,  2.8774e-01,  1.7443e-02, -9.9795e-02,  9.0759e-01,\n",
      "           -7.8013e-01, -6.2619e-02,  4.8131e-05,  1.1730e-01, -5.7262e-01,\n",
      "           -2.5745e-01,  5.3943e-01,  2.2698e-01, -5.8321e-01, -5.6663e-01,\n",
      "            3.4756e-01,  6.3073e-01, -4.6077e-01, -1.9965e-01, -6.1940e-01,\n",
      "           -1.8233e-01, -2.5356e-01],\n",
      "          [ 2.2743e-02,  1.9396e-01,  5.6389e-01, -1.3216e-01, -3.4216e-01,\n",
      "            5.1372e-01,  9.9956e-02,  1.8084e-01, -6.3048e-01, -2.4725e-01,\n",
      "           -1.5670e-01,  1.0939e-01, -1.3407e-01, -1.3594e-01,  7.0595e-01,\n",
      "           -7.0116e-01,  1.7825e-02,  9.1123e-02, -4.0263e-01, -3.1227e-01,\n",
      "           -1.9327e-01,  8.4329e-02,  5.9482e-01,  2.6255e-02, -1.0330e-01,\n",
      "            5.4583e-01,  1.9685e-01, -2.7289e-01, -2.8540e-01, -2.5164e-01,\n",
      "            3.4340e-03, -6.6268e-02]],\n",
      "\n",
      "         [[-3.1089e-01,  2.8882e-01,  5.3125e-01,  1.8608e-02, -2.7287e-01,\n",
      "            3.7620e-01, -2.0476e-01,  5.7916e-01, -9.7827e-01, -3.3428e-01,\n",
      "           -1.8894e-01,  3.2843e-01, -1.1799e-01, -8.5572e-02,  8.8086e-01,\n",
      "           -8.1772e-01,  7.1988e-02,  1.2672e-01, -1.3833e-01, -5.0478e-01,\n",
      "           -2.4418e-01,  4.6371e-01,  5.0196e-01, -4.7195e-01, -5.0514e-01,\n",
      "            5.1434e-01,  4.9891e-01, -4.0585e-01, -3.7613e-01, -4.5500e-01,\n",
      "           -1.0745e-01, -2.0410e-01],\n",
      "          [-1.6155e-01,  2.9121e-01,  6.2465e-01,  1.3851e-02, -2.1545e-01,\n",
      "            1.3077e-01, -4.4423e-01,  6.5979e-01, -8.4529e-01, -2.7748e-01,\n",
      "            2.6471e-03,  2.8678e-01,  2.0646e-02, -1.0013e-01,  9.0822e-01,\n",
      "           -7.7924e-01, -6.5803e-02, -2.9482e-03,  1.2335e-01, -5.7423e-01,\n",
      "           -2.5776e-01,  5.4122e-01,  2.2047e-01, -5.8584e-01, -5.6808e-01,\n",
      "            3.4361e-01,  6.3385e-01, -4.6207e-01, -1.9548e-01, -6.2329e-01,\n",
      "           -1.8410e-01, -2.5473e-01],\n",
      "          [ 6.6339e-02,  1.7052e-01,  5.4875e-01, -1.6731e-01, -3.7299e-01,\n",
      "            6.0731e-01,  2.3236e-01,  6.5015e-02, -5.7944e-01, -2.4028e-01,\n",
      "           -1.9613e-01,  6.6876e-02, -1.7209e-01, -1.4449e-01,  6.5707e-01,\n",
      "           -6.8256e-01,  3.8716e-02,  1.1449e-01, -5.3076e-01, -2.4878e-01,\n",
      "           -1.7766e-01, -2.6169e-02,  6.8653e-01,  1.7434e-01,  9.0154e-03,\n",
      "            5.9547e-01,  9.0842e-02, -2.2700e-01, -3.0804e-01, -1.6119e-01,\n",
      "            4.9031e-02, -2.0589e-02]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 128])\n",
      "tensor([[[-3.1089e-01,  2.8882e-01,  5.3125e-01,  1.8608e-02, -2.7287e-01,\n",
      "           3.7620e-01, -2.0476e-01,  5.7916e-01, -9.7827e-01, -3.3428e-01,\n",
      "          -1.8894e-01,  3.2843e-01, -1.1799e-01, -8.5572e-02,  8.8086e-01,\n",
      "          -8.1772e-01,  7.1988e-02,  1.2672e-01, -1.3833e-01, -5.0478e-01,\n",
      "          -2.4418e-01,  4.6371e-01,  5.0196e-01, -4.7195e-01, -5.0514e-01,\n",
      "           5.1434e-01,  4.9891e-01, -4.0585e-01, -3.7613e-01, -4.5500e-01,\n",
      "          -1.0745e-01, -2.0410e-01, -3.1089e-01,  2.8882e-01,  5.3125e-01,\n",
      "           1.8608e-02, -2.7287e-01,  3.7620e-01, -2.0476e-01,  5.7916e-01,\n",
      "          -9.7827e-01, -3.3428e-01, -1.8894e-01,  3.2843e-01, -1.1799e-01,\n",
      "          -8.5572e-02,  8.8086e-01, -8.1772e-01,  7.1988e-02,  1.2672e-01,\n",
      "          -1.3833e-01, -5.0478e-01, -2.4418e-01,  4.6371e-01,  5.0196e-01,\n",
      "          -4.7195e-01, -5.0514e-01,  5.1434e-01,  4.9891e-01, -4.0585e-01,\n",
      "          -3.7613e-01, -4.5500e-01, -1.0745e-01, -2.0410e-01, -3.1089e-01,\n",
      "           2.8882e-01,  5.3125e-01,  1.8608e-02, -2.7287e-01,  3.7620e-01,\n",
      "          -2.0476e-01,  5.7916e-01, -9.7827e-01, -3.3428e-01, -1.8894e-01,\n",
      "           3.2843e-01, -1.1799e-01, -8.5572e-02,  8.8086e-01, -8.1772e-01,\n",
      "           7.1988e-02,  1.2672e-01, -1.3833e-01, -5.0478e-01, -2.4418e-01,\n",
      "           4.6371e-01,  5.0196e-01, -4.7195e-01, -5.0514e-01,  5.1434e-01,\n",
      "           4.9891e-01, -4.0585e-01, -3.7613e-01, -4.5500e-01, -1.0745e-01,\n",
      "          -2.0410e-01, -3.1089e-01,  2.8882e-01,  5.3125e-01,  1.8608e-02,\n",
      "          -2.7287e-01,  3.7620e-01, -2.0476e-01,  5.7916e-01, -9.7827e-01,\n",
      "          -3.3428e-01, -1.8894e-01,  3.2843e-01, -1.1799e-01, -8.5572e-02,\n",
      "           8.8086e-01, -8.1772e-01,  7.1988e-02,  1.2672e-01, -1.3833e-01,\n",
      "          -5.0478e-01, -2.4418e-01,  4.6371e-01,  5.0196e-01, -4.7195e-01,\n",
      "          -5.0514e-01,  5.1434e-01,  4.9891e-01, -4.0585e-01, -3.7613e-01,\n",
      "          -4.5500e-01, -1.0745e-01, -2.0410e-01],\n",
      "         [-1.5750e-01,  2.9127e-01,  6.2718e-01,  1.3722e-02, -2.1389e-01,\n",
      "           1.2412e-01, -4.5072e-01,  6.6198e-01, -8.4168e-01, -2.7594e-01,\n",
      "           7.8393e-03,  2.8565e-01,  2.4403e-02, -1.0053e-01,  9.0896e-01,\n",
      "          -7.7819e-01, -6.9537e-02, -6.4623e-03,  1.3044e-01, -5.7611e-01,\n",
      "          -2.5813e-01,  5.4332e-01,  2.1284e-01, -5.8893e-01, -5.6979e-01,\n",
      "           3.3899e-01,  6.3751e-01, -4.6359e-01, -1.9058e-01, -6.2785e-01,\n",
      "          -1.8618e-01, -2.5610e-01, -1.8702e-01,  2.9080e-01,  6.0872e-01,\n",
      "           1.4662e-02, -2.2524e-01,  1.7264e-01, -4.0338e-01,  6.4604e-01,\n",
      "          -8.6797e-01, -2.8717e-01, -3.0034e-02,  2.9388e-01, -3.0015e-03,\n",
      "          -9.7648e-02,  9.0355e-01, -7.8580e-01, -4.2299e-02,  1.9170e-02,\n",
      "           7.8709e-02, -5.6238e-01, -2.5544e-01,  5.2800e-01,  2.6849e-01,\n",
      "          -5.6642e-01, -5.5735e-01,  3.7274e-01,  6.1083e-01, -4.5248e-01,\n",
      "          -2.2629e-01, -5.9458e-01, -1.7103e-01, -2.4609e-01, -1.6500e-01,\n",
      "           2.9115e-01,  6.2249e-01,  1.3960e-02, -2.1677e-01,  1.3644e-01,\n",
      "          -4.3869e-01,  6.5793e-01, -8.4836e-01, -2.7879e-01, -1.7802e-03,\n",
      "           2.8774e-01,  1.7443e-02, -9.9795e-02,  9.0759e-01, -7.8013e-01,\n",
      "          -6.2619e-02,  4.8131e-05,  1.1730e-01, -5.7262e-01, -2.5745e-01,\n",
      "           5.3943e-01,  2.2698e-01, -5.8321e-01, -5.6663e-01,  3.4756e-01,\n",
      "           6.3073e-01, -4.6077e-01, -1.9965e-01, -6.1940e-01, -1.8233e-01,\n",
      "          -2.5356e-01, -1.6155e-01,  2.9121e-01,  6.2465e-01,  1.3851e-02,\n",
      "          -2.1545e-01,  1.3077e-01, -4.4423e-01,  6.5979e-01, -8.4529e-01,\n",
      "          -2.7748e-01,  2.6471e-03,  2.8678e-01,  2.0646e-02, -1.0013e-01,\n",
      "           9.0822e-01, -7.7924e-01, -6.5803e-02, -2.9482e-03,  1.2335e-01,\n",
      "          -5.7423e-01, -2.5776e-01,  5.4122e-01,  2.2047e-01, -5.8584e-01,\n",
      "          -5.6808e-01,  3.4361e-01,  6.3385e-01, -4.6207e-01, -1.9548e-01,\n",
      "          -6.2329e-01, -1.8410e-01, -2.5473e-01],\n",
      "         [ 2.0647e-01,  9.9972e-02,  5.0850e-01, -2.7358e-01, -4.6265e-01,\n",
      "           8.7528e-01,  6.1773e-01, -2.7959e-01, -4.1783e-01, -2.1595e-01,\n",
      "          -3.0391e-01, -6.3741e-02, -2.7870e-01, -1.7111e-01,  5.1126e-01,\n",
      "          -6.2423e-01,  9.3723e-02,  1.7742e-01, -9.0198e-01, -6.1351e-02,\n",
      "          -1.3137e-01, -3.5488e-01,  9.4676e-01,  6.1430e-01,  3.4406e-01,\n",
      "           7.3526e-01, -2.2102e-01, -9.1905e-02, -3.6584e-01,  1.0180e-01,\n",
      "           1.8208e-01,  1.1422e-01,  4.2333e-02,  1.8754e-01,  5.6430e-01,\n",
      "          -1.4221e-01, -3.4792e-01,  5.2767e-01,  1.2504e-01,  1.5250e-01,\n",
      "          -6.0962e-01, -2.4248e-01, -1.5820e-01,  9.5449e-02, -1.3780e-01,\n",
      "          -1.3905e-01,  6.9366e-01, -6.9406e-01,  1.6817e-02,  9.1210e-02,\n",
      "          -4.2542e-01, -2.9798e-01, -1.8958e-01,  5.7319e-02,  6.0646e-01,\n",
      "           6.1956e-02, -7.5062e-02,  5.5122e-01,  1.7395e-01, -2.6287e-01,\n",
      "          -2.8276e-01, -2.3474e-01,  1.2362e-02, -5.6025e-02,  2.2743e-02,\n",
      "           1.9396e-01,  5.6389e-01, -1.3216e-01, -3.4216e-01,  5.1372e-01,\n",
      "           9.9956e-02,  1.8084e-01, -6.3048e-01, -2.4725e-01, -1.5670e-01,\n",
      "           1.0939e-01, -1.3407e-01, -1.3594e-01,  7.0595e-01, -7.0116e-01,\n",
      "           1.7825e-02,  9.1123e-02, -4.0263e-01, -3.1227e-01, -1.9327e-01,\n",
      "           8.4329e-02,  5.9482e-01,  2.6255e-02, -1.0330e-01,  5.4583e-01,\n",
      "           1.9685e-01, -2.7289e-01, -2.8540e-01, -2.5164e-01,  3.4340e-03,\n",
      "          -6.6268e-02,  6.6339e-02,  1.7052e-01,  5.4875e-01, -1.6731e-01,\n",
      "          -3.7299e-01,  6.0731e-01,  2.3236e-01,  6.5015e-02, -5.7944e-01,\n",
      "          -2.4028e-01, -1.9613e-01,  6.6876e-02, -1.7209e-01, -1.4449e-01,\n",
      "           6.5707e-01, -6.8256e-01,  3.8716e-02,  1.1449e-01, -5.3076e-01,\n",
      "          -2.4878e-01, -1.7766e-01, -2.6169e-02,  6.8653e-01,  1.7434e-01,\n",
      "           9.0154e-03,  5.9547e-01,  9.0842e-02, -2.2700e-01, -3.0804e-01,\n",
      "          -1.6119e-01,  4.9031e-02, -2.0589e-02]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([128, 4])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0139, -0.0787, -0.0272, -0.0417],\n",
      "        [ 0.0033,  0.0494, -0.0123,  0.0227],\n",
      "        [ 0.0840,  0.0823,  0.0379, -0.0151],\n",
      "        [-0.0278,  0.0584,  0.0317,  0.0743],\n",
      "        [ 0.0762, -0.0099, -0.0417,  0.0459],\n",
      "        [ 0.0649,  0.0091, -0.0589,  0.0485],\n",
      "        [ 0.0458, -0.0723,  0.0485, -0.0311],\n",
      "        [ 0.0279,  0.0563,  0.0791, -0.0248],\n",
      "        [-0.0813,  0.0755, -0.0165,  0.0383],\n",
      "        [-0.0721, -0.0364, -0.0574,  0.0814],\n",
      "        [-0.0438, -0.0211,  0.0055, -0.0661],\n",
      "        [ 0.0021,  0.0126, -0.0437, -0.0395],\n",
      "        [ 0.0412, -0.0169, -0.0531,  0.0762],\n",
      "        [ 0.0454,  0.0125,  0.0438,  0.0251],\n",
      "        [ 0.0287,  0.0397,  0.0766, -0.0685],\n",
      "        [-0.0439, -0.0862,  0.0521,  0.0354],\n",
      "        [ 0.0287, -0.0362,  0.0043, -0.0332],\n",
      "        [-0.0180,  0.0598, -0.0022,  0.0810],\n",
      "        [ 0.0312, -0.0346, -0.0249,  0.0128],\n",
      "        [ 0.0677,  0.0725,  0.0375, -0.0278],\n",
      "        [ 0.0479,  0.0114, -0.0526, -0.0683],\n",
      "        [-0.0165, -0.0462,  0.0869, -0.0703],\n",
      "        [ 0.0455,  0.0610,  0.0053,  0.0742],\n",
      "        [ 0.0757, -0.0677,  0.0271,  0.0347],\n",
      "        [-0.0526, -0.0091,  0.0032, -0.0064],\n",
      "        [-0.0776,  0.0104,  0.0064, -0.0518],\n",
      "        [-0.0017, -0.0029, -0.0549, -0.0834],\n",
      "        [-0.0227,  0.0364,  0.0527, -0.0069],\n",
      "        [ 0.0085,  0.0365, -0.0376, -0.0070],\n",
      "        [ 0.0820, -0.0390, -0.0115, -0.0306],\n",
      "        [ 0.0498, -0.0566,  0.0319, -0.0084],\n",
      "        [-0.0578, -0.0742,  0.0627, -0.0273],\n",
      "        [-0.0837, -0.0356, -0.0646,  0.0619],\n",
      "        [-0.0584, -0.0646,  0.0578, -0.0586],\n",
      "        [ 0.0294, -0.0348, -0.0340,  0.0118],\n",
      "        [-0.0396, -0.0184,  0.0042,  0.0138],\n",
      "        [ 0.0701, -0.0292,  0.0398, -0.0565],\n",
      "        [ 0.0478, -0.0783,  0.0089, -0.0451],\n",
      "        [ 0.0826, -0.0510,  0.0791,  0.0765],\n",
      "        [-0.0505, -0.0013, -0.0316, -0.0613],\n",
      "        [ 0.0244,  0.0527, -0.0418, -0.0360],\n",
      "        [ 0.0488,  0.0199,  0.0736,  0.0449],\n",
      "        [-0.0180,  0.0355, -0.0553,  0.0215],\n",
      "        [-0.0083,  0.0779, -0.0510, -0.0657],\n",
      "        [ 0.0020, -0.0527, -0.0741,  0.0346],\n",
      "        [ 0.0375,  0.0752, -0.0377, -0.0659],\n",
      "        [ 0.0219,  0.0304,  0.0374, -0.0870],\n",
      "        [-0.0281, -0.0418,  0.0484,  0.0157],\n",
      "        [-0.0478,  0.0550,  0.0397, -0.0394],\n",
      "        [ 0.0020, -0.0341,  0.0732, -0.0505],\n",
      "        [ 0.0496,  0.0150, -0.0263, -0.0473],\n",
      "        [-0.0515, -0.0541, -0.0173, -0.0739],\n",
      "        [ 0.0284,  0.0858,  0.0706,  0.0280],\n",
      "        [-0.0219,  0.0860,  0.0841, -0.0408],\n",
      "        [-0.0817, -0.0685,  0.0569,  0.0070],\n",
      "        [-0.0781,  0.0541,  0.0586, -0.0666],\n",
      "        [-0.0822, -0.0732,  0.0389, -0.0767],\n",
      "        [-0.0720, -0.0872,  0.0849, -0.0393],\n",
      "        [ 0.0702, -0.0119,  0.0197,  0.0544],\n",
      "        [-0.0763,  0.0492,  0.0762, -0.0804],\n",
      "        [-0.0458,  0.0242,  0.0344, -0.0160],\n",
      "        [ 0.0139, -0.0879,  0.0617,  0.0035],\n",
      "        [-0.0723, -0.0875, -0.0360, -0.0717],\n",
      "        [ 0.0703,  0.0624, -0.0128, -0.0182],\n",
      "        [-0.0372, -0.0452,  0.0723,  0.0477],\n",
      "        [-0.0867, -0.0091,  0.0572,  0.0341],\n",
      "        [-0.0241, -0.0047, -0.0325, -0.0600],\n",
      "        [ 0.0211,  0.0089, -0.0471,  0.0287],\n",
      "        [ 0.0486,  0.0764,  0.0096, -0.0662],\n",
      "        [ 0.0428, -0.0168, -0.0484,  0.0304],\n",
      "        [ 0.0791,  0.0326,  0.0070,  0.0849],\n",
      "        [ 0.0162,  0.0343,  0.0489,  0.0249],\n",
      "        [-0.0175,  0.0154, -0.0133, -0.0546],\n",
      "        [-0.0223, -0.0140,  0.0862,  0.0644],\n",
      "        [-0.0730, -0.0561, -0.0175,  0.0270],\n",
      "        [-0.0750,  0.0791,  0.0761,  0.0207],\n",
      "        [ 0.0588,  0.0386, -0.0708, -0.0120],\n",
      "        [-0.0486,  0.0058, -0.0176, -0.0872],\n",
      "        [ 0.0420,  0.0211, -0.0453, -0.0756],\n",
      "        [-0.0469,  0.0648,  0.0840,  0.0771],\n",
      "        [ 0.0167, -0.0466,  0.0435, -0.0558],\n",
      "        [-0.0180,  0.0093, -0.0242, -0.0033],\n",
      "        [ 0.0672, -0.0508,  0.0832, -0.0102],\n",
      "        [ 0.0063, -0.0791, -0.0377, -0.0634],\n",
      "        [ 0.0389,  0.0140, -0.0615,  0.0771],\n",
      "        [-0.0714,  0.0663, -0.0419,  0.0436],\n",
      "        [-0.0005, -0.0116,  0.0021, -0.0355],\n",
      "        [ 0.0094,  0.0812,  0.0483, -0.0468],\n",
      "        [-0.0453,  0.0196,  0.0874, -0.0572],\n",
      "        [ 0.0470,  0.0742, -0.0554,  0.0737],\n",
      "        [-0.0046, -0.0622, -0.0864, -0.0697],\n",
      "        [-0.0350,  0.0755, -0.0144, -0.0533],\n",
      "        [-0.0733, -0.0141, -0.0706,  0.0772],\n",
      "        [ 0.0318, -0.0718, -0.0039, -0.0808],\n",
      "        [ 0.0250, -0.0836, -0.0437, -0.0564],\n",
      "        [ 0.0294, -0.0549, -0.0642, -0.0373],\n",
      "        [-0.0052,  0.0771, -0.0031,  0.0305],\n",
      "        [-0.0191, -0.0395,  0.0577,  0.0202],\n",
      "        [-0.0204,  0.0647, -0.0778, -0.0585],\n",
      "        [ 0.0814,  0.0275, -0.0803,  0.0322],\n",
      "        [ 0.0391,  0.0696, -0.0112,  0.0406],\n",
      "        [-0.0811, -0.0443, -0.0128, -0.0556],\n",
      "        [ 0.0855,  0.0882,  0.0047, -0.0368],\n",
      "        [ 0.0583, -0.0531,  0.0071,  0.0782],\n",
      "        [-0.0406, -0.0030,  0.0457, -0.0841],\n",
      "        [ 0.0442, -0.0754, -0.0803, -0.0494],\n",
      "        [-0.0486, -0.0157,  0.0512,  0.0465],\n",
      "        [-0.0004, -0.0288, -0.0781,  0.0676],\n",
      "        [-0.0460, -0.0629, -0.0148,  0.0751],\n",
      "        [ 0.0104, -0.0334, -0.0388, -0.0608],\n",
      "        [-0.0189,  0.0272,  0.0264, -0.0059],\n",
      "        [-0.0798, -0.0168, -0.0396, -0.0421],\n",
      "        [-0.0075,  0.0069, -0.0432, -0.0495],\n",
      "        [-0.0047,  0.0769, -0.0368,  0.0721],\n",
      "        [-0.0162,  0.0626, -0.0485,  0.0189],\n",
      "        [ 0.0079,  0.0517, -0.0361, -0.0346],\n",
      "        [ 0.0057,  0.0109, -0.0509, -0.0678],\n",
      "        [ 0.0801, -0.0706,  0.0351, -0.0406],\n",
      "        [-0.0175,  0.0669,  0.0229,  0.0174],\n",
      "        [-0.0324,  0.0067, -0.0462, -0.0483],\n",
      "        [ 0.0860,  0.0160,  0.0122,  0.0359],\n",
      "        [ 0.0542, -0.0770, -0.0258,  0.0683],\n",
      "        [-0.0590, -0.0269,  0.0553,  0.0716],\n",
      "        [ 0.0781,  0.0203, -0.0462,  0.0629],\n",
      "        [ 0.0124, -0.0418,  0.0457,  0.0071],\n",
      "        [-0.0077,  0.0158,  0.0176, -0.0724],\n",
      "        [ 0.0464, -0.0335, -0.0344,  0.0326],\n",
      "        [-0.0237,  0.0807, -0.0869,  0.0878]], requires_grad=True)\n",
      "spliced Wo: torch.Size([128, 4])\n",
      "tensor([[ 0.0139, -0.0787, -0.0272, -0.0417],\n",
      "        [ 0.0033,  0.0494, -0.0123,  0.0227],\n",
      "        [ 0.0840,  0.0823,  0.0379, -0.0151],\n",
      "        [-0.0278,  0.0584,  0.0317,  0.0743],\n",
      "        [ 0.0762, -0.0099, -0.0417,  0.0459],\n",
      "        [ 0.0649,  0.0091, -0.0589,  0.0485],\n",
      "        [ 0.0458, -0.0723,  0.0485, -0.0311],\n",
      "        [ 0.0279,  0.0563,  0.0791, -0.0248],\n",
      "        [-0.0813,  0.0755, -0.0165,  0.0383],\n",
      "        [-0.0721, -0.0364, -0.0574,  0.0814],\n",
      "        [-0.0438, -0.0211,  0.0055, -0.0661],\n",
      "        [ 0.0021,  0.0126, -0.0437, -0.0395],\n",
      "        [ 0.0412, -0.0169, -0.0531,  0.0762],\n",
      "        [ 0.0454,  0.0125,  0.0438,  0.0251],\n",
      "        [ 0.0287,  0.0397,  0.0766, -0.0685],\n",
      "        [-0.0439, -0.0862,  0.0521,  0.0354],\n",
      "        [ 0.0287, -0.0362,  0.0043, -0.0332],\n",
      "        [-0.0180,  0.0598, -0.0022,  0.0810],\n",
      "        [ 0.0312, -0.0346, -0.0249,  0.0128],\n",
      "        [ 0.0677,  0.0725,  0.0375, -0.0278],\n",
      "        [ 0.0479,  0.0114, -0.0526, -0.0683],\n",
      "        [-0.0165, -0.0462,  0.0869, -0.0703],\n",
      "        [ 0.0455,  0.0610,  0.0053,  0.0742],\n",
      "        [ 0.0757, -0.0677,  0.0271,  0.0347],\n",
      "        [-0.0526, -0.0091,  0.0032, -0.0064],\n",
      "        [-0.0776,  0.0104,  0.0064, -0.0518],\n",
      "        [-0.0017, -0.0029, -0.0549, -0.0834],\n",
      "        [-0.0227,  0.0364,  0.0527, -0.0069],\n",
      "        [ 0.0085,  0.0365, -0.0376, -0.0070],\n",
      "        [ 0.0820, -0.0390, -0.0115, -0.0306],\n",
      "        [ 0.0498, -0.0566,  0.0319, -0.0084],\n",
      "        [-0.0578, -0.0742,  0.0627, -0.0273],\n",
      "        [-0.0837, -0.0356, -0.0646,  0.0619],\n",
      "        [-0.0584, -0.0646,  0.0578, -0.0586],\n",
      "        [ 0.0294, -0.0348, -0.0340,  0.0118],\n",
      "        [-0.0396, -0.0184,  0.0042,  0.0138],\n",
      "        [ 0.0701, -0.0292,  0.0398, -0.0565],\n",
      "        [ 0.0478, -0.0783,  0.0089, -0.0451],\n",
      "        [ 0.0826, -0.0510,  0.0791,  0.0765],\n",
      "        [-0.0505, -0.0013, -0.0316, -0.0613],\n",
      "        [ 0.0244,  0.0527, -0.0418, -0.0360],\n",
      "        [ 0.0488,  0.0199,  0.0736,  0.0449],\n",
      "        [-0.0180,  0.0355, -0.0553,  0.0215],\n",
      "        [-0.0083,  0.0779, -0.0510, -0.0657],\n",
      "        [ 0.0020, -0.0527, -0.0741,  0.0346],\n",
      "        [ 0.0375,  0.0752, -0.0377, -0.0659],\n",
      "        [ 0.0219,  0.0304,  0.0374, -0.0870],\n",
      "        [-0.0281, -0.0418,  0.0484,  0.0157],\n",
      "        [-0.0478,  0.0550,  0.0397, -0.0394],\n",
      "        [ 0.0020, -0.0341,  0.0732, -0.0505],\n",
      "        [ 0.0496,  0.0150, -0.0263, -0.0473],\n",
      "        [-0.0515, -0.0541, -0.0173, -0.0739],\n",
      "        [ 0.0284,  0.0858,  0.0706,  0.0280],\n",
      "        [-0.0219,  0.0860,  0.0841, -0.0408],\n",
      "        [-0.0817, -0.0685,  0.0569,  0.0070],\n",
      "        [-0.0781,  0.0541,  0.0586, -0.0666],\n",
      "        [-0.0822, -0.0732,  0.0389, -0.0767],\n",
      "        [-0.0720, -0.0872,  0.0849, -0.0393],\n",
      "        [ 0.0702, -0.0119,  0.0197,  0.0544],\n",
      "        [-0.0763,  0.0492,  0.0762, -0.0804],\n",
      "        [-0.0458,  0.0242,  0.0344, -0.0160],\n",
      "        [ 0.0139, -0.0879,  0.0617,  0.0035],\n",
      "        [-0.0723, -0.0875, -0.0360, -0.0717],\n",
      "        [ 0.0703,  0.0624, -0.0128, -0.0182],\n",
      "        [-0.0372, -0.0452,  0.0723,  0.0477],\n",
      "        [-0.0867, -0.0091,  0.0572,  0.0341],\n",
      "        [-0.0241, -0.0047, -0.0325, -0.0600],\n",
      "        [ 0.0211,  0.0089, -0.0471,  0.0287],\n",
      "        [ 0.0486,  0.0764,  0.0096, -0.0662],\n",
      "        [ 0.0428, -0.0168, -0.0484,  0.0304],\n",
      "        [ 0.0791,  0.0326,  0.0070,  0.0849],\n",
      "        [ 0.0162,  0.0343,  0.0489,  0.0249],\n",
      "        [-0.0175,  0.0154, -0.0133, -0.0546],\n",
      "        [-0.0223, -0.0140,  0.0862,  0.0644],\n",
      "        [-0.0730, -0.0561, -0.0175,  0.0270],\n",
      "        [-0.0750,  0.0791,  0.0761,  0.0207],\n",
      "        [ 0.0588,  0.0386, -0.0708, -0.0120],\n",
      "        [-0.0486,  0.0058, -0.0176, -0.0872],\n",
      "        [ 0.0420,  0.0211, -0.0453, -0.0756],\n",
      "        [-0.0469,  0.0648,  0.0840,  0.0771],\n",
      "        [ 0.0167, -0.0466,  0.0435, -0.0558],\n",
      "        [-0.0180,  0.0093, -0.0242, -0.0033],\n",
      "        [ 0.0672, -0.0508,  0.0832, -0.0102],\n",
      "        [ 0.0063, -0.0791, -0.0377, -0.0634],\n",
      "        [ 0.0389,  0.0140, -0.0615,  0.0771],\n",
      "        [-0.0714,  0.0663, -0.0419,  0.0436],\n",
      "        [-0.0005, -0.0116,  0.0021, -0.0355],\n",
      "        [ 0.0094,  0.0812,  0.0483, -0.0468],\n",
      "        [-0.0453,  0.0196,  0.0874, -0.0572],\n",
      "        [ 0.0470,  0.0742, -0.0554,  0.0737],\n",
      "        [-0.0046, -0.0622, -0.0864, -0.0697],\n",
      "        [-0.0350,  0.0755, -0.0144, -0.0533],\n",
      "        [-0.0733, -0.0141, -0.0706,  0.0772],\n",
      "        [ 0.0318, -0.0718, -0.0039, -0.0808],\n",
      "        [ 0.0250, -0.0836, -0.0437, -0.0564],\n",
      "        [ 0.0294, -0.0549, -0.0642, -0.0373],\n",
      "        [-0.0052,  0.0771, -0.0031,  0.0305],\n",
      "        [-0.0191, -0.0395,  0.0577,  0.0202],\n",
      "        [-0.0204,  0.0647, -0.0778, -0.0585],\n",
      "        [ 0.0814,  0.0275, -0.0803,  0.0322],\n",
      "        [ 0.0391,  0.0696, -0.0112,  0.0406],\n",
      "        [-0.0811, -0.0443, -0.0128, -0.0556],\n",
      "        [ 0.0855,  0.0882,  0.0047, -0.0368],\n",
      "        [ 0.0583, -0.0531,  0.0071,  0.0782],\n",
      "        [-0.0406, -0.0030,  0.0457, -0.0841],\n",
      "        [ 0.0442, -0.0754, -0.0803, -0.0494],\n",
      "        [-0.0486, -0.0157,  0.0512,  0.0465],\n",
      "        [-0.0004, -0.0288, -0.0781,  0.0676],\n",
      "        [-0.0460, -0.0629, -0.0148,  0.0751],\n",
      "        [ 0.0104, -0.0334, -0.0388, -0.0608],\n",
      "        [-0.0189,  0.0272,  0.0264, -0.0059],\n",
      "        [-0.0798, -0.0168, -0.0396, -0.0421],\n",
      "        [-0.0075,  0.0069, -0.0432, -0.0495],\n",
      "        [-0.0047,  0.0769, -0.0368,  0.0721],\n",
      "        [-0.0162,  0.0626, -0.0485,  0.0189],\n",
      "        [ 0.0079,  0.0517, -0.0361, -0.0346],\n",
      "        [ 0.0057,  0.0109, -0.0509, -0.0678],\n",
      "        [ 0.0801, -0.0706,  0.0351, -0.0406],\n",
      "        [-0.0175,  0.0669,  0.0229,  0.0174],\n",
      "        [-0.0324,  0.0067, -0.0462, -0.0483],\n",
      "        [ 0.0860,  0.0160,  0.0122,  0.0359],\n",
      "        [ 0.0542, -0.0770, -0.0258,  0.0683],\n",
      "        [-0.0590, -0.0269,  0.0553,  0.0716],\n",
      "        [ 0.0781,  0.0203, -0.0462,  0.0629],\n",
      "        [ 0.0124, -0.0418,  0.0457,  0.0071],\n",
      "        [-0.0077,  0.0158,  0.0176, -0.0724],\n",
      "        [ 0.0464, -0.0335, -0.0344,  0.0326],\n",
      "        [-0.0237,  0.0807, -0.0869,  0.0878]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.2210,  0.0973, -0.0113,  0.0162],\n",
      "         [ 0.1159,  0.1137, -0.0827,  0.0687],\n",
      "         [ 0.1911, -0.0500,  0.0287, -0.0069]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[ 1.1705, -0.1432,  0.5766,  1.5009],\n",
      "         [ 1.1767, -1.2033,  0.9331,  2.0614],\n",
      "         [ 1.7695,  0.9544, -1.6644, -0.0465]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 1.1705, -0.1432,  0.5766,  1.5009],\n",
      "         [ 1.1767, -1.2033,  0.9331,  2.0614],\n",
      "         [ 1.7695,  0.9544, -1.6644, -0.0465]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 1.1740, -0.1436,  0.5784,  1.5055],\n",
      "         [ 0.8345, -0.8534,  0.6618,  1.4620],\n",
      "         [ 1.3557,  0.7312, -1.2752, -0.0356]]], grad_fn=<MulBackward0>)\n",
      "dim: 4\n",
      "skip: 0\n",
      "spliced scale: torch.Size([4])\n",
      "tensor([0., 0., 0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 1.1740, -0.1436,  0.5784,  1.5055],\n",
      "         [ 0.8345, -0.8534,  0.6618,  1.4620],\n",
      "         [ 1.3557,  0.7312, -1.2752, -0.0356]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 4])\n",
      "tensor([[[ 1.1740, -0.1436,  0.5784,  1.5055],\n",
      "         [ 0.8345, -0.8534,  0.6618,  1.4620],\n",
      "         [ 1.3557,  0.7312, -1.2752, -0.0356]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 4\n",
      "d_skip: 0\n",
      "i_dim: 16\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([4, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.4920,  0.3010,  0.1499, -0.3392,  0.4856, -0.4726,  0.0867,  0.2430,\n",
      "          0.3942,  0.4743, -0.2700, -0.3373,  0.3804, -0.2024, -0.3690, -0.1290],\n",
      "        [-0.4186, -0.3264, -0.4179,  0.2408, -0.1365,  0.1968,  0.0011, -0.4685,\n",
      "         -0.1507,  0.2918,  0.3232,  0.0377, -0.1145, -0.4494, -0.2852, -0.0918],\n",
      "        [-0.3881, -0.4971, -0.3846,  0.2184,  0.2203,  0.3713,  0.1256, -0.4725,\n",
      "          0.1326,  0.1626,  0.4547, -0.0986,  0.3189,  0.3653,  0.2537,  0.3719],\n",
      "        [ 0.2424, -0.1054,  0.2797, -0.0835, -0.3293,  0.3435,  0.2210,  0.4816,\n",
      "          0.2108,  0.2477, -0.1237, -0.2128,  0.3488, -0.4129, -0.1048,  0.3282]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([4, 16])\n",
      "tensor([[ 0.4920,  0.3010,  0.1499, -0.3392,  0.4856, -0.4726,  0.0867,  0.2430,\n",
      "          0.3942,  0.4743, -0.2700, -0.3373,  0.3804, -0.2024, -0.3690, -0.1290],\n",
      "        [-0.4186, -0.3264, -0.4179,  0.2408, -0.1365,  0.1968,  0.0011, -0.4685,\n",
      "         -0.1507,  0.2918,  0.3232,  0.0377, -0.1145, -0.4494, -0.2852, -0.0918],\n",
      "        [-0.3881, -0.4971, -0.3846,  0.2184,  0.2203,  0.3713,  0.1256, -0.4725,\n",
      "          0.1326,  0.1626,  0.4547, -0.0986,  0.3189,  0.3653,  0.2537,  0.3719],\n",
      "        [ 0.2424, -0.1054,  0.2797, -0.0835, -0.3293,  0.3435,  0.2210,  0.4816,\n",
      "          0.2108,  0.2477, -0.1237, -0.2128,  0.3488, -0.4129, -0.1048,  0.3282]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bgate: torch.Size([16])\n",
      "Parameter containing:\n",
      "tensor([-0.4296, -0.2656, -0.3730, -0.4881,  0.2735,  0.3810, -0.2995, -0.2457,\n",
      "         0.3296, -0.1207, -0.1850,  0.1151, -0.1815,  0.0565,  0.1133,  0.2197],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([16])\n",
      "tensor([-0.4296, -0.2656, -0.3730, -0.4881,  0.2735,  0.3810, -0.2995, -0.2457,\n",
      "         0.3296, -0.1207, -0.1850,  0.1151, -0.1815,  0.0565,  0.1133,  0.2197],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.3487, -0.3116,  0.0618, -0.9204,  0.4949,  0.5299,  0.2075,\n",
      "           0.5587,  1.2080,  0.8612, -0.4716, -0.6637,  0.9910, -0.5269,\n",
      "          -0.2900,  0.7905],\n",
      "         [ 0.4358, -0.2190,  0.2632, -0.9543,  0.4596,  0.5666,  0.1781,\n",
      "           0.7483,  1.1830,  0.4959, -0.5660, -0.5749,  0.9545, -0.0908,\n",
      "           0.0634,  0.9163],\n",
      "         [ 0.4175,  0.5413,  0.0051, -1.0475,  0.5628, -0.6015, -0.3491,\n",
      "           0.3264,  0.5772,  0.5195, -0.8900, -0.1812, -0.1686, -0.9976,\n",
      "          -0.9153, -0.5083]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.2219, -0.1177,  0.0324, -0.1645,  0.3413,  0.3719,  0.1208,\n",
      "           0.3977,  1.0709,  0.6937, -0.1503, -0.1682,  0.8316, -0.1576,\n",
      "          -0.1119,  0.6209],\n",
      "         [ 0.2913, -0.0905,  0.1589, -0.1622,  0.3112,  0.4049,  0.1016,\n",
      "           0.5784,  1.0430,  0.3422, -0.1617, -0.1625,  0.7924, -0.0421,\n",
      "           0.0333,  0.7515],\n",
      "         [ 0.2763,  0.3821,  0.0026, -0.1544,  0.4014, -0.1647, -0.1269,\n",
      "           0.2049,  0.4145,  0.3628, -0.1662, -0.0776, -0.0730, -0.1589,\n",
      "          -0.1648, -0.1554]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.4397,  0.0639,  0.3251, -0.4417, -0.1198, -0.4560,  0.3213, -0.1483,\n",
      "         -0.0726, -0.1437, -0.1375,  0.2211,  0.4713, -0.1941,  0.2534, -0.1524],\n",
      "        [-0.1683,  0.0169, -0.3806, -0.3985,  0.0782, -0.4636, -0.4176, -0.0484,\n",
      "         -0.3432, -0.1917,  0.4930,  0.3492, -0.0744,  0.1426, -0.3753,  0.4084],\n",
      "        [-0.2476,  0.0072, -0.3127, -0.3605,  0.3117,  0.4990,  0.0193,  0.1382,\n",
      "          0.2498,  0.1919,  0.2682, -0.3924, -0.0709, -0.2175, -0.3473,  0.1599],\n",
      "        [ 0.4299,  0.3039,  0.2940, -0.4826,  0.1980, -0.0223, -0.0833, -0.0818,\n",
      "          0.1703, -0.4762, -0.0675,  0.1609, -0.0555, -0.1132, -0.4620,  0.0077]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([4, 16])\n",
      "tensor([[-0.4397,  0.0639,  0.3251, -0.4417, -0.1198, -0.4560,  0.3213, -0.1483,\n",
      "         -0.0726, -0.1437, -0.1375,  0.2211,  0.4713, -0.1941,  0.2534, -0.1524],\n",
      "        [-0.1683,  0.0169, -0.3806, -0.3985,  0.0782, -0.4636, -0.4176, -0.0484,\n",
      "         -0.3432, -0.1917,  0.4930,  0.3492, -0.0744,  0.1426, -0.3753,  0.4084],\n",
      "        [-0.2476,  0.0072, -0.3127, -0.3605,  0.3117,  0.4990,  0.0193,  0.1382,\n",
      "          0.2498,  0.1919,  0.2682, -0.3924, -0.0709, -0.2175, -0.3473,  0.1599],\n",
      "        [ 0.4299,  0.3039,  0.2940, -0.4826,  0.1980, -0.0223, -0.0833, -0.0818,\n",
      "          0.1703, -0.4762, -0.0675,  0.1609, -0.0555, -0.1132, -0.4620,  0.0077]],\n",
      "       grad_fn=<AliasBackward0>)\n",
      "Bup: torch.Size([16])\n",
      "Parameter containing:\n",
      "tensor([ 0.3836,  0.0685, -0.3794,  0.2138,  0.0660,  0.0780, -0.2228,  0.0113,\n",
      "        -0.1320,  0.3062, -0.2345,  0.4163, -0.4161,  0.3209,  0.2370, -0.2584],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([16])\n",
      "tensor([ 0.3836,  0.0685, -0.3794,  0.2138,  0.0660,  0.0780, -0.2228,  0.0113,\n",
      "        -0.1320,  0.3062, -0.2345,  0.4163, -0.4161,  0.3209,  0.2370, -0.2584],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.3957,  0.6027,  0.3186, -1.1826,  0.3925, -0.1356,  0.1002,\n",
      "          -0.1990,  0.2329, -0.4409, -0.4133,  0.6410,  0.0233, -0.2237,\n",
      "          -0.3079, -0.3919],\n",
      "         [ 0.6250,  0.5565,  0.4395, -0.7588,  0.3950,  0.3908,  0.2928,\n",
      "          -0.0992,  0.5145, -0.2193, -0.6912,  0.2783, -0.0874, -0.2722,\n",
      "          -0.1364, -0.6170],\n",
      "         [-0.0351,  0.1474,  0.1713, -0.1996, -0.4438, -1.5148, -0.1143,\n",
      "          -0.3985, -0.8059, -0.2566, -0.4000,  1.4661,  0.2609,  0.4433,\n",
      "           0.7654, -0.3706]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 16])\n",
      "tensor([[[ 8.7787e-02, -7.0928e-02,  1.0322e-02,  1.9449e-01,  1.3394e-01,\n",
      "          -5.0445e-02,  1.2108e-02, -7.9148e-02,  2.4941e-01, -3.0583e-01,\n",
      "           6.2096e-02, -1.0782e-01,  1.9390e-02,  3.5259e-02,  3.4459e-02,\n",
      "          -2.4333e-01],\n",
      "         [ 1.8210e-01, -5.0371e-02,  6.9842e-02,  1.2308e-01,  1.2293e-01,\n",
      "           1.5823e-01,  2.9763e-02, -5.7388e-02,  5.3665e-01, -7.5044e-02,\n",
      "           1.1177e-01, -4.5234e-02, -6.9258e-02,  1.1461e-02, -4.5457e-03,\n",
      "          -4.6371e-01],\n",
      "         [-9.6989e-03,  5.6315e-02,  4.4059e-04,  3.0832e-02, -1.7816e-01,\n",
      "           2.4942e-01,  1.4507e-02, -8.1660e-02, -3.3404e-01, -9.3076e-02,\n",
      "           6.6476e-02, -1.1372e-01, -1.9055e-02, -7.0419e-02, -1.2612e-01,\n",
      "           5.7568e-02]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([16, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.1754,  0.0761,  0.1232,  0.1366],\n",
      "        [-0.0635,  0.1460,  0.0160, -0.1254],\n",
      "        [ 0.0276, -0.1514, -0.0825, -0.1608],\n",
      "        [ 0.0913, -0.0020,  0.2442,  0.1662],\n",
      "        [ 0.1354,  0.1174,  0.2448, -0.2325],\n",
      "        [-0.0123,  0.0650,  0.1438, -0.1913],\n",
      "        [-0.0013,  0.2201,  0.1408,  0.0611],\n",
      "        [-0.1373,  0.1180,  0.1358, -0.1107],\n",
      "        [ 0.1830,  0.2221, -0.1693,  0.2425],\n",
      "        [-0.1213, -0.0698,  0.0168,  0.1718],\n",
      "        [-0.0677, -0.2102, -0.0295, -0.1922],\n",
      "        [ 0.0277,  0.0011,  0.1526, -0.0894],\n",
      "        [ 0.2469,  0.1777,  0.0815,  0.0353],\n",
      "        [-0.1205, -0.1902, -0.1469, -0.0820],\n",
      "        [ 0.0319, -0.0125,  0.0571, -0.1865],\n",
      "        [ 0.0971,  0.0427,  0.2102,  0.0165]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([16, 4])\n",
      "tensor([[-0.1754,  0.0761,  0.1232,  0.1366],\n",
      "        [-0.0635,  0.1460,  0.0160, -0.1254],\n",
      "        [ 0.0276, -0.1514, -0.0825, -0.1608],\n",
      "        [ 0.0913, -0.0020,  0.2442,  0.1662],\n",
      "        [ 0.1354,  0.1174,  0.2448, -0.2325],\n",
      "        [-0.0123,  0.0650,  0.1438, -0.1913],\n",
      "        [-0.0013,  0.2201,  0.1408,  0.0611],\n",
      "        [-0.1373,  0.1180,  0.1358, -0.1107],\n",
      "        [ 0.1830,  0.2221, -0.1693,  0.2425],\n",
      "        [-0.1213, -0.0698,  0.0168,  0.1718],\n",
      "        [-0.0677, -0.2102, -0.0295, -0.1922],\n",
      "        [ 0.0277,  0.0011,  0.1526, -0.0894],\n",
      "        [ 0.2469,  0.1777,  0.0815,  0.0353],\n",
      "        [-0.1205, -0.1902, -0.1469, -0.0820],\n",
      "        [ 0.0319, -0.0125,  0.0571, -0.1865],\n",
      "        [ 0.0971,  0.0427,  0.2102,  0.0165]], grad_fn=<AliasBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([-0.0242,  0.1740,  0.0925,  0.2142], requires_grad=True)\n",
      "Bdown spliced: torch.Size([4])\n",
      "tensor([-0.0242,  0.1740,  0.0925,  0.2142], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 4])\n",
      "tensor([[[ 0.0661,  0.2236,  0.0469,  0.2468],\n",
      "         [ 0.0176,  0.2609, -0.0197,  0.2936],\n",
      "         [-0.0914,  0.1025,  0.1323,  0.1445]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[ 1.2365,  0.0804,  0.6235,  1.7477],\n",
      "         [ 1.1942, -0.9424,  0.9134,  2.3550],\n",
      "         [ 1.6781,  1.0569, -1.5321,  0.0980]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 4])\n",
      "tensor([[[ 1.2365,  0.0804,  0.6235,  1.7477],\n",
      "         [ 1.1942, -0.9424,  0.9134,  2.3550],\n",
      "         [ 1.6781,  1.0569, -1.5321,  0.0980]]], grad_fn=<AddBackward0>)\n",
      "Level 1 from range(2)\n",
      "Model 0 from range(2)\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.6289,  0.4586],\n",
      "         [-0.6905,  1.2571],\n",
      "         [ 0.7723,  3.0443]]])\n",
      "normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[ 1.1427,  0.8332],\n",
      "         [-0.6809,  1.2395],\n",
      "         [ 0.3478,  1.3708]]])\n",
      "dim: 2\n",
      "skip: 0\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[ 1.1427,  0.8332],\n",
      "         [-0.6809,  1.2395],\n",
      "         [ 0.3478,  1.3708]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 2])\n",
      "d_skip: 0\n",
      "models_in_this_level: 2\n",
      "h_dim: 16\n",
      "h_skip: 0\n",
      "self.Wqkv: torch.Size([4, 192])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3301,  0.3833,  0.4804,  0.3799, -0.3784,  0.0511,  0.3683,  0.1008,\n",
      "         -0.2285, -0.0197,  0.0348,  0.1120, -0.3621, -0.2547,  0.4441, -0.4699,\n",
      "         -0.3608, -0.4284,  0.1076,  0.4837, -0.0746,  0.1340, -0.4201, -0.0809,\n",
      "          0.1141, -0.1077,  0.0644,  0.3932,  0.0016,  0.0923, -0.3204, -0.1327,\n",
      "         -0.3313, -0.1889, -0.1339,  0.0845,  0.1315, -0.0367, -0.3632, -0.3186,\n",
      "         -0.0542, -0.2293, -0.4923, -0.4534, -0.3555, -0.4473, -0.2461,  0.3808,\n",
      "         -0.2491, -0.0109, -0.3062,  0.1362, -0.3017,  0.0435,  0.1770,  0.4863,\n",
      "         -0.2845, -0.0166,  0.4380,  0.1111,  0.3862,  0.1065,  0.2000, -0.2471,\n",
      "         -0.0877, -0.4372,  0.4612,  0.0399,  0.2884,  0.0085, -0.3506,  0.0517,\n",
      "          0.2164, -0.1687,  0.3631,  0.2543, -0.0103,  0.4510,  0.3025, -0.0815,\n",
      "         -0.0690,  0.4011, -0.0612, -0.3017, -0.4315, -0.0644, -0.0030,  0.1530,\n",
      "         -0.3891, -0.2778, -0.2134, -0.0432,  0.4629, -0.0109, -0.0543,  0.3298,\n",
      "          0.1822, -0.2275, -0.2900, -0.0932, -0.0859,  0.1376,  0.4553,  0.2669,\n",
      "         -0.1041, -0.4566,  0.4032,  0.4596,  0.0240,  0.2582,  0.4123,  0.0547,\n",
      "          0.2229,  0.0054, -0.2271,  0.2998,  0.3078,  0.1199,  0.0656, -0.4977,\n",
      "         -0.1922, -0.1638, -0.0580, -0.1722, -0.0207, -0.2017, -0.4377, -0.2301,\n",
      "          0.1247,  0.4458,  0.0637, -0.4035, -0.1888, -0.3908,  0.4974,  0.0636,\n",
      "         -0.3287,  0.3867, -0.3518, -0.0131,  0.0452, -0.4103, -0.0953,  0.3811,\n",
      "          0.1975,  0.4332,  0.1069,  0.0880, -0.3776,  0.1469, -0.3684,  0.2586,\n",
      "          0.0258,  0.2367, -0.4173, -0.4539,  0.0401,  0.3268,  0.4671,  0.3535,\n",
      "          0.3936,  0.4408,  0.4123,  0.1102, -0.4779,  0.2480,  0.2368, -0.1251,\n",
      "         -0.3720,  0.2231, -0.0660,  0.0564, -0.3418, -0.2669,  0.4652, -0.4518,\n",
      "         -0.3219,  0.2803, -0.4718, -0.3934, -0.3866, -0.2158,  0.4596,  0.4616,\n",
      "          0.3334,  0.4542, -0.2137, -0.0657, -0.2930,  0.2428,  0.0470, -0.3103],\n",
      "        [-0.3085,  0.2079, -0.0558,  0.4727, -0.0572,  0.4325, -0.4721, -0.3933,\n",
      "         -0.4037,  0.2223, -0.3074, -0.4287,  0.4854, -0.1563,  0.2378,  0.3082,\n",
      "          0.2669, -0.4955, -0.4475,  0.1592, -0.2937,  0.1021, -0.2588,  0.0658,\n",
      "          0.1464,  0.0969, -0.4747, -0.1736, -0.2305,  0.3631,  0.2602,  0.4455,\n",
      "         -0.4695, -0.4149,  0.2928, -0.2088, -0.2163,  0.2472,  0.2846, -0.3211,\n",
      "         -0.2504,  0.3590, -0.4542, -0.4578,  0.0834, -0.2651, -0.3716,  0.0151,\n",
      "         -0.0984,  0.0979,  0.0320,  0.0527, -0.4798, -0.0049,  0.3435, -0.2685,\n",
      "          0.0051, -0.3738, -0.0213,  0.3898,  0.4789,  0.2627, -0.2252,  0.1004,\n",
      "         -0.4895, -0.0907, -0.1520, -0.2988,  0.0696,  0.3762, -0.0568,  0.3029,\n",
      "          0.2704,  0.1733,  0.0607, -0.3022,  0.0958,  0.2246,  0.0864,  0.1957,\n",
      "         -0.1262,  0.0189, -0.3558,  0.3480, -0.4218,  0.3846,  0.2977, -0.2668,\n",
      "         -0.4719, -0.3136,  0.0346, -0.2221, -0.0843, -0.1978,  0.3260, -0.1030,\n",
      "          0.2973,  0.2319, -0.3665,  0.3388, -0.4193, -0.3237,  0.3003,  0.1727,\n",
      "          0.1312,  0.3687,  0.3775, -0.1518, -0.0341, -0.4558,  0.4252,  0.1452,\n",
      "          0.4464, -0.0587,  0.2010,  0.3056,  0.1037, -0.2781,  0.1119,  0.4075,\n",
      "          0.3024,  0.4253,  0.3235, -0.2391, -0.1235, -0.2415,  0.3376,  0.0750,\n",
      "          0.4636, -0.2067, -0.2724,  0.2813,  0.3083, -0.1645,  0.4072, -0.3367,\n",
      "         -0.4844,  0.4903, -0.2458,  0.3983,  0.4745,  0.1093,  0.3478, -0.3051,\n",
      "          0.2002, -0.0206,  0.2579, -0.2674, -0.3509,  0.4186, -0.1168,  0.3344,\n",
      "          0.4014, -0.1908,  0.1122, -0.2635, -0.2765, -0.4911,  0.3065,  0.2265,\n",
      "         -0.4627, -0.0526, -0.4488,  0.0989, -0.0009,  0.3884,  0.4852, -0.1694,\n",
      "         -0.1860, -0.0708, -0.4281,  0.1182, -0.2934,  0.0983, -0.2847,  0.1333,\n",
      "          0.3494,  0.2996, -0.4393,  0.2396,  0.0914, -0.1308,  0.4602,  0.1519,\n",
      "          0.0964,  0.2173, -0.3352,  0.2081, -0.3686,  0.4634,  0.1625,  0.1153],\n",
      "        [-0.4529, -0.1572, -0.0539,  0.3450,  0.4261,  0.2598,  0.2026, -0.0352,\n",
      "          0.2761, -0.3891,  0.3350, -0.2972, -0.0346, -0.0281, -0.1072,  0.4468,\n",
      "          0.0352,  0.2435, -0.1058,  0.3529,  0.0723,  0.2023,  0.3687, -0.4046,\n",
      "         -0.4893, -0.1057,  0.0078, -0.4062,  0.2501,  0.4469,  0.2588,  0.3636,\n",
      "         -0.4504, -0.4279, -0.2497, -0.4591, -0.4598, -0.1506,  0.0301,  0.1856,\n",
      "         -0.3996,  0.0477, -0.2218,  0.4408,  0.3175, -0.2819, -0.3891, -0.2715,\n",
      "         -0.0119, -0.1215,  0.1585,  0.4206, -0.1848,  0.1778,  0.4221, -0.1031,\n",
      "          0.1428,  0.2237, -0.3037, -0.0875,  0.3658,  0.3300,  0.1022, -0.1516,\n",
      "         -0.4768,  0.0470,  0.2139,  0.3245, -0.2838, -0.2326,  0.3793, -0.4275,\n",
      "          0.0624, -0.1127,  0.3912, -0.2025, -0.1955,  0.0486, -0.3604,  0.1436,\n",
      "         -0.2875, -0.3124, -0.1595, -0.0667, -0.0748, -0.3298, -0.3057,  0.2011,\n",
      "          0.1131, -0.0505, -0.2193,  0.2211,  0.3423, -0.1574, -0.4652,  0.2528,\n",
      "          0.1820,  0.1900, -0.2276, -0.0111, -0.4589, -0.4369,  0.2751,  0.2070,\n",
      "          0.2440, -0.3492,  0.1704, -0.0839, -0.4068,  0.1689, -0.2187, -0.2404,\n",
      "         -0.4594,  0.1789, -0.4587, -0.1004,  0.4327,  0.1572,  0.1019,  0.1318,\n",
      "          0.3062,  0.3306,  0.1911,  0.4272,  0.0892,  0.2553,  0.4253,  0.1659,\n",
      "         -0.0153, -0.3781,  0.4079,  0.0030,  0.4418, -0.4477,  0.2635, -0.3938,\n",
      "          0.0555, -0.3003, -0.2474, -0.3613,  0.0365,  0.0919,  0.0444, -0.0326,\n",
      "          0.1988, -0.0043,  0.4960, -0.2726,  0.2577, -0.1399,  0.0490, -0.2605,\n",
      "         -0.2334,  0.1966, -0.3514, -0.0269, -0.2524, -0.0522,  0.3142, -0.0619,\n",
      "         -0.2309,  0.3951, -0.2231,  0.4976, -0.0028, -0.4960, -0.3880,  0.3877,\n",
      "         -0.3430,  0.3044,  0.0261,  0.3276, -0.1741, -0.0334,  0.0502,  0.0558,\n",
      "         -0.2254,  0.2467,  0.3906, -0.3988, -0.2671,  0.3635, -0.2941, -0.4413,\n",
      "         -0.2717, -0.1500,  0.1175, -0.0211, -0.1650,  0.1372, -0.1467, -0.4682],\n",
      "        [ 0.1006,  0.1690, -0.2365, -0.2573, -0.3941, -0.1949,  0.1145,  0.4879,\n",
      "          0.3939, -0.4653,  0.3927, -0.3928, -0.1648, -0.3126, -0.2240, -0.2955,\n",
      "          0.4953,  0.0201, -0.4571, -0.3099,  0.1722, -0.3074, -0.2650, -0.0343,\n",
      "          0.2155,  0.1219,  0.4715,  0.1549,  0.0180, -0.3136, -0.1017,  0.0442,\n",
      "         -0.0807,  0.3326, -0.2793,  0.4693,  0.2938,  0.2537,  0.4737, -0.4222,\n",
      "         -0.1041,  0.0470, -0.1856,  0.1775, -0.1044,  0.3396,  0.3710,  0.1461,\n",
      "          0.3862,  0.0915,  0.0245, -0.4862,  0.2971, -0.4279, -0.0600, -0.3252,\n",
      "          0.0575,  0.0737, -0.1541,  0.0132, -0.3831,  0.1708,  0.1641,  0.3259,\n",
      "          0.2080, -0.2994,  0.3289, -0.1726,  0.1728, -0.4535,  0.2559,  0.0939,\n",
      "          0.1923, -0.4447,  0.4706,  0.1378,  0.3362,  0.3585, -0.4693,  0.0737,\n",
      "         -0.0180, -0.1533,  0.1602,  0.0328,  0.3808,  0.0183, -0.4225,  0.2609,\n",
      "         -0.2920, -0.3166,  0.0291,  0.4230,  0.3254, -0.1631,  0.0777,  0.0084,\n",
      "          0.3930,  0.0684,  0.3393, -0.4065,  0.0184,  0.1434, -0.4191, -0.2668,\n",
      "         -0.2583, -0.3938,  0.2692,  0.3379, -0.1610, -0.4425, -0.3757, -0.3587,\n",
      "         -0.4249,  0.2199,  0.3004,  0.0443,  0.3836,  0.0263,  0.2958, -0.1507,\n",
      "          0.0537, -0.0687,  0.1514, -0.4911,  0.0052,  0.0761, -0.1913,  0.1853,\n",
      "         -0.0124, -0.3541,  0.0277, -0.3630, -0.3104, -0.1769, -0.1923,  0.1557,\n",
      "          0.4131,  0.2811,  0.3978,  0.4198,  0.4174,  0.1364, -0.3672,  0.0243,\n",
      "         -0.0475, -0.2074, -0.3462,  0.3039,  0.0092, -0.2067,  0.4334, -0.2698,\n",
      "         -0.4175,  0.4882,  0.1428,  0.1868, -0.2379, -0.2610,  0.2044,  0.1492,\n",
      "         -0.4313, -0.2646,  0.0871, -0.2397,  0.1344,  0.3381, -0.0484,  0.2644,\n",
      "         -0.2736, -0.4856, -0.1567,  0.0605,  0.1655,  0.1458,  0.1922, -0.2275,\n",
      "          0.3972, -0.1485, -0.0114,  0.1299,  0.2137,  0.2654,  0.2137, -0.3936,\n",
      "         -0.4087,  0.1286,  0.3506, -0.1719, -0.0443, -0.4216, -0.0135,  0.2737]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([4, 128])\n",
      "tensor([[ 0.3301,  0.3833,  0.4804,  0.3799, -0.3784,  0.0511,  0.3683,  0.1008,\n",
      "         -0.2285, -0.0197,  0.0348,  0.1120, -0.3621, -0.2547,  0.4441, -0.4699,\n",
      "         -0.3608, -0.4284,  0.1076,  0.4837, -0.0746,  0.1340, -0.4201, -0.0809,\n",
      "          0.1141, -0.1077,  0.0644,  0.3932,  0.0016,  0.0923, -0.3204, -0.1327,\n",
      "         -0.3313, -0.1889, -0.1339,  0.0845,  0.1315, -0.0367, -0.3632, -0.3186,\n",
      "         -0.0542, -0.2293, -0.4923, -0.4534, -0.3555, -0.4473, -0.2461,  0.3808,\n",
      "         -0.2491, -0.0109, -0.3062,  0.1362, -0.3017,  0.0435,  0.1770,  0.4863,\n",
      "         -0.2845, -0.0166,  0.4380,  0.1111,  0.3862,  0.1065,  0.2000, -0.2471,\n",
      "         -0.0877, -0.4372,  0.4612,  0.0399,  0.2884,  0.0085, -0.3506,  0.0517,\n",
      "          0.2164, -0.1687,  0.3631,  0.2543, -0.0103,  0.4510,  0.3025, -0.0815,\n",
      "         -0.0690,  0.4011, -0.0612, -0.3017, -0.4315, -0.0644, -0.0030,  0.1530,\n",
      "         -0.3891, -0.2778, -0.2134, -0.0432,  0.4629, -0.0109, -0.0543,  0.3298,\n",
      "          0.1822, -0.2275, -0.2900, -0.0932, -0.0859,  0.1376,  0.4553,  0.2669,\n",
      "         -0.1041, -0.4566,  0.4032,  0.4596,  0.0240,  0.2582,  0.4123,  0.0547,\n",
      "          0.2229,  0.0054, -0.2271,  0.2998,  0.3078,  0.1199,  0.0656, -0.4977,\n",
      "         -0.1922, -0.1638, -0.0580, -0.1722, -0.0207, -0.2017, -0.4377, -0.2301],\n",
      "        [-0.3085,  0.2079, -0.0558,  0.4727, -0.0572,  0.4325, -0.4721, -0.3933,\n",
      "         -0.4037,  0.2223, -0.3074, -0.4287,  0.4854, -0.1563,  0.2378,  0.3082,\n",
      "          0.2669, -0.4955, -0.4475,  0.1592, -0.2937,  0.1021, -0.2588,  0.0658,\n",
      "          0.1464,  0.0969, -0.4747, -0.1736, -0.2305,  0.3631,  0.2602,  0.4455,\n",
      "         -0.4695, -0.4149,  0.2928, -0.2088, -0.2163,  0.2472,  0.2846, -0.3211,\n",
      "         -0.2504,  0.3590, -0.4542, -0.4578,  0.0834, -0.2651, -0.3716,  0.0151,\n",
      "         -0.0984,  0.0979,  0.0320,  0.0527, -0.4798, -0.0049,  0.3435, -0.2685,\n",
      "          0.0051, -0.3738, -0.0213,  0.3898,  0.4789,  0.2627, -0.2252,  0.1004,\n",
      "         -0.4895, -0.0907, -0.1520, -0.2988,  0.0696,  0.3762, -0.0568,  0.3029,\n",
      "          0.2704,  0.1733,  0.0607, -0.3022,  0.0958,  0.2246,  0.0864,  0.1957,\n",
      "         -0.1262,  0.0189, -0.3558,  0.3480, -0.4218,  0.3846,  0.2977, -0.2668,\n",
      "         -0.4719, -0.3136,  0.0346, -0.2221, -0.0843, -0.1978,  0.3260, -0.1030,\n",
      "          0.2973,  0.2319, -0.3665,  0.3388, -0.4193, -0.3237,  0.3003,  0.1727,\n",
      "          0.1312,  0.3687,  0.3775, -0.1518, -0.0341, -0.4558,  0.4252,  0.1452,\n",
      "          0.4464, -0.0587,  0.2010,  0.3056,  0.1037, -0.2781,  0.1119,  0.4075,\n",
      "          0.3024,  0.4253,  0.3235, -0.2391, -0.1235, -0.2415,  0.3376,  0.0750],\n",
      "        [-0.4529, -0.1572, -0.0539,  0.3450,  0.4261,  0.2598,  0.2026, -0.0352,\n",
      "          0.2761, -0.3891,  0.3350, -0.2972, -0.0346, -0.0281, -0.1072,  0.4468,\n",
      "          0.0352,  0.2435, -0.1058,  0.3529,  0.0723,  0.2023,  0.3687, -0.4046,\n",
      "         -0.4893, -0.1057,  0.0078, -0.4062,  0.2501,  0.4469,  0.2588,  0.3636,\n",
      "         -0.4504, -0.4279, -0.2497, -0.4591, -0.4598, -0.1506,  0.0301,  0.1856,\n",
      "         -0.3996,  0.0477, -0.2218,  0.4408,  0.3175, -0.2819, -0.3891, -0.2715,\n",
      "         -0.0119, -0.1215,  0.1585,  0.4206, -0.1848,  0.1778,  0.4221, -0.1031,\n",
      "          0.1428,  0.2237, -0.3037, -0.0875,  0.3658,  0.3300,  0.1022, -0.1516,\n",
      "         -0.4768,  0.0470,  0.2139,  0.3245, -0.2838, -0.2326,  0.3793, -0.4275,\n",
      "          0.0624, -0.1127,  0.3912, -0.2025, -0.1955,  0.0486, -0.3604,  0.1436,\n",
      "         -0.2875, -0.3124, -0.1595, -0.0667, -0.0748, -0.3298, -0.3057,  0.2011,\n",
      "          0.1131, -0.0505, -0.2193,  0.2211,  0.3423, -0.1574, -0.4652,  0.2528,\n",
      "          0.1820,  0.1900, -0.2276, -0.0111, -0.4589, -0.4369,  0.2751,  0.2070,\n",
      "          0.2440, -0.3492,  0.1704, -0.0839, -0.4068,  0.1689, -0.2187, -0.2404,\n",
      "         -0.4594,  0.1789, -0.4587, -0.1004,  0.4327,  0.1572,  0.1019,  0.1318,\n",
      "          0.3062,  0.3306,  0.1911,  0.4272,  0.0892,  0.2553,  0.4253,  0.1659],\n",
      "        [ 0.1006,  0.1690, -0.2365, -0.2573, -0.3941, -0.1949,  0.1145,  0.4879,\n",
      "          0.3939, -0.4653,  0.3927, -0.3928, -0.1648, -0.3126, -0.2240, -0.2955,\n",
      "          0.4953,  0.0201, -0.4571, -0.3099,  0.1722, -0.3074, -0.2650, -0.0343,\n",
      "          0.2155,  0.1219,  0.4715,  0.1549,  0.0180, -0.3136, -0.1017,  0.0442,\n",
      "         -0.0807,  0.3326, -0.2793,  0.4693,  0.2938,  0.2537,  0.4737, -0.4222,\n",
      "         -0.1041,  0.0470, -0.1856,  0.1775, -0.1044,  0.3396,  0.3710,  0.1461,\n",
      "          0.3862,  0.0915,  0.0245, -0.4862,  0.2971, -0.4279, -0.0600, -0.3252,\n",
      "          0.0575,  0.0737, -0.1541,  0.0132, -0.3831,  0.1708,  0.1641,  0.3259,\n",
      "          0.2080, -0.2994,  0.3289, -0.1726,  0.1728, -0.4535,  0.2559,  0.0939,\n",
      "          0.1923, -0.4447,  0.4706,  0.1378,  0.3362,  0.3585, -0.4693,  0.0737,\n",
      "         -0.0180, -0.1533,  0.1602,  0.0328,  0.3808,  0.0183, -0.4225,  0.2609,\n",
      "         -0.2920, -0.3166,  0.0291,  0.4230,  0.3254, -0.1631,  0.0777,  0.0084,\n",
      "          0.3930,  0.0684,  0.3393, -0.4065,  0.0184,  0.1434, -0.4191, -0.2668,\n",
      "         -0.2583, -0.3938,  0.2692,  0.3379, -0.1610, -0.4425, -0.3757, -0.3587,\n",
      "         -0.4249,  0.2199,  0.3004,  0.0443,  0.3836,  0.0263,  0.2958, -0.1507,\n",
      "          0.0537, -0.0687,  0.1514, -0.4911,  0.0052,  0.0761, -0.1913,  0.1853]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([4, 32])\n",
      "tensor([[ 0.1247,  0.4458,  0.0637, -0.4035, -0.1888, -0.3908,  0.4974,  0.0636,\n",
      "         -0.3287,  0.3867, -0.3518, -0.0131,  0.0452, -0.4103, -0.0953,  0.3811,\n",
      "          0.1975,  0.4332,  0.1069,  0.0880, -0.3776,  0.1469, -0.3684,  0.2586,\n",
      "          0.0258,  0.2367, -0.4173, -0.4539,  0.0401,  0.3268,  0.4671,  0.3535],\n",
      "        [ 0.4636, -0.2067, -0.2724,  0.2813,  0.3083, -0.1645,  0.4072, -0.3367,\n",
      "         -0.4844,  0.4903, -0.2458,  0.3983,  0.4745,  0.1093,  0.3478, -0.3051,\n",
      "          0.2002, -0.0206,  0.2579, -0.2674, -0.3509,  0.4186, -0.1168,  0.3344,\n",
      "          0.4014, -0.1908,  0.1122, -0.2635, -0.2765, -0.4911,  0.3065,  0.2265],\n",
      "        [-0.0153, -0.3781,  0.4079,  0.0030,  0.4418, -0.4477,  0.2635, -0.3938,\n",
      "          0.0555, -0.3003, -0.2474, -0.3613,  0.0365,  0.0919,  0.0444, -0.0326,\n",
      "          0.1988, -0.0043,  0.4960, -0.2726,  0.2577, -0.1399,  0.0490, -0.2605,\n",
      "         -0.2334,  0.1966, -0.3514, -0.0269, -0.2524, -0.0522,  0.3142, -0.0619],\n",
      "        [-0.0124, -0.3541,  0.0277, -0.3630, -0.3104, -0.1769, -0.1923,  0.1557,\n",
      "          0.4131,  0.2811,  0.3978,  0.4198,  0.4174,  0.1364, -0.3672,  0.0243,\n",
      "         -0.0475, -0.2074, -0.3462,  0.3039,  0.0092, -0.2067,  0.4334, -0.2698,\n",
      "         -0.4175,  0.4882,  0.1428,  0.1868, -0.2379, -0.2610,  0.2044,  0.1492]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([4, 32])\n",
      "tensor([[ 0.3936,  0.4408,  0.4123,  0.1102, -0.4779,  0.2480,  0.2368, -0.1251,\n",
      "         -0.3720,  0.2231, -0.0660,  0.0564, -0.3418, -0.2669,  0.4652, -0.4518,\n",
      "         -0.3219,  0.2803, -0.4718, -0.3934, -0.3866, -0.2158,  0.4596,  0.4616,\n",
      "          0.3334,  0.4542, -0.2137, -0.0657, -0.2930,  0.2428,  0.0470, -0.3103],\n",
      "        [-0.4627, -0.0526, -0.4488,  0.0989, -0.0009,  0.3884,  0.4852, -0.1694,\n",
      "         -0.1860, -0.0708, -0.4281,  0.1182, -0.2934,  0.0983, -0.2847,  0.1333,\n",
      "          0.3494,  0.2996, -0.4393,  0.2396,  0.0914, -0.1308,  0.4602,  0.1519,\n",
      "          0.0964,  0.2173, -0.3352,  0.2081, -0.3686,  0.4634,  0.1625,  0.1153],\n",
      "        [-0.2309,  0.3951, -0.2231,  0.4976, -0.0028, -0.4960, -0.3880,  0.3877,\n",
      "         -0.3430,  0.3044,  0.0261,  0.3276, -0.1741, -0.0334,  0.0502,  0.0558,\n",
      "         -0.2254,  0.2467,  0.3906, -0.3988, -0.2671,  0.3635, -0.2941, -0.4413,\n",
      "         -0.2717, -0.1500,  0.1175, -0.0211, -0.1650,  0.1372, -0.1467, -0.4682],\n",
      "        [-0.4313, -0.2646,  0.0871, -0.2397,  0.1344,  0.3381, -0.0484,  0.2644,\n",
      "         -0.2736, -0.4856, -0.1567,  0.0605,  0.1655,  0.1458,  0.1922, -0.2275,\n",
      "          0.3972, -0.1485, -0.0114,  0.1299,  0.2137,  0.2654,  0.2137, -0.3936,\n",
      "         -0.4087,  0.1286,  0.3506, -0.1719, -0.0443, -0.4216, -0.0135,  0.2737]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([2, 64])\n",
      "tensor([[ 0.3301,  0.3833,  0.4804,  0.3799, -0.3784,  0.0511,  0.3683,  0.1008,\n",
      "         -0.2285, -0.0197,  0.0348,  0.1120, -0.3621, -0.2547,  0.4441, -0.4699,\n",
      "         -0.3313, -0.1889, -0.1339,  0.0845,  0.1315, -0.0367, -0.3632, -0.3186,\n",
      "         -0.0542, -0.2293, -0.4923, -0.4534, -0.3555, -0.4473, -0.2461,  0.3808,\n",
      "         -0.0877, -0.4372,  0.4612,  0.0399,  0.2884,  0.0085, -0.3506,  0.0517,\n",
      "          0.2164, -0.1687,  0.3631,  0.2543, -0.0103,  0.4510,  0.3025, -0.0815,\n",
      "          0.1822, -0.2275, -0.2900, -0.0932, -0.0859,  0.1376,  0.4553,  0.2669,\n",
      "         -0.1041, -0.4566,  0.4032,  0.4596,  0.0240,  0.2582,  0.4123,  0.0547],\n",
      "        [-0.3085,  0.2079, -0.0558,  0.4727, -0.0572,  0.4325, -0.4721, -0.3933,\n",
      "         -0.4037,  0.2223, -0.3074, -0.4287,  0.4854, -0.1563,  0.2378,  0.3082,\n",
      "         -0.4695, -0.4149,  0.2928, -0.2088, -0.2163,  0.2472,  0.2846, -0.3211,\n",
      "         -0.2504,  0.3590, -0.4542, -0.4578,  0.0834, -0.2651, -0.3716,  0.0151,\n",
      "         -0.4895, -0.0907, -0.1520, -0.2988,  0.0696,  0.3762, -0.0568,  0.3029,\n",
      "          0.2704,  0.1733,  0.0607, -0.3022,  0.0958,  0.2246,  0.0864,  0.1957,\n",
      "          0.2973,  0.2319, -0.3665,  0.3388, -0.4193, -0.3237,  0.3003,  0.1727,\n",
      "          0.1312,  0.3687,  0.3775, -0.1518, -0.0341, -0.4558,  0.4252,  0.1452]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([2, 16])\n",
      "tensor([[ 0.1247,  0.4458,  0.0637, -0.4035, -0.1888, -0.3908,  0.4974,  0.0636,\n",
      "         -0.3287,  0.3867, -0.3518, -0.0131,  0.0452, -0.4103, -0.0953,  0.3811],\n",
      "        [ 0.4636, -0.2067, -0.2724,  0.2813,  0.3083, -0.1645,  0.4072, -0.3367,\n",
      "         -0.4844,  0.4903, -0.2458,  0.3983,  0.4745,  0.1093,  0.3478, -0.3051]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([2, 16])\n",
      "tensor([[ 0.3936,  0.4408,  0.4123,  0.1102, -0.4779,  0.2480,  0.2368, -0.1251,\n",
      "         -0.3720,  0.2231, -0.0660,  0.0564, -0.3418, -0.2669,  0.4652, -0.4518],\n",
      "        [-0.4627, -0.0526, -0.4488,  0.0989, -0.0009,  0.3884,  0.4852, -0.1694,\n",
      "         -0.1860, -0.0708, -0.4281,  0.1182, -0.2934,  0.0983, -0.2847,  0.1333]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([2, 96])\n",
      "tensor([[ 0.3301,  0.3833,  0.4804,  0.3799, -0.3784,  0.0511,  0.3683,  0.1008,\n",
      "         -0.2285, -0.0197,  0.0348,  0.1120, -0.3621, -0.2547,  0.4441, -0.4699,\n",
      "         -0.3313, -0.1889, -0.1339,  0.0845,  0.1315, -0.0367, -0.3632, -0.3186,\n",
      "         -0.0542, -0.2293, -0.4923, -0.4534, -0.3555, -0.4473, -0.2461,  0.3808,\n",
      "         -0.0877, -0.4372,  0.4612,  0.0399,  0.2884,  0.0085, -0.3506,  0.0517,\n",
      "          0.2164, -0.1687,  0.3631,  0.2543, -0.0103,  0.4510,  0.3025, -0.0815,\n",
      "          0.1822, -0.2275, -0.2900, -0.0932, -0.0859,  0.1376,  0.4553,  0.2669,\n",
      "         -0.1041, -0.4566,  0.4032,  0.4596,  0.0240,  0.2582,  0.4123,  0.0547,\n",
      "          0.1247,  0.4458,  0.0637, -0.4035, -0.1888, -0.3908,  0.4974,  0.0636,\n",
      "         -0.3287,  0.3867, -0.3518, -0.0131,  0.0452, -0.4103, -0.0953,  0.3811,\n",
      "          0.3936,  0.4408,  0.4123,  0.1102, -0.4779,  0.2480,  0.2368, -0.1251,\n",
      "         -0.3720,  0.2231, -0.0660,  0.0564, -0.3418, -0.2669,  0.4652, -0.4518],\n",
      "        [-0.3085,  0.2079, -0.0558,  0.4727, -0.0572,  0.4325, -0.4721, -0.3933,\n",
      "         -0.4037,  0.2223, -0.3074, -0.4287,  0.4854, -0.1563,  0.2378,  0.3082,\n",
      "         -0.4695, -0.4149,  0.2928, -0.2088, -0.2163,  0.2472,  0.2846, -0.3211,\n",
      "         -0.2504,  0.3590, -0.4542, -0.4578,  0.0834, -0.2651, -0.3716,  0.0151,\n",
      "         -0.4895, -0.0907, -0.1520, -0.2988,  0.0696,  0.3762, -0.0568,  0.3029,\n",
      "          0.2704,  0.1733,  0.0607, -0.3022,  0.0958,  0.2246,  0.0864,  0.1957,\n",
      "          0.2973,  0.2319, -0.3665,  0.3388, -0.4193, -0.3237,  0.3003,  0.1727,\n",
      "          0.1312,  0.3687,  0.3775, -0.1518, -0.0341, -0.4558,  0.4252,  0.1452,\n",
      "          0.4636, -0.2067, -0.2724,  0.2813,  0.3083, -0.1645,  0.4072, -0.3367,\n",
      "         -0.4844,  0.4903, -0.2458,  0.3983,  0.4745,  0.1093,  0.3478, -0.3051,\n",
      "         -0.4627, -0.0526, -0.4488,  0.0989, -0.0009,  0.3884,  0.4852, -0.1694,\n",
      "         -0.1860, -0.0708, -0.4281,  0.1182, -0.2934,  0.0983, -0.2847,  0.1333]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 96])\n",
      "tensor([[[ 0.1202,  0.6112,  0.5025,  0.8279, -0.4801,  0.4187,  0.0274,\n",
      "          -0.2126, -0.5975,  0.1627, -0.2164, -0.2293, -0.0093, -0.4213,\n",
      "           0.7056, -0.2802, -0.7698, -0.5615,  0.0909, -0.0775, -0.0300,\n",
      "           0.1640, -0.1779, -0.6316, -0.2706,  0.0371, -0.9411, -0.8995,\n",
      "          -0.3367, -0.7320, -0.5909,  0.4477, -0.5081, -0.5752,  0.4004,\n",
      "          -0.2034,  0.3876,  0.3232, -0.4480,  0.3115,  0.4726, -0.0484,\n",
      "           0.4655,  0.0388,  0.0680,  0.7025,  0.4177,  0.0700,  0.4559,\n",
      "          -0.0668, -0.6368,  0.1758, -0.4475, -0.1125,  0.7705,  0.4488,\n",
      "          -0.0096, -0.2145,  0.7753,  0.3987, -0.0010, -0.0848,  0.8254,\n",
      "           0.1834,  0.5287,  0.3372, -0.1541, -0.2268,  0.0412, -0.5836,\n",
      "           0.9077, -0.2078, -0.7792,  0.8504, -0.6068,  0.3168,  0.4470,\n",
      "          -0.3777,  0.1809,  0.1813,  0.0642,  0.4599,  0.0972,  0.2083,\n",
      "          -0.5469,  0.6069,  0.6749, -0.2841, -0.5801,  0.1959, -0.4322,\n",
      "           0.1629, -0.6350, -0.2230,  0.2943, -0.4052],\n",
      "         [-0.6072, -0.0034, -0.3963,  0.3273,  0.1868,  0.5013, -0.8360,\n",
      "          -0.5562, -0.3448,  0.2890, -0.4047, -0.6077,  0.8482, -0.0203,\n",
      "          -0.0077,  0.7019, -0.3563, -0.3856,  0.4540, -0.3163, -0.3577,\n",
      "           0.3314,  0.6000, -0.1810, -0.2735,  0.6011, -0.2278, -0.2587,\n",
      "           0.3455, -0.0240, -0.2931, -0.2406, -0.5470,  0.1852, -0.5025,\n",
      "          -0.3975, -0.1101,  0.4605,  0.1683,  0.3403,  0.1878,  0.3296,\n",
      "          -0.1719, -0.5478,  0.1258, -0.0286, -0.0989,  0.2981,  0.2444,\n",
      "           0.4423, -0.2568,  0.4833, -0.4613, -0.4950,  0.0623,  0.0323,\n",
      "           0.2335,  0.7679,  0.1935, -0.5010, -0.0586, -0.7408,  0.2463,\n",
      "           0.1427,  0.4897, -0.5597, -0.3811,  0.6234,  0.5107,  0.0622,\n",
      "           0.1661, -0.4606, -0.3767,  0.3445, -0.0652,  0.5026,  0.5575,\n",
      "           0.4148,  0.4960, -0.6377, -0.8415, -0.3653, -0.8370,  0.0476,\n",
      "           0.3243,  0.3126,  0.4402, -0.1247,  0.0227, -0.2397, -0.4857,\n",
      "           0.1081, -0.1310,  0.3035, -0.6697,  0.4728],\n",
      "         [-0.3081,  0.4182,  0.0905,  0.7801, -0.2100,  0.6106, -0.5191,\n",
      "          -0.5041, -0.6329,  0.2979, -0.4093, -0.5488,  0.5395, -0.3028,\n",
      "           0.4804,  0.2590, -0.7588, -0.6344,  0.3547, -0.2568, -0.2508,\n",
      "           0.3261,  0.2638, -0.5509, -0.3621,  0.4124, -0.7939, -0.7851,\n",
      "          -0.0092, -0.5189, -0.5950,  0.1531, -0.7016, -0.2764, -0.0480,\n",
      "          -0.3957,  0.1957,  0.5187, -0.1998,  0.4332,  0.4459,  0.1789,\n",
      "           0.2095, -0.3259,  0.1277,  0.4648,  0.2236,  0.2400,  0.4709,\n",
      "           0.2387, -0.6032,  0.4320, -0.6046, -0.3959,  0.5700,  0.3295,\n",
      "           0.1437,  0.3466,  0.6577, -0.0482, -0.0384, -0.5351,  0.7262,\n",
      "           0.2180,  0.6788, -0.1283, -0.3512,  0.2452,  0.3570, -0.3613,\n",
      "           0.7312, -0.4394, -0.7783,  0.8066, -0.4593,  0.5414,  0.6662,\n",
      "           0.0072,  0.4436, -0.2857, -0.4974,  0.0812, -0.4718,  0.1739,\n",
      "          -0.1674,  0.6186,  0.7475, -0.2757, -0.3844, -0.0195, -0.6098,\n",
      "           0.1816, -0.5211,  0.0419, -0.2285,  0.0256]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 64])\n",
      "tensor([[[ 0.1202,  0.6112,  0.5025,  0.8279, -0.4801,  0.4187,  0.0274,\n",
      "          -0.2126, -0.5975,  0.1627, -0.2164, -0.2293, -0.0093, -0.4213,\n",
      "           0.7056, -0.2802, -0.7698, -0.5615,  0.0909, -0.0775, -0.0300,\n",
      "           0.1640, -0.1779, -0.6316, -0.2706,  0.0371, -0.9411, -0.8995,\n",
      "          -0.3367, -0.7320, -0.5909,  0.4477, -0.5081, -0.5752,  0.4004,\n",
      "          -0.2034,  0.3876,  0.3232, -0.4480,  0.3115,  0.4726, -0.0484,\n",
      "           0.4655,  0.0388,  0.0680,  0.7025,  0.4177,  0.0700,  0.4559,\n",
      "          -0.0668, -0.6368,  0.1758, -0.4475, -0.1125,  0.7705,  0.4488,\n",
      "          -0.0096, -0.2145,  0.7753,  0.3987, -0.0010, -0.0848,  0.8254,\n",
      "           0.1834],\n",
      "         [-0.6072, -0.0034, -0.3963,  0.3273,  0.1868,  0.5013, -0.8360,\n",
      "          -0.5562, -0.3448,  0.2890, -0.4047, -0.6077,  0.8482, -0.0203,\n",
      "          -0.0077,  0.7019, -0.3563, -0.3856,  0.4540, -0.3163, -0.3577,\n",
      "           0.3314,  0.6000, -0.1810, -0.2735,  0.6011, -0.2278, -0.2587,\n",
      "           0.3455, -0.0240, -0.2931, -0.2406, -0.5470,  0.1852, -0.5025,\n",
      "          -0.3975, -0.1101,  0.4605,  0.1683,  0.3403,  0.1878,  0.3296,\n",
      "          -0.1719, -0.5478,  0.1258, -0.0286, -0.0989,  0.2981,  0.2444,\n",
      "           0.4423, -0.2568,  0.4833, -0.4613, -0.4950,  0.0623,  0.0323,\n",
      "           0.2335,  0.7679,  0.1935, -0.5010, -0.0586, -0.7408,  0.2463,\n",
      "           0.1427],\n",
      "         [-0.3081,  0.4182,  0.0905,  0.7801, -0.2100,  0.6106, -0.5191,\n",
      "          -0.5041, -0.6329,  0.2979, -0.4093, -0.5488,  0.5395, -0.3028,\n",
      "           0.4804,  0.2590, -0.7588, -0.6344,  0.3547, -0.2568, -0.2508,\n",
      "           0.3261,  0.2638, -0.5509, -0.3621,  0.4124, -0.7939, -0.7851,\n",
      "          -0.0092, -0.5189, -0.5950,  0.1531, -0.7016, -0.2764, -0.0480,\n",
      "          -0.3957,  0.1957,  0.5187, -0.1998,  0.4332,  0.4459,  0.1789,\n",
      "           0.2095, -0.3259,  0.1277,  0.4648,  0.2236,  0.2400,  0.4709,\n",
      "           0.2387, -0.6032,  0.4320, -0.6046, -0.3959,  0.5700,  0.3295,\n",
      "           0.1437,  0.3466,  0.6577, -0.0482, -0.0384, -0.5351,  0.7262,\n",
      "           0.2180]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.5287,  0.3372, -0.1541, -0.2268,  0.0412, -0.5836,  0.9077,\n",
      "          -0.2078, -0.7792,  0.8504, -0.6068,  0.3168,  0.4470, -0.3777,\n",
      "           0.1809,  0.1813],\n",
      "         [ 0.4897, -0.5597, -0.3811,  0.6234,  0.5107,  0.0622,  0.1661,\n",
      "          -0.4606, -0.3767,  0.3445, -0.0652,  0.5026,  0.5575,  0.4148,\n",
      "           0.4960, -0.6377],\n",
      "         [ 0.6788, -0.1283, -0.3512,  0.2452,  0.3570, -0.3613,  0.7312,\n",
      "          -0.4394, -0.7783,  0.8066, -0.4593,  0.5414,  0.6662,  0.0072,\n",
      "           0.4436, -0.2857]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 16])\n",
      "tensor([[[ 0.0642,  0.4599,  0.0972,  0.2083, -0.5469,  0.6069,  0.6749,\n",
      "          -0.2841, -0.5801,  0.1959, -0.4322,  0.1629, -0.6350, -0.2230,\n",
      "           0.2943, -0.4052],\n",
      "         [-0.8415, -0.3653, -0.8370,  0.0476,  0.3243,  0.3126,  0.4402,\n",
      "          -0.1247,  0.0227, -0.2397, -0.4857,  0.1081, -0.1310,  0.3035,\n",
      "          -0.6697,  0.4728],\n",
      "         [-0.4974,  0.0812, -0.4718,  0.1739, -0.1674,  0.6186,  0.7475,\n",
      "          -0.2757, -0.3844, -0.0195, -0.6098,  0.1816, -0.5211,  0.0419,\n",
      "          -0.2285,  0.0256]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[ 0.1202,  0.6112,  0.5025,  0.8279, -0.4801,  0.4187,  0.0274,\n",
      "           -0.2126, -0.5975,  0.1627, -0.2164, -0.2293, -0.0093, -0.4213,\n",
      "            0.7056, -0.2802],\n",
      "          [-0.7698, -0.5615,  0.0909, -0.0775, -0.0300,  0.1640, -0.1779,\n",
      "           -0.6316, -0.2706,  0.0371, -0.9411, -0.8995, -0.3367, -0.7320,\n",
      "           -0.5909,  0.4477],\n",
      "          [-0.5081, -0.5752,  0.4004, -0.2034,  0.3876,  0.3232, -0.4480,\n",
      "            0.3115,  0.4726, -0.0484,  0.4655,  0.0388,  0.0680,  0.7025,\n",
      "            0.4177,  0.0700],\n",
      "          [ 0.4559, -0.0668, -0.6368,  0.1758, -0.4475, -0.1125,  0.7705,\n",
      "            0.4488, -0.0096, -0.2145,  0.7753,  0.3987, -0.0010, -0.0848,\n",
      "            0.8254,  0.1834]],\n",
      "\n",
      "         [[-0.6072, -0.0034, -0.3963,  0.3273,  0.1868,  0.5013, -0.8360,\n",
      "           -0.5562, -0.3448,  0.2890, -0.4047, -0.6077,  0.8482, -0.0203,\n",
      "           -0.0077,  0.7019],\n",
      "          [-0.3563, -0.3856,  0.4540, -0.3163, -0.3577,  0.3314,  0.6000,\n",
      "           -0.1810, -0.2735,  0.6011, -0.2278, -0.2587,  0.3455, -0.0240,\n",
      "           -0.2931, -0.2406],\n",
      "          [-0.5470,  0.1852, -0.5025, -0.3975, -0.1101,  0.4605,  0.1683,\n",
      "            0.3403,  0.1878,  0.3296, -0.1719, -0.5478,  0.1258, -0.0286,\n",
      "           -0.0989,  0.2981],\n",
      "          [ 0.2444,  0.4423, -0.2568,  0.4833, -0.4613, -0.4950,  0.0623,\n",
      "            0.0323,  0.2335,  0.7679,  0.1935, -0.5010, -0.0586, -0.7408,\n",
      "            0.2463,  0.1427]],\n",
      "\n",
      "         [[-0.3081,  0.4182,  0.0905,  0.7801, -0.2100,  0.6106, -0.5191,\n",
      "           -0.5041, -0.6329,  0.2979, -0.4093, -0.5488,  0.5395, -0.3028,\n",
      "            0.4804,  0.2590],\n",
      "          [-0.7588, -0.6344,  0.3547, -0.2568, -0.2508,  0.3261,  0.2638,\n",
      "           -0.5509, -0.3621,  0.4124, -0.7939, -0.7851, -0.0092, -0.5189,\n",
      "           -0.5950,  0.1531],\n",
      "          [-0.7016, -0.2764, -0.0480, -0.3957,  0.1957,  0.5187, -0.1998,\n",
      "            0.4332,  0.4459,  0.1789,  0.2095, -0.3259,  0.1277,  0.4648,\n",
      "            0.2236,  0.2400],\n",
      "          [ 0.4709,  0.2387, -0.6032,  0.4320, -0.6046, -0.3959,  0.5700,\n",
      "            0.3295,  0.1437,  0.3466,  0.6577, -0.0482, -0.0384, -0.5351,\n",
      "            0.7262,  0.2180]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 16])\n",
      "tensor([[[[ 0.5287,  0.3372, -0.1541, -0.2268,  0.0412, -0.5836,  0.9077,\n",
      "           -0.2078, -0.7792,  0.8504, -0.6068,  0.3168,  0.4470, -0.3777,\n",
      "            0.1809,  0.1813]],\n",
      "\n",
      "         [[ 0.4897, -0.5597, -0.3811,  0.6234,  0.5107,  0.0622,  0.1661,\n",
      "           -0.4606, -0.3767,  0.3445, -0.0652,  0.5026,  0.5575,  0.4148,\n",
      "            0.4960, -0.6377]],\n",
      "\n",
      "         [[ 0.6788, -0.1283, -0.3512,  0.2452,  0.3570, -0.3613,  0.7312,\n",
      "           -0.4394, -0.7783,  0.8066, -0.4593,  0.5414,  0.6662,  0.0072,\n",
      "            0.4436, -0.2857]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 16])\n",
      "tensor([[[[ 0.0642,  0.4599,  0.0972,  0.2083, -0.5469,  0.6069,  0.6749,\n",
      "           -0.2841, -0.5801,  0.1959, -0.4322,  0.1629, -0.6350, -0.2230,\n",
      "            0.2943, -0.4052]],\n",
      "\n",
      "         [[-0.8415, -0.3653, -0.8370,  0.0476,  0.3243,  0.3126,  0.4402,\n",
      "           -0.1247,  0.0227, -0.2397, -0.4857,  0.1081, -0.1310,  0.3035,\n",
      "           -0.6697,  0.4728]],\n",
      "\n",
      "         [[-0.4974,  0.0812, -0.4718,  0.1739, -0.1674,  0.6186,  0.7475,\n",
      "           -0.2757, -0.3844, -0.0195, -0.6098,  0.1816, -0.5211,  0.0419,\n",
      "           -0.2285,  0.0256]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[ 0.1202,  0.6112,  0.5025,  0.8279, -0.4801,  0.4187,  0.0274,\n",
      "           -0.2126, -0.5975,  0.1627, -0.2164, -0.2293, -0.0093, -0.4213,\n",
      "            0.7056, -0.2802],\n",
      "          [-0.7698, -0.5615,  0.0909, -0.0775, -0.0300,  0.1640, -0.1779,\n",
      "           -0.6316, -0.2706,  0.0371, -0.9411, -0.8995, -0.3367, -0.7320,\n",
      "           -0.5909,  0.4477],\n",
      "          [-0.5081, -0.5752,  0.4004, -0.2034,  0.3876,  0.3232, -0.4480,\n",
      "            0.3115,  0.4726, -0.0484,  0.4655,  0.0388,  0.0680,  0.7025,\n",
      "            0.4177,  0.0700],\n",
      "          [ 0.4559, -0.0668, -0.6368,  0.1758, -0.4475, -0.1125,  0.7705,\n",
      "            0.4488, -0.0096, -0.2145,  0.7753,  0.3987, -0.0010, -0.0848,\n",
      "            0.8254,  0.1834]],\n",
      "\n",
      "         [[-0.0379, -0.1569, -0.2508,  0.4297,  0.1012,  0.5016, -0.8353,\n",
      "           -0.5686, -0.6972,  0.2427, -0.5079, -0.5402,  0.8626,  0.0079,\n",
      "           -0.0341,  0.6919],\n",
      "          [ 0.0376, -0.6467,  0.5024, -0.2656, -0.3904,  0.3322,  0.6090,\n",
      "           -0.1767, -0.4476,  0.3029, -0.0753, -0.3106,  0.3081, -0.0053,\n",
      "           -0.2740, -0.2438],\n",
      "          [-0.4536, -0.0191, -0.4241, -0.2943, -0.1221,  0.4614,  0.1713,\n",
      "            0.3350, -0.3588,  0.3776, -0.3197, -0.6094,  0.1142, -0.0027,\n",
      "           -0.0936,  0.3041],\n",
      "          [-0.0644, -0.0352, -0.3042,  0.5643, -0.4531, -0.4526,  0.0545,\n",
      "            0.0298,  0.3318,  0.8854,  0.1040, -0.4076, -0.1044, -0.7675,\n",
      "            0.2482,  0.1433]],\n",
      "\n",
      "         [[ 0.7037, -0.0883,  0.3150,  0.9224, -0.3130,  0.6407, -0.5485,\n",
      "           -0.5130, -0.0168,  0.5058, -0.2766, -0.2428,  0.4870, -0.2324,\n",
      "            0.4466,  0.2409],\n",
      "          [ 0.6450, -0.6457,  0.7554,  0.0326, -0.2440,  0.3822,  0.3009,\n",
      "           -0.5560, -0.5393, -0.3944, -0.4306, -0.8254, -0.0589, -0.4790,\n",
      "           -0.5772,  0.1334],\n",
      "          [-0.1135, -0.2806, -0.1626, -0.2574,  0.1664,  0.4632, -0.2136,\n",
      "            0.4244, -0.8235, -0.1722,  0.1406, -0.4432,  0.1640,  0.5200,\n",
      "            0.2105,  0.2552],\n",
      "          [-0.3266, -0.2097, -0.8753,  0.4217, -0.5849, -0.3334,  0.5230,\n",
      "            0.3216,  0.3683,  0.3649,  0.1739,  0.1052, -0.1578, -0.5761,\n",
      "            0.7608,  0.2296]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 16])\n",
      "tensor([[[[ 0.5287,  0.3372, -0.1541, -0.2268,  0.0412, -0.5836,  0.9077,\n",
      "           -0.2078, -0.7792,  0.8504, -0.6068,  0.3168,  0.4470, -0.3777,\n",
      "            0.1809,  0.1813]],\n",
      "\n",
      "         [[ 0.5815, -0.6572, -0.3419,  0.5247,  0.4525,  0.0388,  0.1503,\n",
      "           -0.4492,  0.2086, -0.0070, -0.1805,  0.6049,  0.6057,  0.4177,\n",
      "            0.5010, -0.6458]],\n",
      "\n",
      "         [[ 0.4252, -0.7830, -0.0118,  0.0414,  0.2175, -0.3599,  0.7017,\n",
      "           -0.4289,  0.9411,  0.2323, -0.5781,  0.5929,  0.7239, -0.0334,\n",
      "            0.4889, -0.3011]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[ 0.5287,  0.3372, -0.1541, -0.2268,  0.0412, -0.5836,  0.9077,\n",
      "           -0.2078, -0.7792,  0.8504, -0.6068,  0.3168,  0.4470, -0.3777,\n",
      "            0.1809,  0.1813],\n",
      "          [ 0.5287,  0.3372, -0.1541, -0.2268,  0.0412, -0.5836,  0.9077,\n",
      "           -0.2078, -0.7792,  0.8504, -0.6068,  0.3168,  0.4470, -0.3777,\n",
      "            0.1809,  0.1813],\n",
      "          [ 0.5287,  0.3372, -0.1541, -0.2268,  0.0412, -0.5836,  0.9077,\n",
      "           -0.2078, -0.7792,  0.8504, -0.6068,  0.3168,  0.4470, -0.3777,\n",
      "            0.1809,  0.1813],\n",
      "          [ 0.5287,  0.3372, -0.1541, -0.2268,  0.0412, -0.5836,  0.9077,\n",
      "           -0.2078, -0.7792,  0.8504, -0.6068,  0.3168,  0.4470, -0.3777,\n",
      "            0.1809,  0.1813]],\n",
      "\n",
      "         [[ 0.5815, -0.6572, -0.3419,  0.5247,  0.4525,  0.0388,  0.1503,\n",
      "           -0.4492,  0.2086, -0.0070, -0.1805,  0.6049,  0.6057,  0.4177,\n",
      "            0.5010, -0.6458],\n",
      "          [ 0.5815, -0.6572, -0.3419,  0.5247,  0.4525,  0.0388,  0.1503,\n",
      "           -0.4492,  0.2086, -0.0070, -0.1805,  0.6049,  0.6057,  0.4177,\n",
      "            0.5010, -0.6458],\n",
      "          [ 0.5815, -0.6572, -0.3419,  0.5247,  0.4525,  0.0388,  0.1503,\n",
      "           -0.4492,  0.2086, -0.0070, -0.1805,  0.6049,  0.6057,  0.4177,\n",
      "            0.5010, -0.6458],\n",
      "          [ 0.5815, -0.6572, -0.3419,  0.5247,  0.4525,  0.0388,  0.1503,\n",
      "           -0.4492,  0.2086, -0.0070, -0.1805,  0.6049,  0.6057,  0.4177,\n",
      "            0.5010, -0.6458]],\n",
      "\n",
      "         [[ 0.4252, -0.7830, -0.0118,  0.0414,  0.2175, -0.3599,  0.7017,\n",
      "           -0.4289,  0.9411,  0.2323, -0.5781,  0.5929,  0.7239, -0.0334,\n",
      "            0.4889, -0.3011],\n",
      "          [ 0.4252, -0.7830, -0.0118,  0.0414,  0.2175, -0.3599,  0.7017,\n",
      "           -0.4289,  0.9411,  0.2323, -0.5781,  0.5929,  0.7239, -0.0334,\n",
      "            0.4889, -0.3011],\n",
      "          [ 0.4252, -0.7830, -0.0118,  0.0414,  0.2175, -0.3599,  0.7017,\n",
      "           -0.4289,  0.9411,  0.2323, -0.5781,  0.5929,  0.7239, -0.0334,\n",
      "            0.4889, -0.3011],\n",
      "          [ 0.4252, -0.7830, -0.0118,  0.0414,  0.2175, -0.3599,  0.7017,\n",
      "           -0.4289,  0.9411,  0.2323, -0.5781,  0.5929,  0.7239, -0.0334,\n",
      "            0.4889, -0.3011]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[ 0.0642,  0.4599,  0.0972,  0.2083, -0.5469,  0.6069,  0.6749,\n",
      "           -0.2841, -0.5801,  0.1959, -0.4322,  0.1629, -0.6350, -0.2230,\n",
      "            0.2943, -0.4052],\n",
      "          [ 0.0642,  0.4599,  0.0972,  0.2083, -0.5469,  0.6069,  0.6749,\n",
      "           -0.2841, -0.5801,  0.1959, -0.4322,  0.1629, -0.6350, -0.2230,\n",
      "            0.2943, -0.4052],\n",
      "          [ 0.0642,  0.4599,  0.0972,  0.2083, -0.5469,  0.6069,  0.6749,\n",
      "           -0.2841, -0.5801,  0.1959, -0.4322,  0.1629, -0.6350, -0.2230,\n",
      "            0.2943, -0.4052],\n",
      "          [ 0.0642,  0.4599,  0.0972,  0.2083, -0.5469,  0.6069,  0.6749,\n",
      "           -0.2841, -0.5801,  0.1959, -0.4322,  0.1629, -0.6350, -0.2230,\n",
      "            0.2943, -0.4052]],\n",
      "\n",
      "         [[-0.8415, -0.3653, -0.8370,  0.0476,  0.3243,  0.3126,  0.4402,\n",
      "           -0.1247,  0.0227, -0.2397, -0.4857,  0.1081, -0.1310,  0.3035,\n",
      "           -0.6697,  0.4728],\n",
      "          [-0.8415, -0.3653, -0.8370,  0.0476,  0.3243,  0.3126,  0.4402,\n",
      "           -0.1247,  0.0227, -0.2397, -0.4857,  0.1081, -0.1310,  0.3035,\n",
      "           -0.6697,  0.4728],\n",
      "          [-0.8415, -0.3653, -0.8370,  0.0476,  0.3243,  0.3126,  0.4402,\n",
      "           -0.1247,  0.0227, -0.2397, -0.4857,  0.1081, -0.1310,  0.3035,\n",
      "           -0.6697,  0.4728],\n",
      "          [-0.8415, -0.3653, -0.8370,  0.0476,  0.3243,  0.3126,  0.4402,\n",
      "           -0.1247,  0.0227, -0.2397, -0.4857,  0.1081, -0.1310,  0.3035,\n",
      "           -0.6697,  0.4728]],\n",
      "\n",
      "         [[-0.4974,  0.0812, -0.4718,  0.1739, -0.1674,  0.6186,  0.7475,\n",
      "           -0.2757, -0.3844, -0.0195, -0.6098,  0.1816, -0.5211,  0.0419,\n",
      "           -0.2285,  0.0256],\n",
      "          [-0.4974,  0.0812, -0.4718,  0.1739, -0.1674,  0.6186,  0.7475,\n",
      "           -0.2757, -0.3844, -0.0195, -0.6098,  0.1816, -0.5211,  0.0419,\n",
      "           -0.2285,  0.0256],\n",
      "          [-0.4974,  0.0812, -0.4718,  0.1739, -0.1674,  0.6186,  0.7475,\n",
      "           -0.2757, -0.3844, -0.0195, -0.6098,  0.1816, -0.5211,  0.0419,\n",
      "           -0.2285,  0.0256],\n",
      "          [-0.4974,  0.0812, -0.4718,  0.1739, -0.1674,  0.6186,  0.7475,\n",
      "           -0.2757, -0.3844, -0.0195, -0.6098,  0.1816, -0.5211,  0.0419,\n",
      "           -0.2285,  0.0256]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[ 0.1202,  0.6112,  0.5025,  0.8279, -0.4801,  0.4187,  0.0274,\n",
      "           -0.2126, -0.5975,  0.1627, -0.2164, -0.2293, -0.0093, -0.4213,\n",
      "            0.7056, -0.2802],\n",
      "          [-0.0379, -0.1569, -0.2508,  0.4297,  0.1012,  0.5016, -0.8353,\n",
      "           -0.5686, -0.6972,  0.2427, -0.5079, -0.5402,  0.8626,  0.0079,\n",
      "           -0.0341,  0.6919],\n",
      "          [ 0.7037, -0.0883,  0.3150,  0.9224, -0.3130,  0.6407, -0.5485,\n",
      "           -0.5130, -0.0168,  0.5058, -0.2766, -0.2428,  0.4870, -0.2324,\n",
      "            0.4466,  0.2409]],\n",
      "\n",
      "         [[-0.7698, -0.5615,  0.0909, -0.0775, -0.0300,  0.1640, -0.1779,\n",
      "           -0.6316, -0.2706,  0.0371, -0.9411, -0.8995, -0.3367, -0.7320,\n",
      "           -0.5909,  0.4477],\n",
      "          [ 0.0376, -0.6467,  0.5024, -0.2656, -0.3904,  0.3322,  0.6090,\n",
      "           -0.1767, -0.4476,  0.3029, -0.0753, -0.3106,  0.3081, -0.0053,\n",
      "           -0.2740, -0.2438],\n",
      "          [ 0.6450, -0.6457,  0.7554,  0.0326, -0.2440,  0.3822,  0.3009,\n",
      "           -0.5560, -0.5393, -0.3944, -0.4306, -0.8254, -0.0589, -0.4790,\n",
      "           -0.5772,  0.1334]],\n",
      "\n",
      "         [[-0.5081, -0.5752,  0.4004, -0.2034,  0.3876,  0.3232, -0.4480,\n",
      "            0.3115,  0.4726, -0.0484,  0.4655,  0.0388,  0.0680,  0.7025,\n",
      "            0.4177,  0.0700],\n",
      "          [-0.4536, -0.0191, -0.4241, -0.2943, -0.1221,  0.4614,  0.1713,\n",
      "            0.3350, -0.3588,  0.3776, -0.3197, -0.6094,  0.1142, -0.0027,\n",
      "           -0.0936,  0.3041],\n",
      "          [-0.1135, -0.2806, -0.1626, -0.2574,  0.1664,  0.4632, -0.2136,\n",
      "            0.4244, -0.8235, -0.1722,  0.1406, -0.4432,  0.1640,  0.5200,\n",
      "            0.2105,  0.2552]],\n",
      "\n",
      "         [[ 0.4559, -0.0668, -0.6368,  0.1758, -0.4475, -0.1125,  0.7705,\n",
      "            0.4488, -0.0096, -0.2145,  0.7753,  0.3987, -0.0010, -0.0848,\n",
      "            0.8254,  0.1834],\n",
      "          [-0.0644, -0.0352, -0.3042,  0.5643, -0.4531, -0.4526,  0.0545,\n",
      "            0.0298,  0.3318,  0.8854,  0.1040, -0.4076, -0.1044, -0.7675,\n",
      "            0.2482,  0.1433],\n",
      "          [-0.3266, -0.2097, -0.8753,  0.4217, -0.5849, -0.3334,  0.5230,\n",
      "            0.3216,  0.3683,  0.3649,  0.1739,  0.1052, -0.1578, -0.5761,\n",
      "            0.7608,  0.2296]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[ 0.5287,  0.3372, -0.1541, -0.2268,  0.0412, -0.5836,  0.9077,\n",
      "           -0.2078, -0.7792,  0.8504, -0.6068,  0.3168,  0.4470, -0.3777,\n",
      "            0.1809,  0.1813],\n",
      "          [ 0.5815, -0.6572, -0.3419,  0.5247,  0.4525,  0.0388,  0.1503,\n",
      "           -0.4492,  0.2086, -0.0070, -0.1805,  0.6049,  0.6057,  0.4177,\n",
      "            0.5010, -0.6458],\n",
      "          [ 0.4252, -0.7830, -0.0118,  0.0414,  0.2175, -0.3599,  0.7017,\n",
      "           -0.4289,  0.9411,  0.2323, -0.5781,  0.5929,  0.7239, -0.0334,\n",
      "            0.4889, -0.3011]],\n",
      "\n",
      "         [[ 0.5287,  0.3372, -0.1541, -0.2268,  0.0412, -0.5836,  0.9077,\n",
      "           -0.2078, -0.7792,  0.8504, -0.6068,  0.3168,  0.4470, -0.3777,\n",
      "            0.1809,  0.1813],\n",
      "          [ 0.5815, -0.6572, -0.3419,  0.5247,  0.4525,  0.0388,  0.1503,\n",
      "           -0.4492,  0.2086, -0.0070, -0.1805,  0.6049,  0.6057,  0.4177,\n",
      "            0.5010, -0.6458],\n",
      "          [ 0.4252, -0.7830, -0.0118,  0.0414,  0.2175, -0.3599,  0.7017,\n",
      "           -0.4289,  0.9411,  0.2323, -0.5781,  0.5929,  0.7239, -0.0334,\n",
      "            0.4889, -0.3011]],\n",
      "\n",
      "         [[ 0.5287,  0.3372, -0.1541, -0.2268,  0.0412, -0.5836,  0.9077,\n",
      "           -0.2078, -0.7792,  0.8504, -0.6068,  0.3168,  0.4470, -0.3777,\n",
      "            0.1809,  0.1813],\n",
      "          [ 0.5815, -0.6572, -0.3419,  0.5247,  0.4525,  0.0388,  0.1503,\n",
      "           -0.4492,  0.2086, -0.0070, -0.1805,  0.6049,  0.6057,  0.4177,\n",
      "            0.5010, -0.6458],\n",
      "          [ 0.4252, -0.7830, -0.0118,  0.0414,  0.2175, -0.3599,  0.7017,\n",
      "           -0.4289,  0.9411,  0.2323, -0.5781,  0.5929,  0.7239, -0.0334,\n",
      "            0.4889, -0.3011]],\n",
      "\n",
      "         [[ 0.5287,  0.3372, -0.1541, -0.2268,  0.0412, -0.5836,  0.9077,\n",
      "           -0.2078, -0.7792,  0.8504, -0.6068,  0.3168,  0.4470, -0.3777,\n",
      "            0.1809,  0.1813],\n",
      "          [ 0.5815, -0.6572, -0.3419,  0.5247,  0.4525,  0.0388,  0.1503,\n",
      "           -0.4492,  0.2086, -0.0070, -0.1805,  0.6049,  0.6057,  0.4177,\n",
      "            0.5010, -0.6458],\n",
      "          [ 0.4252, -0.7830, -0.0118,  0.0414,  0.2175, -0.3599,  0.7017,\n",
      "           -0.4289,  0.9411,  0.2323, -0.5781,  0.5929,  0.7239, -0.0334,\n",
      "            0.4889, -0.3011]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[ 0.0642,  0.4599,  0.0972,  0.2083, -0.5469,  0.6069,  0.6749,\n",
      "           -0.2841, -0.5801,  0.1959, -0.4322,  0.1629, -0.6350, -0.2230,\n",
      "            0.2943, -0.4052],\n",
      "          [-0.8415, -0.3653, -0.8370,  0.0476,  0.3243,  0.3126,  0.4402,\n",
      "           -0.1247,  0.0227, -0.2397, -0.4857,  0.1081, -0.1310,  0.3035,\n",
      "           -0.6697,  0.4728],\n",
      "          [-0.4974,  0.0812, -0.4718,  0.1739, -0.1674,  0.6186,  0.7475,\n",
      "           -0.2757, -0.3844, -0.0195, -0.6098,  0.1816, -0.5211,  0.0419,\n",
      "           -0.2285,  0.0256]],\n",
      "\n",
      "         [[ 0.0642,  0.4599,  0.0972,  0.2083, -0.5469,  0.6069,  0.6749,\n",
      "           -0.2841, -0.5801,  0.1959, -0.4322,  0.1629, -0.6350, -0.2230,\n",
      "            0.2943, -0.4052],\n",
      "          [-0.8415, -0.3653, -0.8370,  0.0476,  0.3243,  0.3126,  0.4402,\n",
      "           -0.1247,  0.0227, -0.2397, -0.4857,  0.1081, -0.1310,  0.3035,\n",
      "           -0.6697,  0.4728],\n",
      "          [-0.4974,  0.0812, -0.4718,  0.1739, -0.1674,  0.6186,  0.7475,\n",
      "           -0.2757, -0.3844, -0.0195, -0.6098,  0.1816, -0.5211,  0.0419,\n",
      "           -0.2285,  0.0256]],\n",
      "\n",
      "         [[ 0.0642,  0.4599,  0.0972,  0.2083, -0.5469,  0.6069,  0.6749,\n",
      "           -0.2841, -0.5801,  0.1959, -0.4322,  0.1629, -0.6350, -0.2230,\n",
      "            0.2943, -0.4052],\n",
      "          [-0.8415, -0.3653, -0.8370,  0.0476,  0.3243,  0.3126,  0.4402,\n",
      "           -0.1247,  0.0227, -0.2397, -0.4857,  0.1081, -0.1310,  0.3035,\n",
      "           -0.6697,  0.4728],\n",
      "          [-0.4974,  0.0812, -0.4718,  0.1739, -0.1674,  0.6186,  0.7475,\n",
      "           -0.2757, -0.3844, -0.0195, -0.6098,  0.1816, -0.5211,  0.0419,\n",
      "           -0.2285,  0.0256]],\n",
      "\n",
      "         [[ 0.0642,  0.4599,  0.0972,  0.2083, -0.5469,  0.6069,  0.6749,\n",
      "           -0.2841, -0.5801,  0.1959, -0.4322,  0.1629, -0.6350, -0.2230,\n",
      "            0.2943, -0.4052],\n",
      "          [-0.8415, -0.3653, -0.8370,  0.0476,  0.3243,  0.3126,  0.4402,\n",
      "           -0.1247,  0.0227, -0.2397, -0.4857,  0.1081, -0.1310,  0.3035,\n",
      "           -0.6697,  0.4728],\n",
      "          [-0.4974,  0.0812, -0.4718,  0.1739, -0.1674,  0.6186,  0.7475,\n",
      "           -0.2757, -0.3844, -0.0195, -0.6098,  0.1816, -0.5211,  0.0419,\n",
      "           -0.2285,  0.0256]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[ 0.1760, -0.0108, -0.1606],\n",
      "          [ 0.0821,  0.0667, -0.1502],\n",
      "          [ 0.0677,  0.2592,  0.1408]],\n",
      "\n",
      "         [[-0.0228, -0.3566, -0.1724],\n",
      "          [ 0.1909,  0.0194,  0.1188],\n",
      "          [ 0.0785, -0.0962, -0.0393]],\n",
      "\n",
      "         [[-0.4871,  0.0891,  0.0344],\n",
      "          [ 0.0973, -0.2466, -0.2206],\n",
      "          [-0.0965, -0.0744, -0.3328]],\n",
      "\n",
      "         [[ 0.1563,  0.1715,  0.1580],\n",
      "          [ 0.1945, -0.0991,  0.0903],\n",
      "          [ 0.1629,  0.0320,  0.2190]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[ 1.7595e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 8.2071e-02,  6.6741e-02, -2.3820e+38],\n",
      "          [ 6.7673e-02,  2.5920e-01,  1.4078e-01]],\n",
      "\n",
      "         [[-2.2778e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 1.9094e-01,  1.9422e-02, -2.3820e+38],\n",
      "          [ 7.8508e-02, -9.6163e-02, -3.9298e-02]],\n",
      "\n",
      "         [[-4.8711e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 9.7325e-02, -2.4662e-01, -2.3820e+38],\n",
      "          [-9.6499e-02, -7.4354e-02, -3.3284e-01]],\n",
      "\n",
      "         [[ 1.5625e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 1.9449e-01, -9.9078e-02, -2.3820e+38],\n",
      "          [ 1.6291e-01,  3.2025e-02,  2.1897e-01]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.5038, 0.4962, 0.0000],\n",
      "          [0.3042, 0.3685, 0.3273]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5428, 0.4572, 0.0000],\n",
      "          [0.3665, 0.3078, 0.3258]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5851, 0.4149, 0.0000],\n",
      "          [0.3556, 0.3636, 0.2808]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5729, 0.4271, 0.0000],\n",
      "          [0.3407, 0.2989, 0.3604]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[ 0.0642,  0.4599,  0.0972,  0.2083, -0.5469,  0.6069,  0.6749,\n",
      "           -0.2841, -0.5801,  0.1959, -0.4322,  0.1629, -0.6350, -0.2230,\n",
      "            0.2943, -0.4052],\n",
      "          [-0.3852,  0.0505, -0.3663,  0.1286, -0.1146,  0.4609,  0.5585,\n",
      "           -0.2050, -0.2810, -0.0202, -0.4587,  0.1357, -0.3849,  0.0382,\n",
      "           -0.1840,  0.0305],\n",
      "          [-0.4534,  0.0319, -0.4332,  0.1378, -0.1017,  0.5023,  0.6122,\n",
      "           -0.2226, -0.2939, -0.0351, -0.5101,  0.1488, -0.4120,  0.0577,\n",
      "           -0.2320,  0.0594]],\n",
      "\n",
      "         [[ 0.0642,  0.4599,  0.0972,  0.2083, -0.5469,  0.6069,  0.6749,\n",
      "           -0.2841, -0.5801,  0.1959, -0.4322,  0.1629, -0.6350, -0.2230,\n",
      "            0.2943, -0.4052],\n",
      "          [-0.3499,  0.0826, -0.3299,  0.1348, -0.1485,  0.4723,  0.5676,\n",
      "           -0.2112, -0.3045, -0.0033, -0.4567,  0.1378, -0.4046,  0.0177,\n",
      "           -0.1464, -0.0037],\n",
      "          [-0.3975,  0.0826, -0.3756,  0.1476, -0.1551,  0.5201,  0.6263,\n",
      "           -0.2323, -0.3308, -0.0083, -0.5065,  0.1521, -0.4428,  0.0253,\n",
      "           -0.1727,  0.0054]],\n",
      "\n",
      "         [[ 0.0642,  0.4599,  0.0972,  0.2083, -0.5469,  0.6069,  0.6749,\n",
      "           -0.2841, -0.5801,  0.1959, -0.4322,  0.1629, -0.6350, -0.2230,\n",
      "            0.2943, -0.4052],\n",
      "          [-0.3116,  0.1176, -0.2903,  0.1416, -0.1854,  0.4848,  0.5776,\n",
      "           -0.2180, -0.3300,  0.0152, -0.4544,  0.1402, -0.4259, -0.0046,\n",
      "           -0.1056, -0.0409],\n",
      "          [-0.4228,  0.0535, -0.4022,  0.1402, -0.1236,  0.5032,  0.6100,\n",
      "           -0.2238, -0.3060, -0.0230, -0.5015,  0.1482, -0.4198,  0.0428,\n",
      "           -0.2030,  0.0350]],\n",
      "\n",
      "         [[ 0.0642,  0.4599,  0.0972,  0.2083, -0.5469,  0.6069,  0.6749,\n",
      "           -0.2841, -0.5801,  0.1959, -0.4322,  0.1629, -0.6350, -0.2230,\n",
      "            0.2943, -0.4052],\n",
      "          [-0.3227,  0.1074, -0.3018,  0.1397, -0.1747,  0.4812,  0.5747,\n",
      "           -0.2160, -0.3226,  0.0098, -0.4550,  0.1395, -0.4197,  0.0019,\n",
      "           -0.1174, -0.0301],\n",
      "          [-0.4089,  0.0768, -0.3871,  0.1479, -0.1497,  0.5231,  0.6309,\n",
      "           -0.2334, -0.3294, -0.0119, -0.5122,  0.1533, -0.4433,  0.0299,\n",
      "           -0.1823,  0.0125]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 64])\n",
      "tensor([[[ 0.0642,  0.4599,  0.0972,  0.2083, -0.5469,  0.6069,  0.6749,\n",
      "          -0.2841, -0.5801,  0.1959, -0.4322,  0.1629, -0.6350, -0.2230,\n",
      "           0.2943, -0.4052,  0.0642,  0.4599,  0.0972,  0.2083, -0.5469,\n",
      "           0.6069,  0.6749, -0.2841, -0.5801,  0.1959, -0.4322,  0.1629,\n",
      "          -0.6350, -0.2230,  0.2943, -0.4052,  0.0642,  0.4599,  0.0972,\n",
      "           0.2083, -0.5469,  0.6069,  0.6749, -0.2841, -0.5801,  0.1959,\n",
      "          -0.4322,  0.1629, -0.6350, -0.2230,  0.2943, -0.4052,  0.0642,\n",
      "           0.4599,  0.0972,  0.2083, -0.5469,  0.6069,  0.6749, -0.2841,\n",
      "          -0.5801,  0.1959, -0.4322,  0.1629, -0.6350, -0.2230,  0.2943,\n",
      "          -0.4052],\n",
      "         [-0.3852,  0.0505, -0.3663,  0.1286, -0.1146,  0.4609,  0.5585,\n",
      "          -0.2050, -0.2810, -0.0202, -0.4587,  0.1357, -0.3849,  0.0382,\n",
      "          -0.1840,  0.0305, -0.3499,  0.0826, -0.3299,  0.1348, -0.1485,\n",
      "           0.4723,  0.5676, -0.2112, -0.3045, -0.0033, -0.4567,  0.1378,\n",
      "          -0.4046,  0.0177, -0.1464, -0.0037, -0.3116,  0.1176, -0.2903,\n",
      "           0.1416, -0.1854,  0.4848,  0.5776, -0.2180, -0.3300,  0.0152,\n",
      "          -0.4544,  0.1402, -0.4259, -0.0046, -0.1056, -0.0409, -0.3227,\n",
      "           0.1074, -0.3018,  0.1397, -0.1747,  0.4812,  0.5747, -0.2160,\n",
      "          -0.3226,  0.0098, -0.4550,  0.1395, -0.4197,  0.0019, -0.1174,\n",
      "          -0.0301],\n",
      "         [-0.4534,  0.0319, -0.4332,  0.1378, -0.1017,  0.5023,  0.6122,\n",
      "          -0.2226, -0.2939, -0.0351, -0.5101,  0.1488, -0.4120,  0.0577,\n",
      "          -0.2320,  0.0594, -0.3975,  0.0826, -0.3756,  0.1476, -0.1551,\n",
      "           0.5201,  0.6263, -0.2323, -0.3308, -0.0083, -0.5065,  0.1521,\n",
      "          -0.4428,  0.0253, -0.1727,  0.0054, -0.4228,  0.0535, -0.4022,\n",
      "           0.1402, -0.1236,  0.5032,  0.6100, -0.2238, -0.3060, -0.0230,\n",
      "          -0.5015,  0.1482, -0.4198,  0.0428, -0.2030,  0.0350, -0.4089,\n",
      "           0.0768, -0.3871,  0.1479, -0.1497,  0.5231,  0.6309, -0.2334,\n",
      "          -0.3294, -0.0119, -0.5122,  0.1533, -0.4433,  0.0299, -0.1823,\n",
      "           0.0125]]], grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([128, 4])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0139, -0.0787, -0.0272, -0.0417],\n",
      "        [ 0.0033,  0.0494, -0.0123,  0.0227],\n",
      "        [ 0.0840,  0.0823,  0.0379, -0.0151],\n",
      "        [-0.0278,  0.0584,  0.0317,  0.0743],\n",
      "        [ 0.0762, -0.0099, -0.0417,  0.0459],\n",
      "        [ 0.0649,  0.0091, -0.0589,  0.0485],\n",
      "        [ 0.0458, -0.0723,  0.0485, -0.0311],\n",
      "        [ 0.0279,  0.0563,  0.0791, -0.0248],\n",
      "        [-0.0813,  0.0755, -0.0165,  0.0383],\n",
      "        [-0.0721, -0.0364, -0.0574,  0.0814],\n",
      "        [-0.0438, -0.0211,  0.0055, -0.0661],\n",
      "        [ 0.0021,  0.0126, -0.0437, -0.0395],\n",
      "        [ 0.0412, -0.0169, -0.0531,  0.0762],\n",
      "        [ 0.0454,  0.0125,  0.0438,  0.0251],\n",
      "        [ 0.0287,  0.0397,  0.0766, -0.0685],\n",
      "        [-0.0439, -0.0862,  0.0521,  0.0354],\n",
      "        [ 0.0287, -0.0362,  0.0043, -0.0332],\n",
      "        [-0.0180,  0.0598, -0.0022,  0.0810],\n",
      "        [ 0.0312, -0.0346, -0.0249,  0.0128],\n",
      "        [ 0.0677,  0.0725,  0.0375, -0.0278],\n",
      "        [ 0.0479,  0.0114, -0.0526, -0.0683],\n",
      "        [-0.0165, -0.0462,  0.0869, -0.0703],\n",
      "        [ 0.0455,  0.0610,  0.0053,  0.0742],\n",
      "        [ 0.0757, -0.0677,  0.0271,  0.0347],\n",
      "        [-0.0526, -0.0091,  0.0032, -0.0064],\n",
      "        [-0.0776,  0.0104,  0.0064, -0.0518],\n",
      "        [-0.0017, -0.0029, -0.0549, -0.0834],\n",
      "        [-0.0227,  0.0364,  0.0527, -0.0069],\n",
      "        [ 0.0085,  0.0365, -0.0376, -0.0070],\n",
      "        [ 0.0820, -0.0390, -0.0115, -0.0306],\n",
      "        [ 0.0498, -0.0566,  0.0319, -0.0084],\n",
      "        [-0.0578, -0.0742,  0.0627, -0.0273],\n",
      "        [-0.0837, -0.0356, -0.0646,  0.0619],\n",
      "        [-0.0584, -0.0646,  0.0578, -0.0586],\n",
      "        [ 0.0294, -0.0348, -0.0340,  0.0118],\n",
      "        [-0.0396, -0.0184,  0.0042,  0.0138],\n",
      "        [ 0.0701, -0.0292,  0.0398, -0.0565],\n",
      "        [ 0.0478, -0.0783,  0.0089, -0.0451],\n",
      "        [ 0.0826, -0.0510,  0.0791,  0.0765],\n",
      "        [-0.0505, -0.0013, -0.0316, -0.0613],\n",
      "        [ 0.0244,  0.0527, -0.0418, -0.0360],\n",
      "        [ 0.0488,  0.0199,  0.0736,  0.0449],\n",
      "        [-0.0180,  0.0355, -0.0553,  0.0215],\n",
      "        [-0.0083,  0.0779, -0.0510, -0.0657],\n",
      "        [ 0.0020, -0.0527, -0.0741,  0.0346],\n",
      "        [ 0.0375,  0.0752, -0.0377, -0.0659],\n",
      "        [ 0.0219,  0.0304,  0.0374, -0.0870],\n",
      "        [-0.0281, -0.0418,  0.0484,  0.0157],\n",
      "        [-0.0478,  0.0550,  0.0397, -0.0394],\n",
      "        [ 0.0020, -0.0341,  0.0732, -0.0505],\n",
      "        [ 0.0496,  0.0150, -0.0263, -0.0473],\n",
      "        [-0.0515, -0.0541, -0.0173, -0.0739],\n",
      "        [ 0.0284,  0.0858,  0.0706,  0.0280],\n",
      "        [-0.0219,  0.0860,  0.0841, -0.0408],\n",
      "        [-0.0817, -0.0685,  0.0569,  0.0070],\n",
      "        [-0.0781,  0.0541,  0.0586, -0.0666],\n",
      "        [-0.0822, -0.0732,  0.0389, -0.0767],\n",
      "        [-0.0720, -0.0872,  0.0849, -0.0393],\n",
      "        [ 0.0702, -0.0119,  0.0197,  0.0544],\n",
      "        [-0.0763,  0.0492,  0.0762, -0.0804],\n",
      "        [-0.0458,  0.0242,  0.0344, -0.0160],\n",
      "        [ 0.0139, -0.0879,  0.0617,  0.0035],\n",
      "        [-0.0723, -0.0875, -0.0360, -0.0717],\n",
      "        [ 0.0703,  0.0624, -0.0128, -0.0182],\n",
      "        [-0.0372, -0.0452,  0.0723,  0.0477],\n",
      "        [-0.0867, -0.0091,  0.0572,  0.0341],\n",
      "        [-0.0241, -0.0047, -0.0325, -0.0600],\n",
      "        [ 0.0211,  0.0089, -0.0471,  0.0287],\n",
      "        [ 0.0486,  0.0764,  0.0096, -0.0662],\n",
      "        [ 0.0428, -0.0168, -0.0484,  0.0304],\n",
      "        [ 0.0791,  0.0326,  0.0070,  0.0849],\n",
      "        [ 0.0162,  0.0343,  0.0489,  0.0249],\n",
      "        [-0.0175,  0.0154, -0.0133, -0.0546],\n",
      "        [-0.0223, -0.0140,  0.0862,  0.0644],\n",
      "        [-0.0730, -0.0561, -0.0175,  0.0270],\n",
      "        [-0.0750,  0.0791,  0.0761,  0.0207],\n",
      "        [ 0.0588,  0.0386, -0.0708, -0.0120],\n",
      "        [-0.0486,  0.0058, -0.0176, -0.0872],\n",
      "        [ 0.0420,  0.0211, -0.0453, -0.0756],\n",
      "        [-0.0469,  0.0648,  0.0840,  0.0771],\n",
      "        [ 0.0167, -0.0466,  0.0435, -0.0558],\n",
      "        [-0.0180,  0.0093, -0.0242, -0.0033],\n",
      "        [ 0.0672, -0.0508,  0.0832, -0.0102],\n",
      "        [ 0.0063, -0.0791, -0.0377, -0.0634],\n",
      "        [ 0.0389,  0.0140, -0.0615,  0.0771],\n",
      "        [-0.0714,  0.0663, -0.0419,  0.0436],\n",
      "        [-0.0005, -0.0116,  0.0021, -0.0355],\n",
      "        [ 0.0094,  0.0812,  0.0483, -0.0468],\n",
      "        [-0.0453,  0.0196,  0.0874, -0.0572],\n",
      "        [ 0.0470,  0.0742, -0.0554,  0.0737],\n",
      "        [-0.0046, -0.0622, -0.0864, -0.0697],\n",
      "        [-0.0350,  0.0755, -0.0144, -0.0533],\n",
      "        [-0.0733, -0.0141, -0.0706,  0.0772],\n",
      "        [ 0.0318, -0.0718, -0.0039, -0.0808],\n",
      "        [ 0.0250, -0.0836, -0.0437, -0.0564],\n",
      "        [ 0.0294, -0.0549, -0.0642, -0.0373],\n",
      "        [-0.0052,  0.0771, -0.0031,  0.0305],\n",
      "        [-0.0191, -0.0395,  0.0577,  0.0202],\n",
      "        [-0.0204,  0.0647, -0.0778, -0.0585],\n",
      "        [ 0.0814,  0.0275, -0.0803,  0.0322],\n",
      "        [ 0.0391,  0.0696, -0.0112,  0.0406],\n",
      "        [-0.0811, -0.0443, -0.0128, -0.0556],\n",
      "        [ 0.0855,  0.0882,  0.0047, -0.0368],\n",
      "        [ 0.0583, -0.0531,  0.0071,  0.0782],\n",
      "        [-0.0406, -0.0030,  0.0457, -0.0841],\n",
      "        [ 0.0442, -0.0754, -0.0803, -0.0494],\n",
      "        [-0.0486, -0.0157,  0.0512,  0.0465],\n",
      "        [-0.0004, -0.0288, -0.0781,  0.0676],\n",
      "        [-0.0460, -0.0629, -0.0148,  0.0751],\n",
      "        [ 0.0104, -0.0334, -0.0388, -0.0608],\n",
      "        [-0.0189,  0.0272,  0.0264, -0.0059],\n",
      "        [-0.0798, -0.0168, -0.0396, -0.0421],\n",
      "        [-0.0075,  0.0069, -0.0432, -0.0495],\n",
      "        [-0.0047,  0.0769, -0.0368,  0.0721],\n",
      "        [-0.0162,  0.0626, -0.0485,  0.0189],\n",
      "        [ 0.0079,  0.0517, -0.0361, -0.0346],\n",
      "        [ 0.0057,  0.0109, -0.0509, -0.0678],\n",
      "        [ 0.0801, -0.0706,  0.0351, -0.0406],\n",
      "        [-0.0175,  0.0669,  0.0229,  0.0174],\n",
      "        [-0.0324,  0.0067, -0.0462, -0.0483],\n",
      "        [ 0.0860,  0.0160,  0.0122,  0.0359],\n",
      "        [ 0.0542, -0.0770, -0.0258,  0.0683],\n",
      "        [-0.0590, -0.0269,  0.0553,  0.0716],\n",
      "        [ 0.0781,  0.0203, -0.0462,  0.0629],\n",
      "        [ 0.0124, -0.0418,  0.0457,  0.0071],\n",
      "        [-0.0077,  0.0158,  0.0176, -0.0724],\n",
      "        [ 0.0464, -0.0335, -0.0344,  0.0326],\n",
      "        [-0.0237,  0.0807, -0.0869,  0.0878]], requires_grad=True)\n",
      "spliced Wo: torch.Size([64, 2])\n",
      "tensor([[ 0.0139, -0.0787],\n",
      "        [ 0.0033,  0.0494],\n",
      "        [ 0.0840,  0.0823],\n",
      "        [-0.0278,  0.0584],\n",
      "        [ 0.0762, -0.0099],\n",
      "        [ 0.0649,  0.0091],\n",
      "        [ 0.0458, -0.0723],\n",
      "        [ 0.0279,  0.0563],\n",
      "        [-0.0813,  0.0755],\n",
      "        [-0.0721, -0.0364],\n",
      "        [-0.0438, -0.0211],\n",
      "        [ 0.0021,  0.0126],\n",
      "        [ 0.0412, -0.0169],\n",
      "        [ 0.0454,  0.0125],\n",
      "        [ 0.0287,  0.0397],\n",
      "        [-0.0439, -0.0862],\n",
      "        [-0.0837, -0.0356],\n",
      "        [-0.0584, -0.0646],\n",
      "        [ 0.0294, -0.0348],\n",
      "        [-0.0396, -0.0184],\n",
      "        [ 0.0701, -0.0292],\n",
      "        [ 0.0478, -0.0783],\n",
      "        [ 0.0826, -0.0510],\n",
      "        [-0.0505, -0.0013],\n",
      "        [ 0.0244,  0.0527],\n",
      "        [ 0.0488,  0.0199],\n",
      "        [-0.0180,  0.0355],\n",
      "        [-0.0083,  0.0779],\n",
      "        [ 0.0020, -0.0527],\n",
      "        [ 0.0375,  0.0752],\n",
      "        [ 0.0219,  0.0304],\n",
      "        [-0.0281, -0.0418],\n",
      "        [-0.0372, -0.0452],\n",
      "        [-0.0867, -0.0091],\n",
      "        [-0.0241, -0.0047],\n",
      "        [ 0.0211,  0.0089],\n",
      "        [ 0.0486,  0.0764],\n",
      "        [ 0.0428, -0.0168],\n",
      "        [ 0.0791,  0.0326],\n",
      "        [ 0.0162,  0.0343],\n",
      "        [-0.0175,  0.0154],\n",
      "        [-0.0223, -0.0140],\n",
      "        [-0.0730, -0.0561],\n",
      "        [-0.0750,  0.0791],\n",
      "        [ 0.0588,  0.0386],\n",
      "        [-0.0486,  0.0058],\n",
      "        [ 0.0420,  0.0211],\n",
      "        [-0.0469,  0.0648],\n",
      "        [-0.0052,  0.0771],\n",
      "        [-0.0191, -0.0395],\n",
      "        [-0.0204,  0.0647],\n",
      "        [ 0.0814,  0.0275],\n",
      "        [ 0.0391,  0.0696],\n",
      "        [-0.0811, -0.0443],\n",
      "        [ 0.0855,  0.0882],\n",
      "        [ 0.0583, -0.0531],\n",
      "        [-0.0406, -0.0030],\n",
      "        [ 0.0442, -0.0754],\n",
      "        [-0.0486, -0.0157],\n",
      "        [-0.0004, -0.0288],\n",
      "        [-0.0460, -0.0629],\n",
      "        [ 0.0104, -0.0334],\n",
      "        [-0.0189,  0.0272],\n",
      "        [-0.0798, -0.0168]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.2222, -0.0988],\n",
      "         [ 0.2344, -0.0714],\n",
      "         [ 0.2622, -0.0694]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[ 0.8511,  0.3598],\n",
      "         [-0.4562,  1.1856],\n",
      "         [ 1.0344,  2.9748]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.8511,  0.3598],\n",
      "         [-0.4562,  1.1856],\n",
      "         [ 1.0344,  2.9748]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[ 1.3026,  0.5506],\n",
      "         [-0.5078,  1.3199],\n",
      "         [ 0.4645,  1.3358]]], grad_fn=<MulBackward0>)\n",
      "dim: 2\n",
      "skip: 0\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[ 1.3026,  0.5506],\n",
      "         [-0.5078,  1.3199],\n",
      "         [ 0.4645,  1.3358]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[ 1.3026,  0.5506],\n",
      "         [-0.5078,  1.3199],\n",
      "         [ 0.4645,  1.3358]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 2\n",
      "d_skip: 0\n",
      "i_dim: 8\n",
      "i_skip: 0\n",
      "Wgate: torch.Size([4, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.4920,  0.3010,  0.1499, -0.3392,  0.4856, -0.4726,  0.0867,  0.2430,\n",
      "          0.3942,  0.4743, -0.2700, -0.3373,  0.3804, -0.2024, -0.3690, -0.1290],\n",
      "        [-0.4186, -0.3264, -0.4179,  0.2408, -0.1365,  0.1968,  0.0011, -0.4685,\n",
      "         -0.1507,  0.2918,  0.3232,  0.0377, -0.1145, -0.4494, -0.2852, -0.0918],\n",
      "        [-0.3881, -0.4971, -0.3846,  0.2184,  0.2203,  0.3713,  0.1256, -0.4725,\n",
      "          0.1326,  0.1626,  0.4547, -0.0986,  0.3189,  0.3653,  0.2537,  0.3719],\n",
      "        [ 0.2424, -0.1054,  0.2797, -0.0835, -0.3293,  0.3435,  0.2210,  0.4816,\n",
      "          0.2108,  0.2477, -0.1237, -0.2128,  0.3488, -0.4129, -0.1048,  0.3282]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([2, 8])\n",
      "tensor([[ 0.4920,  0.3010,  0.1499, -0.3392,  0.4856, -0.4726,  0.0867,  0.2430],\n",
      "        [-0.4186, -0.3264, -0.4179,  0.2408, -0.1365,  0.1968,  0.0011, -0.4685]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([16])\n",
      "Parameter containing:\n",
      "tensor([-0.4296, -0.2656, -0.3730, -0.4881,  0.2735,  0.3810, -0.2995, -0.2457,\n",
      "         0.3296, -0.1207, -0.1850,  0.1151, -0.1815,  0.0565,  0.1133,  0.2197],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([8])\n",
      "tensor([-0.4296, -0.2656, -0.3730, -0.4881,  0.2735,  0.3810, -0.2995, -0.2457],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.0192, -0.0533, -0.4078, -0.7974,  0.8309, -0.1262, -0.1859,\n",
      "          -0.1872],\n",
      "         [-1.2320, -0.8493, -1.0007,  0.0020, -0.1534,  0.8808, -0.3420,\n",
      "          -0.9875],\n",
      "         [-0.7602, -0.5618, -0.8615, -0.3240,  0.3166,  0.4244, -0.2577,\n",
      "          -0.7587]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.0094, -0.0255, -0.1393, -0.1695,  0.6622, -0.0568, -0.0793,\n",
      "          -0.0797],\n",
      "         [-0.1343, -0.1680, -0.1586,  0.0010, -0.0673,  0.7141, -0.1252,\n",
      "          -0.1597],\n",
      "         [-0.1700, -0.1613, -0.1675, -0.1208,  0.1977,  0.2820, -0.1027,\n",
      "          -0.1700]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.4397,  0.0639,  0.3251, -0.4417, -0.1198, -0.4560,  0.3213, -0.1483,\n",
      "         -0.0726, -0.1437, -0.1375,  0.2211,  0.4713, -0.1941,  0.2534, -0.1524],\n",
      "        [-0.1683,  0.0169, -0.3806, -0.3985,  0.0782, -0.4636, -0.4176, -0.0484,\n",
      "         -0.3432, -0.1917,  0.4930,  0.3492, -0.0744,  0.1426, -0.3753,  0.4084],\n",
      "        [-0.2476,  0.0072, -0.3127, -0.3605,  0.3117,  0.4990,  0.0193,  0.1382,\n",
      "          0.2498,  0.1919,  0.2682, -0.3924, -0.0709, -0.2175, -0.3473,  0.1599],\n",
      "        [ 0.4299,  0.3039,  0.2940, -0.4826,  0.1980, -0.0223, -0.0833, -0.0818,\n",
      "          0.1703, -0.4762, -0.0675,  0.1609, -0.0555, -0.1132, -0.4620,  0.0077]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([2, 8])\n",
      "tensor([[-0.4397,  0.0639,  0.3251, -0.4417, -0.1198, -0.4560,  0.3213, -0.1483],\n",
      "        [-0.1683,  0.0169, -0.3806, -0.3985,  0.0782, -0.4636, -0.4176, -0.0484]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([16])\n",
      "Parameter containing:\n",
      "tensor([ 0.3836,  0.0685, -0.3794,  0.2138,  0.0660,  0.0780, -0.2228,  0.0113,\n",
      "        -0.1320,  0.3062, -0.2345,  0.4163, -0.4161,  0.3209,  0.2370, -0.2584],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([8])\n",
      "tensor([ 0.3836,  0.0685, -0.3794,  0.2138,  0.0660,  0.0780, -0.2228,  0.0113],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.2817,  0.1610, -0.1655, -0.5811, -0.0470, -0.7713, -0.0342,\n",
      "          -0.2085],\n",
      "         [ 0.3848,  0.0583, -1.0468, -0.0879,  0.2301, -0.3023, -0.9372,\n",
      "           0.0227],\n",
      "         [-0.0454,  0.1207, -0.7368, -0.5238,  0.1148, -0.7531, -0.6314,\n",
      "          -0.1223]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 8])\n",
      "tensor([[[ 2.6620e-03, -4.1053e-03,  2.3059e-02,  9.8516e-02, -3.1128e-02,\n",
      "           4.3785e-02,  2.7129e-03,  1.6618e-02],\n",
      "         [-5.1665e-02, -9.8009e-03,  1.6603e-01, -8.8125e-05, -1.5497e-02,\n",
      "          -2.1587e-01,  1.1738e-01, -3.6191e-03],\n",
      "         [ 7.7102e-03, -1.9468e-02,  1.2344e-01,  6.3297e-02,  2.2696e-02,\n",
      "          -2.1233e-01,  6.4823e-02,  2.0781e-02]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([16, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.1754,  0.0761,  0.1232,  0.1366],\n",
      "        [-0.0635,  0.1460,  0.0160, -0.1254],\n",
      "        [ 0.0276, -0.1514, -0.0825, -0.1608],\n",
      "        [ 0.0913, -0.0020,  0.2442,  0.1662],\n",
      "        [ 0.1354,  0.1174,  0.2448, -0.2325],\n",
      "        [-0.0123,  0.0650,  0.1438, -0.1913],\n",
      "        [-0.0013,  0.2201,  0.1408,  0.0611],\n",
      "        [-0.1373,  0.1180,  0.1358, -0.1107],\n",
      "        [ 0.1830,  0.2221, -0.1693,  0.2425],\n",
      "        [-0.1213, -0.0698,  0.0168,  0.1718],\n",
      "        [-0.0677, -0.2102, -0.0295, -0.1922],\n",
      "        [ 0.0277,  0.0011,  0.1526, -0.0894],\n",
      "        [ 0.2469,  0.1777,  0.0815,  0.0353],\n",
      "        [-0.1205, -0.1902, -0.1469, -0.0820],\n",
      "        [ 0.0319, -0.0125,  0.0571, -0.1865],\n",
      "        [ 0.0971,  0.0427,  0.2102,  0.0165]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([8, 2])\n",
      "tensor([[-0.1754,  0.0761],\n",
      "        [-0.0635,  0.1460],\n",
      "        [ 0.0276, -0.1514],\n",
      "        [ 0.0913, -0.0020],\n",
      "        [ 0.1354,  0.1174],\n",
      "        [-0.0123,  0.0650],\n",
      "        [-0.0013,  0.2201],\n",
      "        [-0.1373,  0.1180]], grad_fn=<SliceBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([-0.0242,  0.1740,  0.0925,  0.2142], requires_grad=True)\n",
      "Bdown spliced: torch.Size([2])\n",
      "tensor([-0.0242,  0.1740], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.0218,  0.1717],\n",
      "         [-0.0091,  0.1531],\n",
      "         [-0.0124,  0.1585]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[ 0.8293,  0.5314],\n",
      "         [-0.4652,  1.3387],\n",
      "         [ 1.0220,  3.1334]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.8293,  0.5314],\n",
      "         [-0.4652,  1.3387],\n",
      "         [ 1.0220,  3.1334]]], grad_fn=<AddBackward0>)\n",
      "Model 1 from range(2)\n",
      "----------------- Layer.forwardTensor() --------------------\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.4059, -0.8723],\n",
      "         [ 0.0212,  1.2475],\n",
      "         [ 0.7738,  0.3308]]])\n",
      "normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.5966, -1.2822],\n",
      "         [ 0.0241,  1.4140],\n",
      "         [ 1.3004,  0.5558]]])\n",
      "dim: 2\n",
      "skip: 2\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.5966, -1.2822],\n",
      "         [ 0.0241,  1.4140],\n",
      "         [ 1.3004,  0.5558]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- Attention Input: torch.Tensor ------------\n",
      "----------------- MultiQueryAttention.forwardTensor() --------------------\n",
      "x shape: torch.Size([1, 3, 2])\n",
      "d_skip: 2\n",
      "models_in_this_level: 2\n",
      "h_dim: 16\n",
      "h_skip: 16\n",
      "self.Wqkv: torch.Size([4, 192])\n",
      "Parameter containing:\n",
      "tensor([[ 0.3301,  0.3833,  0.4804,  0.3799, -0.3784,  0.0511,  0.3683,  0.1008,\n",
      "         -0.2285, -0.0197,  0.0348,  0.1120, -0.3621, -0.2547,  0.4441, -0.4699,\n",
      "         -0.3608, -0.4284,  0.1076,  0.4837, -0.0746,  0.1340, -0.4201, -0.0809,\n",
      "          0.1141, -0.1077,  0.0644,  0.3932,  0.0016,  0.0923, -0.3204, -0.1327,\n",
      "         -0.3313, -0.1889, -0.1339,  0.0845,  0.1315, -0.0367, -0.3632, -0.3186,\n",
      "         -0.0542, -0.2293, -0.4923, -0.4534, -0.3555, -0.4473, -0.2461,  0.3808,\n",
      "         -0.2491, -0.0109, -0.3062,  0.1362, -0.3017,  0.0435,  0.1770,  0.4863,\n",
      "         -0.2845, -0.0166,  0.4380,  0.1111,  0.3862,  0.1065,  0.2000, -0.2471,\n",
      "         -0.0877, -0.4372,  0.4612,  0.0399,  0.2884,  0.0085, -0.3506,  0.0517,\n",
      "          0.2164, -0.1687,  0.3631,  0.2543, -0.0103,  0.4510,  0.3025, -0.0815,\n",
      "         -0.0690,  0.4011, -0.0612, -0.3017, -0.4315, -0.0644, -0.0030,  0.1530,\n",
      "         -0.3891, -0.2778, -0.2134, -0.0432,  0.4629, -0.0109, -0.0543,  0.3298,\n",
      "          0.1822, -0.2275, -0.2900, -0.0932, -0.0859,  0.1376,  0.4553,  0.2669,\n",
      "         -0.1041, -0.4566,  0.4032,  0.4596,  0.0240,  0.2582,  0.4123,  0.0547,\n",
      "          0.2229,  0.0054, -0.2271,  0.2998,  0.3078,  0.1199,  0.0656, -0.4977,\n",
      "         -0.1922, -0.1638, -0.0580, -0.1722, -0.0207, -0.2017, -0.4377, -0.2301,\n",
      "          0.1247,  0.4458,  0.0637, -0.4035, -0.1888, -0.3908,  0.4974,  0.0636,\n",
      "         -0.3287,  0.3867, -0.3518, -0.0131,  0.0452, -0.4103, -0.0953,  0.3811,\n",
      "          0.1975,  0.4332,  0.1069,  0.0880, -0.3776,  0.1469, -0.3684,  0.2586,\n",
      "          0.0258,  0.2367, -0.4173, -0.4539,  0.0401,  0.3268,  0.4671,  0.3535,\n",
      "          0.3936,  0.4408,  0.4123,  0.1102, -0.4779,  0.2480,  0.2368, -0.1251,\n",
      "         -0.3720,  0.2231, -0.0660,  0.0564, -0.3418, -0.2669,  0.4652, -0.4518,\n",
      "         -0.3219,  0.2803, -0.4718, -0.3934, -0.3866, -0.2158,  0.4596,  0.4616,\n",
      "          0.3334,  0.4542, -0.2137, -0.0657, -0.2930,  0.2428,  0.0470, -0.3103],\n",
      "        [-0.3085,  0.2079, -0.0558,  0.4727, -0.0572,  0.4325, -0.4721, -0.3933,\n",
      "         -0.4037,  0.2223, -0.3074, -0.4287,  0.4854, -0.1563,  0.2378,  0.3082,\n",
      "          0.2669, -0.4955, -0.4475,  0.1592, -0.2937,  0.1021, -0.2588,  0.0658,\n",
      "          0.1464,  0.0969, -0.4747, -0.1736, -0.2305,  0.3631,  0.2602,  0.4455,\n",
      "         -0.4695, -0.4149,  0.2928, -0.2088, -0.2163,  0.2472,  0.2846, -0.3211,\n",
      "         -0.2504,  0.3590, -0.4542, -0.4578,  0.0834, -0.2651, -0.3716,  0.0151,\n",
      "         -0.0984,  0.0979,  0.0320,  0.0527, -0.4798, -0.0049,  0.3435, -0.2685,\n",
      "          0.0051, -0.3738, -0.0213,  0.3898,  0.4789,  0.2627, -0.2252,  0.1004,\n",
      "         -0.4895, -0.0907, -0.1520, -0.2988,  0.0696,  0.3762, -0.0568,  0.3029,\n",
      "          0.2704,  0.1733,  0.0607, -0.3022,  0.0958,  0.2246,  0.0864,  0.1957,\n",
      "         -0.1262,  0.0189, -0.3558,  0.3480, -0.4218,  0.3846,  0.2977, -0.2668,\n",
      "         -0.4719, -0.3136,  0.0346, -0.2221, -0.0843, -0.1978,  0.3260, -0.1030,\n",
      "          0.2973,  0.2319, -0.3665,  0.3388, -0.4193, -0.3237,  0.3003,  0.1727,\n",
      "          0.1312,  0.3687,  0.3775, -0.1518, -0.0341, -0.4558,  0.4252,  0.1452,\n",
      "          0.4464, -0.0587,  0.2010,  0.3056,  0.1037, -0.2781,  0.1119,  0.4075,\n",
      "          0.3024,  0.4253,  0.3235, -0.2391, -0.1235, -0.2415,  0.3376,  0.0750,\n",
      "          0.4636, -0.2067, -0.2724,  0.2813,  0.3083, -0.1645,  0.4072, -0.3367,\n",
      "         -0.4844,  0.4903, -0.2458,  0.3983,  0.4745,  0.1093,  0.3478, -0.3051,\n",
      "          0.2002, -0.0206,  0.2579, -0.2674, -0.3509,  0.4186, -0.1168,  0.3344,\n",
      "          0.4014, -0.1908,  0.1122, -0.2635, -0.2765, -0.4911,  0.3065,  0.2265,\n",
      "         -0.4627, -0.0526, -0.4488,  0.0989, -0.0009,  0.3884,  0.4852, -0.1694,\n",
      "         -0.1860, -0.0708, -0.4281,  0.1182, -0.2934,  0.0983, -0.2847,  0.1333,\n",
      "          0.3494,  0.2996, -0.4393,  0.2396,  0.0914, -0.1308,  0.4602,  0.1519,\n",
      "          0.0964,  0.2173, -0.3352,  0.2081, -0.3686,  0.4634,  0.1625,  0.1153],\n",
      "        [-0.4529, -0.1572, -0.0539,  0.3450,  0.4261,  0.2598,  0.2026, -0.0352,\n",
      "          0.2761, -0.3891,  0.3350, -0.2972, -0.0346, -0.0281, -0.1072,  0.4468,\n",
      "          0.0352,  0.2435, -0.1058,  0.3529,  0.0723,  0.2023,  0.3687, -0.4046,\n",
      "         -0.4893, -0.1057,  0.0078, -0.4062,  0.2501,  0.4469,  0.2588,  0.3636,\n",
      "         -0.4504, -0.4279, -0.2497, -0.4591, -0.4598, -0.1506,  0.0301,  0.1856,\n",
      "         -0.3996,  0.0477, -0.2218,  0.4408,  0.3175, -0.2819, -0.3891, -0.2715,\n",
      "         -0.0119, -0.1215,  0.1585,  0.4206, -0.1848,  0.1778,  0.4221, -0.1031,\n",
      "          0.1428,  0.2237, -0.3037, -0.0875,  0.3658,  0.3300,  0.1022, -0.1516,\n",
      "         -0.4768,  0.0470,  0.2139,  0.3245, -0.2838, -0.2326,  0.3793, -0.4275,\n",
      "          0.0624, -0.1127,  0.3912, -0.2025, -0.1955,  0.0486, -0.3604,  0.1436,\n",
      "         -0.2875, -0.3124, -0.1595, -0.0667, -0.0748, -0.3298, -0.3057,  0.2011,\n",
      "          0.1131, -0.0505, -0.2193,  0.2211,  0.3423, -0.1574, -0.4652,  0.2528,\n",
      "          0.1820,  0.1900, -0.2276, -0.0111, -0.4589, -0.4369,  0.2751,  0.2070,\n",
      "          0.2440, -0.3492,  0.1704, -0.0839, -0.4068,  0.1689, -0.2187, -0.2404,\n",
      "         -0.4594,  0.1789, -0.4587, -0.1004,  0.4327,  0.1572,  0.1019,  0.1318,\n",
      "          0.3062,  0.3306,  0.1911,  0.4272,  0.0892,  0.2553,  0.4253,  0.1659,\n",
      "         -0.0153, -0.3781,  0.4079,  0.0030,  0.4418, -0.4477,  0.2635, -0.3938,\n",
      "          0.0555, -0.3003, -0.2474, -0.3613,  0.0365,  0.0919,  0.0444, -0.0326,\n",
      "          0.1988, -0.0043,  0.4960, -0.2726,  0.2577, -0.1399,  0.0490, -0.2605,\n",
      "         -0.2334,  0.1966, -0.3514, -0.0269, -0.2524, -0.0522,  0.3142, -0.0619,\n",
      "         -0.2309,  0.3951, -0.2231,  0.4976, -0.0028, -0.4960, -0.3880,  0.3877,\n",
      "         -0.3430,  0.3044,  0.0261,  0.3276, -0.1741, -0.0334,  0.0502,  0.0558,\n",
      "         -0.2254,  0.2467,  0.3906, -0.3988, -0.2671,  0.3635, -0.2941, -0.4413,\n",
      "         -0.2717, -0.1500,  0.1175, -0.0211, -0.1650,  0.1372, -0.1467, -0.4682],\n",
      "        [ 0.1006,  0.1690, -0.2365, -0.2573, -0.3941, -0.1949,  0.1145,  0.4879,\n",
      "          0.3939, -0.4653,  0.3927, -0.3928, -0.1648, -0.3126, -0.2240, -0.2955,\n",
      "          0.4953,  0.0201, -0.4571, -0.3099,  0.1722, -0.3074, -0.2650, -0.0343,\n",
      "          0.2155,  0.1219,  0.4715,  0.1549,  0.0180, -0.3136, -0.1017,  0.0442,\n",
      "         -0.0807,  0.3326, -0.2793,  0.4693,  0.2938,  0.2537,  0.4737, -0.4222,\n",
      "         -0.1041,  0.0470, -0.1856,  0.1775, -0.1044,  0.3396,  0.3710,  0.1461,\n",
      "          0.3862,  0.0915,  0.0245, -0.4862,  0.2971, -0.4279, -0.0600, -0.3252,\n",
      "          0.0575,  0.0737, -0.1541,  0.0132, -0.3831,  0.1708,  0.1641,  0.3259,\n",
      "          0.2080, -0.2994,  0.3289, -0.1726,  0.1728, -0.4535,  0.2559,  0.0939,\n",
      "          0.1923, -0.4447,  0.4706,  0.1378,  0.3362,  0.3585, -0.4693,  0.0737,\n",
      "         -0.0180, -0.1533,  0.1602,  0.0328,  0.3808,  0.0183, -0.4225,  0.2609,\n",
      "         -0.2920, -0.3166,  0.0291,  0.4230,  0.3254, -0.1631,  0.0777,  0.0084,\n",
      "          0.3930,  0.0684,  0.3393, -0.4065,  0.0184,  0.1434, -0.4191, -0.2668,\n",
      "         -0.2583, -0.3938,  0.2692,  0.3379, -0.1610, -0.4425, -0.3757, -0.3587,\n",
      "         -0.4249,  0.2199,  0.3004,  0.0443,  0.3836,  0.0263,  0.2958, -0.1507,\n",
      "          0.0537, -0.0687,  0.1514, -0.4911,  0.0052,  0.0761, -0.1913,  0.1853,\n",
      "         -0.0124, -0.3541,  0.0277, -0.3630, -0.3104, -0.1769, -0.1923,  0.1557,\n",
      "          0.4131,  0.2811,  0.3978,  0.4198,  0.4174,  0.1364, -0.3672,  0.0243,\n",
      "         -0.0475, -0.2074, -0.3462,  0.3039,  0.0092, -0.2067,  0.4334, -0.2698,\n",
      "         -0.4175,  0.4882,  0.1428,  0.1868, -0.2379, -0.2610,  0.2044,  0.1492,\n",
      "         -0.4313, -0.2646,  0.0871, -0.2397,  0.1344,  0.3381, -0.0484,  0.2644,\n",
      "         -0.2736, -0.4856, -0.1567,  0.0605,  0.1655,  0.1458,  0.1922, -0.2275,\n",
      "          0.3972, -0.1485, -0.0114,  0.1299,  0.2137,  0.2654,  0.2137, -0.3936,\n",
      "         -0.4087,  0.1286,  0.3506, -0.1719, -0.0443, -0.4216, -0.0135,  0.2737]],\n",
      "       requires_grad=True)\n",
      "Wq: torch.Size([4, 128])\n",
      "tensor([[ 0.3301,  0.3833,  0.4804,  0.3799, -0.3784,  0.0511,  0.3683,  0.1008,\n",
      "         -0.2285, -0.0197,  0.0348,  0.1120, -0.3621, -0.2547,  0.4441, -0.4699,\n",
      "         -0.3608, -0.4284,  0.1076,  0.4837, -0.0746,  0.1340, -0.4201, -0.0809,\n",
      "          0.1141, -0.1077,  0.0644,  0.3932,  0.0016,  0.0923, -0.3204, -0.1327,\n",
      "         -0.3313, -0.1889, -0.1339,  0.0845,  0.1315, -0.0367, -0.3632, -0.3186,\n",
      "         -0.0542, -0.2293, -0.4923, -0.4534, -0.3555, -0.4473, -0.2461,  0.3808,\n",
      "         -0.2491, -0.0109, -0.3062,  0.1362, -0.3017,  0.0435,  0.1770,  0.4863,\n",
      "         -0.2845, -0.0166,  0.4380,  0.1111,  0.3862,  0.1065,  0.2000, -0.2471,\n",
      "         -0.0877, -0.4372,  0.4612,  0.0399,  0.2884,  0.0085, -0.3506,  0.0517,\n",
      "          0.2164, -0.1687,  0.3631,  0.2543, -0.0103,  0.4510,  0.3025, -0.0815,\n",
      "         -0.0690,  0.4011, -0.0612, -0.3017, -0.4315, -0.0644, -0.0030,  0.1530,\n",
      "         -0.3891, -0.2778, -0.2134, -0.0432,  0.4629, -0.0109, -0.0543,  0.3298,\n",
      "          0.1822, -0.2275, -0.2900, -0.0932, -0.0859,  0.1376,  0.4553,  0.2669,\n",
      "         -0.1041, -0.4566,  0.4032,  0.4596,  0.0240,  0.2582,  0.4123,  0.0547,\n",
      "          0.2229,  0.0054, -0.2271,  0.2998,  0.3078,  0.1199,  0.0656, -0.4977,\n",
      "         -0.1922, -0.1638, -0.0580, -0.1722, -0.0207, -0.2017, -0.4377, -0.2301],\n",
      "        [-0.3085,  0.2079, -0.0558,  0.4727, -0.0572,  0.4325, -0.4721, -0.3933,\n",
      "         -0.4037,  0.2223, -0.3074, -0.4287,  0.4854, -0.1563,  0.2378,  0.3082,\n",
      "          0.2669, -0.4955, -0.4475,  0.1592, -0.2937,  0.1021, -0.2588,  0.0658,\n",
      "          0.1464,  0.0969, -0.4747, -0.1736, -0.2305,  0.3631,  0.2602,  0.4455,\n",
      "         -0.4695, -0.4149,  0.2928, -0.2088, -0.2163,  0.2472,  0.2846, -0.3211,\n",
      "         -0.2504,  0.3590, -0.4542, -0.4578,  0.0834, -0.2651, -0.3716,  0.0151,\n",
      "         -0.0984,  0.0979,  0.0320,  0.0527, -0.4798, -0.0049,  0.3435, -0.2685,\n",
      "          0.0051, -0.3738, -0.0213,  0.3898,  0.4789,  0.2627, -0.2252,  0.1004,\n",
      "         -0.4895, -0.0907, -0.1520, -0.2988,  0.0696,  0.3762, -0.0568,  0.3029,\n",
      "          0.2704,  0.1733,  0.0607, -0.3022,  0.0958,  0.2246,  0.0864,  0.1957,\n",
      "         -0.1262,  0.0189, -0.3558,  0.3480, -0.4218,  0.3846,  0.2977, -0.2668,\n",
      "         -0.4719, -0.3136,  0.0346, -0.2221, -0.0843, -0.1978,  0.3260, -0.1030,\n",
      "          0.2973,  0.2319, -0.3665,  0.3388, -0.4193, -0.3237,  0.3003,  0.1727,\n",
      "          0.1312,  0.3687,  0.3775, -0.1518, -0.0341, -0.4558,  0.4252,  0.1452,\n",
      "          0.4464, -0.0587,  0.2010,  0.3056,  0.1037, -0.2781,  0.1119,  0.4075,\n",
      "          0.3024,  0.4253,  0.3235, -0.2391, -0.1235, -0.2415,  0.3376,  0.0750],\n",
      "        [-0.4529, -0.1572, -0.0539,  0.3450,  0.4261,  0.2598,  0.2026, -0.0352,\n",
      "          0.2761, -0.3891,  0.3350, -0.2972, -0.0346, -0.0281, -0.1072,  0.4468,\n",
      "          0.0352,  0.2435, -0.1058,  0.3529,  0.0723,  0.2023,  0.3687, -0.4046,\n",
      "         -0.4893, -0.1057,  0.0078, -0.4062,  0.2501,  0.4469,  0.2588,  0.3636,\n",
      "         -0.4504, -0.4279, -0.2497, -0.4591, -0.4598, -0.1506,  0.0301,  0.1856,\n",
      "         -0.3996,  0.0477, -0.2218,  0.4408,  0.3175, -0.2819, -0.3891, -0.2715,\n",
      "         -0.0119, -0.1215,  0.1585,  0.4206, -0.1848,  0.1778,  0.4221, -0.1031,\n",
      "          0.1428,  0.2237, -0.3037, -0.0875,  0.3658,  0.3300,  0.1022, -0.1516,\n",
      "         -0.4768,  0.0470,  0.2139,  0.3245, -0.2838, -0.2326,  0.3793, -0.4275,\n",
      "          0.0624, -0.1127,  0.3912, -0.2025, -0.1955,  0.0486, -0.3604,  0.1436,\n",
      "         -0.2875, -0.3124, -0.1595, -0.0667, -0.0748, -0.3298, -0.3057,  0.2011,\n",
      "          0.1131, -0.0505, -0.2193,  0.2211,  0.3423, -0.1574, -0.4652,  0.2528,\n",
      "          0.1820,  0.1900, -0.2276, -0.0111, -0.4589, -0.4369,  0.2751,  0.2070,\n",
      "          0.2440, -0.3492,  0.1704, -0.0839, -0.4068,  0.1689, -0.2187, -0.2404,\n",
      "         -0.4594,  0.1789, -0.4587, -0.1004,  0.4327,  0.1572,  0.1019,  0.1318,\n",
      "          0.3062,  0.3306,  0.1911,  0.4272,  0.0892,  0.2553,  0.4253,  0.1659],\n",
      "        [ 0.1006,  0.1690, -0.2365, -0.2573, -0.3941, -0.1949,  0.1145,  0.4879,\n",
      "          0.3939, -0.4653,  0.3927, -0.3928, -0.1648, -0.3126, -0.2240, -0.2955,\n",
      "          0.4953,  0.0201, -0.4571, -0.3099,  0.1722, -0.3074, -0.2650, -0.0343,\n",
      "          0.2155,  0.1219,  0.4715,  0.1549,  0.0180, -0.3136, -0.1017,  0.0442,\n",
      "         -0.0807,  0.3326, -0.2793,  0.4693,  0.2938,  0.2537,  0.4737, -0.4222,\n",
      "         -0.1041,  0.0470, -0.1856,  0.1775, -0.1044,  0.3396,  0.3710,  0.1461,\n",
      "          0.3862,  0.0915,  0.0245, -0.4862,  0.2971, -0.4279, -0.0600, -0.3252,\n",
      "          0.0575,  0.0737, -0.1541,  0.0132, -0.3831,  0.1708,  0.1641,  0.3259,\n",
      "          0.2080, -0.2994,  0.3289, -0.1726,  0.1728, -0.4535,  0.2559,  0.0939,\n",
      "          0.1923, -0.4447,  0.4706,  0.1378,  0.3362,  0.3585, -0.4693,  0.0737,\n",
      "         -0.0180, -0.1533,  0.1602,  0.0328,  0.3808,  0.0183, -0.4225,  0.2609,\n",
      "         -0.2920, -0.3166,  0.0291,  0.4230,  0.3254, -0.1631,  0.0777,  0.0084,\n",
      "          0.3930,  0.0684,  0.3393, -0.4065,  0.0184,  0.1434, -0.4191, -0.2668,\n",
      "         -0.2583, -0.3938,  0.2692,  0.3379, -0.1610, -0.4425, -0.3757, -0.3587,\n",
      "         -0.4249,  0.2199,  0.3004,  0.0443,  0.3836,  0.0263,  0.2958, -0.1507,\n",
      "          0.0537, -0.0687,  0.1514, -0.4911,  0.0052,  0.0761, -0.1913,  0.1853]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wk: torch.Size([4, 32])\n",
      "tensor([[ 0.1247,  0.4458,  0.0637, -0.4035, -0.1888, -0.3908,  0.4974,  0.0636,\n",
      "         -0.3287,  0.3867, -0.3518, -0.0131,  0.0452, -0.4103, -0.0953,  0.3811,\n",
      "          0.1975,  0.4332,  0.1069,  0.0880, -0.3776,  0.1469, -0.3684,  0.2586,\n",
      "          0.0258,  0.2367, -0.4173, -0.4539,  0.0401,  0.3268,  0.4671,  0.3535],\n",
      "        [ 0.4636, -0.2067, -0.2724,  0.2813,  0.3083, -0.1645,  0.4072, -0.3367,\n",
      "         -0.4844,  0.4903, -0.2458,  0.3983,  0.4745,  0.1093,  0.3478, -0.3051,\n",
      "          0.2002, -0.0206,  0.2579, -0.2674, -0.3509,  0.4186, -0.1168,  0.3344,\n",
      "          0.4014, -0.1908,  0.1122, -0.2635, -0.2765, -0.4911,  0.3065,  0.2265],\n",
      "        [-0.0153, -0.3781,  0.4079,  0.0030,  0.4418, -0.4477,  0.2635, -0.3938,\n",
      "          0.0555, -0.3003, -0.2474, -0.3613,  0.0365,  0.0919,  0.0444, -0.0326,\n",
      "          0.1988, -0.0043,  0.4960, -0.2726,  0.2577, -0.1399,  0.0490, -0.2605,\n",
      "         -0.2334,  0.1966, -0.3514, -0.0269, -0.2524, -0.0522,  0.3142, -0.0619],\n",
      "        [-0.0124, -0.3541,  0.0277, -0.3630, -0.3104, -0.1769, -0.1923,  0.1557,\n",
      "          0.4131,  0.2811,  0.3978,  0.4198,  0.4174,  0.1364, -0.3672,  0.0243,\n",
      "         -0.0475, -0.2074, -0.3462,  0.3039,  0.0092, -0.2067,  0.4334, -0.2698,\n",
      "         -0.4175,  0.4882,  0.1428,  0.1868, -0.2379, -0.2610,  0.2044,  0.1492]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wv: torch.Size([4, 32])\n",
      "tensor([[ 0.3936,  0.4408,  0.4123,  0.1102, -0.4779,  0.2480,  0.2368, -0.1251,\n",
      "         -0.3720,  0.2231, -0.0660,  0.0564, -0.3418, -0.2669,  0.4652, -0.4518,\n",
      "         -0.3219,  0.2803, -0.4718, -0.3934, -0.3866, -0.2158,  0.4596,  0.4616,\n",
      "          0.3334,  0.4542, -0.2137, -0.0657, -0.2930,  0.2428,  0.0470, -0.3103],\n",
      "        [-0.4627, -0.0526, -0.4488,  0.0989, -0.0009,  0.3884,  0.4852, -0.1694,\n",
      "         -0.1860, -0.0708, -0.4281,  0.1182, -0.2934,  0.0983, -0.2847,  0.1333,\n",
      "          0.3494,  0.2996, -0.4393,  0.2396,  0.0914, -0.1308,  0.4602,  0.1519,\n",
      "          0.0964,  0.2173, -0.3352,  0.2081, -0.3686,  0.4634,  0.1625,  0.1153],\n",
      "        [-0.2309,  0.3951, -0.2231,  0.4976, -0.0028, -0.4960, -0.3880,  0.3877,\n",
      "         -0.3430,  0.3044,  0.0261,  0.3276, -0.1741, -0.0334,  0.0502,  0.0558,\n",
      "         -0.2254,  0.2467,  0.3906, -0.3988, -0.2671,  0.3635, -0.2941, -0.4413,\n",
      "         -0.2717, -0.1500,  0.1175, -0.0211, -0.1650,  0.1372, -0.1467, -0.4682],\n",
      "        [-0.4313, -0.2646,  0.0871, -0.2397,  0.1344,  0.3381, -0.0484,  0.2644,\n",
      "         -0.2736, -0.4856, -0.1567,  0.0605,  0.1655,  0.1458,  0.1922, -0.2275,\n",
      "          0.3972, -0.1485, -0.0114,  0.1299,  0.2137,  0.2654,  0.2137, -0.3936,\n",
      "         -0.4087,  0.1286,  0.3506, -0.1719, -0.0443, -0.4216, -0.0135,  0.2737]],\n",
      "       grad_fn=<SplitWithSizesBackward0>)\n",
      "Wq spliced: torch.Size([2, 64])\n",
      "tensor([[ 0.0352,  0.2435, -0.1058,  0.3529,  0.0723,  0.2023,  0.3687, -0.4046,\n",
      "         -0.4893, -0.1057,  0.0078, -0.4062,  0.2501,  0.4469,  0.2588,  0.3636,\n",
      "         -0.0119, -0.1215,  0.1585,  0.4206, -0.1848,  0.1778,  0.4221, -0.1031,\n",
      "          0.1428,  0.2237, -0.3037, -0.0875,  0.3658,  0.3300,  0.1022, -0.1516,\n",
      "         -0.2875, -0.3124, -0.1595, -0.0667, -0.0748, -0.3298, -0.3057,  0.2011,\n",
      "          0.1131, -0.0505, -0.2193,  0.2211,  0.3423, -0.1574, -0.4652,  0.2528,\n",
      "         -0.4594,  0.1789, -0.4587, -0.1004,  0.4327,  0.1572,  0.1019,  0.1318,\n",
      "          0.3062,  0.3306,  0.1911,  0.4272,  0.0892,  0.2553,  0.4253,  0.1659],\n",
      "        [ 0.4953,  0.0201, -0.4571, -0.3099,  0.1722, -0.3074, -0.2650, -0.0343,\n",
      "          0.2155,  0.1219,  0.4715,  0.1549,  0.0180, -0.3136, -0.1017,  0.0442,\n",
      "          0.3862,  0.0915,  0.0245, -0.4862,  0.2971, -0.4279, -0.0600, -0.3252,\n",
      "          0.0575,  0.0737, -0.1541,  0.0132, -0.3831,  0.1708,  0.1641,  0.3259,\n",
      "         -0.0180, -0.1533,  0.1602,  0.0328,  0.3808,  0.0183, -0.4225,  0.2609,\n",
      "         -0.2920, -0.3166,  0.0291,  0.4230,  0.3254, -0.1631,  0.0777,  0.0084,\n",
      "         -0.4249,  0.2199,  0.3004,  0.0443,  0.3836,  0.0263,  0.2958, -0.1507,\n",
      "          0.0537, -0.0687,  0.1514, -0.4911,  0.0052,  0.0761, -0.1913,  0.1853]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wk spliced: torch.Size([2, 16])\n",
      "tensor([[ 0.1988, -0.0043,  0.4960, -0.2726,  0.2577, -0.1399,  0.0490, -0.2605,\n",
      "         -0.2334,  0.1966, -0.3514, -0.0269, -0.2524, -0.0522,  0.3142, -0.0619],\n",
      "        [-0.0475, -0.2074, -0.3462,  0.3039,  0.0092, -0.2067,  0.4334, -0.2698,\n",
      "         -0.4175,  0.4882,  0.1428,  0.1868, -0.2379, -0.2610,  0.2044,  0.1492]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wv spliced: torch.Size([2, 16])\n",
      "tensor([[-0.2254,  0.2467,  0.3906, -0.3988, -0.2671,  0.3635, -0.2941, -0.4413,\n",
      "         -0.2717, -0.1500,  0.1175, -0.0211, -0.1650,  0.1372, -0.1467, -0.4682],\n",
      "        [ 0.3972, -0.1485, -0.0114,  0.1299,  0.2137,  0.2654,  0.2137, -0.3936,\n",
      "         -0.4087,  0.1286,  0.3506, -0.1719, -0.0443, -0.4216, -0.0135,  0.2737]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "Wqkv_spliced: torch.Size([2, 96])\n",
      "tensor([[ 0.0352,  0.2435, -0.1058,  0.3529,  0.0723,  0.2023,  0.3687, -0.4046,\n",
      "         -0.4893, -0.1057,  0.0078, -0.4062,  0.2501,  0.4469,  0.2588,  0.3636,\n",
      "         -0.0119, -0.1215,  0.1585,  0.4206, -0.1848,  0.1778,  0.4221, -0.1031,\n",
      "          0.1428,  0.2237, -0.3037, -0.0875,  0.3658,  0.3300,  0.1022, -0.1516,\n",
      "         -0.2875, -0.3124, -0.1595, -0.0667, -0.0748, -0.3298, -0.3057,  0.2011,\n",
      "          0.1131, -0.0505, -0.2193,  0.2211,  0.3423, -0.1574, -0.4652,  0.2528,\n",
      "         -0.4594,  0.1789, -0.4587, -0.1004,  0.4327,  0.1572,  0.1019,  0.1318,\n",
      "          0.3062,  0.3306,  0.1911,  0.4272,  0.0892,  0.2553,  0.4253,  0.1659,\n",
      "          0.1988, -0.0043,  0.4960, -0.2726,  0.2577, -0.1399,  0.0490, -0.2605,\n",
      "         -0.2334,  0.1966, -0.3514, -0.0269, -0.2524, -0.0522,  0.3142, -0.0619,\n",
      "         -0.2254,  0.2467,  0.3906, -0.3988, -0.2671,  0.3635, -0.2941, -0.4413,\n",
      "         -0.2717, -0.1500,  0.1175, -0.0211, -0.1650,  0.1372, -0.1467, -0.4682],\n",
      "        [ 0.4953,  0.0201, -0.4571, -0.3099,  0.1722, -0.3074, -0.2650, -0.0343,\n",
      "          0.2155,  0.1219,  0.4715,  0.1549,  0.0180, -0.3136, -0.1017,  0.0442,\n",
      "          0.3862,  0.0915,  0.0245, -0.4862,  0.2971, -0.4279, -0.0600, -0.3252,\n",
      "          0.0575,  0.0737, -0.1541,  0.0132, -0.3831,  0.1708,  0.1641,  0.3259,\n",
      "         -0.0180, -0.1533,  0.1602,  0.0328,  0.3808,  0.0183, -0.4225,  0.2609,\n",
      "         -0.2920, -0.3166,  0.0291,  0.4230,  0.3254, -0.1631,  0.0777,  0.0084,\n",
      "         -0.4249,  0.2199,  0.3004,  0.0443,  0.3836,  0.0263,  0.2958, -0.1507,\n",
      "          0.0537, -0.0687,  0.1514, -0.4911,  0.0052,  0.0761, -0.1913,  0.1853,\n",
      "         -0.0475, -0.2074, -0.3462,  0.3039,  0.0092, -0.2067,  0.4334, -0.2698,\n",
      "         -0.4175,  0.4882,  0.1428,  0.1868, -0.2379, -0.2610,  0.2044,  0.1492,\n",
      "          0.3972, -0.1485, -0.0114,  0.1299,  0.2137,  0.2654,  0.2137, -0.3936,\n",
      "         -0.4087,  0.1286,  0.3506, -0.1719, -0.0443, -0.4216, -0.0135,  0.2737]],\n",
      "       grad_fn=<CatBackward0>)\n",
      "xqkv: torch.Size([1, 3, 96])\n",
      "tensor([[[-0.6561, -0.1711,  0.6492,  0.1869, -0.2639,  0.2735,  0.1198,\n",
      "           0.2854,  0.0155, -0.0932, -0.6092,  0.0437, -0.1723,  0.1355,\n",
      "          -0.0240, -0.2736, -0.4881, -0.0449, -0.1260,  0.3724, -0.2707,\n",
      "           0.4426, -0.1749,  0.4785, -0.1589, -0.2280,  0.3787,  0.0352,\n",
      "           0.2730, -0.4158, -0.2713, -0.3274,  0.1946,  0.3830, -0.1102,\n",
      "          -0.0023, -0.4437,  0.1732,  0.7241, -0.4545,  0.3070,  0.4362,\n",
      "           0.0935, -0.6742, -0.6215,  0.3030,  0.1779, -0.1615,  0.8189,\n",
      "          -0.3887, -0.1116,  0.0031, -0.7500, -0.1276, -0.4401,  0.1146,\n",
      "          -0.2514, -0.1091, -0.3081,  0.3748, -0.0599, -0.2499, -0.0084,\n",
      "          -0.3366, -0.0577,  0.2685,  0.1480, -0.2271, -0.1654,  0.3485,\n",
      "          -0.5850,  0.5014,  0.6746, -0.7432,  0.0265, -0.2235,  0.4556,\n",
      "           0.3658, -0.4496, -0.1544, -0.3748,  0.0432, -0.2184,  0.0714,\n",
      "          -0.1147, -0.5571, -0.0986,  0.7679,  0.6861, -0.0755, -0.5196,\n",
      "           0.2330,  0.1552,  0.4587,  0.1048, -0.0717],\n",
      "         [ 0.7012,  0.0343, -0.6489, -0.4298,  0.2452, -0.4298, -0.3658,\n",
      "          -0.0583,  0.2930,  0.1698,  0.6669,  0.2092,  0.0315, -0.4326,\n",
      "          -0.1376,  0.0713,  0.5458,  0.1265,  0.0385, -0.6773,  0.4157,\n",
      "          -0.6007, -0.0747, -0.4623,  0.0847,  0.1096, -0.2252,  0.0166,\n",
      "          -0.5329,  0.2494,  0.2344,  0.4571, -0.0324, -0.2243,  0.2227,\n",
      "           0.0448,  0.5367,  0.0180, -0.6047,  0.3738, -0.4102, -0.4490,\n",
      "           0.0358,  0.6034,  0.4684, -0.2344,  0.0987,  0.0179, -0.6119,\n",
      "           0.3153,  0.4138,  0.0602,  0.5528,  0.0410,  0.4208, -0.2099,\n",
      "           0.0832, -0.0892,  0.2187, -0.6841,  0.0095,  0.1138, -0.2603,\n",
      "           0.2660, -0.0624, -0.2933, -0.4776,  0.4232,  0.0191, -0.2956,\n",
      "           0.6140, -0.3878, -0.5960,  0.6950,  0.1935,  0.2635, -0.3425,\n",
      "          -0.3703,  0.2966,  0.2095,  0.5562, -0.2040, -0.0067,  0.1740,\n",
      "           0.2958,  0.3840,  0.2952, -0.5672, -0.5844,  0.1783,  0.4985,\n",
      "          -0.2436, -0.0666, -0.5928, -0.0226,  0.3758],\n",
      "         [ 0.3211,  0.3278, -0.3917,  0.2866,  0.1897,  0.0921,  0.3321,\n",
      "          -0.5452, -0.5165, -0.0697,  0.2722, -0.4421,  0.3353,  0.4068,\n",
      "           0.2800,  0.4974,  0.1992, -0.1071,  0.2198,  0.2768, -0.0752,\n",
      "          -0.0066,  0.5156, -0.3149,  0.2177,  0.3319, -0.4805, -0.1064,\n",
      "           0.2628,  0.5240,  0.2241, -0.0160, -0.3839, -0.4915, -0.1184,\n",
      "          -0.0685,  0.1144, -0.4187, -0.6324,  0.4065, -0.0153, -0.2417,\n",
      "          -0.2690,  0.5226,  0.6260, -0.2953, -0.5617,  0.3333, -0.8336,\n",
      "           0.3549, -0.4295, -0.1059,  0.7759,  0.2191,  0.2969,  0.0876,\n",
      "           0.4279,  0.3917,  0.3327,  0.2825,  0.1189,  0.3743,  0.4467,\n",
      "           0.3187,  0.2321, -0.1209,  0.4525, -0.1855,  0.3402, -0.2968,\n",
      "           0.3046, -0.4887, -0.5356,  0.5270, -0.3775,  0.0688, -0.4605,\n",
      "          -0.2129,  0.5223,  0.0025, -0.0724,  0.2383,  0.5016, -0.4464,\n",
      "          -0.2285,  0.6203, -0.2636, -0.7926, -0.5805, -0.1236,  0.3477,\n",
      "          -0.1230, -0.2392, -0.0559, -0.1982, -0.4567]]],\n",
      "       grad_fn=<UnsafeViewBackward0>)\n",
      "xq: torch.Size([1, 3, 64])\n",
      "tensor([[[-0.6561, -0.1711,  0.6492,  0.1869, -0.2639,  0.2735,  0.1198,\n",
      "           0.2854,  0.0155, -0.0932, -0.6092,  0.0437, -0.1723,  0.1355,\n",
      "          -0.0240, -0.2736, -0.4881, -0.0449, -0.1260,  0.3724, -0.2707,\n",
      "           0.4426, -0.1749,  0.4785, -0.1589, -0.2280,  0.3787,  0.0352,\n",
      "           0.2730, -0.4158, -0.2713, -0.3274,  0.1946,  0.3830, -0.1102,\n",
      "          -0.0023, -0.4437,  0.1732,  0.7241, -0.4545,  0.3070,  0.4362,\n",
      "           0.0935, -0.6742, -0.6215,  0.3030,  0.1779, -0.1615,  0.8189,\n",
      "          -0.3887, -0.1116,  0.0031, -0.7500, -0.1276, -0.4401,  0.1146,\n",
      "          -0.2514, -0.1091, -0.3081,  0.3748, -0.0599, -0.2499, -0.0084,\n",
      "          -0.3366],\n",
      "         [ 0.7012,  0.0343, -0.6489, -0.4298,  0.2452, -0.4298, -0.3658,\n",
      "          -0.0583,  0.2930,  0.1698,  0.6669,  0.2092,  0.0315, -0.4326,\n",
      "          -0.1376,  0.0713,  0.5458,  0.1265,  0.0385, -0.6773,  0.4157,\n",
      "          -0.6007, -0.0747, -0.4623,  0.0847,  0.1096, -0.2252,  0.0166,\n",
      "          -0.5329,  0.2494,  0.2344,  0.4571, -0.0324, -0.2243,  0.2227,\n",
      "           0.0448,  0.5367,  0.0180, -0.6047,  0.3738, -0.4102, -0.4490,\n",
      "           0.0358,  0.6034,  0.4684, -0.2344,  0.0987,  0.0179, -0.6119,\n",
      "           0.3153,  0.4138,  0.0602,  0.5528,  0.0410,  0.4208, -0.2099,\n",
      "           0.0832, -0.0892,  0.2187, -0.6841,  0.0095,  0.1138, -0.2603,\n",
      "           0.2660],\n",
      "         [ 0.3211,  0.3278, -0.3917,  0.2866,  0.1897,  0.0921,  0.3321,\n",
      "          -0.5452, -0.5165, -0.0697,  0.2722, -0.4421,  0.3353,  0.4068,\n",
      "           0.2800,  0.4974,  0.1992, -0.1071,  0.2198,  0.2768, -0.0752,\n",
      "          -0.0066,  0.5156, -0.3149,  0.2177,  0.3319, -0.4805, -0.1064,\n",
      "           0.2628,  0.5240,  0.2241, -0.0160, -0.3839, -0.4915, -0.1184,\n",
      "          -0.0685,  0.1144, -0.4187, -0.6324,  0.4065, -0.0153, -0.2417,\n",
      "          -0.2690,  0.5226,  0.6260, -0.2953, -0.5617,  0.3333, -0.8336,\n",
      "           0.3549, -0.4295, -0.1059,  0.7759,  0.2191,  0.2969,  0.0876,\n",
      "           0.4279,  0.3917,  0.3327,  0.2825,  0.1189,  0.3743,  0.4467,\n",
      "           0.3187]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xk: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.0577,  0.2685,  0.1480, -0.2271, -0.1654,  0.3485, -0.5850,\n",
      "           0.5014,  0.6746, -0.7432,  0.0265, -0.2235,  0.4556,  0.3658,\n",
      "          -0.4496, -0.1544],\n",
      "         [-0.0624, -0.2933, -0.4776,  0.4232,  0.0191, -0.2956,  0.6140,\n",
      "          -0.3878, -0.5960,  0.6950,  0.1935,  0.2635, -0.3425, -0.3703,\n",
      "           0.2966,  0.2095],\n",
      "         [ 0.2321, -0.1209,  0.4525, -0.1855,  0.3402, -0.2968,  0.3046,\n",
      "          -0.4887, -0.5356,  0.5270, -0.3775,  0.0688, -0.4605, -0.2129,\n",
      "           0.5223,  0.0025]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xv: torch.Size([1, 3, 16])\n",
      "tensor([[[-0.3748,  0.0432, -0.2184,  0.0714, -0.1147, -0.5571, -0.0986,\n",
      "           0.7679,  0.6861, -0.0755, -0.5196,  0.2330,  0.1552,  0.4587,\n",
      "           0.1048, -0.0717],\n",
      "         [ 0.5562, -0.2040, -0.0067,  0.1740,  0.2958,  0.3840,  0.2952,\n",
      "          -0.5672, -0.5844,  0.1783,  0.4985, -0.2436, -0.0666, -0.5928,\n",
      "          -0.0226,  0.3758],\n",
      "         [-0.0724,  0.2383,  0.5016, -0.4464, -0.2285,  0.6203, -0.2636,\n",
      "          -0.7926, -0.5805, -0.1236,  0.3477, -0.1230, -0.2392, -0.0559,\n",
      "          -0.1982, -0.4567]]], grad_fn=<SplitWithSizesBackward0>)\n",
      "xq reshaped: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[-0.6561, -0.1711,  0.6492,  0.1869, -0.2639,  0.2735,  0.1198,\n",
      "            0.2854,  0.0155, -0.0932, -0.6092,  0.0437, -0.1723,  0.1355,\n",
      "           -0.0240, -0.2736],\n",
      "          [-0.4881, -0.0449, -0.1260,  0.3724, -0.2707,  0.4426, -0.1749,\n",
      "            0.4785, -0.1589, -0.2280,  0.3787,  0.0352,  0.2730, -0.4158,\n",
      "           -0.2713, -0.3274],\n",
      "          [ 0.1946,  0.3830, -0.1102, -0.0023, -0.4437,  0.1732,  0.7241,\n",
      "           -0.4545,  0.3070,  0.4362,  0.0935, -0.6742, -0.6215,  0.3030,\n",
      "            0.1779, -0.1615],\n",
      "          [ 0.8189, -0.3887, -0.1116,  0.0031, -0.7500, -0.1276, -0.4401,\n",
      "            0.1146, -0.2514, -0.1091, -0.3081,  0.3748, -0.0599, -0.2499,\n",
      "           -0.0084, -0.3366]],\n",
      "\n",
      "         [[ 0.7012,  0.0343, -0.6489, -0.4298,  0.2452, -0.4298, -0.3658,\n",
      "           -0.0583,  0.2930,  0.1698,  0.6669,  0.2092,  0.0315, -0.4326,\n",
      "           -0.1376,  0.0713],\n",
      "          [ 0.5458,  0.1265,  0.0385, -0.6773,  0.4157, -0.6007, -0.0747,\n",
      "           -0.4623,  0.0847,  0.1096, -0.2252,  0.0166, -0.5329,  0.2494,\n",
      "            0.2344,  0.4571],\n",
      "          [-0.0324, -0.2243,  0.2227,  0.0448,  0.5367,  0.0180, -0.6047,\n",
      "            0.3738, -0.4102, -0.4490,  0.0358,  0.6034,  0.4684, -0.2344,\n",
      "            0.0987,  0.0179],\n",
      "          [-0.6119,  0.3153,  0.4138,  0.0602,  0.5528,  0.0410,  0.4208,\n",
      "           -0.2099,  0.0832, -0.0892,  0.2187, -0.6841,  0.0095,  0.1138,\n",
      "           -0.2603,  0.2660]],\n",
      "\n",
      "         [[ 0.3211,  0.3278, -0.3917,  0.2866,  0.1897,  0.0921,  0.3321,\n",
      "           -0.5452, -0.5165, -0.0697,  0.2722, -0.4421,  0.3353,  0.4068,\n",
      "            0.2800,  0.4974],\n",
      "          [ 0.1992, -0.1071,  0.2198,  0.2768, -0.0752, -0.0066,  0.5156,\n",
      "           -0.3149,  0.2177,  0.3319, -0.4805, -0.1064,  0.2628,  0.5240,\n",
      "            0.2241, -0.0160],\n",
      "          [-0.3839, -0.4915, -0.1184, -0.0685,  0.1144, -0.4187, -0.6324,\n",
      "            0.4065, -0.0153, -0.2417, -0.2690,  0.5226,  0.6260, -0.2953,\n",
      "           -0.5617,  0.3333],\n",
      "          [-0.8336,  0.3549, -0.4295, -0.1059,  0.7759,  0.2191,  0.2969,\n",
      "            0.0876,  0.4279,  0.3917,  0.3327,  0.2825,  0.1189,  0.3743,\n",
      "            0.4467,  0.3187]]]], grad_fn=<ViewBackward0>)\n",
      "xk reshaped: torch.Size([1, 3, 1, 16])\n",
      "tensor([[[[-0.0577,  0.2685,  0.1480, -0.2271, -0.1654,  0.3485, -0.5850,\n",
      "            0.5014,  0.6746, -0.7432,  0.0265, -0.2235,  0.4556,  0.3658,\n",
      "           -0.4496, -0.1544]],\n",
      "\n",
      "         [[-0.0624, -0.2933, -0.4776,  0.4232,  0.0191, -0.2956,  0.6140,\n",
      "           -0.3878, -0.5960,  0.6950,  0.1935,  0.2635, -0.3425, -0.3703,\n",
      "            0.2966,  0.2095]],\n",
      "\n",
      "         [[ 0.2321, -0.1209,  0.4525, -0.1855,  0.3402, -0.2968,  0.3046,\n",
      "           -0.4887, -0.5356,  0.5270, -0.3775,  0.0688, -0.4605, -0.2129,\n",
      "            0.5223,  0.0025]]]], grad_fn=<ViewBackward0>)\n",
      "xv reshaped: torch.Size([1, 3, 1, 16])\n",
      "tensor([[[[-0.3748,  0.0432, -0.2184,  0.0714, -0.1147, -0.5571, -0.0986,\n",
      "            0.7679,  0.6861, -0.0755, -0.5196,  0.2330,  0.1552,  0.4587,\n",
      "            0.1048, -0.0717]],\n",
      "\n",
      "         [[ 0.5562, -0.2040, -0.0067,  0.1740,  0.2958,  0.3840,  0.2952,\n",
      "           -0.5672, -0.5844,  0.1783,  0.4985, -0.2436, -0.0666, -0.5928,\n",
      "           -0.0226,  0.3758]],\n",
      "\n",
      "         [[-0.0724,  0.2383,  0.5016, -0.4464, -0.2285,  0.6203, -0.2636,\n",
      "           -0.7926, -0.5805, -0.1236,  0.3477, -0.1230, -0.2392, -0.0559,\n",
      "           -0.1982, -0.4567]]]], grad_fn=<ViewBackward0>)\n",
      "rotated xq: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[-0.6561, -0.1711,  0.6492,  0.1869, -0.2639,  0.2735,  0.1198,\n",
      "            0.2854,  0.0155, -0.0932, -0.6092,  0.0437, -0.1723,  0.1355,\n",
      "           -0.0240, -0.2736],\n",
      "          [-0.4881, -0.0449, -0.1260,  0.3724, -0.2707,  0.4426, -0.1749,\n",
      "            0.4785, -0.1589, -0.2280,  0.3787,  0.0352,  0.2730, -0.4158,\n",
      "           -0.2713, -0.3274],\n",
      "          [ 0.1946,  0.3830, -0.1102, -0.0023, -0.4437,  0.1732,  0.7241,\n",
      "           -0.4545,  0.3070,  0.4362,  0.0935, -0.6742, -0.6215,  0.3030,\n",
      "            0.1779, -0.1615],\n",
      "          [ 0.8189, -0.3887, -0.1116,  0.0031, -0.7500, -0.1276, -0.4401,\n",
      "            0.1146, -0.2514, -0.1091, -0.3081,  0.3748, -0.0599, -0.2499,\n",
      "           -0.0084, -0.3366]],\n",
      "\n",
      "         [[ 0.1324, -0.0615, -0.8241, -0.4600,  0.2409, -0.4048, -0.3613,\n",
      "           -0.0595,  0.7484,  0.1620,  0.4320,  0.1299,  0.0558, -0.4561,\n",
      "           -0.1491,  0.0702],\n",
      "          [ 0.2236,  0.0486,  0.1066, -0.6696,  0.4668, -0.6138, -0.0821,\n",
      "           -0.4703,  0.5051,  0.1602, -0.2020, -0.1035, -0.4887,  0.2153,\n",
      "            0.2320,  0.4488],\n",
      "          [ 0.3277,  0.0496,  0.2005, -0.0627,  0.4872,  0.0311, -0.6075,\n",
      "            0.3734, -0.2489, -0.4994,  0.1033,  0.6018,  0.5196, -0.2330,\n",
      "            0.0795,  0.0246],\n",
      "          [-0.4007,  0.3143,  0.3252,  0.1803,  0.5491,  0.0346,  0.4288,\n",
      "           -0.2146, -0.4699,  0.0927,  0.3365, -0.6626,  0.0646,  0.1159,\n",
      "           -0.2468,  0.2623]],\n",
      "\n",
      "         [[ 0.3360,  0.2043, -0.4768,  0.4226,  0.1193,  0.0459,  0.3138,\n",
      "           -0.5626,  0.5069,  0.2657, -0.0120, -0.3147,  0.3663,  0.4146,\n",
      "            0.3004,  0.4777],\n",
      "          [-0.2808, -0.3456,  0.4613,  0.2965, -0.1259, -0.0654,  0.5004,\n",
      "           -0.3141,  0.0905,  0.0466, -0.2576, -0.0034,  0.2426,  0.5200,\n",
      "            0.2563, -0.0272],\n",
      "          [ 0.1737,  0.0060,  0.0635, -0.2462, -0.0122, -0.3829, -0.5956,\n",
      "            0.3944, -0.3427, -0.5477, -0.2870,  0.4660,  0.6363, -0.3404,\n",
      "           -0.6005,  0.3476],\n",
      "          [-0.0422, -0.2002, -0.5431, -0.1976,  0.7368,  0.1757,  0.2681,\n",
      "            0.0762, -0.9361,  0.4891,  0.0145,  0.2280,  0.2707,  0.3965,\n",
      "            0.4646,  0.3217]]]], grad_fn=<TransposeBackward0>)\n",
      "rotated xk: torch.Size([1, 3, 1, 16])\n",
      "tensor([[[[-5.7660e-02,  2.6845e-01,  1.4805e-01, -2.2712e-01, -1.6545e-01,\n",
      "            3.4847e-01, -5.8497e-01,  5.0135e-01,  6.7456e-01, -7.4323e-01,\n",
      "            2.6468e-02, -2.2347e-01,  4.5562e-01,  3.6577e-01, -4.4958e-01,\n",
      "           -1.5441e-01]],\n",
      "\n",
      "         [[ 4.6779e-01, -6.1870e-01, -5.1412e-01,  3.6992e-01,  5.3237e-02,\n",
      "           -2.7433e-01,  6.0436e-01, -3.9144e-01, -3.7450e-01,  4.3159e-01,\n",
      "            3.5398e-02,  3.3420e-01, -3.3884e-01, -3.8633e-01,  3.1588e-01,\n",
      "            2.0256e-01]],\n",
      "\n",
      "         [[ 3.9041e-01, -5.2760e-01,  5.8817e-01, -1.9785e-01,  4.2486e-01,\n",
      "           -2.7105e-01,  2.7100e-01, -4.8849e-01,  4.3387e-01,  1.1836e-01,\n",
      "           -3.7023e-02, -5.0612e-05, -3.8373e-01, -2.4487e-01,  5.4048e-01,\n",
      "           -1.4884e-02]]]], grad_fn=<TransposeBackward0>)\n",
      "repeat_interleaved xk: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[-5.7660e-02,  2.6845e-01,  1.4805e-01, -2.2712e-01, -1.6545e-01,\n",
      "            3.4847e-01, -5.8497e-01,  5.0135e-01,  6.7456e-01, -7.4323e-01,\n",
      "            2.6468e-02, -2.2347e-01,  4.5562e-01,  3.6577e-01, -4.4958e-01,\n",
      "           -1.5441e-01],\n",
      "          [-5.7660e-02,  2.6845e-01,  1.4805e-01, -2.2712e-01, -1.6545e-01,\n",
      "            3.4847e-01, -5.8497e-01,  5.0135e-01,  6.7456e-01, -7.4323e-01,\n",
      "            2.6468e-02, -2.2347e-01,  4.5562e-01,  3.6577e-01, -4.4958e-01,\n",
      "           -1.5441e-01],\n",
      "          [-5.7660e-02,  2.6845e-01,  1.4805e-01, -2.2712e-01, -1.6545e-01,\n",
      "            3.4847e-01, -5.8497e-01,  5.0135e-01,  6.7456e-01, -7.4323e-01,\n",
      "            2.6468e-02, -2.2347e-01,  4.5562e-01,  3.6577e-01, -4.4958e-01,\n",
      "           -1.5441e-01],\n",
      "          [-5.7660e-02,  2.6845e-01,  1.4805e-01, -2.2712e-01, -1.6545e-01,\n",
      "            3.4847e-01, -5.8497e-01,  5.0135e-01,  6.7456e-01, -7.4323e-01,\n",
      "            2.6468e-02, -2.2347e-01,  4.5562e-01,  3.6577e-01, -4.4958e-01,\n",
      "           -1.5441e-01]],\n",
      "\n",
      "         [[ 4.6779e-01, -6.1870e-01, -5.1412e-01,  3.6992e-01,  5.3237e-02,\n",
      "           -2.7433e-01,  6.0436e-01, -3.9144e-01, -3.7450e-01,  4.3159e-01,\n",
      "            3.5398e-02,  3.3420e-01, -3.3884e-01, -3.8633e-01,  3.1588e-01,\n",
      "            2.0256e-01],\n",
      "          [ 4.6779e-01, -6.1870e-01, -5.1412e-01,  3.6992e-01,  5.3237e-02,\n",
      "           -2.7433e-01,  6.0436e-01, -3.9144e-01, -3.7450e-01,  4.3159e-01,\n",
      "            3.5398e-02,  3.3420e-01, -3.3884e-01, -3.8633e-01,  3.1588e-01,\n",
      "            2.0256e-01],\n",
      "          [ 4.6779e-01, -6.1870e-01, -5.1412e-01,  3.6992e-01,  5.3237e-02,\n",
      "           -2.7433e-01,  6.0436e-01, -3.9144e-01, -3.7450e-01,  4.3159e-01,\n",
      "            3.5398e-02,  3.3420e-01, -3.3884e-01, -3.8633e-01,  3.1588e-01,\n",
      "            2.0256e-01],\n",
      "          [ 4.6779e-01, -6.1870e-01, -5.1412e-01,  3.6992e-01,  5.3237e-02,\n",
      "           -2.7433e-01,  6.0436e-01, -3.9144e-01, -3.7450e-01,  4.3159e-01,\n",
      "            3.5398e-02,  3.3420e-01, -3.3884e-01, -3.8633e-01,  3.1588e-01,\n",
      "            2.0256e-01]],\n",
      "\n",
      "         [[ 3.9041e-01, -5.2760e-01,  5.8817e-01, -1.9785e-01,  4.2486e-01,\n",
      "           -2.7105e-01,  2.7100e-01, -4.8849e-01,  4.3387e-01,  1.1836e-01,\n",
      "           -3.7023e-02, -5.0612e-05, -3.8373e-01, -2.4487e-01,  5.4048e-01,\n",
      "           -1.4884e-02],\n",
      "          [ 3.9041e-01, -5.2760e-01,  5.8817e-01, -1.9785e-01,  4.2486e-01,\n",
      "           -2.7105e-01,  2.7100e-01, -4.8849e-01,  4.3387e-01,  1.1836e-01,\n",
      "           -3.7023e-02, -5.0612e-05, -3.8373e-01, -2.4487e-01,  5.4048e-01,\n",
      "           -1.4884e-02],\n",
      "          [ 3.9041e-01, -5.2760e-01,  5.8817e-01, -1.9785e-01,  4.2486e-01,\n",
      "           -2.7105e-01,  2.7100e-01, -4.8849e-01,  4.3387e-01,  1.1836e-01,\n",
      "           -3.7023e-02, -5.0612e-05, -3.8373e-01, -2.4487e-01,  5.4048e-01,\n",
      "           -1.4884e-02],\n",
      "          [ 3.9041e-01, -5.2760e-01,  5.8817e-01, -1.9785e-01,  4.2486e-01,\n",
      "           -2.7105e-01,  2.7100e-01, -4.8849e-01,  4.3387e-01,  1.1836e-01,\n",
      "           -3.7023e-02, -5.0612e-05, -3.8373e-01, -2.4487e-01,  5.4048e-01,\n",
      "           -1.4884e-02]]]], grad_fn=<ViewBackward0>)\n",
      "repeat_interleaved xv: torch.Size([1, 3, 4, 16])\n",
      "tensor([[[[-0.3748,  0.0432, -0.2184,  0.0714, -0.1147, -0.5571, -0.0986,\n",
      "            0.7679,  0.6861, -0.0755, -0.5196,  0.2330,  0.1552,  0.4587,\n",
      "            0.1048, -0.0717],\n",
      "          [-0.3748,  0.0432, -0.2184,  0.0714, -0.1147, -0.5571, -0.0986,\n",
      "            0.7679,  0.6861, -0.0755, -0.5196,  0.2330,  0.1552,  0.4587,\n",
      "            0.1048, -0.0717],\n",
      "          [-0.3748,  0.0432, -0.2184,  0.0714, -0.1147, -0.5571, -0.0986,\n",
      "            0.7679,  0.6861, -0.0755, -0.5196,  0.2330,  0.1552,  0.4587,\n",
      "            0.1048, -0.0717],\n",
      "          [-0.3748,  0.0432, -0.2184,  0.0714, -0.1147, -0.5571, -0.0986,\n",
      "            0.7679,  0.6861, -0.0755, -0.5196,  0.2330,  0.1552,  0.4587,\n",
      "            0.1048, -0.0717]],\n",
      "\n",
      "         [[ 0.5562, -0.2040, -0.0067,  0.1740,  0.2958,  0.3840,  0.2952,\n",
      "           -0.5672, -0.5844,  0.1783,  0.4985, -0.2436, -0.0666, -0.5928,\n",
      "           -0.0226,  0.3758],\n",
      "          [ 0.5562, -0.2040, -0.0067,  0.1740,  0.2958,  0.3840,  0.2952,\n",
      "           -0.5672, -0.5844,  0.1783,  0.4985, -0.2436, -0.0666, -0.5928,\n",
      "           -0.0226,  0.3758],\n",
      "          [ 0.5562, -0.2040, -0.0067,  0.1740,  0.2958,  0.3840,  0.2952,\n",
      "           -0.5672, -0.5844,  0.1783,  0.4985, -0.2436, -0.0666, -0.5928,\n",
      "           -0.0226,  0.3758],\n",
      "          [ 0.5562, -0.2040, -0.0067,  0.1740,  0.2958,  0.3840,  0.2952,\n",
      "           -0.5672, -0.5844,  0.1783,  0.4985, -0.2436, -0.0666, -0.5928,\n",
      "           -0.0226,  0.3758]],\n",
      "\n",
      "         [[-0.0724,  0.2383,  0.5016, -0.4464, -0.2285,  0.6203, -0.2636,\n",
      "           -0.7926, -0.5805, -0.1236,  0.3477, -0.1230, -0.2392, -0.0559,\n",
      "           -0.1982, -0.4567],\n",
      "          [-0.0724,  0.2383,  0.5016, -0.4464, -0.2285,  0.6203, -0.2636,\n",
      "           -0.7926, -0.5805, -0.1236,  0.3477, -0.1230, -0.2392, -0.0559,\n",
      "           -0.1982, -0.4567],\n",
      "          [-0.0724,  0.2383,  0.5016, -0.4464, -0.2285,  0.6203, -0.2636,\n",
      "           -0.7926, -0.5805, -0.1236,  0.3477, -0.1230, -0.2392, -0.0559,\n",
      "           -0.1982, -0.4567],\n",
      "          [-0.0724,  0.2383,  0.5016, -0.4464, -0.2285,  0.6203, -0.2636,\n",
      "           -0.7926, -0.5805, -0.1236,  0.3477, -0.1230, -0.2392, -0.0559,\n",
      "           -0.1982, -0.4567]]]], grad_fn=<ViewBackward0>)\n",
      "transposed xq: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[-0.6561, -0.1711,  0.6492,  0.1869, -0.2639,  0.2735,  0.1198,\n",
      "            0.2854,  0.0155, -0.0932, -0.6092,  0.0437, -0.1723,  0.1355,\n",
      "           -0.0240, -0.2736],\n",
      "          [ 0.1324, -0.0615, -0.8241, -0.4600,  0.2409, -0.4048, -0.3613,\n",
      "           -0.0595,  0.7484,  0.1620,  0.4320,  0.1299,  0.0558, -0.4561,\n",
      "           -0.1491,  0.0702],\n",
      "          [ 0.3360,  0.2043, -0.4768,  0.4226,  0.1193,  0.0459,  0.3138,\n",
      "           -0.5626,  0.5069,  0.2657, -0.0120, -0.3147,  0.3663,  0.4146,\n",
      "            0.3004,  0.4777]],\n",
      "\n",
      "         [[-0.4881, -0.0449, -0.1260,  0.3724, -0.2707,  0.4426, -0.1749,\n",
      "            0.4785, -0.1589, -0.2280,  0.3787,  0.0352,  0.2730, -0.4158,\n",
      "           -0.2713, -0.3274],\n",
      "          [ 0.2236,  0.0486,  0.1066, -0.6696,  0.4668, -0.6138, -0.0821,\n",
      "           -0.4703,  0.5051,  0.1602, -0.2020, -0.1035, -0.4887,  0.2153,\n",
      "            0.2320,  0.4488],\n",
      "          [-0.2808, -0.3456,  0.4613,  0.2965, -0.1259, -0.0654,  0.5004,\n",
      "           -0.3141,  0.0905,  0.0466, -0.2576, -0.0034,  0.2426,  0.5200,\n",
      "            0.2563, -0.0272]],\n",
      "\n",
      "         [[ 0.1946,  0.3830, -0.1102, -0.0023, -0.4437,  0.1732,  0.7241,\n",
      "           -0.4545,  0.3070,  0.4362,  0.0935, -0.6742, -0.6215,  0.3030,\n",
      "            0.1779, -0.1615],\n",
      "          [ 0.3277,  0.0496,  0.2005, -0.0627,  0.4872,  0.0311, -0.6075,\n",
      "            0.3734, -0.2489, -0.4994,  0.1033,  0.6018,  0.5196, -0.2330,\n",
      "            0.0795,  0.0246],\n",
      "          [ 0.1737,  0.0060,  0.0635, -0.2462, -0.0122, -0.3829, -0.5956,\n",
      "            0.3944, -0.3427, -0.5477, -0.2870,  0.4660,  0.6363, -0.3404,\n",
      "           -0.6005,  0.3476]],\n",
      "\n",
      "         [[ 0.8189, -0.3887, -0.1116,  0.0031, -0.7500, -0.1276, -0.4401,\n",
      "            0.1146, -0.2514, -0.1091, -0.3081,  0.3748, -0.0599, -0.2499,\n",
      "           -0.0084, -0.3366],\n",
      "          [-0.4007,  0.3143,  0.3252,  0.1803,  0.5491,  0.0346,  0.4288,\n",
      "           -0.2146, -0.4699,  0.0927,  0.3365, -0.6626,  0.0646,  0.1159,\n",
      "           -0.2468,  0.2623],\n",
      "          [-0.0422, -0.2002, -0.5431, -0.1976,  0.7368,  0.1757,  0.2681,\n",
      "            0.0762, -0.9361,  0.4891,  0.0145,  0.2280,  0.2707,  0.3965,\n",
      "            0.4646,  0.3217]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xk: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[-5.7660e-02,  2.6845e-01,  1.4805e-01, -2.2712e-01, -1.6545e-01,\n",
      "            3.4847e-01, -5.8497e-01,  5.0135e-01,  6.7456e-01, -7.4323e-01,\n",
      "            2.6468e-02, -2.2347e-01,  4.5562e-01,  3.6577e-01, -4.4958e-01,\n",
      "           -1.5441e-01],\n",
      "          [ 4.6779e-01, -6.1870e-01, -5.1412e-01,  3.6992e-01,  5.3237e-02,\n",
      "           -2.7433e-01,  6.0436e-01, -3.9144e-01, -3.7450e-01,  4.3159e-01,\n",
      "            3.5398e-02,  3.3420e-01, -3.3884e-01, -3.8633e-01,  3.1588e-01,\n",
      "            2.0256e-01],\n",
      "          [ 3.9041e-01, -5.2760e-01,  5.8817e-01, -1.9785e-01,  4.2486e-01,\n",
      "           -2.7105e-01,  2.7100e-01, -4.8849e-01,  4.3387e-01,  1.1836e-01,\n",
      "           -3.7023e-02, -5.0612e-05, -3.8373e-01, -2.4487e-01,  5.4048e-01,\n",
      "           -1.4884e-02]],\n",
      "\n",
      "         [[-5.7660e-02,  2.6845e-01,  1.4805e-01, -2.2712e-01, -1.6545e-01,\n",
      "            3.4847e-01, -5.8497e-01,  5.0135e-01,  6.7456e-01, -7.4323e-01,\n",
      "            2.6468e-02, -2.2347e-01,  4.5562e-01,  3.6577e-01, -4.4958e-01,\n",
      "           -1.5441e-01],\n",
      "          [ 4.6779e-01, -6.1870e-01, -5.1412e-01,  3.6992e-01,  5.3237e-02,\n",
      "           -2.7433e-01,  6.0436e-01, -3.9144e-01, -3.7450e-01,  4.3159e-01,\n",
      "            3.5398e-02,  3.3420e-01, -3.3884e-01, -3.8633e-01,  3.1588e-01,\n",
      "            2.0256e-01],\n",
      "          [ 3.9041e-01, -5.2760e-01,  5.8817e-01, -1.9785e-01,  4.2486e-01,\n",
      "           -2.7105e-01,  2.7100e-01, -4.8849e-01,  4.3387e-01,  1.1836e-01,\n",
      "           -3.7023e-02, -5.0612e-05, -3.8373e-01, -2.4487e-01,  5.4048e-01,\n",
      "           -1.4884e-02]],\n",
      "\n",
      "         [[-5.7660e-02,  2.6845e-01,  1.4805e-01, -2.2712e-01, -1.6545e-01,\n",
      "            3.4847e-01, -5.8497e-01,  5.0135e-01,  6.7456e-01, -7.4323e-01,\n",
      "            2.6468e-02, -2.2347e-01,  4.5562e-01,  3.6577e-01, -4.4958e-01,\n",
      "           -1.5441e-01],\n",
      "          [ 4.6779e-01, -6.1870e-01, -5.1412e-01,  3.6992e-01,  5.3237e-02,\n",
      "           -2.7433e-01,  6.0436e-01, -3.9144e-01, -3.7450e-01,  4.3159e-01,\n",
      "            3.5398e-02,  3.3420e-01, -3.3884e-01, -3.8633e-01,  3.1588e-01,\n",
      "            2.0256e-01],\n",
      "          [ 3.9041e-01, -5.2760e-01,  5.8817e-01, -1.9785e-01,  4.2486e-01,\n",
      "           -2.7105e-01,  2.7100e-01, -4.8849e-01,  4.3387e-01,  1.1836e-01,\n",
      "           -3.7023e-02, -5.0612e-05, -3.8373e-01, -2.4487e-01,  5.4048e-01,\n",
      "           -1.4884e-02]],\n",
      "\n",
      "         [[-5.7660e-02,  2.6845e-01,  1.4805e-01, -2.2712e-01, -1.6545e-01,\n",
      "            3.4847e-01, -5.8497e-01,  5.0135e-01,  6.7456e-01, -7.4323e-01,\n",
      "            2.6468e-02, -2.2347e-01,  4.5562e-01,  3.6577e-01, -4.4958e-01,\n",
      "           -1.5441e-01],\n",
      "          [ 4.6779e-01, -6.1870e-01, -5.1412e-01,  3.6992e-01,  5.3237e-02,\n",
      "           -2.7433e-01,  6.0436e-01, -3.9144e-01, -3.7450e-01,  4.3159e-01,\n",
      "            3.5398e-02,  3.3420e-01, -3.3884e-01, -3.8633e-01,  3.1588e-01,\n",
      "            2.0256e-01],\n",
      "          [ 3.9041e-01, -5.2760e-01,  5.8817e-01, -1.9785e-01,  4.2486e-01,\n",
      "           -2.7105e-01,  2.7100e-01, -4.8849e-01,  4.3387e-01,  1.1836e-01,\n",
      "           -3.7023e-02, -5.0612e-05, -3.8373e-01, -2.4487e-01,  5.4048e-01,\n",
      "           -1.4884e-02]]]], grad_fn=<TransposeBackward0>)\n",
      "transposed xv: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[-0.3748,  0.0432, -0.2184,  0.0714, -0.1147, -0.5571, -0.0986,\n",
      "            0.7679,  0.6861, -0.0755, -0.5196,  0.2330,  0.1552,  0.4587,\n",
      "            0.1048, -0.0717],\n",
      "          [ 0.5562, -0.2040, -0.0067,  0.1740,  0.2958,  0.3840,  0.2952,\n",
      "           -0.5672, -0.5844,  0.1783,  0.4985, -0.2436, -0.0666, -0.5928,\n",
      "           -0.0226,  0.3758],\n",
      "          [-0.0724,  0.2383,  0.5016, -0.4464, -0.2285,  0.6203, -0.2636,\n",
      "           -0.7926, -0.5805, -0.1236,  0.3477, -0.1230, -0.2392, -0.0559,\n",
      "           -0.1982, -0.4567]],\n",
      "\n",
      "         [[-0.3748,  0.0432, -0.2184,  0.0714, -0.1147, -0.5571, -0.0986,\n",
      "            0.7679,  0.6861, -0.0755, -0.5196,  0.2330,  0.1552,  0.4587,\n",
      "            0.1048, -0.0717],\n",
      "          [ 0.5562, -0.2040, -0.0067,  0.1740,  0.2958,  0.3840,  0.2952,\n",
      "           -0.5672, -0.5844,  0.1783,  0.4985, -0.2436, -0.0666, -0.5928,\n",
      "           -0.0226,  0.3758],\n",
      "          [-0.0724,  0.2383,  0.5016, -0.4464, -0.2285,  0.6203, -0.2636,\n",
      "           -0.7926, -0.5805, -0.1236,  0.3477, -0.1230, -0.2392, -0.0559,\n",
      "           -0.1982, -0.4567]],\n",
      "\n",
      "         [[-0.3748,  0.0432, -0.2184,  0.0714, -0.1147, -0.5571, -0.0986,\n",
      "            0.7679,  0.6861, -0.0755, -0.5196,  0.2330,  0.1552,  0.4587,\n",
      "            0.1048, -0.0717],\n",
      "          [ 0.5562, -0.2040, -0.0067,  0.1740,  0.2958,  0.3840,  0.2952,\n",
      "           -0.5672, -0.5844,  0.1783,  0.4985, -0.2436, -0.0666, -0.5928,\n",
      "           -0.0226,  0.3758],\n",
      "          [-0.0724,  0.2383,  0.5016, -0.4464, -0.2285,  0.6203, -0.2636,\n",
      "           -0.7926, -0.5805, -0.1236,  0.3477, -0.1230, -0.2392, -0.0559,\n",
      "           -0.1982, -0.4567]],\n",
      "\n",
      "         [[-0.3748,  0.0432, -0.2184,  0.0714, -0.1147, -0.5571, -0.0986,\n",
      "            0.7679,  0.6861, -0.0755, -0.5196,  0.2330,  0.1552,  0.4587,\n",
      "            0.1048, -0.0717],\n",
      "          [ 0.5562, -0.2040, -0.0067,  0.1740,  0.2958,  0.3840,  0.2952,\n",
      "           -0.5672, -0.5844,  0.1783,  0.4985, -0.2436, -0.0666, -0.5928,\n",
      "           -0.0226,  0.3758],\n",
      "          [-0.0724,  0.2383,  0.5016, -0.4464, -0.2285,  0.6203, -0.2636,\n",
      "           -0.7926, -0.5805, -0.1236,  0.3477, -0.1230, -0.2392, -0.0559,\n",
      "           -0.1982, -0.4567]]]], grad_fn=<TransposeBackward0>)\n",
      "scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[ 0.0839, -0.1760, -0.0180],\n",
      "          [ 0.0601,  0.0638,  0.0425],\n",
      "          [-0.0691,  0.1406,  0.0556]],\n",
      "\n",
      "         [[ 0.1658, -0.1311, -0.2714],\n",
      "          [-0.0973,  0.0462,  0.3320],\n",
      "          [-0.0794,  0.0400,  0.1279]],\n",
      "\n",
      "         [[-0.1583,  0.1056,  0.1091],\n",
      "          [ 0.1737, -0.1183, -0.0468],\n",
      "          [ 0.2294, -0.1555, -0.1996]],\n",
      "\n",
      "         [[-0.0044,  0.1434, -0.0067],\n",
      "          [-0.1023, -0.0511, -0.0294],\n",
      "          [-0.3255,  0.2597, -0.0484]]]], grad_fn=<MulBackward0>)\n",
      "mask: torch.Size([1, 1, 3, 3])\n",
      "tensor([[[[ 0.0000e+00, -2.3820e+38, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00, -2.3820e+38],\n",
      "          [ 0.0000e+00,  0.0000e+00,  0.0000e+00]]]])\n",
      "masked scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[ 8.3870e-02, -2.3820e+38, -2.3820e+38],\n",
      "          [ 6.0132e-02,  6.3773e-02, -2.3820e+38],\n",
      "          [-6.9056e-02,  1.4061e-01,  5.5601e-02]],\n",
      "\n",
      "         [[ 1.6580e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [-9.7255e-02,  4.6160e-02, -2.3820e+38],\n",
      "          [-7.9415e-02,  3.9964e-02,  1.2786e-01]],\n",
      "\n",
      "         [[-1.5829e-01, -2.3820e+38, -2.3820e+38],\n",
      "          [ 1.7367e-01, -1.1828e-01, -2.3820e+38],\n",
      "          [ 2.2939e-01, -1.5550e-01, -1.9957e-01]],\n",
      "\n",
      "         [[-4.4009e-03, -2.3820e+38, -2.3820e+38],\n",
      "          [-1.0228e-01, -5.1141e-02, -2.3820e+38],\n",
      "          [-3.2546e-01,  2.5970e-01, -4.8447e-02]]]], grad_fn=<AddBackward0>)\n",
      "softmaxed scores: torch.Size([1, 4, 3, 3])\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4991, 0.5009, 0.0000],\n",
      "          [0.2971, 0.3664, 0.3365]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4642, 0.5358, 0.0000],\n",
      "          [0.2979, 0.3356, 0.3665]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.5725, 0.4275, 0.0000],\n",
      "          [0.4289, 0.2919, 0.2793]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4872, 0.5128, 0.0000],\n",
      "          [0.2430, 0.4363, 0.3206]]]], grad_fn=<SoftmaxBackward0>)\n",
      "attention: torch.Size([1, 4, 3, 16])\n",
      "tensor([[[[-3.7476e-01,  4.3165e-02, -2.1839e-01,  7.1376e-02, -1.1471e-01,\n",
      "           -5.5712e-01, -9.8641e-02,  7.6793e-01,  6.8609e-01, -7.5455e-02,\n",
      "           -5.1961e-01,  2.3305e-01,  1.5524e-01,  4.5872e-01,  1.0478e-01,\n",
      "           -7.1665e-02],\n",
      "          [ 9.1548e-02, -8.0633e-02, -1.1237e-01,  1.2280e-01,  9.0913e-02,\n",
      "           -8.5723e-02,  9.8617e-02,  9.9157e-02,  4.9693e-02,  5.1646e-02,\n",
      "           -9.6046e-03, -5.7316e-03,  4.4123e-02, -6.8021e-02,  4.0977e-02,\n",
      "            1.5248e-01],\n",
      "          [ 6.8072e-02,  1.8290e-02,  1.0146e-01, -6.5248e-02, -2.6120e-03,\n",
      "            1.8390e-01, -9.8663e-03, -2.4640e-01, -2.0563e-01,  1.3225e-03,\n",
      "            1.4529e-01, -6.1419e-02, -5.8786e-02, -9.9740e-02, -4.3852e-02,\n",
      "           -3.7304e-02]],\n",
      "\n",
      "         [[-3.7476e-01,  4.3165e-02, -2.1839e-01,  7.1376e-02, -1.1471e-01,\n",
      "           -5.5712e-01, -9.8641e-02,  7.6793e-01,  6.8609e-01, -7.5455e-02,\n",
      "           -5.1961e-01,  2.3305e-01,  1.5524e-01,  4.5872e-01,  1.0478e-01,\n",
      "           -7.1665e-02],\n",
      "          [ 1.2402e-01, -8.9254e-02, -1.0499e-01,  1.2638e-01,  1.0523e-01,\n",
      "           -5.2896e-02,  1.1235e-01,  5.2586e-02,  5.3768e-03,  6.0497e-02,\n",
      "            2.5911e-02, -2.2360e-02,  3.6386e-02, -1.0470e-01,  3.6535e-02,\n",
      "            1.6809e-01],\n",
      "          [ 4.8509e-02,  3.1734e-02,  1.1652e-01, -8.3913e-02, -1.8641e-02,\n",
      "            1.9023e-01, -2.6914e-02, -2.5209e-01, -2.0452e-01, -7.9194e-03,\n",
      "            1.3996e-01, -5.7428e-02, -6.3783e-02, -8.2826e-02, -4.9012e-02,\n",
      "           -6.2595e-02]],\n",
      "\n",
      "         [[-3.7476e-01,  4.3165e-02, -2.1839e-01,  7.1376e-02, -1.1471e-01,\n",
      "           -5.5712e-01, -9.8641e-02,  7.6793e-01,  6.8609e-01, -7.5455e-02,\n",
      "           -5.1961e-01,  2.3305e-01,  1.5524e-01,  4.5872e-01,  1.0478e-01,\n",
      "           -7.1665e-02],\n",
      "          [ 2.3235e-02, -6.2497e-02, -1.2790e-01,  1.1526e-01,  6.0790e-02,\n",
      "           -1.5478e-01,  6.9719e-02,  1.9713e-01,  1.4292e-01,  3.3026e-02,\n",
      "           -8.4320e-02,  2.9249e-02,  6.0401e-02,  9.1451e-03,  5.0324e-02,\n",
      "            1.1964e-01],\n",
      "          [-1.8621e-02,  2.5537e-02,  4.4458e-02, -4.3256e-02, -2.6689e-02,\n",
      "            4.6350e-02, -2.9772e-02, -5.7546e-02, -3.8432e-02, -1.4834e-02,\n",
      "            1.9751e-02, -5.5079e-03, -1.9669e-02,  8.0953e-03, -1.7013e-02,\n",
      "           -4.8607e-02]],\n",
      "\n",
      "         [[-3.7476e-01,  4.3165e-02, -2.1839e-01,  7.1376e-02, -1.1471e-01,\n",
      "           -5.5712e-01, -9.8641e-02,  7.6793e-01,  6.8609e-01, -7.5455e-02,\n",
      "           -5.1961e-01,  2.3305e-01,  1.5524e-01,  4.5872e-01,  1.0478e-01,\n",
      "           -7.1665e-02],\n",
      "          [ 1.0260e-01, -8.3567e-02, -1.0986e-01,  1.2402e-01,  9.5787e-02,\n",
      "           -7.4550e-02,  1.0329e-01,  8.3306e-02,  3.4609e-02,  5.4659e-02,\n",
      "            2.4836e-03, -1.1391e-02,  4.1490e-02, -8.0506e-02,  3.9465e-02,\n",
      "            1.5779e-01],\n",
      "          [ 1.2838e-01, -2.1011e-03,  1.0481e-01, -4.9832e-02,  2.7912e-02,\n",
      "            2.3100e-01,  2.0302e-02, -3.1496e-01, -2.7435e-01,  1.9836e-02,\n",
      "            2.0271e-01, -8.9099e-02, -6.8028e-02, -1.6511e-01, -4.7942e-02,\n",
      "            1.2030e-04]]]], grad_fn=<UnsafeViewBackward0>)\n",
      "reshaped attention: torch.Size([1, 3, 64])\n",
      "tensor([[[-3.7476e-01,  4.3165e-02, -2.1839e-01,  7.1376e-02, -1.1471e-01,\n",
      "          -5.5712e-01, -9.8641e-02,  7.6793e-01,  6.8609e-01, -7.5455e-02,\n",
      "          -5.1961e-01,  2.3305e-01,  1.5524e-01,  4.5872e-01,  1.0478e-01,\n",
      "          -7.1665e-02, -3.7476e-01,  4.3165e-02, -2.1839e-01,  7.1376e-02,\n",
      "          -1.1471e-01, -5.5712e-01, -9.8641e-02,  7.6793e-01,  6.8609e-01,\n",
      "          -7.5455e-02, -5.1961e-01,  2.3305e-01,  1.5524e-01,  4.5872e-01,\n",
      "           1.0478e-01, -7.1665e-02, -3.7476e-01,  4.3165e-02, -2.1839e-01,\n",
      "           7.1376e-02, -1.1471e-01, -5.5712e-01, -9.8641e-02,  7.6793e-01,\n",
      "           6.8609e-01, -7.5455e-02, -5.1961e-01,  2.3305e-01,  1.5524e-01,\n",
      "           4.5872e-01,  1.0478e-01, -7.1665e-02, -3.7476e-01,  4.3165e-02,\n",
      "          -2.1839e-01,  7.1376e-02, -1.1471e-01, -5.5712e-01, -9.8641e-02,\n",
      "           7.6793e-01,  6.8609e-01, -7.5455e-02, -5.1961e-01,  2.3305e-01,\n",
      "           1.5524e-01,  4.5872e-01,  1.0478e-01, -7.1665e-02],\n",
      "         [ 9.1548e-02, -8.0633e-02, -1.1237e-01,  1.2280e-01,  9.0913e-02,\n",
      "          -8.5723e-02,  9.8617e-02,  9.9157e-02,  4.9693e-02,  5.1646e-02,\n",
      "          -9.6046e-03, -5.7316e-03,  4.4123e-02, -6.8021e-02,  4.0977e-02,\n",
      "           1.5248e-01,  1.2402e-01, -8.9254e-02, -1.0499e-01,  1.2638e-01,\n",
      "           1.0523e-01, -5.2896e-02,  1.1235e-01,  5.2586e-02,  5.3768e-03,\n",
      "           6.0497e-02,  2.5911e-02, -2.2360e-02,  3.6386e-02, -1.0470e-01,\n",
      "           3.6535e-02,  1.6809e-01,  2.3235e-02, -6.2497e-02, -1.2790e-01,\n",
      "           1.1526e-01,  6.0790e-02, -1.5478e-01,  6.9719e-02,  1.9713e-01,\n",
      "           1.4292e-01,  3.3026e-02, -8.4320e-02,  2.9249e-02,  6.0401e-02,\n",
      "           9.1451e-03,  5.0324e-02,  1.1964e-01,  1.0260e-01, -8.3567e-02,\n",
      "          -1.0986e-01,  1.2402e-01,  9.5787e-02, -7.4550e-02,  1.0329e-01,\n",
      "           8.3306e-02,  3.4609e-02,  5.4659e-02,  2.4836e-03, -1.1391e-02,\n",
      "           4.1490e-02, -8.0506e-02,  3.9465e-02,  1.5779e-01],\n",
      "         [ 6.8072e-02,  1.8290e-02,  1.0146e-01, -6.5248e-02, -2.6120e-03,\n",
      "           1.8390e-01, -9.8663e-03, -2.4640e-01, -2.0563e-01,  1.3225e-03,\n",
      "           1.4529e-01, -6.1419e-02, -5.8786e-02, -9.9740e-02, -4.3852e-02,\n",
      "          -3.7304e-02,  4.8509e-02,  3.1734e-02,  1.1652e-01, -8.3913e-02,\n",
      "          -1.8641e-02,  1.9023e-01, -2.6914e-02, -2.5209e-01, -2.0452e-01,\n",
      "          -7.9194e-03,  1.3996e-01, -5.7428e-02, -6.3783e-02, -8.2826e-02,\n",
      "          -4.9012e-02, -6.2595e-02, -1.8621e-02,  2.5537e-02,  4.4458e-02,\n",
      "          -4.3256e-02, -2.6689e-02,  4.6350e-02, -2.9772e-02, -5.7546e-02,\n",
      "          -3.8432e-02, -1.4834e-02,  1.9751e-02, -5.5079e-03, -1.9669e-02,\n",
      "           8.0953e-03, -1.7013e-02, -4.8607e-02,  1.2838e-01, -2.1011e-03,\n",
      "           1.0481e-01, -4.9832e-02,  2.7912e-02,  2.3100e-01,  2.0302e-02,\n",
      "          -3.1496e-01, -2.7435e-01,  1.9836e-02,  2.0271e-01, -8.9099e-02,\n",
      "          -6.8028e-02, -1.6511e-01, -4.7942e-02,  1.2030e-04]]],\n",
      "       grad_fn=<ViewBackward0>)\n",
      "self.Wo: torch.Size([128, 4])\n",
      "Parameter containing:\n",
      "tensor([[ 0.0139, -0.0787, -0.0272, -0.0417],\n",
      "        [ 0.0033,  0.0494, -0.0123,  0.0227],\n",
      "        [ 0.0840,  0.0823,  0.0379, -0.0151],\n",
      "        [-0.0278,  0.0584,  0.0317,  0.0743],\n",
      "        [ 0.0762, -0.0099, -0.0417,  0.0459],\n",
      "        [ 0.0649,  0.0091, -0.0589,  0.0485],\n",
      "        [ 0.0458, -0.0723,  0.0485, -0.0311],\n",
      "        [ 0.0279,  0.0563,  0.0791, -0.0248],\n",
      "        [-0.0813,  0.0755, -0.0165,  0.0383],\n",
      "        [-0.0721, -0.0364, -0.0574,  0.0814],\n",
      "        [-0.0438, -0.0211,  0.0055, -0.0661],\n",
      "        [ 0.0021,  0.0126, -0.0437, -0.0395],\n",
      "        [ 0.0412, -0.0169, -0.0531,  0.0762],\n",
      "        [ 0.0454,  0.0125,  0.0438,  0.0251],\n",
      "        [ 0.0287,  0.0397,  0.0766, -0.0685],\n",
      "        [-0.0439, -0.0862,  0.0521,  0.0354],\n",
      "        [ 0.0287, -0.0362,  0.0043, -0.0332],\n",
      "        [-0.0180,  0.0598, -0.0022,  0.0810],\n",
      "        [ 0.0312, -0.0346, -0.0249,  0.0128],\n",
      "        [ 0.0677,  0.0725,  0.0375, -0.0278],\n",
      "        [ 0.0479,  0.0114, -0.0526, -0.0683],\n",
      "        [-0.0165, -0.0462,  0.0869, -0.0703],\n",
      "        [ 0.0455,  0.0610,  0.0053,  0.0742],\n",
      "        [ 0.0757, -0.0677,  0.0271,  0.0347],\n",
      "        [-0.0526, -0.0091,  0.0032, -0.0064],\n",
      "        [-0.0776,  0.0104,  0.0064, -0.0518],\n",
      "        [-0.0017, -0.0029, -0.0549, -0.0834],\n",
      "        [-0.0227,  0.0364,  0.0527, -0.0069],\n",
      "        [ 0.0085,  0.0365, -0.0376, -0.0070],\n",
      "        [ 0.0820, -0.0390, -0.0115, -0.0306],\n",
      "        [ 0.0498, -0.0566,  0.0319, -0.0084],\n",
      "        [-0.0578, -0.0742,  0.0627, -0.0273],\n",
      "        [-0.0837, -0.0356, -0.0646,  0.0619],\n",
      "        [-0.0584, -0.0646,  0.0578, -0.0586],\n",
      "        [ 0.0294, -0.0348, -0.0340,  0.0118],\n",
      "        [-0.0396, -0.0184,  0.0042,  0.0138],\n",
      "        [ 0.0701, -0.0292,  0.0398, -0.0565],\n",
      "        [ 0.0478, -0.0783,  0.0089, -0.0451],\n",
      "        [ 0.0826, -0.0510,  0.0791,  0.0765],\n",
      "        [-0.0505, -0.0013, -0.0316, -0.0613],\n",
      "        [ 0.0244,  0.0527, -0.0418, -0.0360],\n",
      "        [ 0.0488,  0.0199,  0.0736,  0.0449],\n",
      "        [-0.0180,  0.0355, -0.0553,  0.0215],\n",
      "        [-0.0083,  0.0779, -0.0510, -0.0657],\n",
      "        [ 0.0020, -0.0527, -0.0741,  0.0346],\n",
      "        [ 0.0375,  0.0752, -0.0377, -0.0659],\n",
      "        [ 0.0219,  0.0304,  0.0374, -0.0870],\n",
      "        [-0.0281, -0.0418,  0.0484,  0.0157],\n",
      "        [-0.0478,  0.0550,  0.0397, -0.0394],\n",
      "        [ 0.0020, -0.0341,  0.0732, -0.0505],\n",
      "        [ 0.0496,  0.0150, -0.0263, -0.0473],\n",
      "        [-0.0515, -0.0541, -0.0173, -0.0739],\n",
      "        [ 0.0284,  0.0858,  0.0706,  0.0280],\n",
      "        [-0.0219,  0.0860,  0.0841, -0.0408],\n",
      "        [-0.0817, -0.0685,  0.0569,  0.0070],\n",
      "        [-0.0781,  0.0541,  0.0586, -0.0666],\n",
      "        [-0.0822, -0.0732,  0.0389, -0.0767],\n",
      "        [-0.0720, -0.0872,  0.0849, -0.0393],\n",
      "        [ 0.0702, -0.0119,  0.0197,  0.0544],\n",
      "        [-0.0763,  0.0492,  0.0762, -0.0804],\n",
      "        [-0.0458,  0.0242,  0.0344, -0.0160],\n",
      "        [ 0.0139, -0.0879,  0.0617,  0.0035],\n",
      "        [-0.0723, -0.0875, -0.0360, -0.0717],\n",
      "        [ 0.0703,  0.0624, -0.0128, -0.0182],\n",
      "        [-0.0372, -0.0452,  0.0723,  0.0477],\n",
      "        [-0.0867, -0.0091,  0.0572,  0.0341],\n",
      "        [-0.0241, -0.0047, -0.0325, -0.0600],\n",
      "        [ 0.0211,  0.0089, -0.0471,  0.0287],\n",
      "        [ 0.0486,  0.0764,  0.0096, -0.0662],\n",
      "        [ 0.0428, -0.0168, -0.0484,  0.0304],\n",
      "        [ 0.0791,  0.0326,  0.0070,  0.0849],\n",
      "        [ 0.0162,  0.0343,  0.0489,  0.0249],\n",
      "        [-0.0175,  0.0154, -0.0133, -0.0546],\n",
      "        [-0.0223, -0.0140,  0.0862,  0.0644],\n",
      "        [-0.0730, -0.0561, -0.0175,  0.0270],\n",
      "        [-0.0750,  0.0791,  0.0761,  0.0207],\n",
      "        [ 0.0588,  0.0386, -0.0708, -0.0120],\n",
      "        [-0.0486,  0.0058, -0.0176, -0.0872],\n",
      "        [ 0.0420,  0.0211, -0.0453, -0.0756],\n",
      "        [-0.0469,  0.0648,  0.0840,  0.0771],\n",
      "        [ 0.0167, -0.0466,  0.0435, -0.0558],\n",
      "        [-0.0180,  0.0093, -0.0242, -0.0033],\n",
      "        [ 0.0672, -0.0508,  0.0832, -0.0102],\n",
      "        [ 0.0063, -0.0791, -0.0377, -0.0634],\n",
      "        [ 0.0389,  0.0140, -0.0615,  0.0771],\n",
      "        [-0.0714,  0.0663, -0.0419,  0.0436],\n",
      "        [-0.0005, -0.0116,  0.0021, -0.0355],\n",
      "        [ 0.0094,  0.0812,  0.0483, -0.0468],\n",
      "        [-0.0453,  0.0196,  0.0874, -0.0572],\n",
      "        [ 0.0470,  0.0742, -0.0554,  0.0737],\n",
      "        [-0.0046, -0.0622, -0.0864, -0.0697],\n",
      "        [-0.0350,  0.0755, -0.0144, -0.0533],\n",
      "        [-0.0733, -0.0141, -0.0706,  0.0772],\n",
      "        [ 0.0318, -0.0718, -0.0039, -0.0808],\n",
      "        [ 0.0250, -0.0836, -0.0437, -0.0564],\n",
      "        [ 0.0294, -0.0549, -0.0642, -0.0373],\n",
      "        [-0.0052,  0.0771, -0.0031,  0.0305],\n",
      "        [-0.0191, -0.0395,  0.0577,  0.0202],\n",
      "        [-0.0204,  0.0647, -0.0778, -0.0585],\n",
      "        [ 0.0814,  0.0275, -0.0803,  0.0322],\n",
      "        [ 0.0391,  0.0696, -0.0112,  0.0406],\n",
      "        [-0.0811, -0.0443, -0.0128, -0.0556],\n",
      "        [ 0.0855,  0.0882,  0.0047, -0.0368],\n",
      "        [ 0.0583, -0.0531,  0.0071,  0.0782],\n",
      "        [-0.0406, -0.0030,  0.0457, -0.0841],\n",
      "        [ 0.0442, -0.0754, -0.0803, -0.0494],\n",
      "        [-0.0486, -0.0157,  0.0512,  0.0465],\n",
      "        [-0.0004, -0.0288, -0.0781,  0.0676],\n",
      "        [-0.0460, -0.0629, -0.0148,  0.0751],\n",
      "        [ 0.0104, -0.0334, -0.0388, -0.0608],\n",
      "        [-0.0189,  0.0272,  0.0264, -0.0059],\n",
      "        [-0.0798, -0.0168, -0.0396, -0.0421],\n",
      "        [-0.0075,  0.0069, -0.0432, -0.0495],\n",
      "        [-0.0047,  0.0769, -0.0368,  0.0721],\n",
      "        [-0.0162,  0.0626, -0.0485,  0.0189],\n",
      "        [ 0.0079,  0.0517, -0.0361, -0.0346],\n",
      "        [ 0.0057,  0.0109, -0.0509, -0.0678],\n",
      "        [ 0.0801, -0.0706,  0.0351, -0.0406],\n",
      "        [-0.0175,  0.0669,  0.0229,  0.0174],\n",
      "        [-0.0324,  0.0067, -0.0462, -0.0483],\n",
      "        [ 0.0860,  0.0160,  0.0122,  0.0359],\n",
      "        [ 0.0542, -0.0770, -0.0258,  0.0683],\n",
      "        [-0.0590, -0.0269,  0.0553,  0.0716],\n",
      "        [ 0.0781,  0.0203, -0.0462,  0.0629],\n",
      "        [ 0.0124, -0.0418,  0.0457,  0.0071],\n",
      "        [-0.0077,  0.0158,  0.0176, -0.0724],\n",
      "        [ 0.0464, -0.0335, -0.0344,  0.0326],\n",
      "        [-0.0237,  0.0807, -0.0869,  0.0878]], requires_grad=True)\n",
      "spliced Wo: torch.Size([64, 2])\n",
      "tensor([[ 0.0043, -0.0332],\n",
      "        [-0.0022,  0.0810],\n",
      "        [-0.0249,  0.0128],\n",
      "        [ 0.0375, -0.0278],\n",
      "        [-0.0526, -0.0683],\n",
      "        [ 0.0869, -0.0703],\n",
      "        [ 0.0053,  0.0742],\n",
      "        [ 0.0271,  0.0347],\n",
      "        [ 0.0032, -0.0064],\n",
      "        [ 0.0064, -0.0518],\n",
      "        [-0.0549, -0.0834],\n",
      "        [ 0.0527, -0.0069],\n",
      "        [-0.0376, -0.0070],\n",
      "        [-0.0115, -0.0306],\n",
      "        [ 0.0319, -0.0084],\n",
      "        [ 0.0627, -0.0273],\n",
      "        [ 0.0397, -0.0394],\n",
      "        [ 0.0732, -0.0505],\n",
      "        [-0.0263, -0.0473],\n",
      "        [-0.0173, -0.0739],\n",
      "        [ 0.0706,  0.0280],\n",
      "        [ 0.0841, -0.0408],\n",
      "        [ 0.0569,  0.0070],\n",
      "        [ 0.0586, -0.0666],\n",
      "        [ 0.0389, -0.0767],\n",
      "        [ 0.0849, -0.0393],\n",
      "        [ 0.0197,  0.0544],\n",
      "        [ 0.0762, -0.0804],\n",
      "        [ 0.0344, -0.0160],\n",
      "        [ 0.0617,  0.0035],\n",
      "        [-0.0360, -0.0717],\n",
      "        [-0.0128, -0.0182],\n",
      "        [ 0.0435, -0.0558],\n",
      "        [-0.0242, -0.0033],\n",
      "        [ 0.0832, -0.0102],\n",
      "        [-0.0377, -0.0634],\n",
      "        [-0.0615,  0.0771],\n",
      "        [-0.0419,  0.0436],\n",
      "        [ 0.0021, -0.0355],\n",
      "        [ 0.0483, -0.0468],\n",
      "        [ 0.0874, -0.0572],\n",
      "        [-0.0554,  0.0737],\n",
      "        [-0.0864, -0.0697],\n",
      "        [-0.0144, -0.0533],\n",
      "        [-0.0706,  0.0772],\n",
      "        [-0.0039, -0.0808],\n",
      "        [-0.0437, -0.0564],\n",
      "        [-0.0642, -0.0373],\n",
      "        [-0.0432, -0.0495],\n",
      "        [-0.0368,  0.0721],\n",
      "        [-0.0485,  0.0189],\n",
      "        [-0.0361, -0.0346],\n",
      "        [-0.0509, -0.0678],\n",
      "        [ 0.0351, -0.0406],\n",
      "        [ 0.0229,  0.0174],\n",
      "        [-0.0462, -0.0483],\n",
      "        [ 0.0122,  0.0359],\n",
      "        [-0.0258,  0.0683],\n",
      "        [ 0.0553,  0.0716],\n",
      "        [-0.0462,  0.0629],\n",
      "        [ 0.0457,  0.0071],\n",
      "        [ 0.0176, -0.0724],\n",
      "        [-0.0344,  0.0326],\n",
      "        [-0.0869,  0.0878]], grad_fn=<CatBackward0>)\n",
      "projected output: torch.Size([1, 3, 2])\n",
      "tensor([[[ 0.1324, -0.1417],\n",
      "         [-0.0045, -0.0405],\n",
      "         [ 0.0003,  0.0333]]], grad_fn=<UnsafeViewBackward0>)\n",
      "----------------- END MultiQueryAttention.forwardTensor() --------------------\n",
      "x in layer after MQA & resid connection and before MLP:\n",
      "tensor([[[-0.2735, -1.0140],\n",
      "         [ 0.0167,  1.2070],\n",
      "         [ 0.7741,  0.3640]]], grad_fn=<AddBackward0>)\n",
      "------------- RMSNorm.forward() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.2735, -1.0140],\n",
      "         [ 0.0167,  1.2070],\n",
      "         [ 0.7741,  0.3640]]], grad_fn=<AddBackward0>)\n",
      "normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.3683, -1.3654],\n",
      "         [ 0.0196,  1.4141],\n",
      "         [ 1.2798,  0.6018]]], grad_fn=<MulBackward0>)\n",
      "dim: 2\n",
      "skip: 2\n",
      "spliced scale: torch.Size([2])\n",
      "tensor([0., 0.], grad_fn=<SliceBackward0>)\n",
      "scaled normed x: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.3683, -1.3654],\n",
      "         [ 0.0196,  1.4141],\n",
      "         [ 1.2798,  0.6018]]], grad_fn=<MulBackward0>)\n",
      "------------- END RMSNorm.forward() ------------\n",
      "---------- MLP Input: torch.Tensor ------------\n",
      "------------- MLP.forwardTensor() ------------\n",
      "x: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.3683, -1.3654],\n",
      "         [ 0.0196,  1.4141],\n",
      "         [ 1.2798,  0.6018]]], grad_fn=<MulBackward0>)\n",
      "d_dim: 2\n",
      "d_skip: 2\n",
      "i_dim: 8\n",
      "i_skip: 8\n",
      "Wgate: torch.Size([4, 16])\n",
      "Parameter containing:\n",
      "tensor([[ 0.4920,  0.3010,  0.1499, -0.3392,  0.4856, -0.4726,  0.0867,  0.2430,\n",
      "          0.3942,  0.4743, -0.2700, -0.3373,  0.3804, -0.2024, -0.3690, -0.1290],\n",
      "        [-0.4186, -0.3264, -0.4179,  0.2408, -0.1365,  0.1968,  0.0011, -0.4685,\n",
      "         -0.1507,  0.2918,  0.3232,  0.0377, -0.1145, -0.4494, -0.2852, -0.0918],\n",
      "        [-0.3881, -0.4971, -0.3846,  0.2184,  0.2203,  0.3713,  0.1256, -0.4725,\n",
      "          0.1326,  0.1626,  0.4547, -0.0986,  0.3189,  0.3653,  0.2537,  0.3719],\n",
      "        [ 0.2424, -0.1054,  0.2797, -0.0835, -0.3293,  0.3435,  0.2210,  0.4816,\n",
      "          0.2108,  0.2477, -0.1237, -0.2128,  0.3488, -0.4129, -0.1048,  0.3282]],\n",
      "       requires_grad=True)\n",
      "Wgate spliced: torch.Size([2, 8])\n",
      "tensor([[ 0.1326,  0.1626,  0.4547, -0.0986,  0.3189,  0.3653,  0.2537,  0.3719],\n",
      "        [ 0.2108,  0.2477, -0.1237, -0.2128,  0.3488, -0.4129, -0.1048,  0.3282]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bgate: torch.Size([16])\n",
      "Parameter containing:\n",
      "tensor([-0.4296, -0.2656, -0.3730, -0.4881,  0.2735,  0.3810, -0.2995, -0.2457,\n",
      "         0.3296, -0.1207, -0.1850,  0.1151, -0.1815,  0.0565,  0.1133,  0.2197],\n",
      "       requires_grad=True)\n",
      "Bgate spliced: torch.Size([8])\n",
      "tensor([ 0.3296, -0.1207, -0.1850,  0.1151, -0.1815,  0.0565,  0.1133,  0.2197],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xgate: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.0071, -0.5188, -0.1835,  0.4421, -0.7751,  0.4857,  0.1629,\n",
      "          -0.3654],\n",
      "         [ 0.6302,  0.2328, -0.3510, -0.1878,  0.3179, -0.5202, -0.0300,\n",
      "           0.6910],\n",
      "         [ 0.6261,  0.2365,  0.3224, -0.1392,  0.4364,  0.2754,  0.3749,\n",
      "           0.8931]]], grad_fn=<AddBackward0>)\n",
      "GeLU'ed Xgate: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.0035, -0.1567, -0.0784,  0.2965, -0.1699,  0.3334,  0.0920,\n",
      "          -0.1306],\n",
      "         [ 0.4637,  0.1378, -0.1273, -0.0799,  0.1986, -0.1568, -0.0146,\n",
      "           0.5219],\n",
      "         [ 0.4598,  0.1403,  0.2020, -0.0619,  0.2919,  0.1676,  0.2423,\n",
      "           0.7271]]], grad_fn=<GeluBackward0>)\n",
      "Wup: torch.Size([4, 16])\n",
      "Parameter containing:\n",
      "tensor([[-0.4397,  0.0639,  0.3251, -0.4417, -0.1198, -0.4560,  0.3213, -0.1483,\n",
      "         -0.0726, -0.1437, -0.1375,  0.2211,  0.4713, -0.1941,  0.2534, -0.1524],\n",
      "        [-0.1683,  0.0169, -0.3806, -0.3985,  0.0782, -0.4636, -0.4176, -0.0484,\n",
      "         -0.3432, -0.1917,  0.4930,  0.3492, -0.0744,  0.1426, -0.3753,  0.4084],\n",
      "        [-0.2476,  0.0072, -0.3127, -0.3605,  0.3117,  0.4990,  0.0193,  0.1382,\n",
      "          0.2498,  0.1919,  0.2682, -0.3924, -0.0709, -0.2175, -0.3473,  0.1599],\n",
      "        [ 0.4299,  0.3039,  0.2940, -0.4826,  0.1980, -0.0223, -0.0833, -0.0818,\n",
      "          0.1703, -0.4762, -0.0675,  0.1609, -0.0555, -0.1132, -0.4620,  0.0077]],\n",
      "       requires_grad=True)\n",
      "Wup spliced: torch.Size([2, 8])\n",
      "tensor([[ 0.2498,  0.1919,  0.2682, -0.3924, -0.0709, -0.2175, -0.3473,  0.1599],\n",
      "        [ 0.1703, -0.4762, -0.0675,  0.1609, -0.0555, -0.1132, -0.4620,  0.0077]],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Bup: torch.Size([16])\n",
      "Parameter containing:\n",
      "tensor([ 0.3836,  0.0685, -0.3794,  0.2138,  0.0660,  0.0780, -0.2228,  0.0113,\n",
      "        -0.1320,  0.3062, -0.2345,  0.4163, -0.4161,  0.3209,  0.2370, -0.2584],\n",
      "       requires_grad=True)\n",
      "Bup spliced: torch.Size([8])\n",
      "tensor([-0.1320,  0.3062, -0.2345,  0.4163, -0.4161,  0.3209,  0.2370, -0.2584],\n",
      "       grad_fn=<SliceBackward0>)\n",
      "Xup: torch.Size([1, 3, 8])\n",
      "tensor([[[-0.4564,  0.8857, -0.2411,  0.3411, -0.3141,  0.5555,  0.9957,\n",
      "          -0.3278],\n",
      "         [ 0.1137, -0.3635, -0.3246,  0.6361, -0.4960,  0.1565, -0.4230,\n",
      "          -0.2444],\n",
      "         [ 0.2902,  0.2652,  0.0681,  0.0109, -0.5403, -0.0256, -0.4855,\n",
      "          -0.0491]]], grad_fn=<AddBackward0>)\n",
      "Xfuse: torch.Size([1, 3, 8])\n",
      "tensor([[[ 0.0016, -0.1387,  0.0189,  0.1011,  0.0534,  0.1852,  0.0916,\n",
      "           0.0428],\n",
      "         [ 0.0527, -0.0501,  0.0413, -0.0508, -0.0985, -0.0245,  0.0062,\n",
      "          -0.1275],\n",
      "         [ 0.1334,  0.0372,  0.0138, -0.0007, -0.1577, -0.0043, -0.1176,\n",
      "          -0.0357]]], grad_fn=<MulBackward0>)\n",
      "Wdown: torch.Size([16, 4])\n",
      "Parameter containing:\n",
      "tensor([[-0.1754,  0.0761,  0.1232,  0.1366],\n",
      "        [-0.0635,  0.1460,  0.0160, -0.1254],\n",
      "        [ 0.0276, -0.1514, -0.0825, -0.1608],\n",
      "        [ 0.0913, -0.0020,  0.2442,  0.1662],\n",
      "        [ 0.1354,  0.1174,  0.2448, -0.2325],\n",
      "        [-0.0123,  0.0650,  0.1438, -0.1913],\n",
      "        [-0.0013,  0.2201,  0.1408,  0.0611],\n",
      "        [-0.1373,  0.1180,  0.1358, -0.1107],\n",
      "        [ 0.1830,  0.2221, -0.1693,  0.2425],\n",
      "        [-0.1213, -0.0698,  0.0168,  0.1718],\n",
      "        [-0.0677, -0.2102, -0.0295, -0.1922],\n",
      "        [ 0.0277,  0.0011,  0.1526, -0.0894],\n",
      "        [ 0.2469,  0.1777,  0.0815,  0.0353],\n",
      "        [-0.1205, -0.1902, -0.1469, -0.0820],\n",
      "        [ 0.0319, -0.0125,  0.0571, -0.1865],\n",
      "        [ 0.0971,  0.0427,  0.2102,  0.0165]], requires_grad=True)\n",
      "Wdown spliced: torch.Size([8, 2])\n",
      "tensor([[-0.1693,  0.2425],\n",
      "        [ 0.0168,  0.1718],\n",
      "        [-0.0295, -0.1922],\n",
      "        [ 0.1526, -0.0894],\n",
      "        [ 0.0815,  0.0353],\n",
      "        [-0.1469, -0.0820],\n",
      "        [ 0.0571, -0.1865],\n",
      "        [ 0.2102,  0.0165]], grad_fn=<SliceBackward0>)\n",
      "Bdown: torch.Size([4])\n",
      "Parameter containing:\n",
      "tensor([-0.0242,  0.1740,  0.0925,  0.2142], requires_grad=True)\n",
      "Bdown spliced: torch.Size([2])\n",
      "tensor([0.0925, 0.2142], grad_fn=<SliceBackward0>)\n",
      "outputs: torch.Size([1, 3, 2])\n",
      "tensor([[[0.0962, 0.1484],\n",
      "         [0.0429, 0.2103],\n",
      "         [0.0436, 0.2665]]], grad_fn=<AddBackward0>)\n",
      "------------- END MLP.forwardTensor() ------------\n",
      "layer's final residual state:\n",
      "tensor([[[-0.1773, -0.8656],\n",
      "         [ 0.0596,  1.4173],\n",
      "         [ 0.8177,  0.6306]]], grad_fn=<AddBackward0>)\n",
      "----------------- END Layer.forwardTensor() --------------------\n",
      "forwardTensor() output: torch.Size([1, 3, 2])\n",
      "tensor([[[-0.1773, -0.8656],\n",
      "         [ 0.0596,  1.4173],\n",
      "         [ 0.8177,  0.6306]]], grad_fn=<AddBackward0>)\n",
      "final output: ((tensor([[[ 1.2365,  0.0804,  0.6235,  1.7477],\n",
      "         [ 1.1942, -0.9424,  0.9134,  2.3550],\n",
      "         [ 1.6781,  1.0569, -1.5321,  0.0980]]], grad_fn=<AddBackward0>),), (tensor([[[ 0.8293,  0.5314],\n",
      "         [-0.4652,  1.3387],\n",
      "         [ 1.0220,  3.1334]]], grad_fn=<AddBackward0>), tensor([[[-0.1773, -0.8656],\n",
      "         [ 0.0596,  1.4173],\n",
      "         [ 0.8177,  0.6306]]], grad_fn=<AddBackward0>)))\n",
      "------------- END Layer.forwardTuple() ------------\n",
      "out: ((tensor([[[ 1.2365,  0.0804,  0.6235,  1.7477],\n",
      "         [ 1.1942, -0.9424,  0.9134,  2.3550],\n",
      "         [ 1.6781,  1.0569, -1.5321,  0.0980]]], grad_fn=<AddBackward0>),), (tensor([[[ 0.8293,  0.5314],\n",
      "         [-0.4652,  1.3387],\n",
      "         [ 1.0220,  3.1334]]], grad_fn=<AddBackward0>), tensor([[[-0.1773, -0.8656],\n",
      "         [ 0.0596,  1.4173],\n",
      "         [ 0.8177,  0.6306]]], grad_fn=<AddBackward0>)))\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n"
     ]
    }
   ],
   "source": [
    "# Testing our Layer's forwardTuple()\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4 = config.hidden_size, config.levels, config.max_position_embeddings, config.hidden_size\n",
    "config.hidden_size = 4\n",
    "config.levels = 2\n",
    "config.max_position_embeddings = 3\n",
    "config.vocab_size = 5\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "embedding = torch.randn(config.vocab_size, config.hidden_size)\n",
    "print(f\"embedding: {embedding.shape}\\n{embedding}\")\n",
    "\n",
    "layer = Layer(config)\n",
    "# we need to make sure to send in a tuple of the expected size. above we set hidden_size=4 and levels=2\n",
    "x = ((torch.randn((1,3,config.hidden_size)),),\n",
    "     (torch.randn((1,3,config.hidden_size//config.split)),torch.randn((1,3,config.hidden_size//config.split))))\n",
    "print(f\"x: {x}\")\n",
    "out = layer(x)\n",
    "print(f\"out: {out}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.levels = hold2\n",
    "config.max_position_embeddings = hold3\n",
    "config.vocab_size = hold4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "# clear up memory\n",
    "del hold1, hold2, hold3, hold4, x, layer, out, embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9089c1",
   "metadata": {},
   "source": [
    "# Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e090675",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractalLoss(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, logits, target):\n",
    "        \"\"\"\n",
    "        input: \n",
    "            - logits are a tuple of tuples of tensors each of shape [batch_size, max_seq_len, vocab_size]\n",
    "            - target is a shape [batch_size, max_seq_len] tensor of the integer indices of the correct tokens\n",
    "        output: a tensor containing a single float of the loss value\n",
    "        \"\"\"\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- FractalLoss.forward() ------------\")\n",
    "            print(f\"logits:\\n{logits}\")\n",
    "            \n",
    "        assert type(logits) == tuple # since this function should only be used during training\n",
    "            \n",
    "        # should only be used during training, so we assert input_len == max_position_embeddings\n",
    "        b,t,v = logits[0][0].shape\n",
    "        if verbose: print(f\"b:{b}, t:{t}, v:{v}, b*t:{b*t}\")\n",
    "        assert t == config.max_position_embeddings\n",
    "        \n",
    "        # Calculate losses for each output and stack them. \n",
    "        # i apologize for the weird format instead of regular for loops, but it feels better in my head\n",
    "        loss = torch.stack([ # stacks across levels\n",
    "                            torch.stack( # stacks across models in level\n",
    "                                        [self.criterion(logits_ij.view(b*t, v), # reshapes for CELoss\n",
    "                                                        target.view(b*t)) \n",
    "                                         for logits_ij in logits[i]] # iterates across models in level\n",
    "                            ).sum() # sums across models in level\n",
    "                            for i in range(len(logits))] # iterates across levels\n",
    "                          ).sum() # sums across levels\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"final loss: {loss}\")\n",
    "            print(\"------------- END FractalLoss.forward() ------------\")\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4151cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------- Micro Hyperparameters -------\n",
      "model_count:  [1, 2]\n",
      "model_dim_list:  [4, 2]\n",
      "embedding: torch.Size([5, 4])\n",
      "tensor([[ 0.7798, -1.2821,  1.4271, -1.5708],\n",
      "        [ 0.5209,  0.8941, -0.8535,  1.2495],\n",
      "        [-0.2803,  0.2022,  0.6984,  0.7937],\n",
      "        [-1.9587, -0.5391,  0.3903, -0.3077],\n",
      "        [-0.1755, -0.0080, -1.2165,  0.8529]])\n",
      "logits: ((tensor([[[ 0.1422, -0.3405,  0.2353,  0.6880, -0.8742],\n",
      "         [ 0.1979,  0.9890,  0.0070, -1.8563, -1.5080],\n",
      "         [ 0.0152,  1.4316,  1.1355,  0.5441,  0.2532]],\n",
      "\n",
      "        [[-0.7838, -0.7004, -0.1135, -0.6035,  0.8739],\n",
      "         [-0.7674, -0.7594, -1.9494,  0.0333, -0.8108],\n",
      "         [ 0.6092, -0.2454,  1.0809,  1.0434,  1.2647]]]),), (tensor([[[ 1.4386, -0.1488, -0.2277,  0.3608,  0.3705],\n",
      "         [ 1.2773,  0.7523,  0.6605,  0.9307,  1.2303],\n",
      "         [-1.2080,  0.8718,  0.9394, -0.6804,  0.2274]],\n",
      "\n",
      "        [[-1.4003,  0.4723,  0.7164, -1.9626,  0.6976],\n",
      "         [-3.2143,  0.1564,  1.4848,  0.4646,  1.2435],\n",
      "         [-0.6979,  0.0184, -1.0786, -0.9153,  0.0878]]]), tensor([[[ 0.6393, -0.8985,  0.7073,  0.1996,  2.1958],\n",
      "         [-0.0376,  0.3781, -0.9874, -0.3961,  0.8724],\n",
      "         [ 0.0401, -1.1299, -0.2367,  1.5324, -0.4624]],\n",
      "\n",
      "        [[-0.5976,  1.4669,  1.9744, -1.4081,  1.6576],\n",
      "         [ 0.7153,  1.1098, -0.3809, -1.6481, -0.7548],\n",
      "         [ 0.9231,  1.4474,  0.2814,  0.4342,  0.7594]]])))\n",
      "target: tensor([[[4, 4, 0],\n",
      "         [4, 3, 3]]])\n",
      "------------- FractalLoss.forward() ------------\n",
      "logits:\n",
      "((tensor([[[ 0.1422, -0.3405,  0.2353,  0.6880, -0.8742],\n",
      "         [ 0.1979,  0.9890,  0.0070, -1.8563, -1.5080],\n",
      "         [ 0.0152,  1.4316,  1.1355,  0.5441,  0.2532]],\n",
      "\n",
      "        [[-0.7838, -0.7004, -0.1135, -0.6035,  0.8739],\n",
      "         [-0.7674, -0.7594, -1.9494,  0.0333, -0.8108],\n",
      "         [ 0.6092, -0.2454,  1.0809,  1.0434,  1.2647]]]),), (tensor([[[ 1.4386, -0.1488, -0.2277,  0.3608,  0.3705],\n",
      "         [ 1.2773,  0.7523,  0.6605,  0.9307,  1.2303],\n",
      "         [-1.2080,  0.8718,  0.9394, -0.6804,  0.2274]],\n",
      "\n",
      "        [[-1.4003,  0.4723,  0.7164, -1.9626,  0.6976],\n",
      "         [-3.2143,  0.1564,  1.4848,  0.4646,  1.2435],\n",
      "         [-0.6979,  0.0184, -1.0786, -0.9153,  0.0878]]]), tensor([[[ 0.6393, -0.8985,  0.7073,  0.1996,  2.1958],\n",
      "         [-0.0376,  0.3781, -0.9874, -0.3961,  0.8724],\n",
      "         [ 0.0401, -1.1299, -0.2367,  1.5324, -0.4624]],\n",
      "\n",
      "        [[-0.5976,  1.4669,  1.9744, -1.4081,  1.6576],\n",
      "         [ 0.7153,  1.1098, -0.3809, -1.6481, -0.7548],\n",
      "         [ 0.9231,  1.4474,  0.2814,  0.4342,  0.7594]]])))\n",
      "b:2, t:3, v:5, b*t:6\n",
      "final loss: 5.459757328033447\n",
      "------------- END FractalLoss.forward() ------------\n",
      "out: 5.459757328033447\n",
      "---------- RESET CONFIG --------\n",
      "model_count:  [1, 2, 4]\n",
      "model_dim_list:  [128, 64, 32]\n"
     ]
    }
   ],
   "source": [
    "# Testing our FractalLoss\n",
    "verbose = True\n",
    "\n",
    "print(\"--------- Micro Hyperparameters -------\")\n",
    "hold1, hold2, hold3, hold4 = config.hidden_size, config.levels, config.max_position_embeddings, config.hidden_size\n",
    "config.hidden_size = 4\n",
    "config.levels = 2\n",
    "config.max_position_embeddings = 3\n",
    "config.vocab_size = 5\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "embedding = torch.randn(config.vocab_size, config.hidden_size)\n",
    "print(f\"embedding: {embedding.shape}\\n{embedding}\")\n",
    "\n",
    "loss = FractalLoss(config)\n",
    "# we need to make sure to send in a tuple of the expected size. above we set hidden_size=4 and levels=2\n",
    "logits = ((torch.randn((2,3,config.vocab_size)),),\n",
    "     (torch.randn((2,3,config.vocab_size)),torch.randn((2,3,config.vocab_size))))\n",
    "print(f\"logits: {logits}\")\n",
    "target = torch.randint(config.vocab_size, (2,3)).unsqueeze(0)\n",
    "print(f\"target: {target}\")\n",
    "out = loss(logits, target)\n",
    "print(f\"out: {out}\")\n",
    "\n",
    "verbose = False\n",
    "print(\"---------- RESET CONFIG --------\")\n",
    "config.hidden_size = hold1\n",
    "config.levels = hold2\n",
    "config.max_position_embeddings = hold3\n",
    "config.vocab_size = hold4\n",
    "print(\"model_count: \", config.model_count)\n",
    "print(\"model_dim_list: \", config.model_dim_list)\n",
    "\n",
    "# clear up memory\n",
    "del hold1, hold2, hold3, hold4, embedding, loss, logits, target, out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d511fec",
   "metadata": {},
   "source": [
    "# The Model itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9d0b47ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FractalFormer_base(nn.Module):\n",
    "    def __init__(self, config: Config, tokenizer: tokenizer):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # hyperparameters\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.max_seq_len = config.max_position_embeddings\n",
    "        self.head_dim = config.head_dim\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        ### FractalFormer-specific hyperparameters\n",
    "        self.num_levels = config.levels # the number of levels for sub-models to exist on\n",
    "        self.split = config.split # the number of splits to make at a given level\n",
    "        self.model_count = config.model_count # list of number of models at a given level\n",
    "        self.model_dim_list = config.model_dim_list # list of hidden dimensions corresponding to each given level\n",
    "        self.head_dim_list = config.head_dim_list # list of attention head dimensions corresponding to each given level    \n",
    "\n",
    "        # the embedding matrix. for converting tokens to the first residual state, and the last residual state to logits\n",
    "        self.embedder = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "\n",
    "        # for normalizing the initial embeddings\n",
    "        self.embedder_norm = RMSNorm(config.hidden_size)\n",
    "\n",
    "        # Initialize a sequence of DecoderLayer instances as specified by the number of hidden layers in the config\n",
    "        self.layers = nn.ModuleList(Layer(config) for _ in range(config.num_hidden_layers))\n",
    "\n",
    "        # initializing output layer\n",
    "        self.output_layer = OutputLayer(self.embedder.weight, config)\n",
    "        # i think i need to do this bc in the above version you can't use `self.` inside the init\n",
    "        #@property \n",
    "        #def output_layer(self):\n",
    "            #return OutputLayer(self.embedder.weight, config)\n",
    "\n",
    "        # the loss function\n",
    "        self.criterion = FractalLoss(config)\n",
    "\n",
    "    def forwardTensor(self,\n",
    "                      input_token_ids: torch.Tensor,\n",
    "                      level: int = 0, # integer designating the level of model to use. 0 is largest model, -1 is smallest\n",
    "                      model: int = 0, # integer designating the model in that level to use. 0 is top-left, -1 is bottom right\n",
    "                     ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        inputs: \n",
    "            - input_token_ids (torch.Tensor): a tensor of integers size (batch_size, sequence_length)\n",
    "            - level: integer designating the level of model to use. 0 is largest model, -1 is smallest\n",
    "            - model: integer designating the model in that level to use. 0 is top-left, -1 is bottom right\n",
    "        output: a torch.Tensor shape (batch_size, sequence_length, vocab_size)\n",
    "        \"\"\"\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- FractalFormer.forwardTensor() ------------\")\n",
    "            print(f\"input_token_ids: {input_token_ids.shape}\\n{input_token_ids}\")\n",
    "        \n",
    "        # adjusting everything to the specified level & model\n",
    "        d_dim = self.hidden_size // (2**level)\n",
    "        d_skip = model * d_dim\n",
    "        if verbose:\n",
    "            print(f\"d_dim: {d_dim}\")\n",
    "            print(f\"d_skip: {d_skip}\")\n",
    "        \n",
    "        # turn the input tokens into the first residual state using the embedding matrix\n",
    "        # (batch_size, input_len) & (vocab_size, hidden_size) -> (batch_size, input_len, hidden_size) -> (batch_size, input_len, d_dim)\n",
    "        x = self.embedder(input_token_ids)\n",
    "        if verbose: print(f\"x0: {x.shape}\\n{x}\")\n",
    "\n",
    "        x = x[:,:, d_skip:d_skip + d_dim]\n",
    "        if verbose: print(f\"spliced x0: {x0.shape}\\n{x0}\")\n",
    "        \n",
    "        # Gemma normalizes the embedding by sqrt(hidden_size)\n",
    "        # the question is, should I do this with the full sized hidden_size or do it at the splice size????\n",
    "        # imma do it at the splice size and change it later if i think the models aren't learning well\n",
    "        #x = x * (d_dim**0.5)\n",
    "        # alternatively i could just switch to doing a regular RMSNorm which would be more like me\n",
    "        # if i figure out this different sizes of hyperspheres thing it'd be more in line with that\n",
    "        x = self.embedder_norm(x, model)\n",
    "        if verbose: print(f\"normalized initial x: {x.shape}\\n{x}\")\n",
    "\n",
    "        # Iteratively process the input through each Layer\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if verbose: print(f\"begin layer {i}\")\n",
    "            x = layer(x, model)\n",
    "            if verbose: print(f\"output of layer {i}: {x.shape}\\n{x}\")\n",
    "\n",
    "        logits = self.output_layer(x, model)\n",
    "        if verbose: \n",
    "            print(f\"output logits: {logits.shape}\\n{logits}\")\n",
    "            print(\"------------- END FractalFormer.forwardTensor() ------------\")\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def forwardTuple(self,\n",
    "                     input_token_ids: torch.Tensor,\n",
    "                     target_token_ids: torch.Tensor,\n",
    "                    ) -> torch.Tensor:\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- FractalFormer.forwardTuple() ------------\")\n",
    "            print(f\"input_token_ids: {input_token_ids.shape}\\n{input_token_ids}\")\n",
    "            print(f\"target_token_ids: {target_token_ids.shape}\\n{target_token_ids}\")\n",
    "        \n",
    "        # use the embedding matrix to turn the input tokens into the first residual state of the largest model\n",
    "        # (batch_size, input_len) & (vocab_size, hidden_size) -> (batch_size, input_len, hidden_size)\n",
    "        x0 = self.embedder(input_token_ids)\n",
    "        if verbose: print(f\"initial x: {x.shape}\\n{x}\")\n",
    "\n",
    "        # create the first fractal tuple of residual states\n",
    "        x = ()\n",
    "        for i, models_in_level in enumerate(config.model_count):\n",
    "            if verbose: print(f\"i: {i}, models_in_level: {models_in_level}, iterating over {config.model_count}\")\n",
    "            \n",
    "            x_lvl = ()\n",
    "            for j, d_dim in enumerate(config.model_dim_list):\n",
    "                if verbose: print(f\"j: {j}, d_dim: {d_dim}, iterating over {config.model_dim_list}\")\n",
    "\n",
    "                skip = j * d_dim\n",
    "                if verbose: print(f\"skip: {skip}\")\n",
    "                \n",
    "                x_ij_spliced = x0[:,:,skip:skip + d_dim]\n",
    "                if verbose: print(f\"initial x[{i}][{j}] spliced: {x_ij_spliced.shape}\\n{x_ij_spliced}\")\n",
    "                    \n",
    "                x_ij_spliced_normed = self.embedder_norm(x_ij_spliced, model=j) # * (d_dim**0.5) # if i want to do Gemma normalization instead\n",
    "                if verbose: print(f\"initial x[{i}][{j}] spliced & normed: {x_ij_spliced_normed.shape}\\n{x_ij_spliced_normed}\")\n",
    "                \n",
    "                x_lvl += (x_ij_spliced_normed,)  \n",
    "            x += (x_lvl,)\n",
    "        if verbose: print(f\"full tuple initial x: {x0}\")\n",
    "\n",
    "        # Iteratively process the input through each Layer\n",
    "        for i, layer in enumerate(self.layers):\n",
    "            if verbose: print(f\"begin layer {i}\")\n",
    "            \n",
    "            x = layer(x)\n",
    "            if verbose: print(f\"output of layer {i}: {x}\")\n",
    "\n",
    "        logits = self.output_layer(x)\n",
    "        if verbose: \n",
    "            print(f\"output logits: {logits}\")\n",
    "            print(\"------------- END FractalFormer.forwardTuple() ------------\")\n",
    "\n",
    "        return logits\n",
    "\n",
    "    def forward(self,\n",
    "                input_token_ids: torch.Tensor, # a shape (batch_size, input_seq_len OR max_seq_len)list of integer token ids\n",
    "                target_token_ids: torch.Tensor = None, # a shape (batch_size, max_seq_len) list of token ids to train on\n",
    "                level: int = 0, # integer designating the level of model to use. 0 is largest model\n",
    "                model: int = 0, # integer designating the model in that level to use. 0 is top-left model in level\n",
    "                ):\n",
    "        global verbose\n",
    "        if verbose: \n",
    "            print(\"------------- FractalFormer.forward() ------------\")\n",
    "            print(f\"input_token_ids: {input_token_ids.shape}\\n{input_token_ids}\")\n",
    "            print(f\"target_token_ids: {target_token_ids}\")\n",
    "            print(f\"level: {level}\")\n",
    "            print(f\"model: {model}\")\n",
    "        \n",
    "        if target_token_ids is None: # if we're not training, then we don't need to calculate loss\n",
    "            logits = self.forwardTensor(input_token_ids, level, model)\n",
    "            loss = None\n",
    "        else:\n",
    "            # if we are training\n",
    "            # training uses a tuple of tuples of tensors\n",
    "            logits = self.forwardTuple(input_token_ids, target_token_ids) # -> Tuple[Tuple[Tensor shape (batch_size, max_seq_len, vocab_size)]]\n",
    "            \n",
    "            # custom Fractal CELoss function\n",
    "            loss = self.criterion(logits, target_token_ids) \n",
    "        \n",
    "        if verbose: \n",
    "            print(f\"logits: {logits}\")\n",
    "            print(f\"loss: {loss}\")\n",
    "            print(\"------------- END FractalFormer.forward() ------------\")\n",
    "        \n",
    "        return logits, loss\n",
    "\n",
    "    @torch.no_grad() # no need to keep track of gradients during inference\n",
    "    def Sampler(\n",
    "        self,\n",
    "        logits: torch.Tensor, # shape (batch_size, input_len, vocab_size)\n",
    "        temperature: float, # controls how boring vs random the outputs should be\n",
    "        top_p: float, # the maximum cumulative probability of output options we're willing to consider\n",
    "        top_k: int, # the maximum number of output options we're willing to consider\n",
    "    ) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        The Sampler function is responsible for generating token predictions from Gemma's output.\n",
    "        It supports temperature scaling, top-p (nucleus) sampling, and top-k sampling \n",
    "        The class operates as follows:\n",
    "    \n",
    "        1. Selects the last hidden state for each sequence in the batch\n",
    "    \n",
    "        2. Computes logits by multiplying the selected hidden states with the transposed embedding matrix. \n",
    "    \n",
    "        3. Temperature is used to scale the logits, making the distribution over tokens sharper (lower temperature) \n",
    "        or flatter (higher temperature), which affects the randomness of the sampling (flatter -> more random)\n",
    "    \n",
    "        4. The softmax function is applied to the scaled logits to obtain a probability distribution over the vocabulary.\n",
    "    \n",
    "        5. For top-p sampling, the function computes the cumulative sum of the sorted probabilities and masks out tokens until the \n",
    "        cumulative probability exceeds the threshold defined by `top_ps`. This allows the model to focus on a subset of the most \n",
    "        probable tokens while ignoring the long tail of less likely tokens. \n",
    "        We to ignore long tail probabilities to avoid nonsensical output\n",
    "    \n",
    "        7. For top-k sampling, the function masks out all tokens except the `k` most likely ones, as specified by `top_ks`. \n",
    "        This ensures that the model only considers a fixed number of the most probable tokens for the next token prediction.\n",
    "    \n",
    "        8. After applying both the top-p and top-k masks, the probabilities are re-normalized so that they sum up to 1\n",
    "    \n",
    "        9. The function then samples from the re-normalized probability distribution to select the next token. \n",
    "        \"\"\"\n",
    "        if config.verbose['Sampler']:\n",
    "            print(\"----------------- FractalFormer.Sampler() --------------\")\n",
    "            print(f\"temperature: {temperature}, top_p: {top_p}, top_k: {top_k}\")\n",
    "            \n",
    "        # Select the last element for each sequence.\n",
    "        # (batch_size, input_len, vocab_size) -> (batch_size, vocab_size)\n",
    "        logits = logits[:,-1,:]\n",
    "        if config.verbose['Sampler']: print(f\"logits: {logits.shape}\\n{logits}\")\n",
    "        \n",
    "        # Apply temperature scaling\n",
    "        # (batch_size, vocab_size) / float -> (batch_size, vocab_size)\n",
    "        logits.clone().div_(temperature) # the clone() is because i didn't properly prevent gradient tracking and i'm too lazy to fix the issue at its cause\n",
    "        if config.verbose['Sampler']: print(f\"logits w temperature: {logits.shape}\\n{logits}\")\n",
    "\n",
    "        # Calculate probabilities with softmax.\n",
    "        probs = torch.softmax(logits, dim=-1, dtype=torch.float) # dim=-1 is the vocab_size dimension that we calculate along\n",
    "        if config.verbose['Sampler']: print(f\"probs: {probs.shape}\\n{probs}\")\n",
    "\n",
    "        # sort the probabilities to for use in top-p & top-k\n",
    "        # both are (batch_size, vocab_size)\n",
    "        probs_sort, probs_idx = torch.sort(probs, dim=-1, descending=True)\n",
    "        # probs_sort contains float probabilities while probs_idx contains integer indices\n",
    "        if config.verbose['Sampler']: \n",
    "            print(f\"probs_sort: {probs_sort.shape}\\n{probs_sort}\")\n",
    "            print(f\"probs_idx: {probs_idx.shape}\\n{probs_idx}\")\n",
    "\n",
    "        # calculating top-p\n",
    "        # creates same-size tensor of cumulatve probabilities instead of indivdiual probs\n",
    "        probs_sum = torch.cumsum(probs_sort, dim=-1) \n",
    "        if config.verbose['Sampler']: print(f\"probs_sum: {probs_sum.shape}\\n{probs_sum}\")\n",
    "        # mask where 0's are top-p selections & 1's are to be excluded\n",
    "        top_ps_mask = (probs_sum - probs_sort) > top_p\n",
    "        if config.verbose['Sampler']: print(f\"top_ps_mask: {top_ps_mask.shape}\\n{top_ps_mask}\")\n",
    "        # the original probabilities with excluded tokens changed to 0.0\n",
    "        probs_sort = torch.where(top_ps_mask, 0, probs_sort) \n",
    "        if config.verbose['Sampler']: print(f\"probs_sort: {probs_sort.shape}\\n{probs_sort}\")\n",
    "\n",
    "        # calculating top_k\n",
    "        # create a shape (vocab_size) tensor that just iterates up by 1's\n",
    "        top_ks_mask = torch.arange(probs_idx.shape[-1], device=probs_idx.device) \n",
    "        if config.verbose['Sampler']: print(f\"top_ks_mask: {top_ks_mask.shape}\\n{top_ks_mask}\")\n",
    "        # expand our mask along the batch_size dimension to become size (batch_size, vocab_size)\n",
    "        # \"expand\" means copy the original into this new size, so each length vocab_size row is the same\n",
    "        top_ks_mask = top_ks_mask.expand(probs_idx.shape[0], -1)\n",
    "        if config.verbose['Sampler']: print(f\"top_ks_mask: {top_ks_mask.shape}\\n{top_ks_mask}\")\n",
    "        # top_ks is a list of integers. we keep whichever entries in top_ks_mask are greater than their corresponding entries in top_ks\n",
    "        top_ks_mask = top_ks_mask >= top_k\n",
    "        if config.verbose['Sampler']: print(f\"top_ks_mask: {top_ks_mask.shape}\\n{top_ks_mask}\")\n",
    "\n",
    "        # we'll be combining top-p with top-k and using whichever gives us fewer tokens. a very conservative approach\n",
    "        # this trims probs_sort to also fit within our top_k requirement\n",
    "        probs_sort = torch.where(top_ks_mask, 0, probs_sort)\n",
    "        if config.verbose['Sampler']: print(f\"probs_sort: {probs_sort.shape}\\n{probs_sort}\")\n",
    "\n",
    "        # Re-normalization so that total probabilities add up to 1\n",
    "        probs_sort.div_(probs_sort.sum(dim=-1, keepdim=True))\n",
    "        if config.verbose['Sampler']: print(f\"probs_sort: {probs_sort.shape}\\n{probs_sort}\")\n",
    "        \n",
    "        # now we rearrange the modified probabilities in probs_sort back to their original order according to probs_idx\n",
    "        probs = torch.gather(probs_sort,\n",
    "                             dim=-1,\n",
    "                             index=torch.argsort(probs_idx, dim=-1))\n",
    "        if config.verbose['Sampler']: print(f\"probs: {probs.shape}\\n{probs}\")\n",
    "        \n",
    "        # samples from the distribution\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1)\n",
    "        if config.verbose['Sampler']: print(f\"next_token_id: {next_token_id.shape}\\n{next_token_id}\")\n",
    "        \n",
    "        return next_token_id # returns the predicted token\n",
    "        \n",
    "    def generate(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        output_len: int = 100, # the model will output 100 tokens\n",
    "        temperature: float = 0.7, # 0.95 is pretty close to not even using temperature at all (1.0 would be no effect)\n",
    "        top_p: float = 1.0, # defaulting to 1 means we essentially don't use top-p\n",
    "        top_k: int = config.vocab_size, # setting top_k = vocab_size means we're effectively not using top_k at all\n",
    "        level: int = 0, # which size model we want to perform inference with\n",
    "        model: int = 0, # which model in that level we want to perform inference with\n",
    "    ) -> str: \n",
    "        \n",
    "        # encoding the prompt into token indices\n",
    "        tokens = self.tokenizer.encode(prompt)\n",
    "\n",
    "        # turning it into the right tensor shape\n",
    "        tokens = torch.tensor(tokens, device=config.device).unsqueeze(0)\n",
    "        \n",
    "        # we wouldn't want to go past the maximum context length we trained on\n",
    "        assert len(tokens) + output_len <= self.config.max_position_embeddings\n",
    "\n",
    "        for i in range(output_len):\n",
    "            # get the model's output logits and ignore the loss, which would be a NoneType object\n",
    "            logits, _ = self(tokens[:,:self.max_seq_len], level=level, model=model)\n",
    "            \n",
    "            next_token = self.Sampler(\n",
    "                logits = logits, # the actual output of the model\n",
    "                temperature = temperature,\n",
    "                top_p = top_p,\n",
    "                top_k = top_k\n",
    "            )\n",
    "            #print(next_token)\n",
    "\n",
    "            # add our new token to the sequence\n",
    "            tokens = torch.cat((tokens, next_token), dim=1)\n",
    "\n",
    "        # decode our list of tokens to an actual string\n",
    "        output = self.tokenizer.decode(tokens.squeeze(0).tolist())\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7641ae",
   "metadata": {},
   "source": [
    "# Training-related Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b4604bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test splits\n",
    "data = torch.tensor(tokenizer.encode(text), dtype=torch.long)\n",
    "n = int(0.9*len(data)) # first 90% will be our training dataset, the rest for validation\n",
    "train_data = data[:n]\n",
    "val_data = data[n:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d76bf9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loading for training which generates a small batch of data of inputs x and targets y\n",
    "def get_batch(split, batch_size):\n",
    "    # whether we grab from our training or validation dataset\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - config.max_position_embeddings, (batch_size,))\n",
    "    x = torch.stack([data[i:i+config.max_position_embeddings] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+config.max_position_embeddings+1] for i in ix])\n",
    "    x, y = x.to(config.device), y.to(config.device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "74f0d059",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 83,  80,   1,  57,  54, 112,  58,  57,   1,  94,   1,  72,   1,  52,\n",
      "          47, 122,  58,   0,  17,  52,  95,  50,  53,  54,  43,   1,  88,  66,\n",
      "          45, 126,  42,   1,  28, 115,  60,  53,  80,   2,   1,  35,  46,  53,\n",
      "           1,  41,  39,  81,   5,  42,   1,  87,  93,   1,  94,   1,  50,  78,\n",
      "          43,  12,  75,  28, 115,  60,  53,  80,  71,  26,  79,  43,  66,  57,\n",
      "          69, 104,   1,  72,   1,  41,  59,  56,  44,  43,  61,   1,  56,  59,\n",
      "          52,  45,  85,  16,  33,  23,  17,   1,  34,  21,  26,  15,  17,  26,\n",
      "          32,  21,  27,  71,  26,  53,  58,   1,  21,  57,  39,  98,  50,  12,\n",
      "          75,  28, 115,  60,  53,  80,  71,  26,  53,  85,  16,  33,  23,  17,\n",
      "           1,  34,  21,  26,  15,  17,  26,  32,  21,  27,  71,  32,  87,  63,\n",
      "           1, 100,  81,  66,  72,  52,  66, 110,   5,  58,   1,  98,   1,  50,\n",
      "          79,  45,  85,  28, 115,  60,  53,  80,  71,  35,  92,  58,   1,  41,\n",
      "          53,  51, 105,  58,   1,  74,   1, 105,   1,  15,  50,  39,  59,  42,\n",
      "          47,  53,  12,  75,  16,  33,  23,  17,   1,  34,  21,  26,  15,  17,\n",
      "          26,  32,  21,  27,  71,  32,  87,  93,   5,  57,   1,  57,  53,  83,\n",
      "           1,  69,   1,  46,  53,  54,  43,  85,  28, 115,  60,  53,  80,  71,\n",
      "          21,  58,   1,  74,   1,  39,   1,  40,  96,  58,  68,   1, 118,  54,\n",
      "         114,  63,  85,  16,  33,  23,  17,   1,  34,  21,  26,  15,  17,  26,\n",
      "          32,  21,  27,  71]])\n",
      "mest spirts of the night\n",
      "Envelope you, good Provost! Who call'd here of late?\n",
      "\n",
      "Provost:\n",
      "None, since the curfew rung.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Not Isabel?\n",
      "\n",
      "Provost:\n",
      "No.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "They will, then, ere't be long.\n",
      "\n",
      "Provost:\n",
      "What comfort is for Claudio?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "There's some in hope.\n",
      "\n",
      "Provost:\n",
      "It is a bitter deputy.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "\n",
      "-------\n",
      "tensor([[ 80,   1,  57,  54, 112,  58,  57,   1,  94,   1,  72,   1,  52,  47,\n",
      "         122,  58,   0,  17,  52,  95,  50,  53,  54,  43,   1,  88,  66,  45,\n",
      "         126,  42,   1,  28, 115,  60,  53,  80,   2,   1,  35,  46,  53,   1,\n",
      "          41,  39,  81,   5,  42,   1,  87,  93,   1,  94,   1,  50,  78,  43,\n",
      "          12,  75,  28, 115,  60,  53,  80,  71,  26,  79,  43,  66,  57,  69,\n",
      "         104,   1,  72,   1,  41,  59,  56,  44,  43,  61,   1,  56,  59,  52,\n",
      "          45,  85,  16,  33,  23,  17,   1,  34,  21,  26,  15,  17,  26,  32,\n",
      "          21,  27,  71,  26,  53,  58,   1,  21,  57,  39,  98,  50,  12,  75,\n",
      "          28, 115,  60,  53,  80,  71,  26,  53,  85,  16,  33,  23,  17,   1,\n",
      "          34,  21,  26,  15,  17,  26,  32,  21,  27,  71,  32,  87,  63,   1,\n",
      "         100,  81,  66,  72,  52,  66, 110,   5,  58,   1,  98,   1,  50,  79,\n",
      "          45,  85,  28, 115,  60,  53,  80,  71,  35,  92,  58,   1,  41,  53,\n",
      "          51, 105,  58,   1,  74,   1, 105,   1,  15,  50,  39,  59,  42,  47,\n",
      "          53,  12,  75,  16,  33,  23,  17,   1,  34,  21,  26,  15,  17,  26,\n",
      "          32,  21,  27,  71,  32,  87,  93,   5,  57,   1,  57,  53,  83,   1,\n",
      "          69,   1,  46,  53,  54,  43,  85,  28, 115,  60,  53,  80,  71,  21,\n",
      "          58,   1,  74,   1,  39,   1,  40,  96,  58,  68,   1, 118,  54, 114,\n",
      "          63,  85,  16,  33,  23,  17,   1,  34,  21,  26,  15,  17,  26,  32,\n",
      "          21,  27,  71,  26]])\n",
      "st spirts of the night\n",
      "Envelope you, good Provost! Who call'd here of late?\n",
      "\n",
      "Provost:\n",
      "None, since the curfew rung.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "Not Isabel?\n",
      "\n",
      "Provost:\n",
      "No.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "They will, then, ere't be long.\n",
      "\n",
      "Provost:\n",
      "What comfort is for Claudio?\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "There's some in hope.\n",
      "\n",
      "Provost:\n",
      "It is a bitter deputy.\n",
      "\n",
      "DUKE VINCENTIO:\n",
      "N\n"
     ]
    }
   ],
   "source": [
    "# a demonstration of what a batch with batch_size=1 looks like. Notice the one-token offset in characters\n",
    "xb, yb = get_batch('train', 1)\n",
    "print(xb)\n",
    "print(tokenizer.decode(xb.squeeze(0).tolist()))\n",
    "print(\"-------\")\n",
    "print(yb)\n",
    "print(tokenizer.decode(yb.squeeze(0).tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "78a7c1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model, batch_size, eval_iters = 10): # to estimate loss during the training loop\n",
    "    out = {}\n",
    "    model.eval() # sets model to eval mode\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split, batch_size)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train() # just resets to training mode\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fc68c9",
   "metadata": {},
   "source": [
    "# Instantiating a brand new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06118186",
   "metadata": {},
   "outputs": [],
   "source": [
    "# just to make sure nothing got messed up above. \n",
    "# if an error gets thrown in one of the test cells then the config values won't reset\n",
    "print(config)\n",
    "\n",
    "model = FractalFormer_base(config, tokenizer).to(config.device)\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3003f7",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06f3d856",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a PyTorch optimizer\n",
    "# this is not what they used, but this learning rate & weight decay work for our tiny minGemma\n",
    "learning_rate = 3e-5\n",
    "weight_decay = 0.01\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# how long we want to train for\n",
    "max_iters = 5000\n",
    "\n",
    "# how often we want to check & see how our loss is doing\n",
    "eval_interval = 250\n",
    "\n",
    "# batch size to use\n",
    "batch_size = 12\n",
    "\n",
    "# if you want to do debugging\n",
    "config.verbose['RMSNorm'] = False\n",
    "config.verbose['MLP'] = False\n",
    "config.verbose['MQA'] = False\n",
    "config.verbose['Layer'] = False\n",
    "config.verbose['OutputLayer'] = False\n",
    "config.verbose['FractalLoss'] = False\n",
    "config.verbose['FractalFormer'] = False\n",
    "config.verbose['Sampler'] = False\n",
    "config.verbose['Generate'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9f76c3",
   "metadata": {},
   "source": [
    "# ------------ BOOKMARK ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6ca3e922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: train loss 19.1140, val loss 22.1463, time elapsed: 1.54 seconds\n",
      "step 250: train loss 19.2682, val loss 22.2341, time elapsed: 354.09 seconds\n",
      "step 500: train loss 18.8027, val loss 22.1553, time elapsed: 688.91 seconds\n",
      "step 750: train loss 18.8095, val loss 21.7493, time elapsed: 1021.93 seconds\n",
      "step 1000: train loss 19.0256, val loss 22.6200, time elapsed: 1356.87 seconds\n",
      "step 1250: train loss 18.8943, val loss 22.5097, time elapsed: 1692.22 seconds\n",
      "step 1500: train loss 18.8110, val loss 22.2320, time elapsed: 2028.29 seconds\n",
      "step 1750: train loss 18.9138, val loss 21.8615, time elapsed: 2363.70 seconds\n",
      "step 2000: train loss 18.8240, val loss 22.2428, time elapsed: 2709.21 seconds\n",
      "step 2250: train loss 19.0780, val loss 21.9875, time elapsed: 3042.35 seconds\n",
      "step 2500: train loss 18.7679, val loss 22.1570, time elapsed: 3372.90 seconds\n",
      "step 2750: train loss 18.6269, val loss 22.4533, time elapsed: 3703.29 seconds\n",
      "step 3000: train loss 18.8560, val loss 21.8843, time elapsed: 4034.63 seconds\n",
      "step 3250: train loss 18.4526, val loss 22.2834, time elapsed: 4364.82 seconds\n",
      "step 3500: train loss 18.6428, val loss 22.4813, time elapsed: 4695.05 seconds\n",
      "step 3750: train loss 18.7680, val loss 22.3242, time elapsed: 5025.20 seconds\n",
      "step 4000: train loss 18.7381, val loss 22.0920, time elapsed: 5362.44 seconds\n",
      "step 4250: train loss 18.7113, val loss 21.8017, time elapsed: 5703.41 seconds\n",
      "step 4500: train loss 18.6947, val loss 22.1474, time elapsed: 6033.60 seconds\n",
      "step 4750: train loss 18.7029, val loss 22.4592, time elapsed: 6364.48 seconds\n",
      "step 4999: train loss 18.6812, val loss 22.1771, time elapsed: 6693.84 seconds\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "start_time = time.time()\n",
    "\n",
    "# Enable anomaly detection. uncomment these lines if you need to do extensive debugging\n",
    "#torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # sample a batch of data\n",
    "    xb, yb = get_batch('train', batch_size)\n",
    "    \n",
    "    # train\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # every once in a while evaluate the loss on train and val sets\n",
    "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
    "        current_time = time.time()\n",
    "        elapsed_time = current_time - start_time\n",
    "        losses = estimate_loss(model, batch_size)\n",
    "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}, time elapsed: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Disable anomaly detection after the training loop\n",
    "#torch.autograd.set_detect_anomaly(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac6744d",
   "metadata": {},
   "source": [
    "# Saving your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "090948b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model currently held in memory\n",
    "# the filename specifies the model's class, hyperparameters, and date/time it was saved\n",
    "import os\n",
    "\n",
    "# Ensure the directory exists\n",
    "model_dir = 'models'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "# Create a shorter, more concise filename\n",
    "filename = (f'{model.__class__.__name__}'\n",
    "           f'-v{config.vocab_size}'\n",
    "           f'-max_t{config.max_position_embeddings}'\n",
    "           f'-layers{config.num_hidden_layers}'\n",
    "           f'-heads{config.num_attention_heads}'\n",
    "           f'-kv_heads{config.num_key_value_heads}'\n",
    "           f'-hidden{config.hidden_size}'\n",
    "           f'-intermediate{config.intermediate_size}'\n",
    "           f'-head_dim{config.head_dim}'\n",
    "           f'-theta{config.rope_theta}'\n",
    "           f'-levels{config.levels}'\n",
    "           f'-split{config.split}'\n",
    "           f'-lr{learning_rate}'\n",
    "           f'-decay{weight_decay}'\n",
    "           f'-batch{batch_size}'\n",
    "            f'-train_iter{15000}'\n",
    "           f'--{time.strftime(\"%Y-%m-%d|%H-%M-%S\")}.pth')\n",
    "\n",
    "# Save the model\n",
    "model_path = os.path.join(model_dir, filename)\n",
    "torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859ce832",
   "metadata": {},
   "source": [
    "# Load a Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "21eea23d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "972.672 K parameters\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "FractalFormer_base(\n",
       "  (embedder): Embedding(128, 128)\n",
       "  (embedder_norm): RMSNorm()\n",
       "  (layers): ModuleList(\n",
       "    (0-3): 4 x Layer(\n",
       "      (self_attn): MultiQueryAttention(\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (mlp): MLP(\n",
       "        (drop): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (input_layernorm): RMSNorm()\n",
       "      (post_attention_layernorm): RMSNorm()\n",
       "    )\n",
       "  )\n",
       "  (output_layer): OutputLayer(\n",
       "    (embedding_norm): RMSNorm()\n",
       "    (final_norm): RMSNorm()\n",
       "  )\n",
       "  (criterion): FractalLoss(\n",
       "    (criterion): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize a blank model\n",
    "model = FractalFormer_base(config, tokenizer).to(config.device)  \n",
    "\n",
    "# here's the path to a minGemma model that i've trained with roughly 1m parameters\n",
    "path = 'models/FractalFormer_base-v128-max_t256-layers4-heads4-kv_heads1-hidden128-intermediate512-head_dim32-theta100.0-levels3-split2-lr0.0003-decay0.01-batch12--2024-03-06|07-14-57.pth'\n",
    "\n",
    "# Load the saved state dictionary\n",
    "model.load_state_dict(torch.load(path))\n",
    "# REMEMBER TO CHANGE VALUES IN CONFIG TO MATCH THE MODEL YOU'VE LOADED\n",
    "\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in model.parameters())/1e3, 'K parameters')\n",
    "\n",
    "# If you only plan to do inference, switch to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# If you plan to continue training the model, switch to training mode\n",
    "#model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12b8c0b",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1923eecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "level: 0, model: 0\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romeo shall have the wars,\n",
      "Than some prince then the world of this side,\n",
      "Which all the sacribe of all the presence\n",
      "Than thou hast be a substance of his pride.\n",
      "\n",
      "BUSHY:\n",
      "The grandship of the people of you, sir?\n",
      "\n",
      "FRIAR LAURENCE:\n",
      "The suppose spirit to thee thee, and I should have\n",
      "distraction of the gentle b\n",
      "level: 1, model: 0\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR\n",
      "level: 1, model: 1\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romeo,\n",
      "The cause a madam, and the prince of thee to must,\n",
      "The can some to be a spossion and some to so,\n",
      "To must the prince of your from the compant\n",
      "Where is the peoples and some in his so made\n",
      "To pluck'd the people supposed the pair suppose\n",
      "Is all you shall be propter of the substance\n",
      "T\n",
      "level: 2, model: 0\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR\n",
      "level: 2, model: 1\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR\n",
      "level: 2, model: 2\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou Romes of a make the caughll the provost of all there so am a some the dister the prother, they caws,\n",
      "The some a shall so shall shall a provost the peop of thee all there shall thee, thee would the comss of all.\n",
      "\n",
      "LAUTIO:\n",
      "I was such the dider's the come, thee wound the com the coms of shall a shall the \n",
      "level: 2, model: 3\n",
      "JULIET:\n",
      "O Romeo, Romeo! wherefore art thou RRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRRR\n"
     ]
    }
   ],
   "source": [
    "model.eval() # sets model to eval mode\n",
    "input_str = \"JULIET:\\nO Romeo, Romeo! wherefore art thou R\" # the classic line\n",
    "max_useable_output_len = config.max_position_embeddings - len(input_str)\n",
    "\n",
    "for i in range(config.levels):\n",
    "    for j in range(config.model_count[i]):\n",
    "        print(f\"level: {i}, model: {j}\")\n",
    "        output = model.generate(input_str, \n",
    "                                output_len = max_useable_output_len, \n",
    "                                temperature=0.7, \n",
    "                                top_k = 3, \n",
    "                                top_p = 0.95,\n",
    "                               level = i,\n",
    "                               model = j)\n",
    "        print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a887a1",
   "metadata": {},
   "source": [
    "so there's almost definitely something wrong happening here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a9b78b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
